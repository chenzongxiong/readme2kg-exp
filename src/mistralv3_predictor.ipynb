{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae8e1d9-35dd-4659-a5aa-9dd8d7303bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "sys.path.append('./readme2kg-exp/src/')\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from termcolor import colored\n",
    "from functools import partial, reduce\n",
    "import operator as op\n",
    "import hashlib\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "from predictor import BasePredictor, LABELS\n",
    "from webanno_tsv import webanno_tsv_read_file, Document, Annotation, Token\n",
    "import utils\n",
    "import cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a3be4a1-90bb-4601-963f-7cfe19c501c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'test_unlabeled'\n",
    "base_path = f'./readme2kg-exp/data/{phase}'\n",
    "file_names = [fp for fp in os.listdir(base_path) if os.path.isfile(os.path.join(base_path, fp)) and fp.endswith('.tsv')]\n",
    "model_name = 'Mistral-7B-Instruct-v0.3'\n",
    "output_folder = f'./readme2kg-exp/results/{model_name}/{phase}'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "560ab781-056f-41fe-aad8-6acf457399c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_id = 0\n",
    "prompt_template_path = f'./readme2kg-exp/config/deepseek-chat-prompt-0.txt'\n",
    "if os.path.isfile(prompt_template_path):\n",
    "    with open(prompt_template_path, 'r') as fd:\n",
    "        prompt_template = fd.read()\n",
    "else:\n",
    "    prompt_template = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901a66c-ff2d-4d40-a331-15ff80039bb6",
   "metadata": {},
   "source": [
    "# Load Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a3d58c-edbc-431f-8b96-48e919e4080e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8346b7bf339941d6839bc2fbd8a3d8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/ann/mistral_models/7B-Instruct-v0.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\n",
    "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d75786-539a-4ee6-855b-784ddb994a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['```\\n# DejaVu\\n## Table of Contents =================\\n\\t* [Code](#code)\\n\\t* [Install Requirements](#install-requirements)\\n\\t* [Usage](#usage)\\n\\t* [Example](#example)\\n\\t* [Datasets](#datasets)\\n\\t* [Deployment and Failure Injection Scripts of Train-Ticket](#deployment-and-failure-injection-scripts-of-train-ticket)\\n\\t* [Citation](#citation)\\n\\t* [Supplementary details](#supplementary-details)\\n\\t## Paper\\n\\tA preprint version: <PUBLICATION>https://arxiv.org/abs/2207.09021</PUBLICATION>\\n\\t## Code\\n\\t### Install\\n\\t1.\\n```']\n"
     ]
    }
   ],
   "source": [
    "from mistral_inference.transformer import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n",
    "\n",
    "tokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\n",
    "model = Transformer.from_folder(mistral_models_path)\n",
    "\n",
    "# unit test code\n",
    "sentence_text = \"\"\"# DejaVu ## Table of Contents =================    * [Code](#code)     * [Install Requirements](#install-requirements)     * [Usage](#usage)     * [Example](#example)   * [Datasets](#datasets)   * [Deployment and Failure Injection Scripts of Train-Ticket](#deployment-and-failure-injection-scripts-of-train-ticket)   * [Citation](#citation)   * [Supplementary details](#supplementary-details)    ## Paper A preprint version: https://arxiv.org/abs/2207.09021 ## Code ### Install 1.\"\"\"\n",
    "prompt = prompt_template.replace('{input_text}', sentence_text)\n",
    "\n",
    "# original code\n",
    "#prompt = prompt_template.replace('{input_text}', sentence.text)\n",
    "\n",
    "'''\n",
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful NER annotator\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "'''\n",
    "completion_request = ChatCompletionRequest(messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful NER annotator\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    ")\n",
    "\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "\n",
    "out_tokens, _ = generate([tokens], model, max_tokens=1000, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e1bc1ba-089a-4cb9-95ae-4f691968fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction(sentence, tokens, sid_path):\n",
    "    try:\n",
    "        #print(f\"Process-{os.getpid()} processing {colored(sentence.text, 'red')} ...\")\n",
    "        prompt = prompt_template.replace('{input_text}', sentence.text)\n",
    "\n",
    "        messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful NER annotator\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "        \n",
    "        completion_request = ChatCompletionRequest(messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful NER annotator\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ])\n",
    "        tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "\n",
    "        out_tokens, _ = generate([tokens], model, max_tokens=255, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "        result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "        \n",
    "        #print(f\"Process-{os.getpid()} predict {colored(sentence.text, 'cyan')} successfully\")\n",
    "        with open(sid_path, 'w') as file:\n",
    "            file.write(result)\n",
    "    except Exception as ex:\n",
    "        logging.error(f'[do_prediction] got exception: {ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78fc7e9-36b5-4505-ae93-b6472540e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotation_labels_if_possible(predicted_text):\n",
    "    label_to_text_list = defaultdict(list)\n",
    "    acc_adjusted_pos = 0\n",
    "    for label in LABELS:\n",
    "        regex = f'<{label}>(.*?)</{label}>'\n",
    "        matches = re.finditer(regex, predicted_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        for m in matches:\n",
    "            adjusted_pos = len(label) + 2\n",
    "            label_to_text_list[label].append({\n",
    "                'text': m.group(1),\n",
    "                'start': m.start(1) - adjusted_pos - acc_adjusted_pos,\n",
    "                'end': m.end(1) - adjusted_pos - acc_adjusted_pos,\n",
    "            })\n",
    "            acc_adjusted_pos += adjusted_pos * 2 + 1\n",
    "    return label_to_text_list\n",
    "\n",
    "\n",
    "\n",
    "def post_process(predicted_text, tokens):\n",
    "    cleaned_text = cleaner.Cleaner(predicted_text).clean()\n",
    "    label_to_text_list = extract_annotation_labels_if_possible(cleaned_text)\n",
    "    return label_to_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aefcdd39-2572-4a1e-bc6b-4f5651cb88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, tokens):\n",
    "    path = f'./readme2kg-exp/results/{model_name}/prompt-{prompt_id}/zzz_{file_name}' # NOTE: prefix zzz for directory sorting, non-sense\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    sid = hashlib.sha256(sentence.text.encode()).hexdigest()[:8]\n",
    "    #if not os.path.isfile(f'{path}/{sid}.txt'):   # original code\n",
    "    if os.path.isdir(f'{path}'):\n",
    "        do_prediction(sentence, tokens, f'{path}/{sid}.txt')\n",
    "\n",
    "    with open(f'{path}/{sid}.txt', 'r') as fd:\n",
    "        predicted_text = fd.read()\n",
    "\n",
    "    label_to_text_list = post_process(predicted_text, tokens)\n",
    "    # NOTE: sanity checking\n",
    "    for label, text_list in label_to_text_list.items():\n",
    "        for text in text_list:\n",
    "            if text['text'] != sentence.text[text['start']:text['end']]:\n",
    "                prompt = prompt_template.replace('{input_text}', sentence.text)\n",
    "                #logging.warning(f\"BUG? The predicted text is not exact the same as the original text. \\n\\nPrompt: {prompt}\\nOriginal: {colored(sentence.text, 'green')}\\nGenerated: {colored(text['text'], 'red')}\\n--------------------------------------------------------------------------------\")\n",
    "\n",
    "    span_tokens_to_label_list = []\n",
    "    for label, text_list in label_to_text_list.items():\n",
    "        for text in text_list:\n",
    "            span_tokens_to_label_list.append({\n",
    "                'span_tokens': utils.make_span_tokens(tokens, text['start'], text['end']),\n",
    "                'label': label\n",
    "            })\n",
    "    return span_tokens_to_label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0045624d-f168-44cf-a4c9-f94413793290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_serial(doc: Document):\n",
    "    annotations = []\n",
    "    for sent in doc.sentences:\n",
    "        tokens = doc.sentence_tokens(sent)\n",
    "        span_tokens_to_label_list = predict(sentence=sent, tokens=tokens)\n",
    "        \n",
    "        # create the annotation instances\n",
    "        for span_tokens_to_label in span_tokens_to_label_list:\n",
    "            span_tokens = span_tokens_to_label['span_tokens']\n",
    "            label = span_tokens_to_label['label']\n",
    "            if span_tokens is None:\n",
    "                continue\n",
    "\n",
    "            annotation = utils.make_annotation(tokens=span_tokens, label=label)\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    result = utils.replace_webanno_annotations(doc, annotations=annotations)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee5349e6-b4b2-49f8-bd0e-a5fc25f83a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 7 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 0 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 4 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 6 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 0 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 0 annotations\n",
      "WARNING:root:Predicted 7 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 6 annotations\n",
      "WARNING:root:Predicted 0 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 4 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 5 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 0 annotations\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_names:\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    ref_doc = webanno_tsv_read_file(file_path)\n",
    "    predicted_doc = call_serial(ref_doc)\n",
    "    # Verify\n",
    "    if ref_doc.text != predicted_doc.text:\n",
    "        #logging.warning('content changed')\n",
    "        pass\n",
    "    if len(ref_doc.sentences) == len(predicted_doc.sentences):\n",
    "        #logging.warning('sentences changed')\n",
    "        pass\n",
    "    if len(ref_doc.tokens) == len(predicted_doc.tokens):\n",
    "        #logging.warning('tokens changed')\n",
    "        pass\n",
    "    for s1, s2 in zip(ref_doc.sentences, predicted_doc.sentences):\n",
    "        if s1 == s2:\n",
    "            #logging.warning(f'sentence changed, \\n{s1}\\n{s2}')\n",
    "            pass\n",
    "\n",
    "    for t1, t2 in zip(ref_doc.tokens, predicted_doc.tokens):\n",
    "        if t1 == t2:\n",
    "            #logging.warning(f'token changed: \\n{t1}\\n{t2}')\n",
    "            pass\n",
    "\n",
    "    logging.warning(f\"Predicted {len(predicted_doc.annotations)} annotations\")\n",
    "    prediction_path = os.path.join(output_folder, file_name)\n",
    "    with open(prediction_path, 'w') as fd:\n",
    "        fd.write(predicted_doc.tsv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3d37b-8fbb-4678-9007-8312ac8c29f5",
   "metadata": {},
   "source": [
    "# Scorer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3efc29ef-de50-4b8b-a310-6179f91d7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from webanno_tsv import webanno_tsv_read_file, Document, Annotation\n",
    "from typing import List, Union\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "LABELS = [\n",
    "    'CONFERENCE',\n",
    "    'DATASET',\n",
    "    'EVALMETRIC',\n",
    "    'LICENSE',\n",
    "    'ONTOLOGY',\n",
    "    'PROGLANG',\n",
    "    'PROJECT',\n",
    "    'PUBLICATION',\n",
    "    'SOFTWARE',\n",
    "    'WORKSHOP'\n",
    "]\n",
    "\n",
    "def to_char_bio(src_path: str, ref_path: str) -> List[List[str]]:\n",
    "    ref_doc = webanno_tsv_read_file(ref_path)\n",
    "    # Parse the WebAnno TSV file\n",
    "    doc = webanno_tsv_read_file(src_path)\n",
    "    # Initialize a list to store character-level BIO tags\n",
    "    bio_tags_list = []\n",
    "    for target_label in LABELS:\n",
    "        bio_tags = ['#'] * len(ref_doc.text)  # Default to '#' for all characters\n",
    "        # Pick interested sentences and default them to 'O'\n",
    "        for annotation in ref_doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "            sentences = ref_doc.annotation_sentences(annotation)\n",
    "            for sentence in sentences:\n",
    "                tokens = ref_doc.sentence_tokens(sentence)\n",
    "                start_char, end_char = tokens[0].start, tokens[-1].end\n",
    "                bio_tags[start_char:end_char] = ['O'] * (end_char-start_char)\n",
    "\n",
    "        for annotation in doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "\n",
    "            start_token, end_token = annotation.tokens[0], annotation.tokens[-1]\n",
    "            start_char = start_token.start\n",
    "            end_char = end_token.end\n",
    "            # Sanity check\n",
    "            if ref_doc.text[start_char:end_char] != annotation.text:\n",
    "                msg = f\"ERROR: src: {src_path}, annotated '{annotation.text}', text: '{ref_doc.text[start_char:end_char]}'\"\n",
    "                print(msg)\n",
    "\n",
    "            if 'I-' in bio_tags[start_char]:\n",
    "                # Overlapping, it's annotated by another annotations, we connect them as one annotations\n",
    "                pass\n",
    "            else:\n",
    "                if bio_tags[start_char] != '#':\n",
    "                    # Assign BIO tags to characters in the entity span\n",
    "                    bio_tags[start_char] = f'B-{label}'  # Beginning of the entity\n",
    "\n",
    "            for i in range(start_char + 1, end_char):\n",
    "                if bio_tags[i] != '#':\n",
    "                    bio_tags[i] = f'I-{label}'  # Inside the entity\n",
    "\n",
    "        # Remove unannotated sentences from bio list.\n",
    "        bio_tags = [x for x in filter(lambda x: x != '#', bio_tags)]\n",
    "        bio_tags_list.append(bio_tags)\n",
    "\n",
    "    return bio_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c53d2884-c60b-432a-988d-82bb3486dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    return reduce(lambda x, y: x + y, lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0279cd9-db34-47e4-8051-78288cdbc274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ann/fiz-ddb/notebook/readme2kg-exp/src\n",
      "WARN: 231sm_Low_Resource_KBP_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: ARM-software_keyword-transformer_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: Cardio-AI_3d-mri-domain-adaptation_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: ChopinSharp_ref-nms_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: James-Durant_fisher-information_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: MELALab_nela-gt-2019_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: allenai_aspire_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: alpiges_LinConGauss_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: anonymous-submission-22_dejavu_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: aspiaspace_earthpt_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: benedekrozemberczki_karateclub_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: claws-lab_awesome-crowd-combat-misinformation_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: daijifeng001_TA-FCN_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: dennlinger_tsar-2022-shared-task_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: dylanashley_story-distiller_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: felixxu35_hamiltoniq_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: freedomintelligence_mllm-bench_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: idt-iti_lightweight-face-detector-pruning_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: iqvianlp_llm-onto-infuse_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: juaml_juharmonize_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: lojzezust_slr_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: lucy3_words_as_gatekeepers_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: microsoft_opendatasheets-framework_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: ml-jku_vnegnn_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: mmp2_megaman_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: nokia_codesearch_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: prasunroy_air-writing_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: qiantianwen_nuscenes-qa_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: salesforce_booksum_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: snap-research_test-time-aggregation-for-cf_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: sosuperic_MeanSum_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: stanfordhci_modelsketchbook_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: tiehangd_Para_DPMM_master_readme.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: uwnetworkslab_netcov_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: vaikkunth_PrivacyFL_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: wenzhengzhang_entqa_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: zju-vipa_awesome-neural-trees_main_README.md.tsv is missing, fill 'O' list as default prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      " {\n",
      "  \"overall_accuracy\": 0.9203280609256005,\n",
      "  \"overall_macro_precision\": 0.043825145758361925,\n",
      "  \"overall_macro_recall\": 0.047619047619047616,\n",
      "  \"overall_macro_f1\": 0.045643394636683224,\n",
      "  \"CONFERENCE_macro_precision\": 0.3284625952291745,\n",
      "  \"CONFERENCE_macro_recall\": 0.3333333333333333,\n",
      "  \"CONFERENCE_macro_f1\": 0.3308800402591684,\n",
      "  \"DATASET_macro_precision\": 0.3198942498347654,\n",
      "  \"DATASET_macro_recall\": 0.3333333333333333,\n",
      "  \"DATASET_macro_f1\": 0.3264755480607083,\n",
      "  \"EVALMETRIC_macro_precision\": 0.33243162007814847,\n",
      "  \"EVALMETRIC_macro_recall\": 0.3333333333333333,\n",
      "  \"EVALMETRIC_macro_f1\": 0.3328818660647103,\n",
      "  \"LICENSE_macro_precision\": 0.32488479262672815,\n",
      "  \"LICENSE_macro_recall\": 0.3333333333333333,\n",
      "  \"LICENSE_macro_f1\": 0.3290548424737456,\n",
      "  \"ONTOLOGY_macro_precision\": 0.30246913580246915,\n",
      "  \"ONTOLOGY_macro_recall\": 0.3333333333333333,\n",
      "  \"ONTOLOGY_macro_f1\": 0.31715210355987056,\n",
      "  \"PROGLANG_macro_precision\": 0.3069741900054915,\n",
      "  \"PROGLANG_macro_recall\": 0.3333333333333333,\n",
      "  \"PROGLANG_macro_f1\": 0.3196112064036592,\n",
      "  \"PROJECT_macro_precision\": 0.3226879574184964,\n",
      "  \"PROJECT_macro_recall\": 0.3333333333333333,\n",
      "  \"PROJECT_macro_f1\": 0.32792427315753886,\n",
      "  \"PUBLICATION_macro_precision\": 0.278686327077748,\n",
      "  \"PUBLICATION_macro_recall\": 0.3333333333333333,\n",
      "  \"PUBLICATION_macro_f1\": 0.3035701248448565,\n",
      "  \"SOFTWARE_macro_precision\": 0.31496062992125984,\n",
      "  \"SOFTWARE_macro_recall\": 0.3333333333333333,\n",
      "  \"SOFTWARE_macro_f1\": 0.32388663967611336,\n",
      "  \"WORKSHOP_macro_precision\": 0.3322422258592471,\n",
      "  \"WORKSHOP_macro_recall\": 0.3333333333333333,\n",
      "  \"WORKSHOP_macro_f1\": 0.33278688524590166\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "print(os.getcwd())\n",
    "ref_dir = '../results/Mistral-7B-Instruct-v0.3/test_unlabeled/'\n",
    "pred_dir = '../results/Mistral-7B-Instruct-v0.3/prompt-0/'\n",
    "score_dir = '../results/scores/'\n",
    "\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "os.makedirs(score_dir, exist_ok=True)\n",
    "\n",
    "ref_file_names = sorted([fp for fp in os.listdir(ref_dir) if os.path.isfile(f'{ref_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "if len(ref_file_names) == 0:\n",
    "    raise Exception(\"ERROR: No reference files found, configuration error?\")\n",
    "\n",
    "all_ref_bio_tags_list = []\n",
    "for ref_file_name in ref_file_names:\n",
    "    src_path = os.path.join(ref_dir, ref_file_name)\n",
    "    ref_path = src_path\n",
    "    all_ref_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "\n",
    "pred_file_names = sorted([fp for fp in os.listdir(pred_dir) if os.path.isfile(f'{pred_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "all_pred_bio_tags_list = []\n",
    "for idx, ref_file_name in enumerate(ref_file_names):\n",
    "    try:\n",
    "        src_path = os.path.join(pred_dir, ref_file_name)\n",
    "        ref_path = os.path.join(ref_dir, ref_file_name)\n",
    "        all_pred_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "    except FileNotFoundError:\n",
    "        nbr_labels = len(all_ref_bio_tags_list[idx])\n",
    "        assert nbr_labels == len(LABELS), \"ERROR: reference tags doesn't have ${len(LABELS)} labels.\"\n",
    "        pred = []\n",
    "        for label_idx in range(nbr_labels):\n",
    "            pred.append(['O'] * len(all_ref_bio_tags_list[idx][label_idx]))\n",
    "\n",
    "        print(f\"WARN: {ref_file_name} is missing, fill 'O' list as default prediction\")\n",
    "        all_pred_bio_tags_list.append(pred)\n",
    "# Sanity checking\n",
    "for idx, (ref_list, pred_list) in enumerate(zip(all_ref_bio_tags_list, all_pred_bio_tags_list)):\n",
    "    for label_idx, (ref, pred) in enumerate(zip(ref_list, pred_list)):\n",
    "        assert len(ref) == len(pred), f'ERROR: {ref_file_names[idx]}, label: {LABELS[label_idx]}, reference length: {len(ref)}, prediction length: {len(pred)}'\n",
    "\n",
    "scores = {}\n",
    "################################################################################\n",
    "# Consider whole dataset\n",
    "################################################################################\n",
    "ref_bio_tags_list = flatten(flatten(all_ref_bio_tags_list))\n",
    "pred_bio_tags_list = flatten(flatten(all_pred_bio_tags_list))\n",
    "\n",
    "accuracy = accuracy_score(ref_bio_tags_list, pred_bio_tags_list)\n",
    "scores['overall_accuracy'] = accuracy\n",
    "average = 'macro'\n",
    "ref_bio_tags_list = flatten(flatten(all_ref_bio_tags_list))\n",
    "pred_bio_tags_list = flatten(flatten(all_pred_bio_tags_list))\n",
    "\n",
    "f1 = f1_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "precision = precision_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "recall = recall_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "scores[f\"overall_{average}_precision\"] = precision\n",
    "scores[f\"overall_{average}_recall\"] = recall\n",
    "scores[f\"overall_{average}_f1\"] = f1\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# For each class\n",
    "################################################################################\n",
    "label_to_ref_bio_tags_list = defaultdict(list)\n",
    "label_to_pred_bio_tags_list = defaultdict(list)\n",
    "for ref_bio_tags_list, pred_bio_tags_list in zip(all_ref_bio_tags_list, all_pred_bio_tags_list):\n",
    "    if len(ref_bio_tags_list) != len(LABELS):\n",
    "        print('ERROR: ref bio tags list')\n",
    "    if len(pred_bio_tags_list) != len(LABELS):\n",
    "        print('ERROR: pred bio tags list')\n",
    "\n",
    "    for label, ref_bio_tags, pred_bio_tags in zip(LABELS, ref_bio_tags_list, pred_bio_tags_list):\n",
    "        label_to_ref_bio_tags_list[label].extend(ref_bio_tags)\n",
    "        label_to_pred_bio_tags_list[label].extend(pred_bio_tags)\n",
    "        if len(label_to_ref_bio_tags_list[label]) != len(label_to_pred_bio_tags_list[label]):\n",
    "            print('ERROR: label_to_ref_pred_bio_tags')\n",
    "\n",
    "\n",
    "for label in label_to_ref_bio_tags_list.keys():\n",
    "    ref_bio_tags_list = label_to_ref_bio_tags_list[label]\n",
    "    pred_bio_tags_list = label_to_pred_bio_tags_list[label]\n",
    "    accuracy = accuracy_score(ref_bio_tags_list, pred_bio_tags_list)\n",
    "    f1 = f1_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "    precision = precision_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "    recall = recall_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "    scores[f\"{label}_{average}_precision\"] = precision\n",
    "    scores[f\"{label}_{average}_recall\"] = recall\n",
    "    scores[f\"{label}_{average}_f1\"] = f1\n",
    "\n",
    "print(\"Scores:\\n\", json.dumps(scores, indent=2))\n",
    "\n",
    "with open(os.path.join(score_dir, 'Mistral-7B-Instruct-v0.3-scores.json'), 'w') as fd:\n",
    "    json.dump(scores, fd, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
