{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae8e1d9-35dd-4659-a5aa-9dd8d7303bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "sys.path.append('./readme2kg-exp/src/')\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from termcolor import colored\n",
    "from functools import partial, reduce\n",
    "import operator as op\n",
    "import hashlib\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "from predictor import BasePredictor, LABELS\n",
    "from webanno_tsv import webanno_tsv_read_file, Document, Annotation, Token\n",
    "import utils\n",
    "import cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3be4a1-90bb-4601-963f-7cfe19c501c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'test_unlabeled'\n",
    "base_path = f'./readme2kg-exp/data/{phase}'\n",
    "file_names = [fp for fp in os.listdir(base_path) if os.path.isfile(os.path.join(base_path, fp)) and fp.endswith('.tsv')]\n",
    "model_name = 'Meta-Llama-3-8B-Instruct'\n",
    "output_folder = f'./readme2kg-exp/results/{model_name}/{phase}'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "560ab781-056f-41fe-aad8-6acf457399c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Task:**\n",
      "You are tasked with performing Named Entity Recognition (NER) on the given text. Follow the guidelines strictly to identify and classify entities into their respective categories. Annotate the entities directly in the original text using XML-style tags. Only return the annotated text in Markdown format—no explanations, introductions, or extra text.\n",
      "\n",
      "\n",
      "**Guidelines:**\n",
      "\n",
      "1. **Entity Classes:**\n",
      "   - **CONFERENCE**: Conference events.\n",
      "     *Definition*:\n",
      "     A formal meeting or gathering focused on a particular field of study or topic.\n",
      "     *Example*:\n",
      "     `<CONFERENCE>International Semantic Web Conference 2019</CONFERENCE>`\n",
      "     `<CONFERENCE>ISWC 2019</CONFERENCE>`\n",
      "     `<CONFERENCE>CVPR2023</CONFERENCE> workshop`\n",
      "\n",
      "   - **DATASET**: Structured collections of data.\n",
      "     *Definition*:\n",
      "     A structured collection of data, organized typically for a specific goal such as analysis, research, or reference.\n",
      "     *Example*:\n",
      "     `<DATASET>Maules Creek</DATASET>`\n",
      "     `Download the <DATASET>USTPO MIT</DATASET> dataset from (https://github.com/wenggong-jin/nips17-rexgen/blob/master/USPTO/data.zip)`\n",
      "\n",
      "   - **EVALMETRIC**: Evaluation metrics for models.\n",
      "     *Definition*:\n",
      "     A quantitative measure used to assess the performance and effectiveness of a statistical or machine learning model.\n",
      "     *Example*:\n",
      "     The evaluation metrics used are `<EVALMETRIC>Precision</EVALMETRIC>`, `<EVALMETRIC>Recall</EVALMETRIC>`, `<EVALMETRIC>F1-Score</EVALMETRIC>`, and `<EVALMETRIC>BLEU Score</EVALMETRIC>`.\n",
      "\n",
      "   - **LICENSE**: Licensing terms.\n",
      "     *Definition*:\n",
      "     Legal terms and conditions for using a particular resource.\n",
      "     *Example*:\n",
      "     Available licenses to use: `<LICENSE>cc-by-3</LICENSE>` and `<LICENSE>CC BY-NC 4.0</LICENSE>`\n",
      "\n",
      "   - **ONTOLOGY**: Semantic frameworks for knowledge representation.\n",
      "     *Definition*:\n",
      "     A framework representing knowledge about a domain, including concepts, entities, properties, and relationships.\n",
      "     *Example*:\n",
      "     The `<ONTOLOGY>Intelligence Task Ontology</ONTOLOGY>` is ...\n",
      "\n",
      "   - **PROGLANG**: Programming languages.\n",
      "     *Definition*:\n",
      "     A formal language used for implementing software.\n",
      "     *Example*:\n",
      "     Programming languages such as `<PROGLANG>Python</PROGLANG>`, `<PROGLANG>PHP</PROGLANG>`, and `<PROGLANG>C++</PROGLANG>`\n",
      "\n",
      "   - **PROJECT**: Scientific or business initiatives.\n",
      "     *Definition*:\n",
      "     A planned initiative aimed at addressing a research question or achieving a specific goal.\n",
      "     *Example*:\n",
      "     The `<PROJECT>Paper With Code</PROJECT>` project (https://<PROJECT>paperswithcode</PROJECT>.com/)\n",
      "\n",
      "   - **PUBLICATION**: Scholarly works.\n",
      "     *Definition*:\n",
      "     A creative work resulting from a publishing process, such as a journal article, conference proceeding, or preprint.\n",
      "     *Example*:\n",
      "     `<PUBLICATION>No Length Left Behind: Enhancing Knowledge Tracing for Modeling Sequences of Excessive or Insufficient Lengths</PUBLICATION>`. In `<PUBLICATION>Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</PUBLICATION>`\n",
      "\n",
      "   - **SOFTWARE**: Software tools or programs.\n",
      "     *Definition*:\n",
      "     Programs or tools designed to perform specific tasks on electronic devices.\n",
      "     *Example*:\n",
      "     You can use the `<SOFTWARE>Protege ontology editor</SOFTWARE>` to explore and edit the resource.\n",
      "\n",
      "   - **WORKSHOP**: Workshop events.\n",
      "     *Definition*:\n",
      "     An educational or hands-on session focused on a specific subject.\n",
      "     *Example*:\n",
      "     Refers to the [Thermal Image Super-Resolution](https://<WORKSHOP>pbvs-workshop</WORKSHOP>.github.io/datasets.html)\n",
      "\n",
      "2. **Annotation Rules:**\n",
      "   - Include the entire proper name but exclude standalone generic descriptors (e.g., exclude \"Dataset\" in \"BookSum Dataset\").\n",
      "   - Use a single-class annotation per entity, based on the context.\n",
      "   - Annotate nested entities separately.\n",
      "   - Include punctuation marks only if part of the entity (e.g., titles with \":\" in \"PAV-SOD: Panoramic Audiovisual Saliency Detection\").\n",
      "   - Annotate entities within URLs (e.g., \"llama\" in `https://ai.meta.com/llama`).\n",
      "\n",
      "3. **Output Format:**\n",
      "   - Return the original content with entities directly annotated using XML-style tags for their respective classes. Example:\n",
      "     - `The <SOFTWARE>Protege Ontology Editor</SOFTWARE> is widely used for creating ontologies.`\n",
      "   - Your output should be in Markdown format with all entities tagged as instructed.\n",
      "\n",
      "---\n",
      "Input Text:\n",
      "\n",
      "{input_text}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_id = 0\n",
    "prompt_template_path = f'./readme2kg-exp/config/deepseek-chat-prompt-0.txt'\n",
    "if os.path.isfile(prompt_template_path):\n",
    "    with open(prompt_template_path, 'r') as fd:\n",
    "        prompt_template = fd.read()\n",
    "else:\n",
    "    prompt_template = ''\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901a66c-ff2d-4d40-a331-15ff80039bb6",
   "metadata": {},
   "source": [
    "# Load Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d78df436-8385-4ea4-a549-c746303eccfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a3d58c-edbc-431f-8b96-48e919e4080e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859d999595c6448a851ac53bd4760f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# DejaVu ## Table of Contents =================\n",
      "    * [Code](#code)\n",
      "     * [Install Requirements](#install-requirements)\n",
      "     * [Usage](#usage)\n",
      "     * [Example](#example)\n",
      "   * [Datasets](#datasets)\n",
      "     `<DATASET>DejaVu</DATASET>`\n",
      "   * [Deployment and Failure Injection Scripts of Train-Ticket](#deployment-and-failure-injection-scripts-of-train-ticket)\n",
      "   * [Citation](#citation)\n",
      "   * [Supplementary details](#supplementary-details)\n",
      "    ## Paper\n",
      "     A preprint version: https://arxiv.org/abs/<PUBLICATION>2207.09021</PUBLICATION>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "sentence_text = \"\"\"# DejaVu ## Table of Contents =================    * [Code](#code)     * [Install Requirements](#install-requirements)     * [Usage](#usage)     * [Example](#example)   * [Datasets](#datasets)   * [Deployment and Failure Injection Scripts of Train-Ticket](#deployment-and-failure-injection-scripts-of-train-ticket)   * [Citation](#citation)   * [Supplementary details](#supplementary-details)    ## Paper A preprint version: https://arxiv.org/abs/2207.09021 ## Code ### Install 1.\"\"\"\n",
    "prompt = prompt_template.replace('{input_text}', sentence_text)\n",
    "# original code\n",
    "#prompt = prompt_template.replace('{input_text}', sentence.text)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful NER annotator.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "    \n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=255,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e1bc1ba-089a-4cb9-95ae-4f691968fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction(sentence, tokens, sid_path):\n",
    "    try:\n",
    "        print(f\"Process-{os.getpid()} processing {colored(sentence.text, 'red')} ...\")\n",
    "        prompt = prompt_template.replace('{input_text}', sentence.text)\n",
    "\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "            \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=255,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        result = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "        #print(f\"Process-{os.getpid()} predict {colored(sentence.text, 'cyan')} successfully\")\n",
    "        with open(sid_path, 'w') as file:\n",
    "            file.write(result)\n",
    "    except Exception as ex:\n",
    "        logging.error(f'[do_prediction] got exception: {ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a78fc7e9-36b5-4505-ae93-b6472540e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotation_labels_if_possible(predicted_text):\n",
    "    label_to_text_list = defaultdict(list)\n",
    "    acc_adjusted_pos = 0\n",
    "    for label in LABELS:\n",
    "        regex = f'<{label}>(.*?)</{label}>'\n",
    "        matches = re.finditer(regex, predicted_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        for m in matches:\n",
    "            adjusted_pos = len(label) + 2\n",
    "            label_to_text_list[label].append({\n",
    "                'text': m.group(1),\n",
    "                'start': m.start(1) - adjusted_pos - acc_adjusted_pos,\n",
    "                'end': m.end(1) - adjusted_pos - acc_adjusted_pos,\n",
    "            })\n",
    "            acc_adjusted_pos += adjusted_pos * 2 + 1\n",
    "    return label_to_text_list\n",
    "\n",
    "\n",
    "\n",
    "def post_process(predicted_text, tokens):\n",
    "    cleaned_text = cleaner.Cleaner(predicted_text).clean()\n",
    "    label_to_text_list = extract_annotation_labels_if_possible(cleaned_text)\n",
    "    return label_to_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aefcdd39-2572-4a1e-bc6b-4f5651cb88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, tokens):\n",
    "    path = f'./readme2kg-exp/results/{model_name}/prompt-{prompt_id}/zzz_{file_name}' # NOTE: prefix zzz for directory sorting, non-sense\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    sid = hashlib.sha256(sentence.text.encode()).hexdigest()[:8]\n",
    "    #if not os.path.isfile(f'{path}/{sid}.txt'):   # original code\n",
    "    if os.path.isdir(f'{path}'):\n",
    "        do_prediction(sentence, tokens, f'{path}/{sid}.txt')\n",
    "\n",
    "    with open(f'{path}/{sid}.txt', 'r') as fd:\n",
    "        predicted_text = fd.read()\n",
    "\n",
    "    label_to_text_list = post_process(predicted_text, tokens)\n",
    "    # NOTE: sanity checking\n",
    "    for label, text_list in label_to_text_list.items():\n",
    "        for text in text_list:\n",
    "            if text['text'] != sentence.text[text['start']:text['end']]:\n",
    "                prompt = prompt_template.replace('{input_text}', sentence.text)\n",
    "                #logging.warning(f\"BUG? The predicted text is not exact the same as the original text. \\n\\nPrompt: {prompt}\\nOriginal: {colored(sentence.text, 'green')}\\nGenerated: {colored(text['text'], 'red')}\\n--------------------------------------------------------------------------------\")\n",
    "\n",
    "    span_tokens_to_label_list = []\n",
    "    for label, text_list in label_to_text_list.items():\n",
    "        for text in text_list:\n",
    "            span_tokens_to_label_list.append({\n",
    "                'span_tokens': utils.make_span_tokens(tokens, text['start'], text['end']),\n",
    "                'label': label\n",
    "            })\n",
    "    return span_tokens_to_label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d259f5-d4c3-4a30-9f63-2ff5ea4b0633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0045624d-f168-44cf-a4c9-f94413793290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_serial(doc: Document):\n",
    "    annotations = []\n",
    "    for sent in doc.sentences:\n",
    "        tokens = doc.sentence_tokens(sent)\n",
    "        span_tokens_to_label_list = predict(sentence=sent, tokens=tokens)\n",
    "        \n",
    "        # create the annotation instances\n",
    "        for span_tokens_to_label in span_tokens_to_label_list:\n",
    "            span_tokens = span_tokens_to_label['span_tokens']\n",
    "            label = span_tokens_to_label['label']\n",
    "            if span_tokens is None:\n",
    "                continue\n",
    "\n",
    "            annotation = utils.make_annotation(tokens=span_tokens, label=label)\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    result = utils.replace_webanno_annotations(doc, annotations=annotations)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee5349e6-b4b2-49f8-bd0e-a5fc25f83a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# DejaVu ## Table of Contents =================    * [Code](#code)     * [Install Requirements](#install-requirements)     * [Usage](#usage)     * [Example](#example)   * [Datasets](#datasets)   * [Deployment and Failure Injection Scripts of Train-Ticket](#deployment-and-failure-injection-scripts-of-train-ticket)   * [Citation](#citation)   * [Supplementary details](#supplementary-details)    ## Paper A preprint version: https://arxiv.org/abs/2207.09021 ## Code ### Install 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAll the software requirements are already pre-installed in the Docker image below.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe requirements are also listed in `requirements.txt` and `requirements-dev.txt`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote that `DGL 0.8` is not released yet when I did this work, so I installed `DGL 0.8` manually from the source code.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPyTorch version should be equal to or greater than 1.11.0.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```bash    docker pull lizytalk/dejavu    ``` 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPull the code from GitHub    ```bash    git pull https://github.com/NetManAIOps/DejaVu.git DejaVu    ``` 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload the datasets following the link in the GitHub repo and extract the datasets into `.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/DejaVu/data` 4.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mI use the command `realpath` in the example commands below, which is not bundled in macOS and Windows.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOn macOS, you can install it by `brew install coreutils`. 5.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mStart a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRun `direnv allow` in the shell of the Docker container to set the environment variables. 7.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRun experiments in the shell of the Docker container following the usage table as follows.   ### Usage |Algorithm|Usage| |---|---| |DejaVu|Run for dataset A1: `python exp/run_GAT_node_classification.py -H=4 -L=8 -fe=GRU -bal=True --data_dir=data/A1`| |JSS'20|Run for dataset A1: `python exp/DejaVu/run_JSS20.py --data_dir=data/A1`| |iSQUAD|Run for dataset A1: `python exp/DejaVu/run_iSQ.py --data_dir=data/A1`| |Decision Tree|Run for dataset A1: `python exp/run_DT_node_classification.py --data_dir=data/A1`| |RandomWalk@Metric|Run for dataset A1: `python exp/DejaVu/run_random_walk_single_metric.py --data_dir=data/A1 --window_size 60 10 --score_aggregation_method=min`| |RandomWalk@FI|Run for dataset A1: `python exp/DejaVu/run_random_walk_failure_instance.py --data_dir=data/A1 --window_size 60 10 --anomaly_score_aggregation_method=min --corr_aggregation_method=max`| |Global interpretation|Run `notebooks/explain.py` as a jupyter notebook with `jupytext`| |Local interpretation|`DejaVu/explanability/similar_faults.py`|  The commands would print a `one-line summary` in the end, including the following fields: `A@1`, `A@2`, `A@3`, `A@5`, `MAR`, `Time`, `Epoch`, `Valid Epoch`, `output_dir`, `val_loss`, `val_MAR`, `val_A@1`, `command`, `git_commit_url`, which are the desrired results.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTotally, the main experiment commands of DejaVu should output as follows: - FDG message, including the data paths, edge types, the number of nodes (failure units), the number of metrics, the metrics of each failure class. - Traning setup message: the faults used for training, validation and testing. - Model architecture: model parameters in each part, total params - Training process: the training/validation/testing loss and accuracy - Time Report. - command output one-line summary.  ### Example See https://github.com/NetManAIOps/DejaVu/issues/4  ## Datasets  The datasets A, B, C, D are public at : - https://www.dropbox.com/sh/ist4ojr03e2oeuw/AAD5NkpAFg1nOI2Ttug3h2qja?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mdl=0 - https://doi.org/10.5281/zenodo.6955909 (including the raw data of the Train-Ticket dataset) In each dataset, `graph.yml` or `graphs/*.yml` are FDGs, `metrics.csv` is metrics, and `faults.csv` is failures (including ground truths).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`FDG.pkl` is a pickle of the FDG object, which contains all the above data.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote that the pickle files are not compatible in different Python and Pandas versions.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSo if you cannot load the pickles, just ignore and delete them.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThey are only used to speed up data load.  ## Deployment and Failure Injection Scripts of Train-Ticket https://github.com/lizeyan/train-ticket  ## Citation ``` bibtex @inproceedings{li2022actionable,   title = {Actionable and Interpretable Fault Localization for Recurring Failures in Online Service Systems},   booktitle = {Proceedings of the 2022 30th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},   author = {Li, Zeyan and Zhao, Nengwen and Li, Mingjie and Lu, Xianglin and Wang, Lixin and Chang, Dongdong and Nie, Xiaohui and Cao, Li and Zhang, Wenchi and Sui, Kaixin and Wang, Yanhua and Du, Xu and Duan, Guoqing and Pei, Dan},   year = {2022},   month = nov,   series = {{{ESEC}}/{{FSE}} 2022} } ```  ## Supplementary details ### Local interpretation !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[local interpretation](figures/local_interpretation.png)   Since the DejaVu model is trained with historical failures, it is straightforward to interpret how it diagnoses a given failure by figuring out from which historical failures it learns to localize the root causes.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTherefore, we propose a pairwise failure similarity function based on the aggregated features extracted by the DejaVu model.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCompared with raw metrics, the extracted features are of much lower dimension and contain little useless information, which the DejaVu model ignores.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHowever, computing failure similarity is not trivial due to the generalizability of DejaVu.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor example, suppose that the features are $1$ for root-cause failure units and $0$ for other failure units and there are four failure units ($v_1$, $v_2$, $v_3$, $v_4$).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThen for two similar failures which occur at $v_1$ and $v_2$ respectively, their feature vectors are $(1, 0, 0, 0)$ and $(0, 1, 0, 0)$ respectively, which are dissimilar with respect to common similarity metrics (e.g., Manhattan or Euclidean).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo solve this problem, we calculate similarities based on failure classes rather than single failure units.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAs shown in \\cref{fig:local-interpretation}, for each failure units at an in-coming failure $T_1$, we compare it with each unit of the corresponding failure classes at a historical failure $T_2$ and take the minimal similarity as its similarity to $T_2$.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThen, we average the similarities to T2 if all units with their suspicious scores (of $T_1$) as the weights.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is because we only care about those failure units that matter in the current failure when finding similar historical failures.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn summary, the similarity function to compare $T_1$ and $T_2$ can be formalized as follows: $$ d(T_1, T_2)=\\frac{1}{|V|}\\sum_{v\\in V}s_{T_1}(v)(\\min_{v' \\in N_c(v;G)}||\\boldsymbol{\\hat{f}}^{(T_1, v)}-\\boldsymbol{\\hat{f}}^{(T_2, v')}||_1) $$ where $N_c(v;G)$ denotes the failure units of the same class as $v$ in $G$, and $||\\cdot||_1$ denotes $L1$ norm.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor an in-coming failure, we calculate its similarity to each historical failure and recommend the top-k most similar ones to engineers.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOur model is believed to learn localizing the root causes from these similar historical failures.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFurthermore, engineers can also directly refer to the failure tickets of these historical failures for their diagnosis and mitigation process.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote that sometimes the most similar historical failures may have different failure classes to the localization results due to imperfect similarity calculation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn such cases, we discard and ignore such historical failures.    ### Global interpretation The selected time-series features are listed as follows: !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[the list of selected time-series features](figures/global_interpretation_time_series_features.png)\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m[!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[License: MIT](https://img.shields.io/badge/license-MIT-blue.svg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=flat-square)](https://opensource.org/licenses/MIT) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[DOI](https://zenodo.org/badge/DOI/10.48550/arXiv.2404.13971.svg)](https://doi.org/10.48550/arXiv.2404.13971)  # !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[HamilToniQ_logo](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/figures/HamilToniQ_logo.png)HamilToniQ: An Open-Source Benchmark Toolkit for Quantum Computers  Table of Contents:  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Introduction](#introduction) 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Quick Start](#quickstart) 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[H-Scores](#hscores) 4.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Architecture](#architecture) 5.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[How to cite](#cite)  <a name=\"introduction\"></a>  ## Introduction   HamilToniQ is an application-oriented benchmarking toolkit for the comprehensive evaluation of QPUs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHighlighted features include:  - Easy to use: Users only need to call QPUs, run the main function, and get the scores. - Comprehensive scoring system: The toolkit generates a single score, termed H-Score, capturing all relevant QPU performance factors such as error rates, circuit compilation, and quantum error mitigation. - Standardized and comparable results: H-Scores ensure consistent and comparable evaluation across different platforms. - Focusing on real-world application: The toolkit is based on optimization algorithms to reflect actual application scenarios. - Multi-QPU support: Utility in multi-QPU resource management has been demonstrated.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<a name=\"quickstart\"></a>  ## Quick Start   ### Installation  Install the *HamilToniQ* toolkit by running the following code in the Terminal.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```shell cd /path/to/your/directory git clone https://github.com/FelixXu35/hamiltoniq.git cd hamiltoniq pip install -e . ```  ### Bechmark a backend  Simply copy and run the following Python code:  ```python from hamiltoniq.bechmark import Toniq  toniq = Toniq() backend = <your_backend> n_qubits = <your_prefered_number_of_qubits> n_layers = <your_prefered_number_of_layers> n_cores = <number_of_cores_in_your_PC>  score = toniq.simulator_run(backend=backend, n_qubits=n_qubits, n_layers=n_layers, n_cores=n_cores) ```  An example is given in [this notebook](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/example_code.ipynb).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<a name=\"hscores\"></a>  ## H-Scores  The following results were obtained on the built-in Q matrices.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote that comparison across different numbers of qubits is meaningless.  ##### 3 qubits  <p align=center><img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/hamiltoniq/H_Scores/qubit_3.png\" alt=\"n_qubits=3\" width=\"700\" /></p>  ##### 4 qubits  <p align=center><img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/hamiltoniq/H_Scores/qubit_4.png\" alt=\"n_qubits=4\" width=\"700\" /></p>  ##### 5 qubits  <p align=center><img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/hamiltoniq/H_Scores/qubit_5.png\" alt=\"n_qubits=4\" width=\"700\" /></p>  ##### 6 qubits  <p align=center><img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/hamiltoniq/H_Scores/qubit_6.png\" alt=\"n_qubits=4\" width=\"700\" /></p>  <a name=\"architecture\"></a>  ## Architecture  For more technical details please visit our [arXiv Paper](https://arxiv.org/abs/2404.13971).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe HamilToniQ’s benchmarking workflow, shown in the figure below, commences with the characterization of QPUs, where each QPU is classified according to its type, topology, and multi-QPU system.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis initial step ensures a tailored approach to the benchmarking process, considering the unique attributes of each QPU.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSubsequently, the process engages in quantum circuit compilation, employing a specific strategy designed to optimize the execution of quantum circuits on the identified QPUs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIntegral to the workflow is Quantum Error Mitigation (QEM), which strategically addresses computational noise and errors that could affect the fidelity of the quantum processes.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe culmination of this rigorous workflow is the benchmarking result, which quantifies the performance of the QPU in terms of reliability—represented by the H-Score and Execution Time.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese metrics provide a quantitative and objective measure of the QPU’s performance, reflecting the effectiveness of the benchmarking process implemented by HamilToniQ.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAdditionally, the H-score can help manage computational resources in a Quantum-HPC system.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<p align=center><img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/figures/benchmarking_scheme.png\" alt=\"scheme\" width=\"400\" /></p>  HamilToniQ primarily comprises two components: the reference part, also known as ground truth, and the scoring part, as depicted in the figure below.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe reference part, which is optional, utilizes a noiseless simulator to find the scoring curve.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUsers will only need this part when they are benchmarking on their own Q matrices.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn the scoring part, the Quantum Approximate Optimization Algorithm (QAOA) is executed a certain number of times, and the scoring curve is used to determine the score of each iteration based on its accuracy.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe final H-Score is computed as the average of all individual scores.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<p align=center><img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/figures/benchmarking_flow.png\" alt=\"flow\" width=\"400\" /></p>  <a name=\"cite\"></a>  ## How to cite  If you used this package or framework for your research, please cite:  ```text @article{xu2024hamiltoniq,   title={HamilToniQ: An Open-Source Benchmark Toolkit for Quantum Computers},   author={Xu, Xiaotian and Chen, Kuan-Cheng and Wille, Robert},   journal={arXiv preprint arXiv:2404.13971},   year={2024} } ```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 3 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# `LinConGauss` ### _Integrals and samples of Gaussians under linear domain constraints_  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Multilevel Splitting](https://repository-images.githubusercontent.com/243241472/797da100-5891-11ea-857f-0cca52af9239 \"Multilevel Splitting\")   ## Setup Clone the repository and run `setup.py` ```bash git clone https://github.com/alpiges/LinConGauss.git ~/LinConGauss cd ~/LinConGauss python setup.py install ```  ## Usage For usage, please refer to the tutorials in the `notebook` section.  ## How to cite If you are using `LinConGauss` for your research, consider citing the [paper](https://arxiv.org/abs/1910.09328)  ``` @inproceedings{GessnerKH2020,     title     = {Integrals over Gaussians under Linear Domain Constraints},     author    = {Alexandra Gessner and Oindrila Kanjilal and Philipp Hennig},     booktitle = {Proceedings of Machine Learning Research},     publisher = {PMLR},     year      = {2020},     url       = {https://arxiv.org/abs/1910.09328} } ```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# 3D CMR-Domain-Adaptation  This repo contains code to train a deep learning model for **Unsupervised Domain Adaptation (UDA)** of 3D cardiac magnetic resonance (CMR) cine images to **transform from axial to short-axis orientation**.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe task associated to the domain adaptation is to perform a **segmentation task via a pre-trained fixed network**, and the results are leveraged to guide the transformation process (rigid transform via spatial transformer networks).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe trained model is able to transform an axial (AX) CMR into the patient specific short-axis (SAX) direction.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe model learns from paired AX/SAX CMR image pairs and a pre-trained SAX segmentation model.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe following gif exemplary visualizes the learning progress of this model.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSlices along z-direction are shown horizontally.   !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Unsupervised Domain adaptation learning](https://github.com/Cardio-AI/3d-mri-domain-adaption/blob/master/reports/ax_sax_learning_example.gif \"learning progress\")   Each temporal frame shows the AX2SAX prediction of the model after successive epochs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAt the end of the learning process, the model is able to transform the data set such that it corresponds to a short-axis view, which can be segmented more reliably by the pre-trained short-axis segmentation module.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFinally the inverse transformation could be applied to the segmentation, which results in a automatically segmented AX CMR.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor the training of this transformation model, neither AX nor SAX ground truth segmentations are required.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mA special **Euler2Affine Layer** was implemented to restrict the transform of the spatial transformer network to be rigid and invertible\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m# Overview  - The repository dependencies are saved as conda environment (environment.yaml) file\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- The Deep Learning models/layers are build with TF 2.X. - Setup instruction for the repository are given here: [Install requirements](https://github.com/Cardio-AI/3d-mri-domain-adaptation#setup-instructions-tested-with-osx-and-ubuntu) - An overview of all files and there usage is given here: [Repository Structure](https://github.com/Cardio-AI/3d-mri-domain-adaptation#repository-structure)  # Paper:  Please cite the following paper if you use/modify or adapt part of the code from this repository:  S.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mKoehler, T.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHussain, Z.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBlair, T.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHuffaker, F.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRitzmann, A.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTandon, T.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPickardt, S.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSarikouch, H.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLatus, G.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mGreil, I.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWolf, S.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mEngelhardt, \"Unsupervised Domain Adaptation from Axial to Short-Axis Multi-Slice Cardiac MR Images by Incorporating Pretrained Task Networks,\" in IEEE Transactions on Medical Imaging (TMI), early access, doi: 10.1109/TMI.2021.3052972\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Link to original paper: [IEEE TMI paper link](https://ieeexplore.ieee.org/document/9328840) (accepted 13.1.2021) - A pre-print version is available here: [arxiv-Preprint](http://arxiv.org/abs/2101.07653)  Bibtex:  >@article{Koehler_2021, <br> >   title={Unsupervised Domain Adaptation from Axial to Short-Axis Multi-Slice Cardiac MR Images by Incorporating Pretrained Task Networks},<br> >   ISSN={1558-254X},<br> >   url={http://dx.doi.org/10.1109/TMI.2021.3052972},<br> >   DOI={10.1109/tmi.2021.3052972},<br> >   journal={IEEE Transactions on Medical Imaging},<br> >   publisher={Institute of Electrical and Electronics Engineers (IEEE)},<br> >   author={Koehler, Sven and Hussain, Tarique and Blair, Zach and Huffaker, Tyler and Ritzmann, Florian and Tandon, Animesh and Pickardt, Thomas and Sarikouch, Samir and Sarikouch, Samir and Latus, Heiner and Greil, Gerald and Wolf, Ivo and Engelhardt, Sandy}, >   year={2021},<br> >   pages={1–1}<br> >}   # Institutions:  >- [Heidelberg University Hospital, Artificial Intelligence in Cardiovascular Medicine (AICM) Group](https://www.klinikum.uni-heidelberg.de/chirurgische-klinik-zentrum/herzchirurgie/forschung/ag-artificial-intelligence-in-cardiovascular-medicine) >- [German Competence network for Congenital heart defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/) >- [UT Southwestern Medical Center, Pediatric Cardiology](https://www.utsouthwestern.edu/education/medical-school/departments/pediatrics/divisions/cardiology/) >- [Department of Computer Science, University Of Applied Science Mannheim](https://www.informatik.hs-mannheim.de/wir/menschen/professoren/prof-dr-ivo-wolf.html)   # How to use:  This repository splits the source code into:  - interactive notebooks (/notebooks),  - python source modules (/src) and  - the experiment related files such as the experiment configs (/reports) or trained models (/models).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mEach experiment includes the following artefacts: - One config file, which represents all experimental hyper-parameters which are neccessary to reproduce the experiment or to load it for later predictions - Three tensorboard logfiles per training to keep track of the trainings, evaluation and visual output progress\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- One dataframe which orchestrates the nrrd files with the metadata and the experiment splitting\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Model graph description either as json or tensorflow protobuf file and the corresponding model weights as h5 file.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe tensorflow model and layer definitions are within /src/models.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe transformation layer is built on the neuron project, which is also part of the current Voxelmorph approach (https://github.com/voxelmorph/voxelmorph).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUse the Notebooks to interact (train, predict or evaluate) with the python functions.   ## Repository Structure      ├── LICENSE     ├── Makefile           <- Makefile with commands like 'make environment'     ├── README.md          <- The top-level README for developers using this project.     ├── data     │   ├── metadata       <- Excel and csv files with additional metadata are stored here     │   ├── interim        <- Intermediate data that has been transformed\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m│   ├── predicted      <- Model predictions, will be used for the evaluation scripts     │   └── raw            <- The original, immutable data dump\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m│     ├── models             <- Trained and serialized model definitions and weights     │     ├── notebooks          <- Jupyter notebooks\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m│   ├── Dataset        <- Create, map, split or pre-process the dataset     │   ├── Evaluate       <- Evaluate the model predictions, create dataframes and plots     │   ├── Predict        <- Load an experiment config and a pre-trained model,      │   │                     transform AX CMR into the SAX domain, apply the task network,      │   │                     transform the predicted mask back into the AX domain,      │   │                     undo the generator steps and save the prediction to disk        │   └── Train          <- Inspect the generators, define an experiment config,     │                              load and inject a task network, build the graph and train a new AX2SAX model     │     ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m│   ├── configs        <- Experiment config files as json with all hyperparameters and paths     │   ├── figures        <- Generated graphics and figures to be used in reporting     │   ├── history        <- Tensorboard trainings history files     │   └── tensorboard_logs  <- Trainings-scalars and images of the intermediate predictions     │     ├── environment.yaml   <- Conda requirements file.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mGenerated with `conda env export > environment.yml`     │     ├── setup.py           <- Makes project pip installable (pip install -e .) so src can be imported     ├── src                <- Python modules with classes and functions that will be orchestrated in the notebooks.         ├── data           <- Python modules - Generators, preprocess and postprocess functions         ├── models         <- Python modules - TF.keras 2.X Model and Layer definition         ├── utils          <- Python modules - Metrics, losses, prediction, evaluation code, TF-callbacks and io-utils         └── visualization  <- Python modules - Plot functions for the evaluation and data description  ## Dataset For this work a multi-centric heterogeneous cine-SSFPs CMR TOF data set from the [German Competence Network for Congenital Heart Defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/)) was used (study identifier: NCT00266188, title: Non-invasive Imaging and Exercise Tolerance Tests in Post-repair Tetralogy of Fallot -Intervention and Course in Patients Over 8 Years Old).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis TOF dataset constitutes one of the largest compiled data set of this pathology to date.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe data was acquired at 14 different German sites between 2005-2008 on 1.5T and 3T machines;  further descriptions can be found in [original study](https://www.ahajournals.org/doi/epub/10.1161/CIRCIMAGING.111.963637), [eprint previous work](https://arxiv.org/abs/2002.04392) [1],[2]\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[1] Sarikouch S, Koerperich H, Dubowy KO, Boethig D, Boettler P, Mir TS, Peters B, Kuehne T, Beerbaum P; German Competence Network for Congenital Heart Defects Investigators.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mImpact of gender and age on cardiovascular function late after repair of tetralogy of Fallot: percentiles based on cardiac magnetic resonance.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCirc Cardiovasc Imaging. 2011 Nov;4(6):703-11. doi: 10.1161/CIRCIMAGING.111.963637.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mEpub 2011 Sep 9.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPMID: 21908707\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[2] Köhler, Sven, Animesh Tandon, Tarique Hussain, H.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLatus, T.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPickardt, S.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSarikouch, P.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBeerbaum, G.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mGreil, S.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mEngelhardt and Ivo Wolf.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m“How well do U-Net-based segmentation trained on adult cardiac magnetic resonance imaging data generalise to rare congenital heart diseases for surgical planning?”\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMedical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=\"_blank\" href=\"https://www.anaconda.com/download/#macos\">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=\"_blank\" href=\"https://www.tensorflow.org/install/gpu\">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name \"ax2sax kernel\" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 5 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m<p align=\"center\">     <img src=\"assets/emoji.png\" alt=\"earthPT\" width=\"150\"/> </p>  # EarthPT  <p align=\"center\">     <img src=\"assets/timeseries.png\" alt=\"prediction\" width=\"600\"/> </p>  A simple repository for training time series large observation models.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis repository began its life as Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT), and has been altered so that it is usable for time series data.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`train.py` reproduces [EarthPT-700M](https://arxiv.org/abs/2309.07207) when trained on 14B time series 'tokens' of ClearSky EO data within the TL UK National Grid tile.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWhen run, `train.py` takes ~5 days to achieve Chinchilla 🐭  completion on a single 8xA100 40GB node.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWithin `train.py` you will find a ~300-line boilerplate training loop and within `model.py` you will find a ~300-line GPT model definition with an MLP tokeniser and a regressive loss.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe have purposefully kept the code as simple and hackable as possible so that it is easy for all to hack this base to their needs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe release this code under the MIT licence in the hope that it will prove useful to others working on EO and timeseries large observation models.  ## install  Dependencies:  - `pip install -r requirements.txt`  ## results  Our EarthPT-700M model is able to predict future satellite passes well into the future, and also learns semantically meaningful information about the timeseries that it is fed:  <p align=\"center\">     <img src=\"assets/3d.gif\" alt=\"embeddings\" width=\"400\"/> </p>  You can find a plot with less angular momentum and further results in our paper [here](https://arxiv.org/abs/2309.07207).  ## pretrained weights  You can find our weights for all the EarthPT models on [HuggingFace](https://doi.org/10.57967/hf/1598) 🤗 .  ## citation  If you find EarthPT useful in your work please do drop us a cite:  ```bibtex @article{ref_smith2023,     author = {Smith, M.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mJ. and Fleming, L. and Geach, J.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mE.},     title = {{EarthPT: a time series foundation model for Earth Observation}},     journal = {arXiv},     year = {2023},     eprint = {2309.07207},     doi = {10.48550/arXiv.2309.07207} } ```  This work is also in the [proceedings](https://www.climatechange.ai/papers/neurips2023/2) of the 2023 CCAI NeurIPS workshop.\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m### Aspire Repository accompanying paper for modeling fine grained similarity between documents:   **Title**: \"Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity\"  **Authors**: Sheshera Mysore, Arman Cohan, Tom Hope  **Abstract**: We present a new scientific document similarity model based on matching fine-grained aspects of texts.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSuch co-citations not only reflect close paper relatedness, but also provide textual descriptions of how the co-cited papers are related.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis novel form of textual supervision is used for learning to match aspects across papers.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe develop multi-vector representations where vectors correspond to sentence-level aspects of documents, and present two methods for aspect matching: (1) A fast method that only matches single aspects, and (2) a method that makes sparse multiple matches with an Optimal Transport mechanism that computes an Earth Mover's Distance between aspects.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOur approach improves performance on document similarity tasks in four datasets.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFurther, our fast single-match method achieves competitive results, paving the way for applying fine-grained similarity to large scientific corpora.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe pre-print can be accessed here: https://arxiv.org/abs/2111.08366  **NEWS:** This work has been accepted to NAACL 2022, stay tuned for the camera-ready paper and additional artifacts.  ### Contents 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Artifacts](#artifacts)     1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[HF Models](#models)     1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Evaluation Datasets](#evaldata) 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Model Usage Instructions](#modelusage) 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Repository Contents](#repocontents) 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Acknowledgements](#acks) 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Citation](#citation) 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[TODOs](#todos)   ### Artifacts <a name=\"artifacts\"></a>  #### Models <a name=\"models\"></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=\"modelusage\"></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': \"Multi-Vector Models with Textual Guidance for Fine-Grained Scientific\"               \" Document Similarity\",      'ABSTRACT': [\"We present a new scientific document similarity model based on \"                   \"matching fine-grained aspects of texts.\",                   \"To train our model, we exploit a naturally-occurring source of \"                   \"supervision: sentences in the full-text of papers that cite multiple \"                   \"papers together (co-citations).\"]},     {'TITLE': \"CSFCube -- A Test Collection of Computer Science Research Articles for \"               \"Faceted Query by Example\",      'ABSTRACT': [\"Query by Example is a well-known information retrieval task in which\"                   \" a document is chosen by the user as the search query and the goal is \"                   \"to retrieve relevant documents from a large collection.\",                   \"However, a document often covers multiple aspects of a topic.\",                   \"To address this scenario we introduce the task of faceted Query by \"                   \"Example in which users can also specify a finer grained aspect in \"                   \"addition to the input query document. \"]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mView example usage and sample document matches here: [`examples/demo-contextualsentence-multim.ipynb`](https://github.com/allenai/aspire/blob/main/examples/demo-contextualsentence-multim.ipynb)  ##### `SPECTER-CoCite`  The `SPECTER-CoCite` bi-encoder model can be used via the `transformers` library as:  ```python from transformers import AutoModel, AutoTokenizer aspire_bienc = AutoModel.from_pretrained('allenai/aspire-biencoder-compsci-spec') aspire_tok = AutoTokenizer.from_pretrained('allenai/aspire-biencoder-compsci-spec') title = \"Multi-Vector Models with Textual Guidance for Fine-Grained Scientific \"         \"Document Similarity\" abstract = \"We present a new scientific document similarity model based on matching \"            \"fine-grained aspects of texts.\" d=[title + aspire_tok.sep_token + abstract] inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=\"pt\", max_length=512) result = aspire_bienc(**inputs) clsrep = result.last_hidden_state[:, 0, :] ```  However, note that the Hugging Face models don't have a set of additional scalar-mix parameters to compute a learned weighted sum of the representations from different layers of the transformer encoder.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese are used in our paper and are important for performance in some datasets.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mObtain the model zip files:  - [`aspire-biencoder-biomed-scib-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-scib-full.zip) - [`aspire-biencoder-biomed-spec-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-spec-full.zip) - [`aspire-biencoder-compsci-spec-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip)  ```bash wget -O aspire-biencoder-compsci-spec-full.zip https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip unzip aspire-biencoder-compsci-spec-full.zip ```  Now it may be used as:  ```python  import os, json, codecs, torch from transformers import AutoTokenizer from examples.ex_aspire_bienc import AspireBiEnc  # Directory where zipped model was downloaded and unzipped. model_path = '.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/aspire-biencoder-compsci-spec-full'  # Load hyperparameters from disk. with codecs.open(os.path.join(model_path, 'run_info.json'), 'r') as fp:     hparams = json.load(fp)     model_hparams = hparams['all_hparams']  # Initialize the tokenizer and model. aspire_tok = AutoTokenizer.from_pretrained(model_hparams['base-pt-layer']) aspire_bienc = AspireBiEnc(model_hparams)  # Load model parameters from disk. model_fname = os.path.join(model_path, 'model_cur_best.pt') aspire_bienc.load_state_dict(torch.load(model_fname))  # Encode example input. title = \"Multi-Vector Models with Textual Guidance for Fine-Grained Scientific \"         \"Document Similarity\" abstract = \"We present a new scientific document similarity model based on matching \"            \"fine-grained aspects of texts.\" d = [title + aspire_tok.sep_token + abstract]  inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=\"pt\", max_length=512) clsrep = aspire_bienc.forward(inputs)  ```   #### Evaluation Datasets <a name=\"evaldata\"></a>  The paper uses the following evaluation datasets:  - RELISH was created in [Brown et al. 2019](https://academic.oup.com/database/article/doi/10.1093/database/baz085/5608006?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mlogin=true).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWhile I wasn't able to access the link in the publication.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mI was able to obtain a copy of the dataset from: [link](http://pubannotation.org/projects/RELISH-DB).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDataset splits are created in `pre_proc_relish.py`\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- TRECCOVID presents an ad-hoc search dataset.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe versions of the dataset used may be accessed here: [query topics](https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml), [relevance annotations](https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt), and the metadata for papers is obtained from the [CORD-19](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html) dataset in the [2021-06-21](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-06-21/metadata.csv) release.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe function `get_qbe_pools` in `pre_proc_treccovid.py`, converts the dataset in its original form to the reformulated form, TRECCOVID-RF, used in the paper.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDataset splits are created in `pre_proc_treccovid.py`\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- SciDocs is obtained from: [link](https://github.com/allenai/scidocs).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe dataset splits supplied alongside the original dataset are used as is\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- CSFCube is obtained from: [link](https://github.com/iesl/CSFCube).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe dataset splits supplied alongside the original dataset are used as is.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mComplete evaluation datasets used in the paper can be downloaded here: [`datasets/datasets.md`](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name=\"repocontents\"></a>      ├── bin     ├── config     │             └── models_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid_models         │             │             ├── disent_models.py         │             │             ├── pair_distances.py         │             │             └── sentsim_models.py         │             ├── main_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre_process             ├── extract_entities.py             ├── pp_settings.py             ├── pre_proc_cocits.py             ├── pre_proc_gorc.py             ├── pre_proc_relish.py             ├── pre_proc_scidocs.py             ├── pre_proc_treccovid.py             ├── pp_gen_nearest.py             └── pre_proc_buildreps.py   **The repository is organized broadly as:**  `src/pre_process/`: Scripts to 1) generate gather and filter co-citations data from the [S2ORC](https://github.com/allenai/s2orc) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`src/learning/`: Classes for implementing models, training, batching data, and a main script to train and save the model.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`src/evaluation/`: Scripts to evaluate model performances on various evaluation datasets.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee `src/evaluation/evaluate.md` for help.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`config/models_config`: JSON files with hyper-parameters for models in the paper consumed by code in `src/learning/`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSince we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`bin`: Shell scripts to call the scripts in all the `src` sub-directories with appropriate command line arguments.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`scripts`: Miscellaneous glue code.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**The following files are the main entry points into the repository:**  `src/learning/main_fsim.py`: The main script called from `bin/learning/run_main_fsim-ddp.sh` to initialize and train a model.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe models consume json config files in `config/models_config/{<domain>}`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mA mapping from the model names/classes/configs in the repository to the models reported in the paper is as follows:  `src/evaluation/evaluate.py`: Contain code to generate rankings over the evaluation datasets.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSupports trained & downloaded models, and it is simple to add new models to the flow.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor instructions, read the detailed help snippets of argument parser.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`src/pre_process/pre_proc_gorc.py:` Code to gather full text articles from the [S2ORC](https://github.com/allenai/s2orc) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis code assumes the 2019-09-28 release of S2ORC.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`src/pre_process/pre_proc_cocits.py:` Generate training data for the models reported in the paper.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCo-citations are used for training sentence level encoder models and whole abstract models, training data for both these model types are generated from functions in this script.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese are the `filter_cocitation_sentences` and `filter_cocitation_papers` functions respectively.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFunctions listed under `write_examples` generate training positive pairs for various models (negatives are generated with in-batch negative sampling).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCSFCube data format matches the assumed format.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDetails about each dataset are as follows:  `src/pre_process/extract_entities.py`:  Use PURE's Entity Model () to extract named entities from abstracts.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn some experiments, these are added to the abstract as additional sentences as an augmented input, improving results.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor info on how to run this file see src/pre_process/README_NER     <div style=\"margin-left: auto;             margin-right: auto;             width: 95%\">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=\"acks\"></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=\"citation\"></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=\"todos\"></a>  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRelease trained model parameters.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m(in-progress)     - Currently released models are _per-domain_ models for computer science and biomedical papers which were used in the paper.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe coming months will also see release of domain independent models trained on data across different scientific domains. 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRelease training training data\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Co-citation data used to train the above model will also be released, this is co-citation pairs on the order of a few million pairs of papers. 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTraining code usage instructions\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- This will be released for reproducibility.\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# A Survey on the Role of Crowds in Combating Online Misinformation: Annotators, Evaluators, and Creators  A curated list of papers on \"A Survey on the Role of Crowds in Combating Online Misinformation: Annotators, Evaluators, and Creators\" ([Paper link](https://arxiv.org/abs/2310.02095))   ## Citation ```  @article{he2023survey,   title={A Survey on the Role of Crowds in Combating Online Misinformation: Annotators, Evaluators, and Creators},   author={He, Bing and Hu, Yibo and Lee, Yeon-Chang and Oh, Soyoung and Verma, Gaurav and Kumar, Srijan},   journal={arXiv preprint arXiv:2310.02095},   year={2023} }  ```   Online misinformation poses a global risk with significant real-world consequences.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo combat misinformation, current research relies on professionals like journalists and fact-checkers for annotating and debunking false information, while also developing automated machine learning methods for detecting misinformation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mComplementary to these approaches, recent research has increasingly concentrated on utilizing the power of ordinary social media users, a.k.a.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m“the crowd”, who act as eyes-on-the-ground proactively questioning and countering misinformation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNotably, recent studies show that 96% of counter-misinformation responses originate from them.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAcknowledging their prominent role, we present the first systematic and comprehensive survey of research papers that actively leverage the crowds to combat misinformation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn this survey, we first identify 88 papers related to crowd-based efforts, following a meticulous annotation process adhering to the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe then present key statistics related to misinformation, counter-misinformation, and crowd input in different formats and topics.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUpon holistic analysis of the papers, we introduce a novel taxonomy of the roles played by the crowds in combating misinformation: (i) crowds as annotators who actively identify misinformation; (ii) crowds as evaluators who assess counter-misinformation effectiveness; (iii) crowds as creators who create counter-misinformation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/assets/crowd_role_visual.jpg\" alt=\"\"/>    This taxonomy explores the crowd’s capabilities in misinformation detection, identifies the prerequisites for effective counter-misinformation, and analyzes crowd-generated counter-misinformation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn each assigned role, we conduct a detailed analysis to categorize the specific utilization of the crowd.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mParticularly, we delve into (i) distinguishing individual, collaborative, and machine-assisted labeling for annotators; (ii) analyzing the effectiveness of counter-misinformation through surveys, interviews, and in-lab experiments for evaluators; and (iii) characterizing creation patterns and creator profiles for creators.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFinally, we conclude this survey by outlining potential avenues for future research in this field.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/assets/taxonomy.jpg\" alt=\"\" />    ## References  ## 2022 | Title                                                                                                                                                       | Category   |   Year | Venue                                                                                          | Publisher                                            | URL                                                                  | |:------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------|-------:|:-----------------------------------------------------------------------------------------------|:-----------------------------------------------------|:---------------------------------------------------------------------| | Does Evidence from Peers Help Crowd Workers in Assessing Truthfulness?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| Annotator  |   2022 | WWW 2022 - Companion Proceedings of the Web Conference 2022                                    | Association for Computing Machinery, Inc             | https://www.doi.org/10.1145/3487553.3524236                          | | Birds of a feather don't fact-check each other: Partisanship and the evaluation of news in Twitter's Birdwatch crowdsourced fact-checking program           | Annotator  |   2022 | Conference on Human Factors in Computing Systems - Proceedings                                 | Association for Computing Machinery                  | https://www.doi.org/10.1145/3491102.3502040                          | | Crowd Sourcing and Blockchain-Based Incentive Mechanism to Combat Fake News                                                                                 | Annotator  |   2022 | Studies in Computational Intelligence                                                          | Springer Science and Business Media Deutschland GmbH | https://www.doi.org/10.1007/978-3-030-90087-8_15                     | | Social Debunking of Misinformation on WhatsApp: The Case for Strong and In-group Ties                                                                       | Evaluator  |   2022 | Proceedings of the ACM on Human-Computer Interaction                                           | Association for Computing Machinery                  | https://www.doi.org/10.1145/3512964                                  | | Election Fraud and Misinformation on Twitter: Author, Cluster, and Message Antecedents                                                                      | Evaluator  |   2022 | Media and Communication                                                                        | Cogitatio Press                                      | https://www.doi.org/10.17645/mac.v10i2.5168                          | | Debunking health myths on the internet: the persuasive effect of (visual) online communication                                                              | Evaluator  |   2022 | Journal of Public Health (Germany)                                                             | Springer Science and Business Media Deutschland GmbH | https://www.doi.org/10.1007/s10389-022-01694-3                       | | A comparison of prebunking and debunking interventions for implied versus explicit misinformation                                                           | Evaluator  |   2022 | British Journal of Psychology                                                                  | nan                                                  | https://pubmed.ncbi.nlm.nih.gov/34967004/                            | | The effects of self-generated and other-generated eWOM in inoculating against misinformation                                                                | Evaluator  |   2022 | Telematics and Informatics                                                                     | Elsevier Ltd                                         | https://www.doi.org/10.1016/j.tele.2022.101835                       | | Does Wording Matter?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mExamining the Effect of Phrasing on Memory for Negated Political Fact Checks                                                           | Evaluator  |   2022 | Journal of Applied Research in Memory and Cognition                                            | Society for Applied Research in Memory and Cognition | https://www.doi.org/10.1037/mac0000022                               | | Effects of corrections on COVID-19-related misinformation: cross-media empirical analyses in Japan                                                          | Evaluator  |   2022 | International Journal of Web Based Communities                                                 | Inderscience Publishers                              | https://www.doi.org/10.1504/IJWBC.2022.122389                        | | Factors influencing fake news rebuttal acceptance during the COVID-19 pandemic and the moderating effect of cognitive ability                               | Evaluator  |   2022 | Computers in Human Behavior                                                                    | Elsevier Ltd                                         | https://www.doi.org/10.1016/j.chb.2021.107174                        | | The Influence of Media Trust and Normative Role Expectations on the Credibility of Fact Checkers                                                            | Evaluator  |   2022 | Journalism Practice                                                                            | Routledge                                            | https://www.doi.org/10.1080/17512786.2022.2080102                    | | What If Unmotivated Is More Dangerous?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe Motivation-Contingent Effectiveness of Misinformation Correction on Social Media                                 | Evaluator  |   2022 | International Journal of Communication                                                         | University of Southern California                    | https://ijoc.org/index.php/ijoc/article/view/17510                   | | Correcting science misinformation in an authoritarian country: An experiment from China                                                                     | Evaluator  |   2022 | Telematics and Informatics                                                                     | Elsevier Ltd                                         | https://www.doi.org/10.1016/j.tele.2021.101749                       | | Science Factionalism: How Group Identity Language Affects Public Engagement With Misinformation and Debunking Narratives on a Popular Q&A Platform in China | Creator    |   2022 | Social Media and Society                                                                       | SAGE Publications Ltd                                | https://www.doi.org/10.1177/20563051221077019                        | | Exploring the Effect of Spreading Fake News Debunking Based on Social Relationship Networks                                                                 | Creator    |   2022 | Frontiers in Physics                                                                           | Frontiers Media S.A\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.3389/fphy.2022.833385                         | | The impact of psycholinguistic patterns in discriminating between fake news spreaders and fact checkers                                                     | Creator    |   2022 | Data and Knowledge Engineering                                                                 | Elsevier B.V\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1016/j.datak.2021.101960                      | | Reading between the lies: A classification scheme of types of reply to misinformation in public discussion threads                                          | Creator    |   2022 | CHIIR 2022 - Proceedings of the 2022 Conference on Human Information Interaction and Retrieval | Association for Computing Machinery, Inc             | https://www.doi.org/10.1145/3498366.3505823                          | | Investigation of the determinants for misinformation correction effectiveness on social media during COVID-19 pandemic                                      | Creator    |   2022 | Information Processing and Management                                                          | Elsevier Ltd                                         | https://www.doi.org/10.1016/j.ipm.2022.102935                        | | The use of emotions in conspiracy and debunking videos to engage publics on YouTube                                                                         | Creator    |   2022 | New Media and Society                                                                          | SAGE Publications Ltd                                | https://www.doi.org/10.1177/14614448221105877                        | | Let's fight the infodemic: the third-person effect process of misinformation during public health emergencies                                               | Creator    |   2022 | Internet Research                                                                              | Emerald Group Holdings Ltd\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1108/INTR-03-2021-0194                        | | Taking corrective action when exposed to fake news: The role of fake news literacy                                                                          | Creator    |   2022 | Journal of Media Literacy Education                                                            | National Association for Media Literacy Education    | https://www.doi.org/10.23860/JMLE-2022-14-2-1                        | | COVID-19 Vaccine Fact-Checking Posts on Facebook: Observational Study                                                                                       | Creator    |   2022 | Journal of Medical Internet Research                                                           | JMIR Publications Inc\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.2196/38423                                    | | The Disinformation Warfare: How users use every means possible in the political battlefield on social media                                                 | Creator    |   2022 | Online Information Review                                                                      | nan                                                  | https://www.emerald.com/insight/content/doi/10.1108/OIR-05-2020-0197 | | Collective social correction: addressing misinformation through group practices of information verification on WhatsApp                                     | Creator    |   2022 | Digital Journalism                                                                             | Taylor \\& Francis                                    | https://www.tandfonline.com/doi/full/10.1080/21670811.2021.1972020   |   ## 2021 | Title                                                                                                                                                | Category   |   Year | Venue                                                                                                                     | Publisher                                            | URL                                               | |:-----------------------------------------------------------------------------------------------------------------------------------------------------|:-----------|-------:|:--------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------|:--------------------------------------------------| | HawkEye: A robust reputation system for community-based counter-misinformation                                                                       | Annotator  |   2021 | Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2021 | Association for Computing Machinery, Inc             | https://www.doi.org/10.1145/3487351.3488343       | | Can the crowd judge truthfulness?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mA longitudinal study on recent misinformation about COVID-19                                                       | Annotator  |   2021 | Personal and Ubiquitous Computing                                                                                         | Springer Science and Business Media Deutschland GmbH | https://www.doi.org/10.1007/s00779-021-01604-6    | | The many dimensions of truthfulness: Crowdsourcing misinformation assessments on a multidimensional scale                                            | Annotator  |   2021 | Information Processing and Management                                                                                     | Elsevier Ltd                                         | https://www.doi.org/10.1016/j.ipm.2021.102710     | | ‘It infuriates me': examining young adults’ reactions to and recommendations to fight misinformation about COVID-19                                  | Evaluator  |   2021 | Journal of Youth Studies                                                                                                  | Routledge                                            | https://www.doi.org/10.1080/13676261.2021.1965108 | | The Effects of a News Literacy Video and Real-Time Corrections to Video Misinformation Related to Sunscreen and Skin Cancer                          | Evaluator  |   2021 | Health Communication                                                                                                      | Routledge                                            | https://www.doi.org/10.1080/10410236.2021.1910165 | | Scaling up fact-checking using the wisdom of crowds                                                                                                  | Evaluator  |   2021 | Science Advances                                                                                                          | American Association for the Advancement of Science  | https://www.doi.org/10.1126/sciadv.abf4393        | | You’re definitely wrong, maybe: Correction style has minimal effect on corrections of misinformation online                                          | Evaluator  |   2021 | Media and Communication                                                                                                   | Cogitatio Press                                      | https://www.doi.org/10.17645/mac.v9i1.3519        | | The Role of Influence of Presumed Influence and Anticipated Guilt in Evoking Social Correction of COVID-19 Misinformation                            | Evaluator  |   2021 | Health Communication                                                                                                      | Routledge                                            | https://www.doi.org/10.1080/10410236.2021.1888452 | | Evaluating Rumor Debunking Effectiveness During the COVID-19 Pandemic Crisis: Utilizing User Stance in Comments on Sina Weibo                        | Evaluator  |   2021 | Frontiers in Public Health                                                                                                | Frontiers Media S.A\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.3389/fpubh.2021.770111     | | Why do citizens share covid‐19 fact‐checks posted by chinese government social media accounts?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe elaboration likelihood model                      | Evaluator  |   2021 | International Journal of Environmental Research and Public Health                                                         | MDPI                                                 | https://www.doi.org/10.3390/ijerph181910058       | | Prevalence of anger, engaged in sadness: engagement in misinformation, correction, and emotional tweets during mass shootings                        | Creator    |   2021 | Online Information Review                                                                                                 | Emerald Group Holdings Ltd\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1108/OIR-03-2021-0121      | | Young Adults' Ability To Detect Fake News And Their New Media Literacy Level In The Wake Of The Covid-19 Pandemic                                    | Creator    |   2021 | Journal of Content, Community and Communication                                                                           | nan                                                  | https://www.amity.edu/gwalior/jccc/pdf/jun_31.pdf | | Arresting fake news sharing on social media: a theory of planned behavior approach                                                                   | Creator    |   2021 | Management Research Review                                                                                                | Emerald Group Holdings Ltd\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1108/MRR-05-2020-0286      | | \"There is No Corona; It’s a Conspiracy\": Addressing the Perceptions of People about COVID-19 through the Narrative of Their Comments on Social Media | Creator    |   2021 | Journal of Consumer Health on the Internet                                                                                | Routledge                                            | https://www.doi.org/10.1080/15398285.2020.1867412 | | Raising the flag: Monitoring user perceived disinformation on reddit                                                                                 | Creator    |   2021 | Information (Switzerland)                                                                                                 | MDPI AG                                              | https://www.doi.org/10.3390/info12010004          | | SAMS: Human-in-the-loop approach to combat the sharing of digital misinformation                                                                     | Creator    |   2021 | CEUR Workshop Proceedings                                                                                                 | CEUR-WS                                              | https://arodes.hes-so.ch/record/8922              | | Fighting disaster misinformation in Latin America: the #19S Mexican earthquake case study                                                            | Creator    |   2021 | Personal and Ubiquitous Computing                                                                                         | Springer Science and Business Media Deutschland GmbH | https://www.doi.org/10.1007/s00779-020-01411-5    | | Dynamics of social corrections to peers sharing COVID-19 misinformation on WhatsApp in Brazil                                                        | Creator    |   2021 | Journal of the American Medical Informatics Association : JAMIA                                                           | NLM (Medline)                                        | https://www.doi.org/10.1093/jamia/ocab219         |   ## 2020 | Title                                                                                                                                                                                    | Category   |   Year | Venue                                                                           | Publisher                                              | URL                                                            | |:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------|-------:|:--------------------------------------------------------------------------------|:-------------------------------------------------------|:---------------------------------------------------------------| | Investigating Differences in Crowdsourced News Credibility Assessment: Raters, Tasks, and Expert Criteria                                                                                | Annotator  |   2020 | Proceedings of the ACM on Human-Computer Interaction                            | Association for Computing Machinery                    | https://www.doi.org/10.1145/3415164                            | | WhistleBlower: Towards A Decentralized and Open Platform for Spotting Fake News                                                                                                          | Annotator  |   2020 | Proceedings - 2020 IEEE International Conference on Blockchain, Blockchain 2020 | Institute of Electrical and Electronics Engineers Inc. | https://www.doi.org/10.1109/Blockchain50366.2020.00026         | | A reliable weighting scheme for the aggregation of crowd intelligence to detect fake news                                                                                                | Annotator  |   2020 | Information (Switzerland)                                                       | MDPI AG                                                | https://www.doi.org/10.3390/INFO11060319                       | | “Is It the Message or the Messenger?”\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m: Conspiracy Endorsement and Media Sources                                                                                                          | Evaluator  |   2020 | Social Science Computer Review                                                  | SAGE Publications Inc\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1177/0894439320965107                   | | Seeking Formula for Misinformation Treatment in Public Health Crises: The Effects of Corrective Information Type and Source                                                              | Evaluator  |   2020 | Health Communication                                                            | Routledge                                              | https://www.doi.org/10.1080/10410236.2019.1573295              | | Exploring “Angry” and “Like” Reactions on Uncivil Facebook Comments That Correct Misinformation in the News                                                                              | Evaluator  |   2020 | Digital Journalism                                                              | Routledge                                              | https://www.doi.org/10.1080/21670811.2020.1835512              | | An Eye Tracking Approach to Understanding Misinformation and Correction Strategies on Social Media: The Mediating Role of Attention and Credibility to Reduce HPV Vaccine Misperceptions | Evaluator  |   2020 | Health Communication                                                            | Routledge                                              | https://www.doi.org/10.1080/10410236.2020.1787933              | | Feeling angry: the effects of vaccine misinformation and refutational messages on negative emotions and vaccination attitude                                                             | Evaluator  |   2020 | Journal of Health Communication                                                 | Bellwether Publishing, Ltd\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1080/10810730.2020.1838671              | | Countering Fake News: A Comparison of Possible Solutions Regarding User Acceptance and Effectiveness                                                                                     | Evaluator  |   2020 | Proceedings of the ACM on Human-Computer Interaction                            | Association for Computing Machinery                    | https://www.doi.org/10.1145/3415211                            | | Caution: Rumors ahead—A case study on the debunking of false information on Twitter                                                                                                      | Evaluator  |   2020 | Big Data and Society                                                            | SAGE Publications Ltd                                  | https://www.doi.org/10.1177/2053951720980127                   | | The Battle is On: Factors that Motivate People to Combat Anti-Vaccine Misinformation                                                                                                     | Creator    |   2020 | Health Communication                                                            | Routledge                                              | https://www.doi.org/10.1080/10410236.2020.1838108              | | Information literacy competence in curtailing fake news about the COVID-19 pandemic among undergraduates in Nigeria                                                                      | Creator    |   2020 | Reference Services Review                                                       | Emerald Group Holdings Ltd\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1108/RSR-06-2020-0037                   | | To correct or not to correct?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSocial identity threats increase willingness to denounce fake news through presumed media influence and hostile media perceptions                          | Creator    |   2020 | Communication Research Reports                                                  | Routledge                                              | https://www.doi.org/10.1080/08824096.2020.1841622              | | The Role of the Crowd in Countering Misinformation: A Case Study of the COVID-19 Infodemic                                                                                               | Creator    |   2020 | Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020     | Institute of Electrical and Electronics Engineers Inc. | https://www.doi.org/10.1109/BigData50022.2020.9377956          | | COVID-19 and the 5G conspiracy theory: Social network analysis of twitter data                                                                                                           | Creator    |   2020 | Journal of Medical Internet Research                                            | JMIR Publications Inc\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.2196/19458                              | | Misinformation debunking and cross-platform information sharing through Twitter during Hurricanes Harvey and Irma: a case study on shelters and ID checks                                | Creator    |   2020 | Natural Hazards                                                                 | Springer                                               | https://www.doi.org/10.1007/s11069-020-04016-6                 | | Characterizing COVID-19 misinformation communities using a novel twitter dataset                                                                                                         | Creator    |   2020 | CEUR Workshop Proceedings                                                       | CEUR-WS                                                | https://arxiv.org/abs/2008.00791                               | | Refuting fake news on social media: nonprofits, crisis response strategies and issue involvement                                                                                         | Creator    |   2020 | Journal of Product and Brand Management                                         | Emerald Group Holdings Ltd\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1108/JPBM-12-2018-2146                  | | Social media rumor refuter feature analysis and crowd identification based on XG Boost and NLP                                                                                           | Creator    |   2020 | Applied Sciences (Switzerland)                                                  | MDPI AG                                                | https://www.doi.org/10.3390/app10144711                        | | #Arson Emergency and Australia’s “Black Summer”: Polarisation and Misinformation on Social Media                                                                                         | Creator    |   2020 | Disinformation in Open Online Media                                             | Springer                                               | https://link.springer.com/chapter/10.1007/978-3-030-61841-4_11 |   ## 2019 | Title                                                                                                                                                                | Category   |   Year | Venue                                                                                                                        | Publisher                                | URL                                               | |:---------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------|-------:|:-----------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------|:--------------------------------------------------| | Exposure to Countering Messages Online: Alleviating or Strengthening False Belief?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| Evaluator  |   2019 | Cyberpsychology, Behavior, and Social Networking                                                                             | Mary Ann Liebert Inc\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1089/cyber.2019.0227       | | Fighting misinformation on social media using crowdsourced judgments of news source quality                                                                          | Evaluator  |   2019 | Proceedings of the National Academy of Sciences of the United States of America                                              | National Academy of Sciences             | https://www.doi.org/10.1073/pnas.1806781116       | | Temporal trends in anti-vaccine discourse on Twitter                                                                                                                 | Creator    |   2019 | Vaccine                                                                                                                      | Elsevier Ltd                             | https://www.doi.org/10.1016/j.vaccine.2019.06.086 | | Learning from fact-checkers: Analysis and generation of fact-checking language                                                                                       | Creator    |   2019 | SIGIR 2019 - Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval | Association for Computing Machinery, Inc | https://www.doi.org/10.1145/3331184.3331248       | | Is citizen journalism better than professional journalism for fact-checking rumours in China?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHow Weibo users verified information following the 2015 Tianjin blasts | Creator    |   2019 | Global Media and China                                                                                                       | SAGE Publications Ltd                    | https://www.doi.org/10.1177/2059436419834124      | | Diffusion of pro- and anti-false information tweets: the Black Panther movie case                                                                                    | Creator    |   2019 | Computational and Mathematical Organization Theory                                                                           | Springer New York LLC                    | https://www.doi.org/10.1007/s10588-018-09286-x    | | Different faces of false: The spread and curtailment of false information in the black Panther Twitter discussion                                                    | Creator    |   2019 | Journal of Data and Information Quality                                                                                      | Association for Computing Machinery      | https://www.doi.org/10.1145/3339468               | | Debunking rumors on social media: The use of denials                                                                                                                 | Creator    |   2019 | Computers in Human Behavior                                                                                                  | Elsevier Ltd                             | https://www.doi.org/10.1016/j.chb.2019.02.022     |   ## 2018 | Title                                                                                                  | Category   |   Year | Venue                                                                                             | Publisher                                              | URL                                               | |:-------------------------------------------------------------------------------------------------------|:-----------|-------:|:--------------------------------------------------------------------------------------------------|:-------------------------------------------------------|:--------------------------------------------------| | Hybrid machine-crowd approach for fake news detection                                                  | Annotator  |   2018 | Proceedings - 4th IEEE International Conference on Collaboration and Internet Computing, CIC 2018 | Institute of Electrical and Electronics Engineers Inc. | https://www.doi.org/10.1109/CIC.2018.00048        | | See Something, Say Something: Correction of Global Health Misinformation on Social Media               | Evaluator  |   2018 | Health Communication                                                                              | Routledge                                              | https://www.doi.org/10.1080/10410236.2017.1331312 | | Salient beliefs about sharing rumor denials on the Internet                                            | Creator    |   2018 | ACM International Conference Proceeding Series                                                    | Association for Computing Machinery                    | https://www.doi.org/10.1145/3164541.3164578       | | Rumor response, debunking response, and decision makings of misinformed Twitter users during disasters | Creator    |   2018 | Natural Hazards                                                                                   | Springer Netherlands                                   | https://www.doi.org/10.1007/s11069-018-3344-6     |   ## 2017 | Title                                                                              | Category   |   Year | Venue                                                                                                                                | Publisher                           | URL                                              | |:-----------------------------------------------------------------------------------|:-----------|-------:|:-------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------|:-------------------------------------------------| | An analysis of rumor and counter-rumor messages in social media                    | Annotator  |   2017 | Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) | Springer Verlag                     | https://www.doi.org/10.1007/978-3-319-70232-2_22 | | The Retransmission of Rumor and Rumor Correction Messages on Twitter               | Creator    |   2017 | American Behavioral Scientist                                                                                                        | SAGE Publications Inc\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1177/0002764217717561     | | A study of tweet veracity to separate rumours from counter-rumours                 | Creator    |   2017 | ACM International Conference Proceeding Series                                                                                       | Association for Computing Machinery | https://www.doi.org/10.1145/3097286.3097290      | | A closer look at the self-correcting crowd: Examining corrections in online rumors | Creator    |   2017 | Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW                                                       | Association for Computing Machinery | https://www.doi.org/10.1145/2998181.2998294      | | Does KFC sell rat?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAnalysis of tweets in the wake of a rumor outbreak              | Creator    |   2017 | Aslib Journal of Information Management                                                                                              | Emerald Group Publishing Ltd\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| https://www.doi.org/10.1108/AJIM-01-2017-0026    |   ## 2016 | Title                                                                                                  | Category   |   Year | Venue                                                                                                                     | Publisher                                              | URL                                              | |:-------------------------------------------------------------------------------------------------------|:-----------|-------:|:--------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------|:-------------------------------------------------| | Changing conspiracy beliefs through rationality and ridiculing                                         | Evaluator  |   2016 | Frontiers in Psychology                                                                                                   | Frontiers Research Foundation                          | https://www.doi.org/10.3389/fpsyg.2016.01525     | | Analysing how people orient to and spread rumours in social media by looking at conversational threads | Creator    |   2016 | PLoS ONE                                                                                                                  | Public Library of Science                              | https://www.doi.org/10.1371/journal.pone.0150989 | | An exploration of rumor combating behavior on social media in the context of social crises             | Creator    |   2016 | Computers in Human Behavior                                                                                               | Elsevier Ltd                                           | https://www.doi.org/10.1016/j.chb.2015.11.054    | | Toward understanding how users respond to rumours in social media                                      | Creator    |   2016 | Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016 | Institute of Electrical and Electronics Engineers Inc. | https://www.doi.org/10.1109/ASONAM.2016.7752326  |  ## 2015 | Title                                                                               | Category   |   Year | Venue                                                                                   | Publisher                                | URL                                                     | |:------------------------------------------------------------------------------------|:-----------|-------:|:----------------------------------------------------------------------------------------|:-----------------------------------------|:--------------------------------------------------------| | CREDBANK: A large-scale social media corpus with associated credibility annotations | Annotator  |   2015 | Proceedings of the 9th International Conference on Web and Social Media, ICWSM 2015     | AAAI Press                               | https://ojs.aaai.org/index.php/ICWSM/article/view/14625 | | Reducing conspiracy theory beliefs                                                  | Evaluator  |   2015 | Psihologija                                                                             | Serbian Psychological Society            | https://www.doi.org/10.2298/PSI1503251S                 | | Crowdsourced rumour identification during emergencies                               | Evaluator  |   2015 | WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web | Association for Computing Machinery, Inc | https://www.doi.org/10.1145/2740908.2742573             |   ## 2013 | Title                                                                     | Category   |   Year | Venue                                          |   Publisher | URL                                         | |:--------------------------------------------------------------------------|:-----------|-------:|:-----------------------------------------------|------------:|:--------------------------------------------| | Analysis of microblog rumors and correction texts for disaster situations | Creator    |   2013 | ACM International Conference Proceeding Series |         nan | https://www.doi.org/10.1145/2539150.2539184 |\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Improving Deep Metric Learning by Divide and Conquer ## About  PyTorch implementation for the paper _Improving Deep Metric  Learning by Divide and Conquer_ accepted to **TPAMI** (Sep. 2021), which is our follow-up paper of [_Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019)_](https://github.com/CompVis/metric-learning-divide-and-conquer)  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * TPAMI early access: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * PyTorch 1.1.0 * Faiss-GPU >= 1.5.0, [Link](https://github.com/facebookresearch/faiss) * albumentations >= 0.4.5, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling `python train.py` and setting the respective params, all of which are listed and explained  in `/experiment/margin_loss_resnet50.py` (the default setup for all our experiments).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe params provided in command line will override those default ones.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \\                 --dataset=sop -i=$NAME -seed=4 \\                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \\                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \\                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \\                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \\                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \\                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor larger number of clusters, you may need to change the default  limit of opened files allowed for each process.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor example, if you are a ubuntu user, `ulimit -n <twice the number of the default>` usually will do the trick.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBesides, please take the total number of different classes in the dataset and the sampling strategy used into account when you setting this value. - **--dataset-dir**: the path to the datasets, check the *Datasets* section below. - **--sampler**: batchminer used to sample pairs or triplets (in embedding space) to create learning signal, check `/metriclearning/sampler` for details. - **--batch-sampler**: data sampler used to generate training batches, check `/dataset/sampler.py` for details. - **--nb-epochs**: the maximum training epochs. - **--mod-epoch**: division frequency in the paper - the number of training epochs between consecutive divisions. - **--wandb-enabled**: by setting this flag, you will enable the [Weights&Biases](https://wandb.ai/site) logging.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease change the w&b initial setting at the last part of `/experiment/margin_loss_resnet50.py` accordingly.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**_Note:_** For exact settings for different datasets, please check the original paper.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor arguments not mentioned above,  please check `/experiment/margin_loss_resnet50.py` for explanation.   ### Evaluate or check results during:  * **evaluate a trained model**: `eval_model.py <log and model checkpoint path>`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is suggested to put only those models (checkpoints and logs) you want to eval into one folder otherwise it will evaluate all the models ind the folder  * **check intermediate results**: the model checkpoints and log files are saved in the selected log-directory (by default: `/log`).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`<$datadir/cars196>`, pass `$datadir` as input to `--dataset-dir`, by default.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSome deviations in results based on different PyTorch/Cuda/Faiss versions and hardware (e.g. between P100 and RTX GPUs) are to be expected._   ## Related Repos  * Divide and Conquer the Embedding Space for Metric Learning (our previous paper on CVPR 2019):  https://github.com/CompVis/metric-learning-divide-and-conquer * An easy-to-use repo to start your DML research, containing collections of models, losses, and samplers implemented in PyTorch: https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch  ## BibTex If you use this code in your research, please cite the following papers:  ``` @InProceedings{dcesml,   title={Divide and Conquer the Embedding Space for Metric Learning},   author={Sanakoyeu, Artsiom and Tschernezki, Vadim and B\\\"uchler, Uta and Ommer, Bj\\\"orn},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   year={2019} }  @article{sanakoyeu2021improving,   title={Improving Deep Metric Learning by Divide and Conquer},    author={Artsiom Sanakoyeu and Pingchuan Ma and Vadim Tschernezki and Björn Ommer},   journal={IEEE Transactions on pattern analysis and machine intelligence},   year={2021},   publisher={IEEE} } ```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# TAG-CF: Test-time Aggregation for CF  Source code for the paper **[How Does Message Passing Improve Collaborative Filtering?]\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m(https://arxiv.org/abs/2404.08660)** accepted at NeurIPS 2024.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m>by [Mingxuan Ju](https://scholar.google.com/citations?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31muser=qNoO67AAAAAJ&hl=en&oi=ao), [William Shiao](https://scholar.google.com/citations?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31muser=TIq-P5AAAAAJ&hl=en&oi=ao), [Zhichun Guo](https://scholar.google.com/citations?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31muser=BOFfWR0AAAAJ&hl=en&oi=ao), [Fanny Ye](https://scholar.google.com/citations?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31muser=egjr888AAAAJ&hl=en&oi=ao), [Yozen Liu](https://scholar.google.com/citations?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31muser=i3U2JjEAAAAJ&hl=en&oi=ao), [Neil Shah](https://scholar.google.com/citations?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31muser=Qut69OgAAAAJ&hl=en&oi=ao) and [Tong Zhao](https://scholar.google.com/citations?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31muser=05cRc-MAAAAJ&hl=en&oi=ao).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe paper proposes TAG-CF which is a test-time aggregation framework that can be utilized as a plug-and-play module to enhance the performance of matrix factorization models.  ## 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInstallation  Please install all dependencies using the command: ``` conda create --name <env> --file requirements.txt ```  ## 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload data  Please download the data following the [instruction](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/dataset/README.md).  ## 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRun experiments This is one example of reproducing results for Movielens-1M with BPR.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease use other provided config files to reproduce results for other datasets. ``` # Train a basic matrix factorization model make run_pipeline MODEL_YAML_PATH=config/MF.yaml DATA_YAML_PATH=config/bpr_ml.yaml  # Train a LightGCN model. make run_pipeline MODEL_YAML_PATH=config/LGCN.yaml DATA_YAML_PATH=config/bpr_ml.yaml  # Conduct test-time aggregation on matrix factorization model. make run_pipeline MODEL_YAML_PATH=config/TAGCF_bpr_ml.yaml  DATA_YAML_PATH=config/bpr_ml.yaml ```  ## 4.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mResults This is a list containing performance of MF trained by BPR and DirectAU, along with the relative performance improvement brought by TAG-CF.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe provide hyper-parameter configs in [the config folder](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/config).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo play around with ```m``` and ```n``` in TAG-CF, feel free to accordingly change individual config files.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo conduct grid-search over them, simply remove them from the config file.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mReference If you find this repo and our work useful to you, please kindly cite us using:  ``` @article{ju2024does,   title={How Does Message Passing Improve Collaborative Filtering?}\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m,   author={Ju, Mingxuan and Shiao, William and Guo, Zhichun and Ye, Yanfang and Liu, Yozen and Shah, Neil and Zhao, Tong},   journal={Advances in neural information processing systems},   year={2024} } ```  ## 6.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mContact Please contact mju@snap.com for any questions.\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# [AAAI 2024] NuScenes-QA  Official repository for the AAAI 2024 paper **[NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://arxiv.org/pdf/2305.14836.pdf)**.  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[DataConstruction](docs/data_construction.png)  ## :fire: News  - `2024.11.01`  CenterPoint feature released\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `2024.10.11`  Training and Testing code released. - `2023.12.09`  Our paper is accepted by AAAI 2024!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `2023.09.04`  Our NuScenes-QA dataset v1.0 released.  ## :hourglass_flowing_sand: To Do  - [x] Release question & anwswer data - [x] Release visual feature - [x] Release training and testing code  ## :running: Getting Started  ### Data Preparation  We have released our question-answer annotations, please download it from [HERE](https://drive.google.com/drive/folders/1jIkICT23wZWZYPrWCa0x-ubjpClSzOuU?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor the visual data, you can download **CenterPoint** feature that we have extracted from [HERE](https://drive.google.com/file/d/1TNsK6cpQ4pd9fH1s7WFxTSXilrT2uNYb/view?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAs an alternative, you can also download the origin nuScenes dataset from [HERE](https://www.nuscenes.org/download), and extract the object-level features refer to this [LINK](https://mmdetection3d.readthedocs.io/en/v0.16.0/datasets/nuscenes_det.html) with different backbones.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor specific details on feature extraction, you can refer to the **Visual Feature Extraction** and **Object Embedding** sections of our paper.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 3 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# ModelSketchBook — Getting Started  [Paper](https://hci.stanford.edu/publications/2023/Lam_ModelSketching_CHI23.pdf) | [DOI](https://doi.org/10.1145/3544548.3581290) |  [Video](https://youtu.be/-zaeXENVTfk) | [Sample NB](https://github.com/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) |  <a target=\"_blank\" href=\"https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb\">   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a>  <p align=\"center\"> <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/ModelSketchBook.png\" alt=\"ModelSketchBook logo\" width=\"75%\"> </p>  **ModelSketchBook** is a Python package introduced as part of an ACM CHI 2023 paper:  **Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design**.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m*Michelle S.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLam, Zixian Ma, Anne Li, Izequiel Freitas, Dakuo Wang, James A.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLanday, Michael S.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBernstein*.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mProceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[PDF](https://hci.stanford.edu/publications/2023/Lam_ModelSketching_CHI23.pdf) | [Arxiv](https://arxiv.org/abs/2303.02884)  ## tl;dr Machine learning practitioners often end up tunneling on **low-level technical details** like model architectures and performance metrics.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCould early model development instead focus on **high-level questions of which factors a model ought to pay attention to**?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mModelSketchBook instantiates a vision of **model sketching**, a technical framework for rapidly iterating over a machine learning model's decision-making logic.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mModel sketching refocuses practitioner attention on **composing high-level, human-understandable concepts** that the model is expected to reason over (e.g., profanity, racism, or sarcasm in a content moderation task) using zero-shot concept instantiation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/model_sketching_header.png\" alt=\"Summary of the model sketching process from concepts to sketch models to evaluating and iterating\" width=\"100%\">  With **ModelSketchBook**, you can **create concepts** using zero-shot methods,  <p align=\"center\"> <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/create_concept_model.gif\" alt=\"Demo of the create_concept_model function\" width=\"75%\"> </p>  **create sketches** that combine these concept building blocks, <p align=\"center\"> <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/create_sketch_model.gif\" alt=\"Demo of the create_sketch_model function\" width=\"75%\"> </p>  and continue to iterate on concepts and sketches to explore the ML model design space before diving into technical implementation details\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m> 🚧  This repo is a work-in-progress research prototype and will continue to be updated!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mModelSketchBook is designed to work with datasets in Pandas DataFrame format.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can load CSVs to dataframes using the Pandas [`read_csv` function](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Schema**.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo set up your sketchbook, you need to specify the schema of your dataframe.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn the schema, the **keys** are the string names of the columns in your dataframe that you'd like to use, and the **values** indicate the type of data contained in that column.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe currently-supported input types are:  - `InputType.Text`: (string) Raw text fields of arbitrary length.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mStrings may be truncated to a max token length for prompts to GPT. - `InputType.Image`: (string) URLs to remotely-hosted images\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `InputType.ImageLocal`: (string) Filepaths to locally-stored images. - `InputType.GroundTruth`: (float or bool) Ground truth labels.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCurrently expected to be in the domain [0, 1].\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Ground truth**.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe current API version requires exactly one ground truth column in the schema, though you can later add datasets that don't contain this ground truth column.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Credentials**.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease supply your own OpenAI organization and API key if you would like to use GPT-based concepts.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOtherwise, you can provide empty strings.  ``` sb = msb.create_model_sketchbook(     goal='Example sketchbook goal here',     schema={         # Specify your data schema here         \"picture_url\": msb.InputType.Image,         \"neighborhood_overview\": msb.InputType.Text,         \"description\": msb.InputType.Text,         \"overall_rating\": msb.InputType.GroundTruth,  # Required     },     credentials={         \"organization\": \"org-INSERT\",         \"api_key\": \"sk-INSERT\"     } ) ```  ### Add your dataset(s) Then, add your dataset from a Pandas dataframe (recommended: 40-50 rows).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf the dataframe contains images, this step will involve caching the images to speed up processing later.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou should also specify exactly one dataset to be the **default dataset** with the `default` argument.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt's recommended that your **training set** be the default dataset.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWith the notebook-widget functions documented here, the default dataset is used to set up concepts and sketches, and other datasets can only be used to evaluate trained sketches. ``` your_df = pd.read_csv(\"path_to_your_data/data.csv\") # Insert logic to load your dataframe  sb.add_dataset(     df=your_df,     default=True,  # If dataset should be used by default, otherwise omit argument ) ```  ### Create concepts You can then go ahead and create image and text concepts with the following function.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis function will display widgets to specify your concept term, input field, and output type. ``` msb.create_concept_model(sb) ```  <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/create_concept_model.png\" alt=\"create_concept_model widgets\" width=\"35%\">  **Models**.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCurrently, text concepts can be served by GPT-3 (using the `text-davinci-002` model) or OpenCLIP (using the `ViT-B-32-quickgelu` model), and image concepts can only be served by OpenCLIP.  ### Create sketches Then, you can combine concepts together into sketches with the following function.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis function will display widgets to select concepts and an aggregator. ``` msb.create_sketch_model(sb) ```  <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/create_sketch_model.png\" alt=\"create_sketch_model widgets\" width=\"50%\">  ## 2: Other functions ### Tune concepts (optional) You may optionally tune your existing concepts by binarizing them at a threshold, normalizing the values, or calibrating them between specified values.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis function will display widgets to select an existing concept, a tuning method, and tuning-related parameters. ``` msb.tune_concept(sb) ```  <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/tune_concept.png\" alt=\"tune_concept widgets\" width=\"50%\">  ### Logical concepts (AND, OR) Logical concepts can be applied to any number of existing binary concepts.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis function will display widgets to select the concepts and the logical operator (AND or OR) to apply to those concept scores. ``` msb.create_logical_concept_model(sb) ```  <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/create_logical_concept_model.png\" alt=\"create_logical_concept_model widgets\" width=\"50%\">  ### Keyword concepts Keyword concepts can be applied to any text-based input fields.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe text of each example will be compared to the specified list of keywords, and if any of the keywords appear in the example, the example will be given a positive (True) label.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis function will display widgets to select the input field and the comma-separated list of keywords, which can be treated in a case-sensitive or case-insensitive manner. ``` msb.create_keyword_concept_model(sb) ```  <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/create_keyword_concept_model.png\" alt=\"create_keyword_concept_model widgets\" width=\"35%\">  ### Take notes You can select from among your created concepts and sketches and enter free-text notes into the text field.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese notes will be displayed with your concept or sketch in calls to `show_concepts()` or `show_sketches()`. ``` msb.take_note(sb) ```  <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/take_note.png\" alt=\"take_note widgets\" width=\"50%\">  ## 3: Concept helper functions ### View existing concepts Prints out a summary of all existing concepts with concept ID, concept term, input field, output type, and associated notes (if provided). ``` msb.show_concepts(sb) ```  ### Get concept term suggestions Prints a set of synonyms for the provided concept term to assist in term ideation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCurrently surfaced using [WordNet](https://wordnet.princeton.edu/) synsets. ``` msb.get_similar_concepts(\"your concept term\") ```  ### Compare concepts to ground truth labels This function displays the Pearson correlation coefficients between each of the selected concepts and the ground truth label column.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt also displays a table view that shows the difference between the concept score and ground truth label for each example in the default dataset. ``` msb.compare_concepts_to_gt(sb) ```  <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/compare_concepts_to_gt.png\" alt=\"compare_concepts_to_gt widgets\" width=\"50%\">  ### Compare concepts to each other This function displays the Pearson correlation coefficients between the two selected concepts.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt also displays a table view that shows the difference between the concept scores for each example in the default dataset.  ``` msb.compare_two_concepts(sb) ```  <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/compare_two_concepts.png\" alt=\"compare_two_concepts widgets\" width=\"50%\">  ## 4: Sketch helper functions ### View existing sketches Prints out a summary of all existing sketches with sketch ID, model type, output type, concepts, and associated notes (if provided). ``` msb.show_sketches(sb) ```  ### Test an existing sketch on a dataset Runs a selected (trained) sketch model on the selected dataset. ``` msb.test_sketch(sb) ```  <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/test_sketch.png\" alt=\"test_sketch widgets\" width=\"50%\">  ### Compare sketch performance Plots performance metrics (Mean Absolute Error, F1 Score, Classification accuracy, precision, and recall) for all of the selected sketches on the specified dataset. ``` msb.compare_sketches(sb) ```  <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/docs/media/compare_sketches.png\" alt=\"compare_sketches widgets\" width=\"50%\">   ## Contributing  Interested in contributing?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCheck out the contributing guidelines.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease note that this project is released with a Code of Conduct.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBy contributing to this project, you agree to abide by its terms.  ## License  `model_sketch_book` was created by Michelle Lam.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is licensed under the terms of the MIT license.  ## Credits  `model_sketch_book` was created with [`cookiecutter`](https://cookiecutter.readthedocs.io/en/latest/) and the `py-pkgs-cookiecutter` [template](https://github.com/py-pkgs/py-pkgs-cookiecutter).\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Keyword Transformer: A Self-Attention Model for Keyword Spotting  <img src=\"kwt.png\" alt=\"drawing\" width=\"200\"/>  This is the official repository for the paper [Keyword Transformer: A Self-Attention Model for Keyword Spotting](https://arxiv.org/abs/2104.00769), presented at Interspeech 2021.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mConsider citing our paper if you find this work useful.  ``` @inproceedings{berg21_interspeech,   author={Axel Berg and Mark O’Connor and Miguel Tairum Cruz},   title={{Keyword Transformer: A Self-Attention Model for Keyword Spotting}},   year=2021,   booktitle={Proc.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInterspeech 2021},   pages={4249--4253},   doi={10.21437/Interspeech.2021-1286} } ```  ## Setup  ### Download Google Speech Commands  There are two versions of the dataset, V1 and V2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo download and extract dataset V2, run:  ```shell wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz mkdir data2 mv .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/speech_commands_v0.02.tar.gz .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/data2 cd .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/data2 tar -xf .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/speech_commands_v0.01.tar.gz .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/data1 cd .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/data1 tar -xf .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/venv3 source .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/venv3/bin/activate ```  To install dependencies, run  ```shell pip install -r requirements.txt ```  Tested using Tensorflow 2.4.0rc1 with CUDA 11.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Note**: Installing the correct Tensorflow version is important for reproducibility!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUsing more recent versions of Tensorflow results in small accuracy differences each time the model is evaluated.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis might be due to a change in how the random seed generator is implemented, and therefore changes the sampling of the \"unknown\"  keyword class.  ## Model The Keyword-Transformer model is defined [here](kws_streaming/models/kws_transformer.py).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt takes the mel scale spectrogram as input, which has shape 98 x 40 using the default settings, corresponding to the 98 time windows with 40 frequency coefficients.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThere are three variants of the Keyword-Transformer model:  * **Time-domain attention**: each time-window is treated as a patch, self-attention is computed between time-windows * **Frequency-domain attention**: each frequency is treated as a patch self-attention is computed between frequencies * **Combination of both**: The signal is fed into both a time- and a frequency-domain transformer and the outputs are combined * **Patch-wise attention**: Similar to the vision transformer, it extracts rectangular patches from the spectrogram, so attention happens both in the time and frequency domain simultaneously.  ## Training a model from scratch To train KWT-3 from scratch on Speech Commands V2, run    ```shell sh train.sh ```  Please note that the train directory (given by the argument  `--train_dir`) cannot exist prior to start script.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe model-specific arguments for KWT are:  ```shell --num_layers 12 \\ #number of sequential transformer encoders --heads 3 \\ #number of attentions heads --d_model 192 \\ #embedding dimension --mlp_dim 768 \\ #mlp-dimension --dropout1 0. \\ #dropout in mlp/multi-head attention blocks --attention_type 'time' \\ #attention type: 'time', 'freq', 'both' or 'patch' --patch_size '1,40' \\ #spectrogram patch_size, if patch attention is used --prenorm False \\ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (Att-MH-RNN), similar to the approach in [DeIT](https://github.com/facebookresearch/deit).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo train KWT-3 with hard distillation from a pre-trained model, run  ```shell sh distill.sh ```  ## Run inference using a pre-trained model  Pre-trained weights for KWT-3, KWT-2 and KWT-1 are provided in .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/models_data_v2_12_labels.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m|Model name|embedding dim|mlp-dim|heads|depth|#params|V2-12 accuracy|pre-trained| |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:| |KWT-1|64|128|1|12|607K|97.7|[here](models_data_v2_12_labels/kwt1)| |KWT-2|128|256|2|12|2.4M|98.2|[here](models_data_v2_12_labels/kwt2)| |KWT-3|192|768|3|12|5.5M|98.7|[here](models_data_v2_12_labels/kwt3)|  To perform inference on Google Speech Commands v2 with 12 labels, run  ```shell sh eval.sh ```  ## Acknowledgements  The code heavily borrows from the [KWS streaming work](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor a more detailed description of the code structure, see the original authors' [README](kws_streaming/README.md).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe also exploit training techniques from [DeiT](https://github.com/facebookresearch/deit).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe thank the authors for sharing their code.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease consider citing them as well if you use our code.  ## License  The source files in this repository are released under the [Apache 2.0](LICENSE.txt) license.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSome source files are derived from the [KWS streaming repository](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese are also released under the Apache 2.0 license, the text of which can be seen in the LICENSE file on their repository.\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m* We then import and extend some new event types based on an [automatically-labeled event data](https://github.com/acl2017submission/event-data), from Freebase and Wikipedia, constrained to specific domains such as music, film, sports, education, etc.  ## Data Structure In \"*Few-Shot_ED.json.zip*\"，the key is \"*event type label*\", the value is the event instances.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m{event_type_label1: \\[instance1, instance2, ...\\], event_type_label2: ...}  ### example of an event instance:  (Note that the sentence should be cleaned, e.g., To expand abbreviations)  \\[\"In trucks and on foot they came to the town of Safwan\", \"came\", \\[7, 6, 12\\]\\]   * \"In trucks and on foot they came to the town of Safwan\" is an event mention * \"came\" is trigger * 7 means the distance from trigger \"came\" to the beginning of the sentence * 6 means the distance from trigger \"came\" to the ending of the sentence * 12 means the sentence length\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Code search  This project contains the code to reproduce the experiments in the paper [Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent](https://arxiv.org/abs/2008.12193).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt implements retrieval systems for annotated code snippets: pairs of a code snippet and a short natural language description.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOur pretrained models and datasets are hosted on Zenodo (https://zenodo.org/record/4001602).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe models and datasets will be downloaded automatically when calling `load_model`, `load_snippet_collection`, etc.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m(see the code examples below).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn addition, the project also implements some *code-only* retrieval models (BM25, NCS, UNIF) for snippet collections that do not come with descriptions.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe experiments in the paper are done on Python snippets, but the code preprocessing currently also supports java, javascript, and bash.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe project is developed by a research team in the [Application Platforms and Software Systems Lab](https://www.bell-labs.com/our-research/areas/applications-and-platforms/) of [Nokia Bell Labs](https://www.bell-labs.com/).   ## Installation  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInstall the codesearch library: `pip install .` 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInstall the tree-sitter parsers (for preprocessing the code snippets): e.g., `codesearch install_parsers python java` or simply `codesearch install_parsers` to install parsers for all supported languages.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBy default, parsers are installed under the `codesearch/parsers` directory this can be customized by setting the `TREE_SITTER_DIR` variable. 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInstall spacy (for preprocessing descriptions/code comments): `python -m spacy download en_core_web_md`   ## Code structure  ``` codesearch ├── codesearch          // Contains the library modules: model code, utilities to download and evaluate models, etc. ├── nbs                 // Contains examples notebooks and notebooks to reproduce the experiments ├── tests               // Contains some unit tests, mostly for verifying the code preprocessing ```  ## Models  We provide some pretrained embedding models to create a retrieval system.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe pretrained models also expose a consistent interface to embed snippets and queries:  #### Example: Query a snippet collection with a pretrained embedding model  ```python from codesearch.utils import load_model from codesearch.embedding_retrieval import EmbeddingRetrievalModel  query = \"plot a bar chart\" snippets = [{                           # a dummy snippet collection with 1 snippet     \"id\": \"1\",     \"description\": \"Hello world\",     \"code\": \"print('hello world')\",     \"language\": \"python\"     }]  embedding_model = load_model(\"use-embedder-pacs\") retrieval_model = EmbeddingRetrievalModel(embedding_model) retrieval_model.add_snippets(snippets) retrieval_model.query(query) ```  #### Example: Embed snippets or queries with a pre-trained embedding model  ```python from codesearch.utils import load_model  model_name = \"use-embedder-pacs\" queries = [\"plot a bar chart\"] snippets = [{     \"description\": \"Hello world\",     \"code\": \"print('hello world')\",     \"language\": \"python\"     }]  embedding_model = load_model(model_name) query_embs = embedding_model.embed_queries(queries) snippet_embs = embedding_model.embed_snippets(snippets) ```  ### Available models  Below you find a table with the pretrained models.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor each model, we mention based on what information it computes a snippet embedding: the description and/or the code\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = \"so-ds-feb20\" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe staqc-py-cleaned, conala-curated, and codesearchnet collections are derived from existing datasets.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor staqc-py and conala-curated we did some additional processing, for the codesearchnet collections we merely load the original dataset in a format that is consistent with our code.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you were to use any of these datasets in your research, please make sure to cite the respective works\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| name                                          | description                                                                                                                  | |-----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------| | so-ds-feb20                                   | Mined from Python Stack Overflow posts related to data science.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mStack Overflow dumps can be found here: https://archive.org/details/stackexchange, [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                             | | staqc-py-cleaned                     | Derived from the Python StaQC snippets (additional cleaning was done as decribed in the paper).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset, [LICENSE](https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset/blob/master/LICENSE.txt)                               | | conala-curated                                | Derived from the curated snippets of the CoNaLa benchmark.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee https://conala-corpus.github.io/ , [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                                                         | | codesearchnet-{language}-{train\\|valid\\|test} | The CodeSearchNet snippet collections used for training/MRR validation/MRR testing.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee https://github.com/github/CodeSearchNet.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLicenses of the individial snippets can be found in pkl files\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| | codesearchnet-{language}                      | The CodeSearchNet snippet collections used for the weights and biases benchmark.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee https://github.com/github/CodeSearchNet.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLicenses of the individial snippets can be found in pkl files.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Note**: not all of these snippets have descriptions |  ### Evaluation data Evaluation datasets link queries to relevant snippets in one of the above snippet collections.   #### Example: load an evaluation dataset ```python from codesearch.data import load_eval_dataset queries, query2ids = load_eval_dataset(\"so-ds-feb20-valid\") ```  #### Available evaluation datasets | name                           | description                                                                     | |--------------------------------|---------------------------------------------------------------------------------| | so-ds-feb20-{valid\\|test}      | Queries paired to relevant snippets in the so-ds-feb20 snippet collection\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| | staqc-py-cleaned-{valid\\|test} | Queries paired to relevant snippets in the staqc-py-cleaned snippet collection. | | conala-curated-0.5-test        | Queries paired to relevant snippets in the CoNaLa benchmark                     |   It is also possible to load a snippet collection as evaluation data.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe descriptions will be used as queries.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote that this only makes sense to evaluate code-only models (i.e., models that do not use the description field).  #### Example: load a snippet collection as evaluation data ```python queries, query2ids = load_eval_dataset(\"codesearchnet-python-valid\") ```   ### Training data  The different models we implement use different kinds of training data.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCode-only models are trained on pairs of code snippets and descriptions.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor these models, the snippet collections are used as training data (of course you should never train on a snippet collection when you intent to use that load that collection as evaluation data as well).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe USE model is fine-tuned on titles of duplicate Stack Overflow posts.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can take a look our notebooks (e.g., nbs/ncs/ncs.ipynb, nbs/tuse/tuse_tuned) to find out how the training is done/how the training data is loaded.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo download and load the title pairs from Stack Overflow duplicate posts run:  ```python from codesearch.data import load_train_dataset duplicate_records = load_train_dataset(\"so-duplicates-pacs-train\") ```  These duplicate records have been filtered to ensure that there is no overlap with the `so-ds-feb20` and `staqc-py` evaluation datasets.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo download a text file with Stack Overflow post titles tagged with Python (used for the TNBOW baseline) run:   ```python from codesearch.data import load_train_dataset filename = load_train_dataset(\"so-python-question-titles-feb20\") ```  ## Demo notebook   You can run the demo notebook `nbs/demo/demo.ipynb` to quickly try out any of the pretrained models on one of the snippet collections.  ## Benchmark on PACS  To replicate the results of our paper or evaluate your own model on the PACS benchmark, have a look at `nbs/evaluate.ipynb` and `codesearch/benchmark.ipynb`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mA custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 3 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# NetCov NetCov is an open-source tool that can be used with [Batfish](https://github.com/batfish/batfish) to analyze test coverage for network configurations.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mGiven a set of Batfish queries, it analyzes which lines of configurations has/has not been covered.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe result can be used to assess the rigorousness of the test suite and help discover the blind spots.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease refer to our [paper](https://www.usenix.org/conference/nsdi23/presentation/xu) for technical details.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNetCov is written in Python and can be used in concert with [pybatfish](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html), Batfish's Python API.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[<img src=\"screenshot_demo_video.png\"  width=\"500\">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you’d like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe'll try to add support, and it would help if you share a configuration example.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou may want to use an [anonymizer](https://github.com/intentionet/netconan) before sharing.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNetCov reports configuration coverage as the percentage of configuration lines that are covered, such as:  <img src=\"screenshot_aggregate.png\"  width=\"500\">  NetCov can also report fine-grained coverage results as colored annotations on source configurations (lines in blue are covered, lines in red are not covered):  <img src=\"screenshot_annotation.png\"  width=\"500\">  You can find an example of the coverage report [here](https://rawcdn.githack.com/UWNetworksLab/netcov/main/examples/fattree4/coverage/HTML_REPORT/index.html).     ## Installing NetCov Install NetCov using `pip`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mA virtual environment and Python version 3.7 is recommended.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```sh $ pip install netcov ```  NetCov leverages [LCOV](https://github.com/linux-test-project/lcov) to generate HTML report.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt provides a hooked version of pybatfish APIs that automatically tracks coverage during the execution of supported pybatfish questions.   ### Using NetCov for an existing pybatfish script/notebook  It takes only two simple steps to measure coverage for your existing pybatfish scripts/notebooks.  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor import, replace pybatfish client session with the one provided by NetCov: ```python #from pybatfish.client.session import Session from netcov import NetCovSession as Session ``` 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mGenerate coverage results at the end of your script.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo print coverage metrics to the console, use `bf.cov.result()`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo generate HTML report, use `bf.cov.html_report()`.  ### Use NetCov for a new pybatfish script/notebook   We provide a [demo video](https://www.youtube.com/watch?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mv=FcBD2LhxqOQ) and an [example](examples/demo.ipynb) to help you get started.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you are new to pybatfish, we recommend reading the [pybatfish doc](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html) first.    ### Advanced  Sometimes not all information retrieved by Batfish questions are meant to be tested, for example, when you retrieve all RIB entries but only assert on a subset of them.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo help NetCov model coverage more accurately, you can pause coverage tracking and add tested information use a NetCov API: ``` # pause coverage tracking to avoid over-estimation bf.cov.pause() routes = bf.q.routes(nodes=\"edge-0000\").answer().frame() bf.cov.resume()  # filter RIB entries to test tested = routes[routes[\"Network\"] == '0.0.0.0/0'].head(1)  # add tested route to coverage trace bf.cov.add_tested_routes(tested) ```  `bf.cov.result()` prints coverage metrics using `logging` module and writes to `stderr` by default.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo save the coverage report to file, you can customize logger by: ```python import logging fh = logging.FileHandler('cov.log') logging.getLogger('netcov').addHandler(fh) ```  ## References ``` @inproceedings {netcov-nsdi-2023,   author = {Xieyang Xu and Weixin Deng and Ryan Beckett and Ratul Mahajan and David Walker},   title = {Test Coverage for Network Configurations},   booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},   year = {2023},   isbn = {978-1-939133-33-5},   address = {Boston, MA},   pages = {1717--1732},   url = {https://www.usenix.org/conference/nsdi23/presentation/xu},   publisher = {USENIX Association},   month = apr, }```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# EntQA  This repo provides the code for our ICLR 2022 paper [EntQA: Entitly Linking as Question Answering](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup  ``` conda create --name entqa python=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch faiss-gpu cudatoolkit=11.0  ```  ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing), you can skip following preprocess steps.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOr preprocess by yourself:  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload KILT wikipedia knowledge base [here](https://github.com/facebookresearch/KILT) and put it under a kb directory like /raw_kb/  \\ 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload BLINK pretrained retriever model [here](https://github.com/facebookresearch/BLINK)  \\ 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload AIDA CoNLL datasets [here](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) and place them under a raw aida directory like /raw_aida/ \\ 4.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload entity title map dictionary [here](https://drive.google.com/file/d/1QE3N8S_tVkGhYz_5fjRahLHfkIwghi-4/view?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \\ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \\ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \\ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \\ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \\ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \\ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \\ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \\ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \\ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \\ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt takes up 32G GPU memory for the main GPU and 23.8G GPU memory for other GPUs. ### Retriever Local Evaluation 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can train the retriever by yourself using the above scripts to get your trained retriever and entity embeddings.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can also Download our trained retriever [here](https://drive.google.com/file/d/1bHS5rxGbHJ5omQ-t8rjQogw7QJq-qYFO/view?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing), our cached entity embeddings [here](https://drive.google.com/file/d/1znMYd5HS80XpLpvpp_dFkQMbJiaFsQIn/view?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing).  2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUse the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \\ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \\ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \\ --model /model_reader/reader.pt   --data_dir /reader_input/  \\ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \\ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \\ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \\ --k 3  --stride 16 --max_passage_len 32  --filter_span  \\ --type_encoder squad2_electra_large  \\ --type_span_loss sum_log  --type_rank_loss sum_log  \\ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt takes up 36G GPU memory for the main GPU and 32G GPU memory for the other GPU. ### Reader Local Evaluation 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can follow the above instructions to train your reader or you can download our trained reader [here](https://drive.google.com/file/d/1A4I1fJZKxmROIE1fd0mdXN6b1emP_xt4/view?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing) 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can run retriever local evaluation for your trained retriever or our trained retriever to get reader input data.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOr you can download our reader input data [here](https://drive.google.com/drive/folders/1xfEgXCREe6pbSmAsnidsMuVYMK_mlOao?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing) 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUse the above reader training scripts and set `--epochs` to be 0 for evaluation.  ### Reader Results  |   val F1  |  test F1  |  val Recall |  test Recall |  val Precision  |  test Precision  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## GERBIL evaluation Our GERBIL evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload our snapshot of GERBIL repo [here](https://drive.google.com/file/d/1Sp-G9631ormzIYfenCDsaWgiBPVBkF6F/view?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing), our pretrained retriever [here](https://drive.google.com/file/d/1bHS5rxGbHJ5omQ-t8rjQogw7QJq-qYFO/view?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing), our cached entities embeddings [here](https://drive.google.com/file/d/1znMYd5HS80XpLpvpp_dFkQMbJiaFsQIn/view?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing) and our pretrained reader [here](https://drive.google.com/file/d/1A4I1fJZKxmROIE1fd0mdXN6b1emP_xt4/view?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing) 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOne one terminal/screen run GERBIL by: ``` cd gerbil .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/start.sh  ``` 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOn another terminal/screen run: ``` cd gerbil-SpotWrapNifWS4Test/ mvn clean -Dmaven.tomcat.port=1235 tomcat:run  ``` 4.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOn a third terminal/screen run: ``` python gerbil_experiments/server.py  \\ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \\ --retriever_path /model_retriever/retriever.pt   \\ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \\ --ents_path /kb/entities_kilt.json  \\ --reader_path /model_reader/reader.pt   \\ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Lightweight-Face-Detector-Pruning  Pruning Lightweight Face Detectors **EXTD** and **EResFD** using NNI's `FPGMPruner` and `L1NormPruner`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRepository updated in April 2024, for correction / completion of the paper-related materials, and for releasing scripts that facilitate the Android deployment of pruned EResFD models.  ## Project Structure  The repository is organized into 4 folders:  - `EXTD_Pytorch-master/`: Contains code and resources specific to the EXTD model. - `EResFD-main/`: Contains code and resources for the EResFD model. - `Pruned_Models/`: A collection of pre-pruned model weights (`.pth` files) for both EXTD and EResFD.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe pruned models that are evaluated in Tables 1 and 2 of our paper are provided, i.e. each of the EXTD, EResFD Face Detectors is pruned using one of the FPGM, L1 pruning techniques, for target pruning rates equal to 10%, 20%, 30%, 40% and 50%.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe naming convention we follow for the pruned models is straightforward; for instance, `ERES10_FPGM` refers to the EResFD model pruned with 10% sparsity using the FPGM technique. - `torchscript/`: All the required files for android deployment of the EResFD model (and its pruned versions) using the torchscript framework.  ## Prerequisites  Before running the pruning scripts, the user needs to prepare the necessary dataset:  ### WIDER FACE Dataset  The models are trained and evaluated using the WIDER FACE dataset.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo use this dataset:  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload the WIDER FACE dataset from [here](https://shuoyang1213.me/WIDERFACE/). 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mExtract and place the `WIDER` folder in the same directory as the `EXTD` and `EResFD` folders.  ## Dependencies  A requirements.txt file is provided with all the necessary python dependencies.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAdditionally, the code was developed using Python 3.11.7, CUDA 11.4 and Ubuntu 20.04.06 LTS.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo ensure compatibility and proper functionality of the pruning scripts, please install the specific versions of the python packages listed in the requirements.txt file, using the following command:  ```bash pip install -r requirements.txt ```  ## Running the Scripts for Pruning a Face Detector  The pruning script executes the model pruning process as outlined in Section 4.2 of our paper.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt prunes and trains the model iteratively for 200 epochs, following which the pruning is halted and the model is fine-tuned for an additional 10 epochs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo execute the pruning process, use the following commands based on the desired pruning algorithm.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mE.g., for pruning with the Geometric Median (FPGM) algorithm the EResFD model:   ```bash   python fpgm.py --pruning_rate 0.1 --pruned_eres '.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/weights/eres10'   ``` Here, `fpgm.py` specifies the pruning methd to be used (in this example, FPGM), the value of `pruning_rate` specifies the sparsity per layer that the user wishes to introduce (`0.1` denotes a 10% target pruning rate), and `pruned_eres` is the prefix of the file where the pruned models will be saved, e.g.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`pruned_eres10`.  ## Android Deployment  Simply move the convert_to_torchscript.py script inside the EResFD-main/ directory and run it.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMake sure to change the path within the script to convert the desired model to torchscript form.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSubsequently, move the created lite_scripted_model.ptl to your android application`s asset folder.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe EResFD.java file contains a java class that can be used in order to load the model from the assets folder and use it for inference.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAdditionally, a sample build.gradle.kts file is provided with the necessary dependencies to build the android app.  ## License This code is provided for academic, non-commercial use only.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease also check for any restrictions applied in the code parts and datasets used here from other sources.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor the materials not covered by any such restrictions, redistribution and use in source and binary forms, with or without modification, are permitted for academic non-commercial use provided that the following conditions are met:  Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation provided with the distribution.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis software is provided by the authors \"as is\" and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn no event shall the authors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software, even if advised of the possibility of such damage.  ## Citation  If you find our pruning method or pruned models useful in your work, please cite the following publication where this approach was proposed:  K.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mGkrispanis, N.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mGkalelis, V.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMezaris, \"Filter-Pruning of Lightweight Face Detectors Using a Geometric Median Criterion\", Proc.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW 2024), Waikoloa, Hawaii, USA, Jan. 2024.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[https://arxiv.org/abs/2311.16613](https://arxiv.org/abs/2311.16613)  BibTex: ``` @inproceedings{Gkrispanis_WACVW2024, author={Gkrispanis, Konstantinos and Gkalelis, Nikolaos and Mezaris, Vasileios}, title={Filter-Pruning of Lightweight Face Detectors Using a Geometric Median Criterion}, year={2024}, month={Jan.}, booktitle={IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW 2024)} } ```  ## Acknowledgements  This work was supported by the EU Horizon 2020 programme under grant agreement H2020-951911 AI4Media.\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 1 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis repository contains the modifications made by our team for the (winning) entry in the English shared task category.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe further include modifications made for Spanish and Portuguese, obtaining SotA in those languages as well.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Find the arXiv version of our paper here: https://arxiv.org/abs/2301.01764**  In case you find these results useful, please consider citing our work in addition to the shared task paper (see below).  ``` @article{aumiller-gertz-2023-unihd, author = {Aumiller, Dennis and Gertz, Michael}, title = {{UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?}}\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m, journal = {CoRR}, volume = {abs/2301.01764}, eprinttype = {arXiv}, eprint = {2301.01764}, url = {https://arxiv.org/abs/2301.01764} } ```   ## Setup for Reproduction To run the results for the respective subsets, you need to provide your own OpenAI API key in a file named `config.py`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mA template can be found in `config.py.template`, which already contains the correct variable name and structure.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOther required packages can be installed with `python3 -m pip install -r requirements.txt` from the main repository folder.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mObtaining predictions from the API should be possible by running `context_predictor.py` for English results, and the `_es.py` and `\\_pt.py` files for Spanish and Portuguese predictions, respectively.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOur predictions submitted to the workshop can be found either in the subfolder `en/`, or the fiel `UniHD.zip`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLater additions of the Spanish and Portuguese predictions are in the respective folders `es/` (for Spanish) and `pt/` (Portuguese).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAn exact reproduction of the system outputs may not be possible, as the generation temperature and other, server-side factors, such as the random seed, can potentially lead to differing predictions.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn our experience, there is no possibility of fixing a particular prediction to occur with 100% likelihood.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe also do not make any promises that the used prompt templates work with later models.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn our experiments, we used the (relatively \"outdated\" `text-davinci-002` model, which may respond differently to prompt variants than later additions\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m# Original README: TSAR-2022-Shared-Task Datasets and Evaluation Scripts TSAR2022 Shared Task on Lexical Simplification for English (en), Spanish (es) and Portuguese (pt) - Datasets and Evaluation scripts  Please look at the website of the Shared Task for more details about the Evaluation Benchmark, Guidelines, Registration Form, etc...\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br/>[TSAR-2022 Shared-Task website](https://taln.upf.edu/pages/tsar2022-st/)  ## Datasets  There is no training dataset for the TSAR-2022 Shared Task.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHowever, a sample of 10 or 12 instances with gold standard annotations is provided here as a trial/sample dataset.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br/>  <br/>  Format of the files: - Format of the *trial_none* and *test_none* files: <span style=\"font-weight:normal\">Sentence&lt;TAB&gt;ComplexWord</span> - Format of the *trial_gold* and *test_gold* files: <span style=\"font-weight:normal\">Sentence&lt;TAB&gt;ComplexWord&lt;TAB&gt;Annotation1&lt;TAB&gt;Annotation2&lt;TAB&gt;...\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m&lt;TAB&gt;AnnotationN</span>   ### Trial dataset The trial dataset consists of a set of 10 instances (for English and Portuguese) and 12 instances (for Spanish) of a sentence, a target complex word.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe *trial_none* files contain only the instances and the *trial_gold* files contain the instances and set of gold annotations\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- /datasets/trial/tsar2022_en_trial_none.tsv - /datasets/trial/tsar2022_en_trial_gold.tsv - /datasets/trial/tsar2022_es_trial_none.tsv - /datasets/trial/tsar2022_es_trial_gold.tsv - /datasets/trial/tsar2022_pt_trial_none.tsv - /datasets/trial/tsar2022_pt_trial_gold.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/tsar_eval.py --gold_file .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/gold/tsar_es_gold.tsv --predictions_file .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/predicted/TEAMNAME_TRACKNAME_RUNNAME.tsv  --output_file .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/results/TEAMNAME_TRACKNAME_RUNNAME.tsv.eval ```  ## Dataset Compilation and Baselines  A paper describing the compilation of the datasets for English, Portuguese and Spanish that includes several experiments with  two state-of-the-art approaches for Lexical Simplification has been published at this link: https://www.frontiersin.org/articles/10.3389/frai.2022.991242  [Lexical Simplification Benchmarks for English, Portuguese, and Spanish](https://www.frontiersin.org/articles/10.3389/frai.2022.991242).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br/> Sanja Štajner, Daniel Ferrés, Matthew Shardlow, Kai North, Marcos Zampieri and  Horacio Saggion.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br/> Front.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mArtif.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIntell.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSec.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNatural Language Processing.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br/> doi: 10.3389/frai.2022.991242  Preprint available at ArXiV: [https://arxiv.org/abs/2209.05301](https://arxiv.org/abs/2209.05301)   ## License  The python scripts follow [AGPL 3.0v license](LICENSE).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe datasets (under the /datasets directory) are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [CC-BY-NC-SA-4.0](CC-BY-NC-SA-4.0).  ## Contact https://taln.upf.edu/pages/tsar2022-st/#contact\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Open Datasheets  The Open Datasheets framework is a simple, standardized no-code way to document datasets.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is inspired by the concept of [datasheets for datasets](https://arxiv.org/abs/1803.09010) and [Understanding Machine Learning Practitioners' Data Documentation Perceptions, Needs, Challenges, and Desiderata](https://dl.acm.org/doi/10.1145/3555760) research papers.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe framework is designed to be both machine-readable and human-readable, serving as a tool for dataset creators to document their datasets and for dataset consumers to understand the datasets they are using.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBy integrating directly with GitHub, the framework allows you to create, edit, and export your Open Datasheets directly to GitHub.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt leverages the widely used [Data Package](https://specs.frictionlessdata.io/data-package/) standard to describe the dataset and its metadata, benefiting from the support of various tools and libraries.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAdditionally, it extends this standard to incorporate responsible AI aspects into the dataset documentation, derived from [Microsoft’s Aether data documentation](https://www.microsoft.com/en-us/research/uploads/prod/2022/07/aether-datadoc-082522.pdf).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis includes information about the dataset's provenance, collection process, privacy and security considerations, and more.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAdhering to good data documentation practices is crucial for responsible AI, as it helps assess how a dataset may impact machine learning models and other solutions derived from it.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe primary goal of this framework is to facilitate dataset documentation, promote transparency, and aid dataset creators and consumers in making informed decisions about the suitability of specific datasets and the limitations they may have.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis framework achieves its goal by applying a no-code approach, making it accessible to both non-technical data producers and those with a technical background.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor more detailed information, you can refer to our paper titled \"[Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments](https://arxiv.org/abs/2312.06153)\"\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m# Try the No-Code App We have created a no-code web tool to help you create, edit, generate, and export an Open Datasheets `datapackage.json` file for your datasets directly to/from GitHub.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can also work with files located locally on your computer.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can try it out here: [https://microsoft.github.io/opendatasheets](https://microsoft.github.io/opendatasheets).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWith the no-code tool, you have the ability to: - Create a new Open Datasheets datapackage from scratch. - Generate a new Open Datasheets datapackage from data located on GitHub or a local file. - Modify an existing Open Datasheets datapackage file from GitHub or a local file. - Save an Open Datasheets datapackage file to GitHub or locally.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDEMO:  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[demo](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/assets/demo.gif)  # Top level schema  | Property | Description | Key Elements for Fostering Responsible AI | | --- | --- | --- | | name | The name of the data package. |  | | title | The title of the data package. | | | description | The description of the data package. |  | | version | The version of the data package. |  | | created | The date when the data package was created. |  | | licenses | The licenses under which the data package is distributed. |  | | sources | The sources from which the data package is derived. |  | | resources | The resources included in the data package. |  | | contributors | The contributors to the data package. | | | privacy | The privacy considerations for the data package. | &#x2705; | | security | The security considerations for the data package. | &#x2705; | | procedures | The collection, processing, and update procedures for the data package. | &#x2705; | | use | The permitted use of the data package. | &#x2705; |  For the complete schema, please refer to the [Open Datasheets schema](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/examples/datapackage.json).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor more detailed information about the responsible AI properties, such as collection procedures, privacy, and security considerations, please refer to the [Open Datasheets paper](https://arxiv.org/pdf/2312.06153).  ## Contributing  This project welcomes contributions and suggestions.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMost contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor details, visit https://cla.opensource.microsoft.com.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSimply follow the instructions provided by the bot.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou will only need to do this once across all repos using our CLA.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.  ## Trademarks  This project may contain trademarks or logos for projects, products, or services.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAuthorized use of Microsoft  trademarks or logos is subject to and must follow  [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAny use of third-party trademarks or logos are subject to those third-party's policies\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m# Citation  If you use the Open Datasheets framework in your research or project, please cite our paper:  ``` @article{roman2023open,   title={Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments},   author={Roman, Anthony Cintron and Vaughan, Jennifer Wortman and See, Valerie and Ballard, Steph and Schifano, Nicolas and Torres, Jehu and Robinson, Caleb and Ferres, Juan M Lavista},   journal={arXiv preprint arXiv:2312.06153},   year={2023} } ```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# fisher-information ## About the Project **For the most up-to-date work using this framework, and if you are interested in applying the framework to experimental design problems of your own, see the [experimental-design](https://github.com/James-Durant/experimental-design) repository**.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis repository contains the [code](/fisher-information), [data](/fisher-information/data) and [results](/fisher-information/results) for a framework for determining the maximum information gain and optimising experimental design in neutron reflectometry experiments using the Fisher information (FI).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn neutron reflectometry experiments, the FI can be analytically calculated and used to provide sub-second predictions of parameter uncertainties.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese uncertainties can influence real-time decisions about measurement angle, measurement time, contrast choice and other experimental conditions based on parameters of interest.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe FI provides a lower bound on parameter estimation uncertainties and these are shown to decrease with the square root of measurement time, providing useful information for the planning and scheduling of experimental work.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAs the FI is computationally inexpensive to calculate, it can be computed repeatedly during the course of an experiment, saving costly beam time by signalling that sufficient data has been obtained; or saving experimental datasets by signalling that an experiment needs to continue.  ### Citation Please cite the following article if you intend on including elements of this work in your own publications: > Durant, J.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mH., Wilkins, L., Butler, K. and Cooper, J.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mF.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mK.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDetermining the maximum information gain and optimizing experimental design in neutron reflectometry using the Fisher information.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m_J.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAppl.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCryst_. **54**, 1100-1110 (2021).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOr with BibTeX as: ``` @article{Durant2021,    author    = {Durant, J.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mH. and Wilkins, L. and Butler, K. and Cooper, J.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mF.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mK.},    doi       = {10.1107/S160057672100563X},    journal   = {Journal of Applied Crystallography},    month     = {Aug},    number    = {4},    pages     = {1100--1110},    publisher = {International Union of Crystallography ({IUCr})},    title     = {{Determining the maximum information gain and optimizing experimental design in neutron reflectometry using the Fisher information}},    url       = {https://doi.org/10.1107/S160057672100563X},    volume    = {54},    year      = {2021} } ```  For the figures presented in this article, see [figures](/figures).  ## Installation 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo replicate the development environment with the [Anaconda](https://www.anaconda.com/products/individual) distribution, first create an empty conda environment by running: <br /> ```conda create --name fisher-information python=3.8.3```  2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo activate the environment, run: ```conda activate fisher-information```  3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInstall pip by running: ```conda install pip```  4.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRun the following to install the required packages from the [requirements.txt](/requirements.txt) file: <br />    ```pip install -r requirements.txt```  You should now be able to run the code.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease ensure you are running a version of Python >= 3.8.0 \\ If you are running an old version of Anaconda, you may need to reinstall with a newer version for this.  ## Contact Jos Cooper - jos.cooper@stfc.ac.uk \\ James Durant - james.durant@warwick.ac.uk \\ Lucas Wilkins - lucas@lucaswilkins.com  ## Acknowledgements This work has been partially supported by the STFC Facilities Programme Fund through the ISIS Neutron and Muon Source, and Scientific Computing Department of Rutherford Appleton Laboratory, Science and Technology Facilities Council, and by the Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/T001569/1, particularly the \"AI for Science\" theme within that grant and The Alan Turing Institute.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe would also like to thank Luke Clifton for his assistance and expertise in fitting the DMPC data.  ## License Distributed under the GPL-3.0 License.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee [license](/LICENSE) for more information.\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# story-distiller  [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)  This project attempts to embed a story into a music playlist by sorting the playlist (i.e., sequencing it) so that the order of the music follows a narrative arc.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe music tracks are fitted to a fixed narrative template based on the output of a machine learning model, which itself distills each track down to its narrative essence.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor more information on narrative essence and how it generalizes to other forms of media, see *On the Distillation of Stories for Transferring Narrative Arcs in Collections of Independent Media* by Dylan R.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAshley, Vincent Herrmann, Zachary Friggstad, and Jürgen Schmidhuber.   ## Installation  This project is implemented in [Python](https://www.python.org/) and uses models learned with [PyTorch](https://pytorch.org).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBefore installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can now run the tool by directly executing the `__main__.py` file, or—if you're using Linux or macOS—you can use the makefile to compile and then install an executable Python zip archive: ```bash make all sudo make install ```   ## Command-line Tool Usage  To run the program, execute it while passing the audio files as command-line arguments: ```bash sdistil files [files ...] >> playlist.txt ```  [librosa](https://librosa.org/doc/latest/index.html) is used to process the audio files, so most common audio file types are supported.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you want to try out a different template, pass the `-t` argument to the program with the template file as an argument.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSeveral learned templates are included in the templates directory:  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[templates.jpg](https://github.com/dylanashley/story-distiller/blob/main/templates.jpg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mraw=true)   ## Web App Usage  To run the web app, simply execute the `app.py` file: ```bash .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/app.py ```   ## Extras  In addition to the above, this repository also includes all the code needed to reproduce the results presented in *On the Distillation of Stories for Transferring Narrative Arcs in Collections of Independent Media* by Dylan R.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAshley, Vincent Herrmann, Zachary Friggstad, and Jürgen Schmidhuber.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn particular, it includes - the scripts needed to learn the PyTorch models for extracting the narrative essence from either music albums (`album_extractor.py`) or movie frames (`movie_extractor.py`) and compute the lower bounds for the mutual information of the different features, - the scripts for learning a set of template curves from scalar descriptions of items in a set of collections (`template_learner.py`), - the code that can fit the scalar descriptions of items in a collection to a given template curve (`fit_values` in `__main__.py`), - the preprocessed album data used to train the original PyTorch models for the music albums (`data/`), and - the learned PyTorch models and template curves (`results/`).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote that to obtain the preprocessed data and the learned PyTorch models and template curves, you will have to clone the repository without using the `--sparse` option: ```bash git clone git@github.com:dylanashley/story-distiller.git ```  At this time, the raw data used to train the movie frame extractor is not possible to release due to copyright issues.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHowever, `movie_extractor.py` should work on any similar collection with minimal changes.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPreprocessing to extract CLIP features from images can be done straightforwardly using the [official implementation](https://github.com/openai/CLIP).\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# MLLM-Bench MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria <center>  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Python 3.9+](https://img.shields.io/badge/Python-3.9+-lightblue) !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Pytorch 2.0](https://img.shields.io/badge/PyTorch-2.0+-lightblue) !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[transformers](https://img.shields.io/badge/transformers-4.36.0.dev0%2B-lightblue) !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[accelerate](https://img.shields.io/badge/accelerate-0.22+-lightblue) </center>  <p align=\"center\">    📃  <a href=\"https://arxiv.org/abs/2311.13951\" target=\"_blank\">Paper</a> • 🌐  <a href=\"https://mllm-bench.llmzoo.com/\" target=\"_blank\">Website</a> • 🤗  <a href=\"huggingface.com\" target=\"_blank\">HuggingFace</a>    <p align=\"center\"> <img src=\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/image.png\" alt=\"Data Composition\" width=\"550\" height=\"550\">   ## 🌈  Update * **[2024.4.27]** V3 data, benchmark reuslts, leaderboard and arxiv paper are updated.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe keep all the per-sample criteria at evaluation private.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHowever, we provide  <a href=\"https://mllm-bench.llmzoo.com/static/submit.html\" target=\"_blank\">a submission entry</a>  for FREE evaluations.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCheck it out!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m* **[2024.1.7]** V2 data, reuslts and leaderboard are updated\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m* **[2023.11.18]** 🎉 🎉 🎉  This repo is made public!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m🎉 🎉 🎉   ## Leaderboard We present the results of voting using LLaVA-v1.5-13B as anchor.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe numbers denote *win/tie/lose* of a benchmarked model over LLaVA-v1.5-13B.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee more results of different evaluation protocols and anchors in our  [paper](https://arxiv.org/abs/2311.13951).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe information of benchmarked models is [here](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/Model_cards.md)\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | 🏅️   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | 🥈   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | 🥉   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe use `bf16` inference by default.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf this is not supported by your device, set `downcast_bf16` to `false` and `mixed_precision` to `fp16`\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Add model information in [configs/model_configs.yaml](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/configs/model_configs.yaml)  - Create a model worker in [workers/model_workers.py](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/workers/model_workers.py).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe worker should inherit `BaseWorker`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRewrite `init_components()` and `forward()` method.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mExplanations of parameters and outputs of the two methods are in [workers/baseworker.py](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/workers/baseworker.py)\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Run `bash generate.sh`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m</details>  ### Self-Evaluation <details><summary>Click to expand</summary>  - Prepare the data in the format as shown in [data/anchor.json](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/data/anchor.json), note that the key \"unique_idx\", \"gen_model_id\", and \"answer\" are required.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMove your data under [data](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/data/) folder\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Modify the parameters in [evaluate.sh](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/evaluate.sh), especially \"model_name\" and \"model2_path\"\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Put your OpenAI API key in [evaluate.py](.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/scripts/evaluate.py), please make sure you have access to model \"gpt-4-vision-preview\"\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Run `bash evaluate.sh`\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Run `cd scripts & bash evaluate4elo.sh` for elo rating\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- NOTE: The per sample criteria are not provided for self-evaluate and this self-evaluation process is just used for your reference.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you wish your results to be displayed on the leaderboard, please refer to [Submission for Leaderboard](#submission-for-leaderboard).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m</details>  ### Submission for Leaderboard Refer to instructions <a href=\"https://mllm-bench.llmzoo.com/static/submit.html\" target=\"_blank\">here</a>.   ## Citation ```angular2 @misc{ge2024mllmbench,       title={MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria},        author={Wentao Ge and Shunian Chen and Guiming Hardy Chen and Zhihong Chen and Junying Chen and Shuo Yan and Chenghao Zhu and Ziyue Lin and Wenya Xie and Xinyi Zhang and Yichen Chai and Xiaoyu Liu and Nuo Chen and Dingjie Song and Xidong Wang and Anningzhe Gao and Zhiyi Zhang and Jianquan Li and Xiang Wan and Benyou Wang},       year={2024},       eprint={2311.13951},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ## Star History  <a href=\"https://star-history.com/#FreedomIntelligence/MLLM-Bench&Date\">   <picture>     <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mrepos=FreedomIntelligence/MLLM-Bench&type=Date&theme=dark\" />     <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mrepos=FreedomIntelligence/MLLM-Bench&type=Date\" />     <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mrepos=FreedomIntelligence/MLLM-Bench&type=Date\" />   </picture> </a>\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 3 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Sparse-Depth-Completion  This repo contains the implementation of our paper [Sparse and Noisy LiDAR Completion with RGB Guidance and Uncertainty](https://arxiv.org/abs/1902.05356) by [Wouter Van Gansbeke](https://github.com/wvangansbeke), Davy Neven, Bert De Brabandere and Luc Van Gool.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you find this interesting or relevant to your work, consider citing:  ``` @inproceedings{wvangansbeke_depth_2019,     author={Van Gansbeke, Wouter and Neven, Davy and De Brabandere, Bert and Van Gool, Luc},     booktitle={2019 16th International Conference on Machine Vision Applications (MVA)},     title={Sparse and Noisy LiDAR Completion with RGB Guidance and Uncertainty},     year={2019},     pages={1-6},     organization={IEEE} } ```  ## License  This software is released under a creative commons license which allows for personal and research use only.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor a commercial license please contact the authors.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can view a license summary [here](http://creativecommons.org/licenses/by-nc/4.0/)  ## Introduction Monocular depth prediction methods fail to generate absolute and precise depth maps and stereoscopic approaches are still significantly outperformed by LiDAR based approaches.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe goal of the depth completion task is to generate dense depth predictions from sparse and irregular point clouds.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis project makes use of uncertainty to combine multiple sensor data in order to generate accurate depth predictions.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMapped lidar points together with RGB images (monocular) are used in this framework.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis method holds the **1st place** entry on the [KITTI depth completion benchmark](http://www.cvlibs.net/datasets/kitti/eval_depth.php?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mbenchmark=depth_completion) at the time of submission of the paper.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe contribution of this paper is threefold: * Global and local information are combined in order to accurately complete and correct the sparse and noisy LiDAR input.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMonocular RGB images are used for the guidance of this depth completion task. * Confidence maps are learned for the global branch and the local branch in an unsupervised manner.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe predicted depth maps are weighted by their respective confidence map.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis is the late fusion technique used in our framework. * This method ranks first on the KITTI depth completion benchmark without using additional data or postprocessing.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee full demo on [YouTube](https://www.youtube.com/watch?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mv=Kr0W7io5rHw&feature=youtu.be).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe predictions of our model for the KITTI test set can be downloaded [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x).  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements Python 3.7 The most important packages are pytorch, torchvision, numpy, pillow and matplotlib.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m(Works with Pytorch 1.1)   ## Dataset The [Kitti dataset](www.cvlibs.net/datasets/kitti/) has been used.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFirst download the dataset of the depth completion.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSecondly, you'll need to unzip and download the camera images from kitti.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mI used the file `download_raw_files.sh`, but this is at your own risk.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMake sure you understand it, otherwise don't use it.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you want to keep it safe, go to kitti's website.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe complete dataset consists of 85898 training samples, 6852 validation samples, 1000 selected validation samples and 1000 test samples.  ## Preprocessing This step is optional, but allows you to transform the images to jpgs and to downsample the original lidar frames.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis will create a new dataset in $dest.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can find the required preprocessing in: `Datasets/Kitti_loader.py`  Run:  `source Shell/preprocess $datapath $dest $num_samples`  (Firstly, I transformed the png's to jpg - images to save place.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSecondly, two directories are built i.e. one for training and one for validation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee `Datasets/Kitti_loader.py`)  Dataset structure should look like this: ``` |--depth selection |-- Depth      |-- train            |--date                |--sequence1                | ...\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m|--validation |--RGB     |--train          |--date              |--sequence1              | ...\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m|--validation ```   ## Run Code To run the code:  `python main.py --data_path /path/to/data/ --lr_policy plateau`  Flags: - Set flag \"input_type\" to rgb or depth. - Set flag \"pretrained\" to true or false to use a model pretrained on Cityscapes for the global branch. - See `python main.py --help` for more information.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mor   `source Shell/train.sh $datapath`  checkout more details in the bash file.  ## Trained models Our network architecture is based on [ERFNet](https://github.com/Eromera/erfnet_pytorch).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can find the model pretrained on Cityscapes [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis model is used for the global network.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can find a fully trained model and its corresponding predictions for the KITTI test set [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe RMSE is around 802 mm on the selected validation set for this model as reported in the paper.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo test it:  Save the model in a folder in the `Saved` directory.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mand execute the following command:  `source Test/test.sh /path/to/directory_with_saved_model/ $num_samples /path/to/dataset/ /path/to/directory_with_ground_truth_for_selected_validation_files/`  (You might have to recompile the C files for testing, provided by KITTI, if your architecture is different from mine)  ## Results  Comparision with state-of-the-art:  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[results](https://user-images.githubusercontent.com/9694230/59205060-49c32780-8ba2-11e9-8a87-34d8c3f99756.PNG)   ## Discussion  Practical discussion:  - I recently increased the stability of the training process and I also made the convergence faster by adding some skip connections between the global and local networks.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInitially I only used guidance by multiplication with an attention map (=probability), but found out that it is less robust and that differences between a focal MSE and vanilla MSE loss function were now negligible.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBe aware that this change will alter the appearance of the confidence maps since fusion happens at mutliple stages now\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Feel free to experiment with different architectures for the global or local network.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is easy to add new architectures to `Models/__init__.py`  - I used a Tesla V100 GPU for evaluation.  ## Acknowledgement This work was supported by Toyota, and was carried out at the TRACE Lab at KU Leuven (Toyota Research on Automated Cars in Europe - Leuven)\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Scaffolding Learning Regime (SLR)  [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[paper](https://img.shields.io/badge/paper-52b69a?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2206.13263) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[weights](https://img.shields.io/badge/weights-34a0a4?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=for-the-badge&logo=DocuSign&logoColor=white)](#pretrained-models) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[presentation](https://img.shields.io/badge/presentation-168aad?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/watch?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mv=F4sLbbMsoHw) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[cite](https://img.shields.io/badge/bibtex-1a759f?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=for-the-badge&logo=overleaf&logoColor=white)](#cite)   A PyTorch implementation of the Scaffolding Learning Regime (SLR) for training obstacle detection models for aquatic domains.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<p align=\"center\">     <img src=\"resources/annotations.png\" alt=\"SLR annotations\" width=\"400px\"> </p>  **November 2022**: Published in Sensors.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**January 2022**: Preliminary version presented at WACV 2022.  ## About SLR  Scaffolding Learning Regime (SLR) is a method for training semantic segmentation models for maritime obstacle detection using only weak annotations (obstacle bounding boxes and water-edge poly-line).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDespite a **signifficant reduction in annotation time (20x)**, SLR training **improves robustness** of networks for obstacle detection, which is a remarkable result.  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[SLR Architecture](resources/slr.png)  SLR is comprised of three steps. 1) The model is warmed-up using object-wise and global objectives derived from weak annotations and IMU. 2) he learned encoder features and model predictions are used to estimate the pseudo labels. 3) The network is fine-tuned on the estimated pseudo labels.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor additional details please refer to the [paper](https://arxiv.org/abs/2206.13263).   ## Getting started  ### Installation  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mClone the repository     ```bash     git clone https://github.com/lojzezust/SLR     cd SLR     ``` 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInstall the requirements     ```bash     pip install -r requirements.txt     ``` 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInstall SLR.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUse the `-e` flag if you want to make changes.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```bash     pip install -e .     ``` 4.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLink datasets directory and create an output directory.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```bash     ln -s path/to/data data     mkdir output     ```  ### Preparing the data  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload the [MaSTr1325 dataset](https://box.vicos.si/borja/viamaro/index.html) and corresponding [weak annotations](https://github.com/lojzezust/SLR/releases/download/weights_v2/mastr_slr.zip).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe weak annotation archive also includes automatically generated prior obstacle segmentation masks (i.e. using DEXTR). 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUse a script to prepare the data.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```bash     python tools/prepare_data.py     ```     The preparation script performs the following operations:     1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPrepares object masks - converts bounding boxes from weak annotations into masks used in training     2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPrepares pairwise similarity maps - pre-computes the neighbor similarities used by the pairwise loss     3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPrepares partial masks - compute the partial masks used in the warm-up phase.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPartial masks are constructed from weak annotations and IMU horizon masks.     4.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPrepares the prior obstacle segmentation masks - reads RLE encoded Dextr predictions and adds sky and water segmentation based on IMU horizon masks and partial labels.     5.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCreates a dataset file `all_weak.yaml`, which links the prepared dataset directories for training.  ### SLR Training  Use the utility script `tools/train_slr.sh` to train a model using the entire SLR pipeline.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```bash chmod +x tools/train_slr.sh tools/train_slr.sh ```  The script contains the following variables, that can be changed to achieve the desired results\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `MASTR_DIR`: Location of the dataset used for training. - `ARCHITECTURE`: Which architecture to use (use `python tools/train.py warmup --help` for more info). - `MODEL_NAME`: Name of the model.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUsed for saving logs and weights. - `BATCH_SIZE`: Batch size per gpu. - `WARMUP_EPOCHS`: Number of epochs for the warm-up phase. - `FINETUNE_EPOCHS`: Number of epochs for the fine-tuning phase. - `NUM_ITER`: Number of iterations of the SLR pseudo label estimation and fine-tuning.  ### Individual training steps  Individual steps of the SLR pipeline can also be executed separately, with the following python scripts.  #### Step I: Feature warm-up  Train an initial model on partial labels generated from weak annotations and IMU.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUses additional object-wise losses.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/train.py warmup \\ --architecture wasr_resnet101_imu \\ --model-name wasr_slr_warmup \\ --batch-size 4 ```  > [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTIP] > Use the `--help` switch for more details on all possible arguments and settings.  #### Step II: Generate pseudo labels  Generate pseudo labels by refining model predictions with learned features.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/generate_pseudo_labels.py \\ --architecture wasr_resnet101_imu \\ --weights-file output/logs/wasr_slr_warmup/version_0/checkpoints/last.ckpt \\ --output-dir output/pseudo_labels/wasr_slr_warmup_v0 ```  This creates the pseudo-labels and stores them into `output/pseudo_labels/wasr_slr_warmup_v0`\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m> [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTIP] > Use the `--help` switch for more details on all possible arguments and settings.  #### Step III: Fine-tune model  Fine-tune the initial model on the estimated pseudo-labels from the previous step.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe model is initialized with weights of the initial model.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/train.py finetune \\ --architecture wasr_resnet101_imu \\ --model-name wasr_slr \\ --batch-size 4 \\ --pretrained-weights output/logs/wasr_slr_warmup/version_0/checkpoints/last.ckpt \\ --mask-dir output/pseudo_labels/wasr_slr_warmup_v0 ```  > [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTIP] > Use the `--help` switch for more details on all possible arguments and settings.  ### Inference  #### General inference  Run inference using a trained model.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`tools/general_inference.py` script is able to run inference on a directory of images recursively.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt replicates the directory structure in the output directory.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/general_inference.py \\ --architecture wasr_resnet101 \\ --weights-file output/logs/wasr_slr_v2_it1/version_0/checkpoints/last.ckpt \\ --image-dir data/example_dir \\ --output-dir output/predictions/test_predictions ```  Additionally, `--imu-dir` can be used to supply a directory with corresponding IMU horizon masks.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe directory structure should match the one of image dir\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m> [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNOTE] > The IMU dir has to be provided for models architectures relying on IMU data (i.e.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWaSR with IMU)\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m> [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTIP] > Use the `--help` switch for more details on all possible arguments and settings.  #### MODS inference  `tools/mods_inference.py` can be used in a similar fashion to run inference on the MODS benchmark\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m> [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTIP] > Use the `--help` switch for more details on all possible arguments and settings.  ## Pretrained models  Currently available pretrained model weights.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAll models are trained on the MaSTr1325 dataset using SLR and weak annotations and evaluated on the [MODS benchmark](https://github.com/bborja/mods_evaluation).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mF1 obstacle detection scores are reported overall and separately within the 15m critical danger zone around the boat\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m| architecture       | backbone   | IMU | url                                                                                                        | $\\mathrm{F1}$ | $\\mathrm{F1}_D$ | |--------------------|------------|-----|------------------------------------------------------------------------------------------------------------|---------------|-----------------| | wasr_resnet101     | ResNet-101 |     | [weights](https://github.com/lojzezust/SLR/releases/download/weights_v2/wasr_slr_v2_rn101.pth)             | 94.4          | 92.0            | | wasr_resnet101_imu | ResNet-101 |  ✓  | [weights](https://github.com/lojzezust/SLR/releases/download/weights_v2/wasr_slr_v2_rn101_imu.pth)         | 94.9          | 93.7            |   ## Cite  You can use the following BibTeX entry to cite this work:  ```bibtex @article{Zust2022Learning,     author = {Žust, Lojze and Kristan, Matej},     title = {Learning with Weak Annotations for Robust Maritime Obstacle Detection},     journal = {Sensors},     volume = {22},     year = {2022},     number = {23},     article-number = {9139},     url = {https://www.mdpi.com/1424-8220/22/23/9139},     issn = {1424-8220},     doi = {10.3390/s22239139} } ```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 3 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# PrivacyFL: A simulator for privacy-preserving and secure federated learning  This repository contains the source code for running a privacy perserving federated learning simulator.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe source code is currently set up for the configuration of three clients performing secure and differentially private federated learning using logistic regresion on the MNIST dataset.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis library, however, is meant to be modified so as to simulate your own secure federated machine learning configuration.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe hope that this simulation can help users decide whether it is beneficial for them to participate in differentially-private federated learning for a given differentially private algorithm.  ## UPDATE : Paper accepted at the 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT  Paper and Video Link : https://dl.acm.org/doi/10.1145/3340531.3412771  ## Getting Started  These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee deployment for notes on how to deploy the project on a live system.   ### Installing First, clone this repository locally.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThen create a conda enviroment by running: ``` conda env create -f environment.yml -n YourEnvironmentName ``` Activate the new enviornment: ``` source activate YourEnvironmentName ``` To validate correct installation `cd` to `src` and run  ``` python run_simulation.py ``` If you encounter any issues, please let us know so that we can help in getting the simulation up and running.   ### Configuring Your Simulation This library is intended to be modified as needed for your use case.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe have provided a default `config.py` file as an example.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSome simulation behavior can easily be configured by changing the files in the config file.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe file contains Boolean variables `USE_SECURITY` and `USE_DP_PRIVACY` to toggle security and differential privacy features.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe security feature does not affect accuracy, however you can set `USE_DP_PRIVACY` to `False` if you want to see what the federated accuracy would be without differential privacy.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe default `config.py` file also has `USING_CUMULATIVE` set to `True`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWhat that means is that the dataset for a client on iteration `i` containts all of the datapoints in iteration `i-1` as well as len_per_iteration new datapoints.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAs such, this flag also makes it so that each client trains its weights from scratch each iteration.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mConversely, one can set the `USING_CUMULATIVE` flag to `False`, which will make the dataset non-cumulative and clients perform gradient descent from last iteration's federated weights.  ## System Architecure ### Agent  Agent (defined in agent.py) is the base class for this simulation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is not meant to be initialized directly, but subclassed to create an agent with whichever behavior you would like.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe have provided two sample subclasses, **ClientAgent** and **ServerAgent**, which are both used in the sample simulation.   ### ClientAgent  An instance of the **ClientAgent** class represents an entity that is training a machine learning model on the same task as the other client agents.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe initialization arguments for a client are `agent_number`, `train_datasets`, `evaluator`, `sensitivity`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mClient agents are named assigned an `agent_number`, which is then appended to the string `client_agent` to create their name.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor example, in the example simulation there are three client agents named `client_agent0`, `client_agent1`, and `client_agent2`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWhen initialized, clients are also provided their dataset, which in the example is a pyspark dataframe.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe client is also passed an `evaluator` which it will use in the simulation to evaluate its own weights and the federated weights.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`evaluator`is an instance of the ModelEvaluator class defined in `utils/model_evaluator.py`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br/><br/>  There are two important methods of **ClientAgent** that are invoked by the **ServerAgent**.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe first is `compute_weights`, which is called every iteration and prompts the client to perform its machine learning task on its dataset for that iteration.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe second method is `receive_weights` which is called at the end of every iteration when the server has federated weights to return to the client.   ### ServerAgent  An instance of the **ServerAgent** class represents a third-party entity that is responsible for invoking the simulation and corresponding with the client agents.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is possible to configure a simulation to use more than one **ServerAgent**, but the straightforward example in the respository currently only creates one instance.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInitializaing a **ServerAgent** only requires the same default argument for initializing its superclass **Agent**: `agent_number` which should be `0` for the first server agent.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br/><br/>  **ServerAgent** has one method: `request_values`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe `request_values` is called to signal the server agent to start requesting values from clients, thereby starting the online portion of the simulation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe only argument is `iters` which dictates how many iterations to run the simulation for.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote that if iters is too large, the client agents may run out of data.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor the example shown in the repository, only set `iters` to be equal to or less than to the `iterations` argument in the **Initializer** `__init__` method, since that is the method that creates the datasets and distributes them to the clients.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you wish to change the behavior of the simulation, `request_values` is a good place to start and subsequently add/modify any methods that are called in the **ClientAgent** class.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br/><br/>  The `request_values` method first requests weights in parallel from the clients by calling their `compute_weights` method, averages them, and then returns them to the clients in parallel by calling their `receive_weights` method.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn the current example, the client returns a message to the server agent through the `receive_weights` method indicating whether its weights have converged.   ### Initializer  An instance of the **Initializer** class is used to initialize the agents and model evaluator.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn addition, any offline stage logic, such as a diffie-helman key exchange, should occur in this class.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn our example, it loads the MNIST dataset and processes it for the client agent instances.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br/><br/>  To commence the simulation, the initializer's `run_simulation` method is invoked, which then invokes the server agent's `request_values` method. ### Directory The **Directory** class contains a mapping of agent names to agent instances that allows agents to invoke other agents' methods by only having their name.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAn instance of **Directory** is created in the `__init__` method of the **Initializer** class after all the agents have been created.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is then passed on to all the agents using their `set_directory` method.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAn example usage to call some method of client_agent1 would be: `self.directory.clients['client_agent1'].METHOD_TO_CALL()'`  ### Utils Folder This folder contains utilities such as data processing, differential privacy functions, and more.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor the most part, functions in here are implementation specific and you should feel free to add any auxiliary functions scripts.   ## Features  The library is intended to help users simulate secure federated learning to decide whether it would be feasible and beneficial.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mA snippet of a sample output looks like: ``` Performance Metrics for client_agent2 on iteration 1  -------------------------------------------  Personal accuracy: 0.8283333333333334  Personal computation time: 0:00:01.194242  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:07.202375   Performance Metrics for client_agent0 on iteration 1  -------------------------------------------  Personal accuracy: 0.8216666666666667  Personal computation time: 0:00:01.198737  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:09.202375  ``` As you can see, the simulation prints out i) the personal accuracy: the accuracy that the client can obtain by itself on its own dataset, adding no DP noise.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNOTE: this quantitiy does incorporate other client's data if you set the config.USING_CUMULATIVE flag to False, since that indicates to clients that they should start training on this iteration using the federated weights from the previous iteration since the datasets aren't cumulative. ii) the federated accuracy: the accuracy of the federated model which is the average of all the clients' personal weights + differentially private noise for that iteration.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote that while the clients benefit from participating in the simulation in this example, that is not always the case.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn particular, as one increases the amount of differentially private noise, the federated accuracy is expected to decrease.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOn the other hand, the personal accuracy will remain the same since it is assumed you don't add differentially private noise to your personal model since you are not sharing it.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br/><br/>  In addition, this library also allows you to simulate how long it would take to receive the federated values back for each iteration.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`Personal computation time` indicates how long your training took for that iteration while `Simulated time to receive federated weights` takes into account user-defined communication latencies between the clients and the server, as well as how long it took the other clients to compute their weights and the server to average them.  ## Authors  Vaikkunth Mugunthan* Anton Peraire* Lalana Kagal  ## License  This project is licensed under the MIT License   MIT License  Copyright (c) 2020 Vaikkunth, Anton, Lalana (PrivacyFL)  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m \u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m# VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification  [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Open in HuggingFace](https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg)](https://huggingface.co/spaces/ml-jku/vnegnn) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[](https://img.shields.io/badge/paper-arxiv2310.06763-red?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=plastic&logo=GitBook)](https://arxiv.org/abs/2404.07194) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[](https://img.shields.io/badge/model-pink?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=plastic&logo=themodelsresource)](https://huggingface.co/fses91/VNEGNN-MODEL) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[](https://img.shields.io/badge/project_page-blue?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=plastic&logo=internetcomputer)](https://linktr.ee/vnegnn)  ## News  🔥  ***April 10 2024***:  *The trained VNEGNN model and processed dataset are released, as in the paper!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m*  ## Overview  Implementation of the VN-EGNN, state-of-the-art method for protein binding site identfication, by Florian Sestak, Lisa Schneckenreiter, Johannes Brandstetter, Sepp Hochreiter, Andreas Mayr, Günter Klambauer.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis repository contains all code, instructions and model weights necessary to run the method or to retrain a model.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you have any question, feel free to open an issue or reach out to: <sestak@ml.jku.at>.  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[](visualizations/overview.jpg)  ## Installation  [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[](https://img.shields.io/badge/PyTorch-2.1.2-ee4c2c?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mlogo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[lightning](https://img.shields.io/badge/-Lightning_2.2.1-792ee5?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mlogo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[black](https://img.shields.io/badge/Code%20Style-Black-black.svg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mlabelColor=gray)](https://black.readthedocs.io/en/stable/) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[isort](https://img.shields.io/badge/%20imports-isort-%231674b1?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)  To reproduce the results please use Python 3.9, PyTorch version 2.1.2, Cuda 12.1, PyG version 2.3.0.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mClone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe processed datasets will be released soon, because of the large size of the datasets (graph with esm embeddings), we will upload the pipeline to process the raw data.  ### Raw Datasets  #### SC-PDB  <https://github.com/jivankandel/PUResNet/blob/main/scpdb_subset.zip>  #### COACH420/HOLO4K  <https://github.com/rdk/p2rank-datasets>  These two datasets do not include ligands in a pre-extracted format.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInstead, the MOAD database contains the ligand information, according to the criteria of *relevant ligand* as specified in the MOAD database.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLigand Information: <http://www.bindingmoad.org/files/csv/every_bind.csv>  #### PDBBind  <http://www.pdbbind.org.cn/download/PDBbind_v2020_other_PL.tar.gz>  ## Replicate results  If you want to replicated the results from the paper, put the downloaded datasets in the specific folders\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `bindingsite_test_data/coach420/raw`  - `bindingsite_test_data/holo4k/raw`  - `bindingsite_test_data/holo4k_split_chain/raw`    This dataset is the same as the holo4k dataset, but the chains are split into separate files.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLook in the paper for more information\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `bindingsite_test_data/pdbbind2020/raw`  The holo4k and the coach420 dataset were created by extracting the relevant ligans from the MOAD database.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAll ligand protein complexes were put into a different folder, at inference we combine them by there pdb id to evalutate the model based on all the bindingsites for a given protein.  ### Process the datasets  This will create a `lmdb` database for each dataset.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```python python train.py --config-name=config_binding_hetero        # heterogenous model, top performing model in the paper ```  For training on [SLURM](https://www.schedmd.com/) cluster with [submitit](https://github.com/facebookincubator/submitit)  used the `conf/hydra/meluxina.yaml` as blueprint.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = \"/path/to_config/\" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = \"/path/to/your/checkpoint.ckpt\"      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[\"test_coach420\"] # name of dataloader [\"test_coach420\", \"test_pdbbind2020\", \"test_holo4k\"] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe thank Audi.JKU Deep Learning Center, TGW LOGISTICS GROUP GMBH, Silicon Austria Labs (SAL), FILL Gesellschaft mbH, Anyline GmbH, Google, ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Bio- pharma SRL, Merck Healthcare KGaA, Verbund AG, GLS (Univ.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWaterloo) Software Competence Center Hagen- berg GmbH, TÜV Austria, Frauscher Sensonic, TRUMPF and the NVIDIA Corporation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe acknowledge EuroHPC Joint Undertaking for awarding us access to Karolina at IT4Innovations, Czech Republic; MeluXina at LuxProvide, Luxembourg; LUMI at CSC, Finland.  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[](visualizations/1odi_3lpk.png)\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 3 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Awesome Neural Tree Papers <img class=\"emoji\" alt=\":art:\" height=\"30\" width=\"30\" src=\"tree.png\"> Selected papers and possible corresponding codes in our review paper **\"A Survey of Neural Trees\" [[arXiv Version]](https://arxiv.org/abs/2209.03415)**  *If you find there is a missed paper or a possible mistake in our survey, please feel free to email me or pull a request here.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mI am more than glad to receive your advice.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThanks!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m*  ## Introduction Neural trees (NTs) refer to a school of methods that combine neural networks (NNs) and decision trees (DTs), for which we present a comprehensive review in this survey.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOur keynote is to identify how these approaches enhance the model interpretability and suggest possible solutions to the remaining challenges.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBesides, we provide a discussion about other considerations like conditional computation and promising directions towards this field, in hope to advance the practice of NTs.  ## *News!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m* **New work of NTs and possible changes in this survey will be notified here.**  - Add \"ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction\" [[Paper]](https://arxiv.org/pdf/2210.03745.pdf), an interesting work that combines prototype learning, soft decision trees and Graph Neural Networks for compound property prediction\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- Add \"Large-Scale Category Structure Aware Image Categorization\" [[Paper]](https://proceedings.neurips.cc/paper/2011/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf) & \"Leveraging Class Hierarchies with Metric-Guided Prototype Learning\" [[Paper]](https://arxiv.org/pdf/2007.03047.pdf), two methods incorporating the class hierarchy into the regulariser of the general objective function in section 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAlthough a regulariser is certainly feasible in utilize the class hierarchy, it is curiously rare in practice and these two methods are the only two examples we know of. - Add \"Distillation Decision Tree\" [[Paper]](https://arxiv.org/pdf/2206.04661.pdf), which interprets the black-box model by distilling its knowledge into decision trees, belonging to section 2.2.  ### Citation  If you find this survey useful for your research, please consider citing ``` @article{li2022survey,   title={A Survey of Neural Trees},   author={Li, Haoling and Song, Jie and Xue, Mengqi and Zhang, Haofei and Ye, Jingwen and Cheng, Lechao and Song, Mingli},   journal={arXiv preprint arXiv:2209.03415},   year={2022} } ```  Thanks!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m## A Taxonomy of Current Methods In this survey, we present a thorough taxonomy for NTs that expresses the gradual integration and co-evolution of NNs and DTs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<p align='center'>     </br>     <img src='outline_final.png' width='1000'> </p>  ### 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNon-hybrid: NNs and DTs Cooperated Approaches.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese approaches are the very first to combine NNs and DTs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThey employ DTs as auxiliaries for NNs as well as using NNs as tools to improve the design of DTs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn these approaches, \"neural\" and \"tree\" are separated, *i.e.*, one of NN and DT is assigned to accomplish a specific task, while the other performs as its assistant or interpreter.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt can  date back to the early 1990s when DTs were supposed to provide structural priors for NNs or extract rules from a  trained NN.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThey are perceived implicit combinations of NNs and DTs, because NNs and DTs still operate on their  own paradigms and no hybrid model is produced.  #### 1.1 DTs as Structural Priors of NNs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAdopt DTs to approximate the target concept of NNs in terms of logical descriptions, *i.e.*, use DTs to design NNs  with structural priors\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- ***Entropy net: the first practice***   - **\"Entropy nets: from decision trees to neural networks\"**, Proceedings of the IEEE, 1990     - I.K.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/58346/)   - **\"Layered neural net design through decision trees\"**, ISCAS, 1990     - I.K.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/112298)   - **\"Comparison between entropy net and decision tree classifiers\"**, IJCNN, 1990     - I.K.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSethi, Mike Otten     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/5726783)  An example for the original [entropy net](https://ieeexplore.ieee.org/abstract/document/58346/):  <p align='center'>     </br>     <img src='shortened_structural_prior.png' width='1000'> </p>  - ***ID3-based algorithm that converts DTs into hidden layers:***   - **\"A machine learning method for generation of a neural network architecture: a continuous ID3 algorithm\"**, IEEE Transactions on Neural Networks, 1992     - K.J.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCios, N.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLiu     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/125869)  - ***Soft thresholding of entropy net:***   - **\"Neural implementation of tree classifiers\"**, IEEE Transactions on Systems, Man, and Cybernetics, 1995     - I.K.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/398685)  - ***Oblique trees version of entropy net:***   - **\"A mapping from linear tree classifiers to neural net classifiers\"**, ICNN, 1994     - Youngtae Park     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/374145)   - **\"Fast Training Algorithms for Multi-layer Neural Nets\"**, IEEE Transactions on Neural Networks, 1991     - Richard P.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBrent     - [[Paper]](https://citeseerx.ist.psu.edu/viewdoc/download?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mdoi=10.1.1.122.4561&rep=rep1&type=pdf)  - ***A knowledge acquisition system that concerns about the knowledge form DTs and NNs prefer:***   - **\"Implementation and refinement of decision trees using neural networks for hybrid knowledge acquisition\"**,  Artificial Intelligence in Engineering, 1995     - Katsuhiko Tsujino, Shogo Nishida     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/0954181095000054)  - ***Create a disjunctive normal form formula for each class and reformulate the first layer of entropy net:***   - **\"Initializing neural networks using decision trees\"**, published by Rutgers University, 1990     - Arunava Banerjee     - [[Paper]](https://scholarship.libraries.rutgers.edu/esploro/outputs/technicalDocumentation/Initializing-neural-networks-using-decision-trees/991031549998404646)   - **\"Initialization of neural networks by means of decision trees\"**, Knowledge-Based Systems, 1995     - Irena Ivanova, Miroslav Kubat     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/0950705196819174)   - **\"On mapping decision trees and neural networks\"**, Knowledge-Based Systems, 1999     - R.SetionoW, K.Leow     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S095070519900009X)  #### 1.2 DTs for Approximating NNs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mApply DTs to interpret trained NNs by approximating the network (mostly in terms of input-output mapping) and mimic the decision boundaries implicitly learned by the hidden layers.  ##### *1.2.1 Pedagogical Techniques.* Treat NNs as black-box approaches, and extract the input-output rule directly form NNs regardless of its intermediate layers.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mA description of pedagogical techniques:  <p align='center'>     </br>     <img src='shortened_rule_extract.png' width='1000'> </p>  - ***TREPAN: the first practice of DT-based interpretation***   - **\"Extracting Tree-Structured Representations of Trained Networks\"**, NeurIPS, 1995     - Mark Craven, Jude Shavlik     - [[Paper]](https://proceedings.neurips.cc/paper/1995/hash/45f31d16b1058d586fc3be7207b58053-Abstract.html)   - **\"Using Sampling and Queries to Extract Rules from Trained Neural Networks\"**, Machine Learning Proceedings, 1994     - Mark Craven, Jude Shavlik     - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500131)    - ***Remove insignificant input neurons from the trained network before inducing the DT***   - **\"Decision Tree Extraction using Trained Neural Network\"**, SMARTGREENS, 2020     - Nikola Vasilev *et al.*     - [[Paper]](https://pdfs.semanticscholar.org/9528/ece5f2a9c71fac8ac258d6812ff5cc75da63.pdf)  - ***Turn the 𝑚-of-𝑛   type splits into standard splitting tests such as C4.5***   - **\"Decision Tree Extraction from Trained Neural Networks\"**, AAAI, 2004     - Darren Dancey *et al.* *(CART tree)*     - [[Paper]](https://www.aaai.org/Papers/FLAIRS/2004/Flairs04-089.pdf)   - **\"Converting a trained neural network to a decision tree DecText - decision tree extractor\"**, Dissertation, 2000     - Olcay Boz *(propose discretization algorithms to handle continuous features)*     - [[Paper]](https://www.proquest.com/openview/11cd66e5b5aab652403e370ca7bf1eef/1?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mpq-origsite=gscholar&cbl=18750&diss=y)   - **\"Extracting decision trees from trained neural networks\"**, Pattern Recognition, 2020     - Olcay Boz      - [[Paper]](https://dl.acm.org/doi/abs/10.1145/775047.775113)   - **\"NeC4.5: neural ensemble based C4.5\"**, IEEE Transactions on Knowledge and Data Engineering, 2004     - Zhi-Hua Zhou, Yuan Jiang *(use NN ensembles)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1294896)   - **\"Extracting decision trees from trained neural networks\"**, Pattern Recognition, 1999     - R.Krishnan, *et al.* *(adapt genetic algorithms to generate artificial examples)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0031320398001812)  - ***generalize the TREPAN algorithm***   - **\"An investigation of TREPAN utilising a continuous oracle model\"**, Data Analysis Techniques and Strategies, 2011      - William A Young *et al.* *(develop DTs derived from continuous-based models)*     - [[Paper]](https://www.researchgate.net/profile/William-Young-10/publication/227441144_An_investigation_of_TREPAN_utilising_a_continuous_oracle_model/links/00b495258560f7e8d3000000/An-investigation-of-TREPAN-utilising-a-continuous-oracle-model.pdf?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)   - **\"Trepan-Plus: An Extension of a Decision Tree Extraction Algorithm Utilizing Artificial Neural Networks\"**, Intelligent Engineering Systems Through Artificial Neural Networks, 2007     - M Rangwala *et al.* *(extend TREPAN to multi-class regression problems)*     - [[Paper]](https://d1wqtxts1xzle7.cloudfront.net/47121083/TREPAN-PLUS_AN_EXTENSION_OF_A_DECISION_T20160708-16395-1sj4ete-with-cover-page-v2.pdf?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mExpires=1662897159&Signature=C2Z0VL3cIkc56vlIci5ERXC7VADyi9tEqIlvJ3CqHzaLB9bfY9Tma8UVsZpu1BXI1l24paZ0iJArqUUzEqrTvum7s93dAUL-pKmmBKausyKiAw3qj2Bx18I5jzDOat2TCKA6wd2GbiexuK7gtlfNgeu03fBg0Rx2UWt~2KW6RNMeamZinhzp1FCrSzH4fFRqhTuc2~UgajTzsaUlJHbJef-s9u2wOiwqNiEUAfwN8zG0tIEt-fE0GEUdSC9AzBVXLpodLTadDzUZODX7lzUX~o00-ZHxKl7VOnbi0wQ8OZw087zKQjqQNOw1ik16zfKMamZW~YS6e38aUgud0sOrBg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)   - **\"Extracting fuzzy symbolic representation from artificial neural networks\"**, NAFIPS, 1999     - Maciej Faifer *et al.* *(use fuzzy representation during the tree-induction process)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/781764)   - **\"DT+GNN: A Fully Explainable Graph Neural Network using Decision Trees\"**, arXiv, 2022     - Peter Müller *et al.* *(incorporate DTs into graph neural networks)*     - [[Paper]](https://arxiv.org/abs/2205.13234)  - ***NNs perform significance analysis to select attributes***   - **\"ANN-DT: an algorithm for extraction of decision trees from artificial neural networks\"**, IEEE Transactions on Neural Networks, 1999     - Gregor PJ Schmitz *et al.*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/809084)   ##### *1.2.2 Decompositional Techniques.* Decompositional techniques concern units in the hidden layers.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThey extract rules from the trained NN at the level of  individual neurons, thus gaining insight into their inner structures\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- ***Extract axis-aligned trees***   - **\"Rule extraction from neural networks via decision tree induction\"**, IJCNN, 2001     - Makoto Sato, Hiroshi Tsukimoto *(CRED algorithm that deals with NNs with one hidden layer)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/938448)   - **\"DeepRED – Rule Extraction from Deep Neural Networks\"**, International Conference on Discovery Science, 2016     - Jan Ruben Zilke, Eneldo Loza Mencía, Frederik Janssen *(Extend CRED algorithm by deriving intermediate rules for every additional hidden layer)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-46307-0_29)   - **\"Eclectic rule extraction from Neural Networks using aggregated Decision Trees\"**, ICECE, 2012     - MD.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRidwan Al Iqbal *(Simplify rules by logical simplification)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6471502)  - ***Extract oblique and multivariate trees***   - **\"NeuroLinear: From neural networks to oblique decision rules\"**, Neurocomputing, 1997      - Rudy Setiono, Huan Liu *(oblique trees)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0925231297000386)   - **\"Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees\"**, 2020     - Duy T Nguyen, Kathryn E Kasmarik, Hussein A Abbass *(multivariate trees)*     - [[Paper]](http://arxiv-export-lb.library.cornell.edu/abs/2003.04675)  ##### *1.2.3 Eclectic Techniques.* Typically, eclectic techniques model the relationship between the last hidden layer and the outputs, then infer the input-output relationship from the magnitudes of the weights in a NN.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHowever, few of them are based on DTs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTherefore, this survey comprises approaches that extract **partial knowledge** contained in the hidden layers and do not demand their approximation target is the input-output relationship  - ***Extended C-Net algorithm***   - **\"Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees\"**, arXiv, 2020     - Duy T Nguyen, Kathryn E Kasmarik, Hussein A Abbass *(a typical eclectic technique)*     - [[Paper]](http://arxiv-export-lb.library.cornell.edu/abs/2003.04675)  - ***Generalized eclectic techniques***   - **\"Global Model Interpretation Via Recursive Partitioning\"**, HPCC/SmartCity/DSS, 2018     - Chengliang Yang, Anand Rangarajan, Sanjay Ranka *(CART tree learned from the contribution matrix and applied to scene understanding tasks)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8622994) [[Code]](https://github.com/west-gates/GIRP)   - **\"Interpreting CNNs via Decision Trees\"**, CVPR, 2019     - Quanshi Zhang *et al.* *(encodes all potential decision modes of the CNN in a coarse-to-fine manner)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Interpreting_CNNs_via_Decision_Trees_CVPR_2019_paper.html)  ##### *1.2.4 Optional: DTs for Regularizing NNs.* These approaches train NNs that resemble compact DTs through crafted regularization terms, so that they can be well-approximated by small DTs\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- ***Tree regularization***   - **\"Beyond Sparsity: Tree Regularization of Deep Models for Interpretability\"**, AAAI, 2018      - Mike Wu *et al.* *(a complexity penalty function that aims to optimize the deep model for interpretability and human-simulatability)*     - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/11501) [[Code]](https://github.com/dtak/tree-regularization-public)   - **\"Regional Tree Regularization for Interpretability in Deep Neural Networks\"**, AAAI, 2020     - Mike Wu *et al.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m(extended work of tree regularization that encourage a deep model to be approximated by several separate DTs specific to pre-defined regions of the input space)*     - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/6112)  - ***L1-orthogonal regularization***   - **\"Enhancing Decision Tree Based Interpretation of Deep Neural Networks through L1-Orthogonal Regularization\"**, ICMLA, 2019     - Nina Schaaf, Marco Huber, Johannes Maucher *(so that they do not need a surrogate network to estimate the tree loss)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8999213)  #### 1.3 NNs for Designing DTs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNNs specialized for designing more reasonable DTs and do not benefit from it, which is rare in practice\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- ***Induce DTs from more valid data filtered by the trained NNs***   - **\"An integrated approach of neural network and decision tree to classification\"**, ICMLC, 2005     - Xiao-Ye Wang, Xiu-Xia Liang, Ji-Zhou Sun     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1527283)  ### 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSemi-hybrid: NNs Leveraging Class Hierarchies.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDT algorithms have two key ideas: \"decision\" and \"tree\".\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe view that \"decision\" is decision branches implemented by routing functions, and \"tree\" is the model topology that identifies a class hierarchy.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis survey tries to include NNs that draw on a part of ideas from DTs, either the class hierarchy or the decision branches, but not both.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese  approaches are considered to be half and implicit integration of NNs and DTs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThey borrow some inherent ideas from DTs for NNs, instead of designing a NN that works like a DT.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHowever, we only concern about the former in our taxonomy,  *i.e.*, NNs leveraging class hierarchies.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese approaches can not implement the class hierarchy in the network structure due to the absence of decision branches, so they resort to incorporating the hierarchical relations into a NN directly.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThere are three lines of research: hierarchical architecture, hierarchical loss function and label embedding based methods, corresponding to different strategies of employing class hierarchies.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOverview of NNs exploiting class hierarchies:   <p align='center'>     </br>     <img src='class_hierarchy.png' width='1000'> </p>  Functions and notations in the figure are adopted from [Bertinetto *et al.*](https://openaccess.thecvf.com/content_CVPR_2020/html/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.html):  #### 2.1 Hierarchical Architecture.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese methods attempt to incorporate the class hierarchy into the architecture of the classifier, so that the networks are designed to be branched and each branch is tasked to identify the concept abstraction at one specific level of the class hierarchy\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- ***Hierarchical multi-label classification using local neural networks***   - **\"An integrated approach of neural network and decision tree to classification\"**, ICMLC, 2014     - Ricardo Cerri, Rodrigo C Barros, André CPLF De Carvalho     - [[Paper]](https://www.sciencedirect.com/science/article/pii/S0022000013000718)   - **\"Reduction strategies for hierarchical multi-label classification in protein function prediction\"**, BMC Bioinformatics, 2016     - Ricardo Cerri *et al.*     - [[Paper]](https://link.springer.com/article/10.1186/s12859-016-1232-1)   - **\"HD-CNN: Hierarchical Deep Convolutional Neural Networks for Large Scale Visual Recognition\"**, ICCV, 2015     - Zhicheng Yan *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.html) [[Code]](https://github.com/Changgang-Zheng/HD-CNN)   - **\"Your \"Flamingo\" is My \"Bird\": Fine-Grained, or Not\"**, CVPR, 2021     - Dongliang Chang *et al.* *(disentangle coarse and fine features)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Chang_Your_Flamingo_is_My_Bird_Fine-Grained_or_Not_CVPR_2021_paper.html) [[Code]](https://github.com/PRIS-CV/Fine-Grained-or-Not)  - ***Branch the network after a shared trunk***   - **\"Learning to Make Better Mistakes: Semantics-aware Visual Food Recognition\"**, ACM international conference on Multimedia, 2016      - Hui Wu *et al.* *(a single network backbone shared by multiple fully-connected layers)*     - [[Paper]](https://dl.acm.org/doi/abs/10.1145/2964284.2967205)   - **\"Do Convolutional Neural Networks Learn Class Hierarchy?\"\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**, TVCG, 2017     - Alsallakh Bilal *et al.* *(deep CNN with branches at intermediate layers to fit the coarser-grained labels)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8017618)   - **\"Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding\"**, ACM international conference on Multimedia, 2018     - Tianshui Chen *et al.* *(introduce an attention mechanism to incorporate the coarse-grained results for learning finer-grained features)*     - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3240508.3240523)  - ***Interactions between different branches***   - **\"Label Hierarchy Transition: Modeling Class Hierarchies to Enhance Deep Classifiers\"**, arXiv, 2021     - Renzhen Wang, *et al* *(propose label hierarchy transition matrices whose column vectors represent the conditional label distributions of classes between two adjacent hierarchies)*     - [[Paper]](https://arxiv.org/abs/2112.02353)   - **\"Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification\"**, CVPR, 2022     - Jingzhou Chen *et al.* *(propose the hierarchical residual network in which granularity-specific features from parent levels are added to features in children levels.)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Label_Relation_Graphs_Enhanced_Hierarchical_Residual_Network_for_Hierarchical_Multi-Granularity_CVPR_2022_paper.html) [[Code]](https://github.com/MonsterZhZh/HRN)  #### 2.2 Hierarchical Loss Function.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIncorporate the class hierarchy into loss functions, which exploits the underlying hierarchical relationships and produce predictions coherent with the pre-defined class hierarchy\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- ***Predict at each grain separately, *i.e.*, first take a model pre-trained on one level, then tune it using labels from other levels***   - **\"Coherent Hierarchical Multi-Label Classification Networks\"**, NIPS, 2020     - Eleonora Giunchiglia, Thomas Lukasiewicz     - [[Paper]](https://proceedings.neurips.cc/paper/2020/hash/6dd4e10e3296fa63738371ec0d5df818-Abstract.html) [[Code]](https://github.com/EGiunchiglia/C-HMCNN)   - **\"Learning Hierarchical Visual Representations in Deep Neural Networks Using Hierarchical Linguistic Labels\"**, arXiv, 2018     - Joshua C Peterson *et al.*     - [[Paper]](https://arxiv.org/abs/1805.07647)  - ***Utilize the hierarchical constraints directly***   - **\"Learning hierarchical similarity metrics\"**, CVPR, 2012     - Nakul Verma *et al.* *(probabilistic nearest-neighbor classification based framework)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6247938)   - **\"Discriminative Transfer Learning with Tree-based Priors\"**, NIPS, 2013     - Nitish Srivastava, Russ R Salakhutdinov *(DNN with hierarchical priors over the parameters of the classification layer)*     - [[Paper]](https://proceedings.neurips.cc/paper/2013/hash/9ac403da7947a183884c18a67d3aa8de-Abstract.html)   - **\"Large-Scale Object Classification Using Label Relation Graphs\"**, ECCV, 2018     - Jia Deng *et al.* *(encodes semantic relations into a directed acyclic graph and compute a loss defined on it)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_4) [[Code]](https://github.com/kylemin/HEX-graph)   - **\"Making Better Mistakes: Leveraging Class Hierarchies With Deep Networks\"**, CVPR, 2020     - Luca Bertinetto *et al.* *(incorporate class hierarchy into the cross-entropy loss)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.html)  #### 2.3 Label Embedding.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese approaches aim to encode the class hierarchy into embeddings whose relative locations or possible interactions represent the semantic relationships\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- ***DeViSE method that utilizes unannotated Wikipedia text***   - **\"DeViSE: A Deep Visual-Semantic Embedding Model\"**, NIPS, 2013     - Andrea Frome *et al.*     - [[Paper]](https://proceedings.neurips.cc/paper/2013/hash/7cce53cf90577442771720a370c3c723-Abstract.html) [[Code]](https://github.com/jean4599/DeViSE)  - ***Embeddings whose pair-wise dot products correspond to semantic similarity between classes***   - **\"Hierarchy-Based Image Embeddings for Semantic Image Retrieval\"**, WACV, 2019     - Björn Barz, Joachim Denzler     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8658633) [[Code]](https://github.com/cvjena/semantic-embeddings)  - **Employ the entailment cones to learn order-preserving embeddings**   - **\"Hierarchical Image Classification Using Entailment Cone Embeddings\"**, CVPR, 2020     - Ankit Dhall *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_CVPRW_2020/html/w50/Dhall_Hierarchical_Image_Classification_Using_Entailment_Cone_Embeddings_CVPRW_2020_paper.html) [[Code]](https://github.com/ankitdhall/learning_embeddings)  - ***Label embeddings in zero-shot classification***   - **\"Latent Embeddings for Zero-Shot Classification\"**, CVPR, 2016     - Yongqin Xian *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2016/html/Xian_Latent_Embeddings_for_CVPR_2016_paper.html)   - **\"Evaluation of Output Embeddings for Fine-Grained Image Classification\"**, CVPR, 2015     - Zeynep Akata *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2015/html/Akata_Evaluation_of_Output_2015_CVPR_paper.html)   ### 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHybrid: Neural Decision Trees.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNeural Decision Trees (NDTs) are hybrid NN models that implement both the class hierarchy and the decision branches.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTheir core idea is to exploit NNs in the tree design by making the routing functions differentiable, thus allowing gradient descent-based methods to optimize.  #### 3.1 Neural Decision Trees with Class Hierarchies.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf a NDT satisfies this property, it means each internal node is assigned a specific and intermediate classification task.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis \"divide-and-conquer\" strategy and stepwise inference process make the model more interpretable, because each node in the tree is responsible and distinguishable from other nodes.  ##### *3.1.1 Data-Driven NDTs.* Data-driven methods employ data-dependent heuristics to perform local optimization.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe resultant tree will lead to  a recursive partitioning of the input space 𝑋  through a cascade of tests and define the output of each leaf in terms of examples falling within it.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mA comparison between NDTs according to whether it implements a class hierarchy and whether it is data-driven: <p align='center'>     </br>     <img src='NDT_hierarchy.png' width='1000'> </p>  - ***Informativeness-related splitting functions with NN-based routers***   - **\"Neural trees-using neural nets in a tree classifier structure\"**, ICASSP, 1991     - J-E Stromberg, Jalel Zrida, Alf Isaksson *(train a small NN at each internal node, which singles out one unique class that gains the most class purity)*     - [[Paper]](https://www.computer.org/csdl/proceedings-article/icassp/1991/0003137/12OmNzdoMSH)   - **\"Evolutionary design of neural network tree-integration of decision tree, neural network and GA\"**, CEC, 2001     - Qiangfu Zhao *(use genetic algorithms to maximize the information gain ratio at each internal node)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/934395)   - **\"Hybrid decision tree\"**, Knowledge-Based Systems, 2002     - Zhi-Hua Zhou, Zhao-Qian Chen *(Divide the instance space into ordered attributes and unordered attributes)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0950705102000382)   - **\"Classification trees with neural network feature extraction\"**, CVPR, 1992     - Heng Guo, Saul B Gelfand *(find two clusters that minimizes a Gini impurity criterion, then find a good split through back-propagation)*     - [[Paper]](https://www.computer.org/csdl/proceedings-article/cvpr/1992/00223275/12OmNzBOilt)  - ***Decrease the classification error at each node to be extended***   - **\"Growing and pruning neural tree networks\"**, IEEE Transactions on Computers, 1993     - A Sakar *et al.* *(Divide the instance space into ordered attributes and unordered attributes)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/210172)  - ***Grows during competitive learning***   - **\"Competitive neural trees for pattern classification\"**,  IEEE Transactions on Neural Networks, 1998     - Sven Behnke, Nicolaos B Karayiannis     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/728387)  - ***Map examples into the label embedding space and predicts using a NDT***   - **\"Label Embedding Trees for Large Multi-Class Tasks\"**, NIPS, 2010     - Samy Bengio, Jason Weston, David Grangier     - [[Paper]](https://proceedings.neurips.cc/paper/2010/hash/06138bc5af6023646ede0e1f7c1eac75-Abstract.html)   - **\"Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition\"**, NIPS, 2011     - Jia Deng, Sanjeev Satheesh, Alexander Berg, Fei Li     - [[Paper]](https://proceedings.neurips.cc/paper/2011/hash/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html)  - ***Apply data-driven architectures to fuzzy NDTs***   - **\"Globally optimal fuzzy decision trees for classification and regression\"**, IEEE Transactions on Pattern Analysis and Machine Intelligence, 1999     - Alberto Suárez, James F Lutsko *(superimpose a fuzzy structure over the skeleton of a CART tree and introduce a global optimization algorithm)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/817409)   - **\"Budding Trees\"**, ICPR, 2014     - Ozan Irsoy, Olcay Taner Yildiz, Ethem Alpaydin *(a FDT model that can be dynamically adjusted)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6977328) [[Code]](https://github.com/oir/budding-tree)   - **\"Convolutional Decision Trees for Feature Learning and Segmentation\"**, GCPR, 2014    - Dmitry Laptev, Joachim M Buhmann *(applies fuzzy NDTs to image segmentation by extracting the most informative and interpretable features)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-11752-2_8)   - **\"Neural Decision Trees\"**, arXiv, 2017     - Randall Balestriero *(map similar inputs to the same hash value)*     - [[Paper]](https://arxiv.org/abs/1702.07360)   - **\"Neuro-fuzzy decision trees\"**, Neural Systems, 2006     - Rajen B Bhatt, M Gopal *(employs fuzzy ID3 algorithm for DT-renovation)*     - [[Paper]](https://www.worldscientific.com/doi/abs/10.1142/s0129065706000470)  - ***Incremental learning***   - **\"Hybrid decision tree\"**, Knowledge-Based Systems, 2022     - Zhi-Hua Zhou, Zhao-Qian Chen *(two example-incremental tasks, one hypothesis-driven constructive induction mechanism)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0950705102000382)   - **\"Soft decision trees\"**, ICPR, 2012     - Ozan Irsoy, Olcay Taner Yıldız, Ethem Alpaydın *(sigmoid-based FDT whose splits are made by checking if there is an improvement over the validation set)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6460506)   - **\"A Neural Tree with Partial Incremental Learning Capability\"**, ICMLC, 2007     - Mu-Chun Su, Hsu-Hsun Lo *(choose a target class at each internal node and train a small NN to separate the positive patterns from negative ones)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/4370106)  ##### *3.1.2 Bigot NDTs.* A bigot NDT tends to have a pre-defined structure and determine leaves (pure classes or fixed class distributions) by priors or algorithms such that induce the class hierarchy\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- ***Bigot NDTs with pure leaves***   - **\"Structure-driven induction of decision tree classifiers through neural learning\"**, Pattern Recognition, 1997     - Ishwar K Sethi *et al.* *(randomly initialized leaf classes)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0031320397000058)   - **\"NBDT: Neural-Backed Decision Trees\"**, ICCV, 2020     - Alvin Wan *et al.* *(assign a specific concept to each terminal and intermediate node by WordNet)*     - [[Paper]](https://arxiv.org/abs/2004.00221) [[Code]](https://github.com/alvinwan/nbdt-pytorch-image-models)  - ***Bigot NDTs with determined leaf distributions***   - **\"Distilling a Neural Network Into a Soft Decision Tree\"**, AI*IA, 2017     - Nicholas Frosst, Geoffrey Hinton *(jointly optimize leaves and other parameters via back-propagation)*     - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)   - **\"Policy-gradient Methods for Decision Trees\"**, ESANN, 2016     - Aurélia Léon, Ludovic Denoyer *(use Monte Carlo approximation to expect the gradient of the objective function)*     - [[Paper]](http://www.smart-labex.fr/publications/pdf/56eab488c05ef.pdf)   - **\"Deep Neural Decision Forests\"**, ICCV, 2015     - Peter Kontschieder *et al.* *(propose a derivative-free strategy to solely optimize the leaf parameters, as a convex optimization problem)*     - [[Paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.html) [[Code]](https://github.com/jingxil/Neural-Decision-Forests)   - **\"Deep Regression Forests for Age Estimation\"**, CVPR, 2018     - Wei Shen *et al.* *(regression forests for age estimation)*     - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Regression_Forests_CVPR_2018_paper.html) [[Code]](https://github.com/Sakura03/age_trans)   - **\"Neural Decision Forests for Semantic Image Labelling\"**, CVPR, 2014     - Samuel Rota Bulo, Peter Kontschieder *(decision forests for semantic image labelling)*     - [[Paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Bulo_Neural_Decision_Forests_2014_CVPR_paper.html)   - **\"Neural Prototype Trees for Interpretable Fine-Grained Image Recognition\"**, CVPR, 2021     - WMeike Nauta, Ron van Bree, Christin Seifert *(prototype-based ante-hoc methods)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Nauta_Neural_Prototype_Trees_for_Interpretable_Fine-Grained_Image_Recognition_CVPR_2021_paper.html?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mref=https://githubhelp.com) [[Code]](https://github.com/M-Nauta/ProtoTree)   - **\"Learn decision trees with deep visual primitives\"**, SSRN, 2022     - Mengqi Xue, *et al*     - [[Paper]](https://papers.ssrn.com/sol3/papers.cfm?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mabstract_id=4210199)   - ***Bigot NDTs for knowledge distillation***   - **\"Tree-Like Decision Distillation\"**, CVPR, 2021     - Jie Song *et al.* *(layer-wise dissect the decision process of a DNN)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Song_Tree-Like_Decision_Distillation_CVPR_2021_paper.html)   - **\"Tree-Like Branching Network for Multi-class Classification\"**, LNNS, 2021     - Mengqi Xue, Jie Song, Li Sun, Mingli Song *(mine the underlying category relationships from a trained teacher network and determines the appropriate layers on which specialized branches grow)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-93247-3_18)   - **\"Distilling a Neural Network Into a Soft Decision Tree\"**, AI*IA, 2017     - Nicholas Frosst, Geoffrey Hinton *(use a trained NN to provide soft targets for training a fuzzy NDT)*     - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)   - **\"TNT: An Interpretable Tree-Network-Tree Learning Framework using Knowledge Distillation\"**, Entropy, 2020     - Jiawei Li *et al.* *(transfer knowledge between tree models and DNNs)*     - [[Paper]](https://www.mdpi.com/1099-4300/22/11/1203)   - **\"KDExplainer: A Task-oriented Attention Model for Explaining Knowledge Distillation\"**, arXiv, 2021     - Mengqi Xue *et al.*     - [[Paper]](https://arxiv.org/abs/2105.04181)  #### 3.2 Expert NDTs (NDTs without Class Hierarchies) NDTs without class hierarchies restrain themselves little and perform arbitrary predictions at the leaves.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTheir  leaf nodes are usually classifiers rather than determined classes or distributions\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- ***Hierarchical Mixtures of Experts (HME)***   - **\"Hierarchical Mixtures of Experts and the EM Algorithm\"**, Neural computation, 1994      - Michael I Jordan, Robert A Jacobs *(the original HME, a tree-structured model for regression and classification.)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6796382)   - **\"Classification using hierarchical mixtures of experts\"**, NNSP, 1994     - Steve R Waterhouse, Anthony J Robinson *(each leaf expert is non-linear and performs multi-way classification)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/366050)   - **\"Bayesian Hierarchical Mixtures of Experts\"**, arXiv, 2012     - Christopher M Bishop, Naonori Ueda, Steve Waterhouse *(bayesian treatments of the HME model to prevent the severe overfitting caused by maximum likelihood)*     - [[Paper]](https://arxiv.org/abs/1212.2447)     - ***Generalized HMEs in advanced frameworks***   - **\"Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization\"**, CVPR, 2020     - Ruyi Ji *et al.* *(incorporate convolutional operations along edges and use attention transformer modules to capture discriminative features)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ji_Attention_Convolutional_Binary_Neural_Tree_for_Fine-Grained_Visual_Categorization_CVPR_2020_paper.html) [[Code]](https://isrc.iscas.ac.cn/gitlab/research/acnet)   - **\"NDT: Neual Decision Tree Towards Fully Functioned Neural Graph\"**, arXiv, 2017     - Han Xiao *(reformulate the non-differentiable information gain in the form of Dirac symbol and approximate it as a continuous function)*     - [[Paper]](https://arxiv.org/abs/1712.05934)   - **\"Decision Forests, Convolutional Networks and the Models in-Between\"**, arXiv, 2016     - Yani Ioannou *et al.* *(hybrid model between decision forests and convolutional networks)*     - [[Paper]](https://arxiv.org/abs/1603.01250)   - **\"Deep Neural Decision Trees\"**, arXiv, 2018     - Yongxin Yang, Irene Garcia Morillo, Timothy M Hospedales *(bin each feature of the input instance and determine the leaf node it will arrive)*     - [[Paper]](https://arxiv.org/abs/1806.06988) [[Code]](https://github.com/wOOL/DNDT)   - **\"ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder\"**, ICML, 2022     - Sangwon Kim, Jaeyeal Nam, Byoung Chul Ko *(transformer version of ProtoTree with expert leaves)*     - [[Paper]](https://proceedings.mlr.press/v162/kim22g.html) [[Code]](https://github.com/jumpsnack/ViT-NeT)  - ***Expert NDTs with architecture search phase***   - **\"Adaptive Neural Trees\"**, ICML, 2019     - Ryutaro Tanno *et al.* *(greedily choosing the best option between going deeper and splitting the input space)*     - [[Paper]](http://proceedings.mlr.press/v97/tanno19a.html?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mutm_campaign=piqcy&utm_medium=email&utm_source=Revue%20newsletter) [[Code]](https://github.com/rtanno21609/AdaptiveNeuralTrees)   - **\"Self-Born Wiring for Neural Trees\"**, ICCV, 2021     - Ying Chen *et al.* *(NDTs that are self-born from a large search space)*     - [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Self-Born_Wiring_for_Neural_Trees_ICCV_2021_paper.html) [[Code]](https://github.com/HollyLee2000/SeBoW-paddle)\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Air-Writing **A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing.**  *Accepted in The IEEE International Conference on Frontiers in Handwriting Recognition (ICFHR) 2018.*  **[[arXiv]](https://arxiv.org/abs/2303.07989)** <img align='right' height='100' src='https://github.com/prasunroy/air-writing/blob/master/assets/logo.png' />  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_1.svg) !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from \"tensorflow\" (default) to \"theano\" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<br /> <br />   **Made with** :heart: **and GitHub**\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# megaman: Manifold Learning for Millions of Points  <img src=\"https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/word2vec_rmetric_plot_no_digits.png\" height=200><img src=\"https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/spectra_D4000.png\" height=200><img src=\"https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/spectra_Halpha.png\" height=200>  [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Anaconda-Server Badge](https://anaconda.org/conda-forge/megaman/badges/downloads.svg)](https://anaconda.org/conda-forge/megaman) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[build status](http://img.shields.io/travis/mmp2/megaman/master.svg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=flat)](https://travis-ci.org/mmp2/megaman) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[version status](http://img.shields.io/pypi/v/megaman.svg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=flat)](https://pypi.python.org/pypi/megaman) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[license](http://img.shields.io/badge/license-BSD-blue.svg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=flat)](https://github.com/mmp2/megaman/blob/master/LICENSE)  ``megaman`` is a scalable manifold learning package implemented in python.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt has a front-end API designed to be familiar to [scikit-learn](http://scikit-learn.org/) but harnesses the C++ Fast Library for Approximate Nearest Neighbors (FLANN) and the Sparse Symmetric Positive Definite (SSPD) solver Locally Optimal Block Precodition Gradient (LOBPCG) method to scale manifold learning algorithms to large data sets.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOn a personal computer megaman can embed 1 million data points with hundreds of dimensions in 10 minutes. megaman is designed for researchers and as such caches intermediary steps and indices to allow for fast re-computation with new parameters.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPackage documentation can be found at http://mmp2.github.io/megaman/  If you use our software please cite the following JMLR paper:  McQueen, Meila, VanderPlas, & Zhang, \"Megaman: Scalable Manifold Learning in Python\", Journal of Machine Learning Research, Vol 17 no. 14, 2016. http://jmlr.org/papers/v17/16-109.html  You can also find our arXiv paper at http://arxiv.org/abs/1603.02763  ## Examples  - [Tutorial Notebook]( https://github.com/mmp2/megaman/blob/master/examples/megaman_tutorial.ipynb)  ## Installation and Examples in Google Colab  Below it's a tutorial to install megaman on Google Colab, through Conda environment.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt also provides tutorial of using megaman to build spectral embedding on uniform swiss roll dataset\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- [Install & Example script]( https://colab.research.google.com/drive/1ms22YK3TvrIx0gji6UZqG0zoSNRCWtXj?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing) - [You can download the Jupyter Notebook version here]( https://github.com/mmp2/megaman/blob/master/examples/megaman_install_usage_colab.ipynb)  ## ~~Installation with Conda~~  <!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m-- The easiest way to install ``megaman`` and its dependencies is with [conda](http://conda.pydata.org/miniconda.html), the cross-platform package manager for the scientific Python ecosystem.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo install megaman and its dependencies, run  ``` $ conda install megaman --channel=conda-forge ```  Currently builds are available for OSX and Linux, on Python 2.7, 3.4, and 3.5.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor other operating systems, see the full install instructions below. -->  Due to the change of API, `$ conda install -c conda-forge megaman` is no longer supported.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe are currently working on fixing the bug.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \\                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWith ``nose`` installed, type ``` $ make test ``` to run the unit tests.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m``megaman`` is tested on Python versions 2.7, 3.4, and 3.5.  ## Authors - [James McQueen](http://www.stat.washington.edu/people/jmcq/) - [Marina Meila](http://www.stat.washington.edu/mmp/) - [Zhongyue Zhang](https://github.com/Jerryzcn) - [Jake VanderPlas](http://www.vanderplas.com) - [Yu-Chia Chen](https://github.com/yuchaz)  ## Other Contributors  - Xiao Wang: lazy rmetric, Nystrom Extension - [Hangliang Ren (Harry)](https://github.com/Harryahh): Installation tutorials, Spectral Embedding  ## Future Work  See this issues list for what we have planned for upcoming releases:  [Future Work](https://github.com/mmp2/megaman/issues/47)\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Words as Gatekeepers  License: [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)  **Authors**: Li Lucy, Jesse Dodge, David Bamman, Katherine A.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mKeith  **[Paper](https://arxiv.org/abs/2212.09676)**, **[Blog post](https://blog.allenai.org/words-as-gatekeepers-measuring-discipline-specific-terms-and-meanings-in-scholarly-publications-718dc56d08a5)**  <p align=\"center\"> <img src=\"image.png\" width=\"75%\" > </p>  **Abstract**: Scholarly text is often laden with jargon, or specialized language that can facilitate efficient in-group communication within fields but hinder understanding for out-groups.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn this work, we develop and validate an interpretable approach for measuring scholarly jargon from text.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mExpanding the scope of prior work which focuses on word types, we use word sense induction to also identify words that are widespread but overloaded with different meanings across fields.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe then estimate the prevalence of these discipline-specific words and senses across hundreds of subfields, and show that word senses provide a complementary, yet unique view of jargon alongside word types.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe demonstrate the utility of our metrics for science of science and computational sociolinguistics by highlighting two key social implications.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFirst, though most fields reduce their use of jargon when writing for general-purpose venues, and some fields (e.g., biological sciences) do so less than others.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSecond, the direction of correlation between jargon and citation rates varies among fields, but jargon is nearly always negatively correlated with interdisciplinary impact.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBroadly, our findings suggest that though multidisciplinary venues intend to cater to more general audiences, some fields' writing norms may act as barriers rather than bridges, and thus impede the dispersion of scholarly ideas.  ## Scholarly jargon  ### Discipline-specific word senses  - `logs/word_clusters_lemmed/0.0/` includes senses and their top predicted substitutes.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUse the `Word Cluster Analysis` notebook in the code folder to inspect the content of these files\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `logs/fos_senses/es-True_res-0.0/` includes senses and their npmi scores in each field  ### Discipline-specific word types   - `logs/type_npmi/fos_set-False_lemma-True/` includes lists of word types in each subfield and their npmi scores  ## Code pipeline  **Data filtering**  Information on accessing S2ORC can be found [here](https://github.com/allenai/s2orc)\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `data_process/clean_up_wikipedia.py`: sample a subset of Wikipedia  - `create_mag_mapping()` in `val_data_process/fos_analysis.py`: get all MAG IDs to fos\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `langid.py` and `language_id_helper.py`: detect non-English journals for removal - `data_preprocessing.py`: determine how many abstracts we have per journal, also outputs dataframe of paper IDs to journal and FOS to support dataset creation - `General Dataset Statistics.ipynb`: examine the distribution of journal counts, save lists of paper IDs to keep for journal and FOS analysis.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis generates `s2orc_fos.json`  **Word type pipeline**  In the `type_jargon` folder:   FOS - `word_counts_per_fos.py`: count words per field of study.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWikipedia - `word_counts_wikipedia.py`: count words in simple and regular Wikipedia samples  Vocab to lemmatize - `write_mask_preds/wsi_vocab.py`: vocab creation  Journals & FOS - `word_type.py`: calculate NPMI  **WSI pipeline**  There are some additional supporting scripts, but these are the main ones to run.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote that many scripts are modified versions of ones found in the [WSIatScale repo](https://github.com/allenai/WSIatScale).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRun `bash prepare_sense_input.sh 2>&1 | tee temp.log` to do the next three scripts:   - `write_mask_preds/wsi_vocab.py`: determine vocabulary of words to perform WSI - `val_data_process/process_wiktionary.py`: get wiktionary definitions for vocabulary words - `write_mask_preds/wsi_preprocessing.py`: input preparation, also copy vocab file into output folder  Then, run the following script on S2ORC and Wikipedia:   - `write_mask_preds/write_mask_preds.py`: write replacements   We recommend splitting input files into numbered parts and running the script on ranges of file numbers.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUsage example for S2ORC:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset s2orc --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  Usage example for Wikipedia:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset wikipedia --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  In the `WSIatScale` folder run `wsi_pipeline.sh` to do WSI pipeline for the entire vocab.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou should change the file paths to redirect to yours, rather than the placeholders I included\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `create_inverted_index.py`: create inverted index  ``` python create_inverted_index.py --replacements_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --vocab_path /home/lucyl/language-map-of-science/logs/sense_vocab/wsi_vocab_set_98_50.txt --outdir /home/lucyl/language-map-of-science/logs/inverted_index --input_ids_path /home/lucyl/language-map-of-science/data/input_paper_ids/journal_analysis.txt ```  - `cluster_reps_per_token.py`: cluster the reps  Lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  After clustering for the whole dataset, use `Wiktionary Validation.ipynb` notebook to get FOS to words json.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThen use `wiktionary_eval.sh` to run wiktionary evaluation steps for clustering and assigning.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCluster only wiktionary words, lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.0 ```  Can check the coverage of words that appear in FOS in `Wiktionary Validation.ipynb`\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `assign_clusters_to_tokens.py`: assign everyone to a cluster  Lemmatized, specifying resolution:   ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_lemmed --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  Assign only wiktionary words, lemmatized, specifying resolution:  ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_eval --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.5 ```  Sense NPMI - run `get_documentID_maps()` in `get_docID_to_group.py` - `word_sense.py`, for journals, fos, and wiktionary evaluation - `Wiktionary Validation.ipynb` is the notebook that contains Wiktionary evaluation results.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Social implication experiments** - `Domain Language Analysis` is a notebook that generates data for some of the tables of example jargon in the paper.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt also generates the figure that summarizes whether some fields tend to use more jargon than others, and whether a field tends to use lots of distinctive words, or repurpose existing words with distinctive meanings\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `get_discipline_specific.py`: get discipline specific journals and their papers, for the audience design experiment - `jargonyness_per_paper.py`: calculate amount of jargon per abstract  Example usage:  ``` python jargonyness_per_paper.py --cutoff 0.1 --exp_name general_specific  python jargonyness_per_paper.py --cutoff 0.1 --exp_name regression_sample ```  - `expected_max_npmi.py`: expected max NPMI over token positions in abstract, for audience design experiment - `Paper Jargon Rate.ipynb`: audience design plots - `get_paper_time_and_place.py`: get FOS and year of potential papers that may cite the papers in regression study - `General Dataset Statistics`: get data used for regression  - `regression_variables.py`: get some of the simpler regression variables - `citations_per_journal.py`: for calculating the average number of citations per journal, a regression variable - `Paper Success Regression.ipynb`: notebook that runs regressions - `get_fos_citation_matrix.py`: for calculating similarity among disciplines, part of interdisciplinarity regression  **Citation graph**  Future work may want to run analysis on the S2ORC citation graph.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe below script supports the conversion of S2ORC data to a `graph-tool` network, where nodes are papers labeled with paper ID\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- `citation_graph.py`: create citation graph\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Systematic-Generalization-via-Meaningful-Learning This repository is for the paper [Revisit Systematic Generalization via Meaningful Learning](https://aclanthology.org/2022.blackboxnlp-1.6).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m*In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP*, pages 62–79, Abu Dhabi, United Arab Emirates (Hybrid).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAssociation for Computational Linguistics.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[[arXiv](https://arxiv.org/abs/2003.06658)] [[Poster](https://www.shininglab.ai/assets/posters/Revisit%20Systematic%20Generalization%20via%20Meaningful%20Learning.pdf)]  ## Directory + **main/config.py** - Configurations + **main/res** - Resources including model check points, datasets, experiment records, and results + **main/src** - Source code including model structures and utility functions ``` Systematic-Generalization-via-Meaningful-Learning ├── README.md ├── main │   ├── config.py │   ├── res │   │   ├── check_points │   │   ├── data │   │   │   ├── scan │   │   │   ├── geography │   │   │   ├── advising │   │   │   ├── geo_vars.txt │   │   │   ├── adv_vars.txt │   │   │   ├── iwslt14 │   │   │   ├── iwslt15 │   │   │   ├── prepare-iwslt14.sh │   │   │   └── prepare-iwslt15.sh │   │   ├── log │   │   └── result │   ├── src │   │   ├── models │   │   └── utils │   └── train.py └── requirements.txt ```  ## Dependencies + python >= 3.10.6 + tqdm >= 4.64.1 + numpy >= 1.23.4 + torch >= 1.13.0  ## Data All datasets can be downloaded [here](https://drive.google.com/drive/folders/19vFBn5C-nTdjxMeuMgw-BvsPNsLF6DpV?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing) and should be placed under **main/res/data** according to specific tasks.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease refer to [text2sql-data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data) for details. * main/res/data/scan * main/res/data/geography * main/res/data/advising  ### Notes + main/res/data/iwslt14 - both vocabulary augmentation set and the entire dataset for IWSLT14 + main/res/data/iwslt15 - both vocabulary augmentation set and the entire dataset for IWSLT15 + main/res/data/prepare-iwslt14.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT14 + main/res/data/prepare-iwslt15.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT15 + main/res/data/geo_vars.txt - the entity augmentation set for Grography + main/res/data/adv_vars.txt - the entity augmentation set for Advising  ## Setup Please ensure required packages are already installed.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mA virtual environment is recommended. ``` $ cd Systematic-Generalization-via-Meaningful-Learning $ cd main $ pip install pip --upgrade $ pip install -r requirements.txt ```  ## Run Before training, please double check **config.py** to ensure training configurations. ``` $ vim config.py $ python train.py ```  ## Outputs If everything goes well, there should be a similar progressing shown as below. ``` Initialize...\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m*Configuration* model name: bi_lstm_rnn_att trainable parameters:5,027,337 ...\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTraining...\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLoss:1.7061: 100%|██████████| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use [fairseq](https://github.com/facebookresearch/fairseq) for NMT tasks in Section 4.1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease find the example pipeline shown below.  ### Models + LSTM - lstm_luong_wmt_en_de + Transformer - transformer_iwslt_de_en + Dynamic Conv. - lightconv_iwslt_de_en  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \\     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\     --destdir data-bin/iwslt14.tokenized.de-en \\     --workers 20 ```  ### Training LSTM ``` fairseq-train \\     data-bin/iwslt14.tokenized.de-en \\     -s en -t de \\     --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \\     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\     --dropout 0.2 --weight-decay 0.0 \\     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \\     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\     --max-tokens 32768 \\     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \\     data-bin/iwslt14.tokenized.de-en \\     -s en -t de \\     --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \\     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\     --max-tokens 32768 \\     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \\     data-bin/iwslt14.tokenized.de-en \\     -s en -t de \\     --arch lightconv_iwslt_de_en \\     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\     --dropout 0.1 --weight-decay 0.0 \\     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\     --max-tokens 32768 \\     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \\     --path checkpoints/checkpoint_best.pt \\     -s en -t de \\     --batch-size 128 --beam 5 --lenpen 0.6 \\     --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \\     --path checkpoints/checkpoint_best.pt \\     -s en -t de \\     --batch-size 128 --beam 5 --lenpen 0.6 \\     --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = \"Revisit Systematic Generalization via Meaningful Learning\",     author = \"Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan\",     booktitle = \"Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP\",     month = dec,     year = \"2022\",     address = \"Abu Dhabi, United Arab Emirates (Hybrid)\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2022.blackboxnlp-1.6\",     pages = \"62--79\",     abstract = \"Humans can systematically generalize to novel compositions of existing concepts.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRecent studies argue that neural networks appear inherently ineffective in such cognitive capacity, leading to a pessimistic view and a lack of attention to optimistic results.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe revisit this controversial topic from the perspective of meaningful learning, an exceptional capability of humans to learn novel concepts by connecting them with known ones.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe reassess the compositional skills of sequence-to-sequence models conditioned on the semantic links between new and old concepts.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOur observations suggest that models can successfully one-shot generalize to novel concepts and compositions through semantic linking, either inductively or deductively.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe demonstrate that prior knowledge plays a key role as well.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn addition to synthetic tests, we further conduct proof-of-concept experiments in machine translation and semantic parsing, showing the benefits of meaningful learning in applications.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe hope our positive findings will encourage excavating modern neural networks{'} potential in systematic generalization through more advanced learning schemes.\", } ```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 3 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# NELA-GT-2019  This repository contain examples of how to use the NELA-GT-2019 data set with Python 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m__Download the dataset from here__: https://dataverse.harvard.edu/dataset.xhtml?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mpersistentId=doi:10.7910/DVN/O7FWPO  __For more details about this dataset, check the paper__: https://arxiv.org/abs/2003.08444  If you use this dataset in your work, please cite us as follows: <br> ``` @misc{     gruppi2020nelagt2019,     title={NELA-GT-2019: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles},     author={Maurício Gruppi and Benjamin D.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHorne and Sibel Adalı},     year={2020},     eprint={2003.08444},     archivePrefix={arXiv},     primaryClass={cs.CY} } ``` ## Data  Metadata|| ---|--- Dataset name|`NELA-GT-2019` Formats|`Sqlite3`,`JSON` No. of articles|`1118821` No. of sources|`261` Collection period|`2019-01-01` to `2019-12-31`  ### Fields  Each data point collected corresponds to an article and contains the fields described below.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m|Field | Type | Description| ---|---|--- `id` | string | ID of the article `date` | string | date of publication (`YYYY-MM-DD`) `source` | string | name of the source `title` | string | article's headline `content` | string | article's body text `author` | string | author who signed the article `published` | string | date time string as provided by source `published_utc` | integer | unix timestamp of publication `collection_utc` | integer | unix timestamp of collection date  ### Aggregated labels  We provide aggregated labels based on Media Bias/Fact Check reports, classifying each source as:  * _Reliable_ - class 0 * _Mixed_ - class 1 * _Unreliable_ - class 2  These labels can be found in `labels.csv`  __Note__: the labels used in this aggregation were collected from Media Bias/Fact Check on Mar 20, 2020.   ## Examples ###  load-sqlite3.py  * How to load the data from the Sqlite3 database using SQL queries\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m+ Loading data from single or multiple sources from the database   + Loading data from the database into a Pandas dataframe  Usage: ``` python3 load-sqlite3.py <path-to-database> ```  ###  load-json.py  * How to load NELA in JSON format with Python 3\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m+ Loading a single source's JSON   + Loading a directory of NELA JSON files - **WARNING**: this consumes a lot of memory  Usage: ``` python3 load-json.py <path-to-file> ```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m## Ontological knowledge infusion in embedding-Large Language Models  The code of this repository implements a novel approach to improve an embedding-Large Language  Model (embedding-LLM) of interest by infusing the knowledge formalized by a reference ontology: ontological  knowledge infusion aims at boosting the ability of the considered embedding-LLM to effectively model  the knowledge domain described by the infused ontology.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe ontological knowledge infusion approach is described into details by the article  [Towards Ontology-Enhanced Representation Learning for Large Language Models](https://arxiv.org/abs/2405.20527).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m`RESOURCE_FOLDER` where all the data / models generated to support  and evaluate ontological knowledge infusion will be stored: inside the resource-folder create three sub-folders  with the following names: `ONTO_FUSE_SYNTH_DATA`, `CHROMA_DB`, `EVAL_RESULT`. - Modify the line 3 of the file [constant_config.py](constant_config.py) by setting the value of `BASE_DATA_FOLDER_PATH`  equal to the absolute local path of the resource-folder.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe python scripts included in this repository  should be able to read files from and write files to the resorce-folder.  ### 2) Preload MONDO ontology Run the [onto/load.py](onto/load.py) script: in the resource-folder a MONDO ontology pickle object file  exploited to store the contents of the ontology to support ontological knowledge infusion, will be  created.  ### 3) Generate synthetic definitions of MONDO ontology concepts - Add your OpenAI access credentials to prompt GPT-3.5-turbo in the module [synth_data_gen/constants.py](synth_data_gen/constants.py).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe module [synth_data_gen/prompt_openai.py](synth_data_gen/prompt_openai.py) wraps interactions with OpenAI GPT-3.5-turbo model. - Run [synth_data_gen/generate_synth_data.py](synth_data_gen/generate_synth_data.py) script: this script will generate a synthetic  concept definition from each synonym of each concept of the MONDO ontology.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe synthetic concept  definitions generated by running this script are stored in a CSV file inside the `ONTO_FUSE_SYNTH_DATA`  folder (that in turns is a sub-folder of the resource-folder).  ### 4) Perform ontological knowledge infusion: fine-tuning embedding-LLMs by a contrastive objective Run [llm_train/train_and_evaluate_onto_knowldge_infusion.py](llm_train/train_and_evaluate_onto_knowldge_infusion.py)  by providing the following two command line arguments: - **ARGUMENT 1**: embedding-LLM base model identifier, to perform ontological knowledge infusion  on such embedding-LLM.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOne of: sapbert_cls, pubmedbert_mean, pubmedbert_cls, gist, gtebase. - **ARGUMENT 2**: name of the CSV file with synthetic definitions generated by running the script  [synth_data_gen/generate_synth_data.py](synth_data_gen/generate_synth_data.py), as described at  step 3 and stored in the `ONTO_FUSE_SYNTH_DATA` folder (that is a sub-folder of the resource-folder).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis script will perform the following actions: - read real definitions of concepts from the MONDO ontology, together with synthetically generated ones   (as described at step 3) - ontology-driven generation of positive pairs of definitions by synonym substitution - ontology-driven generation of hard-negative pairs of definitions by: (i) indexing all definitions  in a collection of the [Chroma DB embedding database](https://www.trychroma.com/); (ii) identifying  for each definition the associated hard-negative one(s), relying on the is-a relations specified by  the MONDO ontology - infuse disease knowledge from the MONDO ontology in the embedding-LLM selected through command-line  **ARGUMENT 1**: this is obtained by relying on a contrastive learning framework relying on  InfoNCE loss (for more details refer the [paper on ontological knowledge infusion](https://arxiv.org/abs/2405.20527))\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mEvaluation results are stored in .eval text  files in the `EVAL_RESULT` folder (that is a sub-folder of the resource-folder).    ### Reference publication  The code shared by this repository implements the ontological knowledge infusion approach presented by the following publication: - **TITLE**: Towards Ontology-Enhanced Representation Learning for Large Language Models - **AUTHORS**: Francesco Ronzano and Jay Nanavati - **AFFILIATION**: IQVIA - **ARXIV LINK**: [https://arxiv.org/abs/2405.20527](https://arxiv.org/abs/2405.20527) - **DATE**: 30 May 2024\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m  \u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Version](https://badge.fury.io/py/karateclub.svg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mstyle=plastic)  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[License](https://img.shields.io/github/license/benedekrozemberczki/karateclub.svg) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/karateclub.svg)](https://github.com/benedekrozemberczki/karateclub/archive/master.zip)  [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Arxiv](https://img.shields.io/badge/ArXiv-2003.04819-orange.svg)](https://arxiv.org/abs/2003.04819) [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[build badge](https://github.com/benedekrozemberczki/karateclub/workflows/CI/badge.svg)](https://github.com/benedekrozemberczki/karateclub/actions?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mquery=workflow%3ACI)  [!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[coverage badge](https://codecov.io/gh/benedekrozemberczki/karateclub/branch/master/graph/badge.svg)](https://codecov.io/github/benedekrozemberczki/karateclub?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mbranch=master) <p align=\"center\">   <img width=\"90%\" src=\"https://github.com/benedekrozemberczki/karateclub/blob/master/karatelogo.jpg?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31msanitize=true\" /> </p>  ------------------------------------------------------   **Karate Club** is an unsupervised machine learning extension library for [NetworkX](https://networkx.github.io/).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease look at the **[Documentation](https://karateclub.readthedocs.io/)**, relevant **[Paper](https://arxiv.org/abs/2003.04819)**, **[Promo Video](https://www.youtube.com/watch?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mv=t212-ntxu2U)**, and **[External Resources](https://karateclub.readthedocs.io/en/latest/notes/resources.html)**.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m*Karate Club* consists of state-of-the-art methods to do unsupervised learning on graph structured data.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo put it simply it is a Swiss Army knife for small-scale graph mining research.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFirst, it provides network embedding techniques at the node and graph level.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSecond, it includes a variety of overlapping and non-overlapping community detection methods.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mImplemented methods cover a wide range of network science ([NetSci](https://netscisociety.net/home), [Complenet](https://complenet.weebly.com/)), data mining ([ICDM](http://icdm2019.bigke.org/), [CIKM](http://www.cikm2019.net/), [KDD](https://www.kdd.org/kdd2020/)), artificial intelligence ([AAAI](http://www.aaai.org/Conferences/conferences.php), [IJCAI](https://www.ijcai.org/)) and machine learning ([NeurIPS](https://nips.cc/), [ICML](https://icml.cc/), [ICLR](https://iclr.cc/)) conferences, workshops, and pieces from prominent journals.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe newly introduced graph classification datasets are available at [SNAP](https://snap.stanford.edu/data/#disjointgraphs), [TUD Graph Kernel Datasets](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets), and [GraphLearning.io](https://chrsmrrs.github.io/datasets/).  --------------------------------------------------------------  **Citing**  If you find *Karate Club* and the new datasets useful in your research, please consider citing the following paper:  ```bibtex @inproceedings{karateclub,        title = {{Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs}},        author = {Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},        year = {2020},        pages = {3125–3132},        booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},        organization = {ACM}, } ``` ----------------------------------------------------------------  **A simple example**  *Karate Club* makes the use of modern community detection techniques quite easy (see [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor example, this is all it takes to use on a Watts-Strogatz graph [Ego-splitting](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf):  ```python import networkx as nx from karateclub import EgoNetSplitter  g = nx.newman_watts_strogatz_graph(1000, 20, 0.05)  splitter = EgoNetSplitter(1.0)  splitter.fit(g)  print(splitter.get_memberships()) ```  ----------------------------------------------------------------  **Models included**  In detail, the following community detection and embedding methods were implemented.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mid=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOnline Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and Févotte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor a quick start, check out our [examples](https://github.com/benedekrozemberczki/karateclub/tree/master/examples.py).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you notice anything unexpected, please open an [issue](https://github.com/benedekrozemberczki/karateclub/issues) and let us know.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/karateclub/issues).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe are motivated to constantly make Karate Club even better.   --------------------------------------------------------------------------------  **Installation**  Karate Club can be installed with the following pip command.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```sh $ pip install karateclub ```  As we create new releases frequently, upgrading the package casually might be beneficial.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m```sh $ pip install karateclub --upgrade ```  --------------------------------------------------------------------------------  **Running examples**  As part of the documentation we provide a number of use cases to show how the clusterings and embeddings can be utilized for downstream learning.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese can accessed [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) with detailed line-by-line explanations.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mBesides the case studies we provide synthetic examples for each model.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese can be tried out by running the example scripts.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIn order to run one of the examples, the Graph2Vec snippet:  ```sh $ cd examples/whole_graph_embedding/ $ python graph2vec_example.py ```  --------------------------------------------------------------------------------  **Running tests**  From the project's root-level directory:  ```sh $ pytest ```  --------------------------------------------------------------------------------  **License**  - [GNU General Public License v3.0](https://github.com/benedekrozemberczki/karateclub/blob/master/LICENSE)\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 3 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Ref-NMS Official codebase for AAAI 2021 paper [\"Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding\"](https://arxiv.org/abs/2009.01449).  ## Prerequisites The following dependencies should be enough.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee [environment.yml](environment.yml) for complete environment settings. - python 3.7.6 - pytorch 1.1.0 - torchvision 0.3.0 - tensorboard 2.1.0 - spacy 2.2.3  ## Data Preparation Follow instructions in `data/README.md` to setup `data` directory.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRun following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<dataset>_<split_by>.txt` - selected GloVe feature: `std_glove_<dataset>_<split_by>.npy` - referring expression database: `std_refdb_<dataset>_<split_by>.json` - critical objects database: `std_ctxdb_<dataset>_<split_by>.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset refcoco --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset refcoco --split-by unc ```  We use tensorboard to monitor the training process.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe log file can be found in `tb` folder.  ## Evaluate Recall **Save Ref-NMS proposals:** ``` PYTHONPATH=$PWD python tools/save_ref_nms_proposals.py --dataset refcoco --split-by unc --tid <tid> --m <loss_type> ``` `<loss_type>` can be either `att_vanilla` for binary XE loss or `att_rank` for rank loss.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Evaluate recall on referent object:** ``` PYTHONPATH=$PWD python tools/eval_proposal_hit_rate.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ``` `conf` parameter is the score threshold used to filter Ref-NMS proposals.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt should be picked properly so that the recall of the referent is high while the number of proposals per expression is around 8-10.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Evaluate recall on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset refcoco --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe provide altered version of [MAttNet](https://github.com/ChopinSharp/MAttNet) and [CM-A-E](https://github.com/ChopinSharp/CM-Erase-REG) for downstream REG task evaluation.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFirst, follow the README in each repository to reproduce the original reported results as baseline (c.f.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTable 2 in our paper).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThen, run the following commands to evaluate on REC and RES task: ``` # Evaluate REC performance python tools/extract_mrcn_ref_feats.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate RES performance python tools/run_propose_to_mask.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **MAttNet-style detection file** (note the MattNet-style detection files can be directly used to evaluate downstream REG task performance).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWith these files, one can easily reproduce our reported results.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[[Google Drive]](https://drive.google.com/drive/folders/1BPqWW0LrAEBFna7b-ORF2TcrY7K_DDvM?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31musp=sharing) [[Baidu Disk]](https://pan.baidu.com/s/1G4k7APKSUs-_5StXoYaNrA) (extraction code: 5a9r)  ## Citation ``` @inproceedings{chen2021ref,   title={Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding},   author={Chen, Long and Ma, Wenbo and Xiao, Jun and Zhang, Hanwang and Chang, Shih-Fu},   booktitle={AAAI},   year={2021} } ```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m \u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m####################################################################  Source code and datasets of Para-DPMM model for single cell transcriptomic clustering to reproduce results in paper \"[Parallel Clustering of Single Cell Transcriptomic Data with Split-Merge Sampling on Dirichlet Process Mixtures](https://arxiv.org/pdf/1812.10048.pdf)\", Author: Tiehang Duan; José P.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPinto; Xiaohui Xie;   # 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mData Preparation:  In the datasets folder, we included the mat files that are used in the paper's experiment part.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou can also prepare your own data following the procedures below.  ## 1.1  Download raw datasets from 10X genomics website (https://support.10xgenomics.com/single-cell-gene-expression/datasets) and store the files in the datasets folder;  ## 1.2  Follow the comments in \"data_preparation.m\" to modify the file names based on the downloaded data files;  ## 1.3  Follow the comments in \"data_preparation.m\" to set the number of cells (randomly selected) and top variable genes;  ## 1.4  Run \"data_preparation.m\";     # 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUsage (two options):  ## Option 1: Run Source Code   Enter the \"Para_DPMM_Source_Code\" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the \"eigen\" directory);  (2) Start Matlab, enter the \"main\" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is \"Mirkin's\" index and HI is \"Hubert's\" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/Para_DPMM  ### (2) Follow guidelines printed in the console (an example):        1> Please enter dataset path:  .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/datasets/data_matrix_1_S_Set.mat        2> Please enter number of processors:  16        3> Please set the value of alpha:  1        4> Please enter computing time limit (seconds):  40    ### (3) Result  The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is \"Mirkin's\" index and HI is \"Hubert's\" index\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m# 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mComparison Methods  In the paper, we performed comparison with several current widely used single cell clustering methods.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMost of the methods are available in the form of R package, and in the \"interface to comparison methods\" folders, we provide interface programs (written in R) to use these datasets with the available R packages for comparison.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease install the related R pacakges before using the interface programs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease feel free to use it for academic purposes.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote: The Para-DPMM project depend heavily on the open source Dirichlet Process Mixtures package(http://people.csail.mit.edu/jchang7/code.php) written by Jason Chang.  ###############################################################################\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m## Fully Convolutional Instance-aware Semantic Segmentation  The major contributors of this repository include [Haozhi Qi](https://github.com/Oh233), [Yi Li](https://github.com/liyi14), [Guodong Zhang](https://github.com/gd-zhang), [Haochen Zhang](https://github.com/Braininvat), [Jifeng Dai](https://github.com/daijifeng001), and [Yichen Wei](https://github.com/YichenWei).  ### Introduction  **FCIS** is a fully convolutional end-to-end solution for instance segmentation, which won the first place in COCO segmentation challenge 2016.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFCIS is initially described in a [CVPR 2017 spotlight paper](https://arxiv.org/abs/1611.07709).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is worth noticing that: * FCIS provides a simple, fast and accurate framework for instance segmentation. * Different from [MNC](https://github.com/daijifeng001/MNC), FCIS performs instance mask estimation and categorization jointly and simultanously, and estimates class-specific masks. * We did not exploit the various techniques & tricks in the Mask RCNN system, like increasing RPN anchor numbers (from 12 to 15), training on anchors out of image boundary, enlarging the image (shorter side from 600 to 800 pixels), utilizing FPN features and aligned ROI pooling.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThese techniques & tricks should be orthogonal to our simple baseline.   ### Resources  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mVisual results on the first 5k images from COCO test set of our ***COCO 2016 challenge entry***: [OneDrive](https://onedrive.live.com/?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mauthkey=%21ABB_CV2zvCEoNK0&id=F371D9563727B96F%2192190&cid=F371D9563727B96F). 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSlides in [ImageNet ILSVRC and COCO workshop 2016](http://image-net.org/challenges/ilsvrc+coco2016): [OneDrive](https://onedrive.live.com/?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mcid=f371d9563727b96f&id=F371D9563727B96F%2197213&authkey=%21AEYOyOirjIutSVk).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m<img src='data/readme_img.png' width='800'>   ### Disclaimer  This is an official implementation for [Fully Convolutional Instance-aware Semantic Segmentation](https://arxiv.org/abs/1611.07709) (FCIS) based on MXNet.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is worth noticing that:    * The original implementation is based on our internal Caffe version on Windows.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThere are slight differences in the final accuracy and running time due to the plenty details in platform switch\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m* The code is tested on official [MXNet@(commit 62ecb60)](https://github.com/dmlc/mxnet/tree/62ecb60) with the extra operators for FCIS\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m* We trained our model based on the ImageNet pre-trained [ResNet-v1-101](https://github.com/KaimingHe/deep-residual-networks) using a [model converter](https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef846/tools/caffe_converter).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe converted model produces slightly lower accuracy (Top-1 Error on ImageNet val: 24.0% v.s. 23.6%)\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m* This repository used code from [MXNet rcnn example](https://github.com/dmlc/mxnet/tree/master/example/rcnn) and [mx-rfcn](https://github.com/giorking/mx-rfcn).   ### License  © Microsoft, 2017.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mLicensed under an MIT license.  ### Citing FCIS  If you find FCIS useful in your research, please consider citing: ``` @inproceedings{li2016fully,   Author = {Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji and Yichen Wei}   Title = {Fully Convolutional Instance-aware Semantic Segmentation},   Conference = {CVPR},   year = {2017} } ```  ### Main Results  |                                 | training data  | testing data | mAP^r@0.5 | mAP^r@0.7 | time   | |:---------------------------------:|:----------------:|:--------------:|:-----------:|:-----------:|:--------:| | FCIS, ResNet-v1-101             | VOC 2012 train | VOC 2012 val | 66.0      | 51.9      |   0.23s    |  |                                 | <sub>training data</sub> | <sub>testing data</sub>  | <sub>mAP^r</sub>  | <sub>mAP^r@0.5</sub> | <sub>mAP^r@0.75</sub>| <sub>mAP^r@S</sub> | <sub>mAP^r@M</sub> | <sub>mAP^r@L</sub> | |:---------------------------------:|:---------------:|:---------------:|:------:|:---------:|:---------:|:-------:|:-------:|:-------:| | <sub>FCIS, ResNet-v1-101, OHEM </sub> | <sub>coco trainval35k</sub> | <sub>coco minival</sub> | 29.2 | 50.8 | 29.7 | 7.9 | 31.4 | 51.1 | | <sub>FCIS, ResNet-v1-101, OHEM </sub> | <sub>coco trainval35k</sub> | <sub>coco test-dev</sub>| 29.6 | 51.4 | 30.2 | 8.0 | 31.0 | 49.7 |  *Running time is counted on a single Maxwell Titan X GPU (mini-batch size is 1 in inference).*  ### Requirements: Software  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMXNet from [the offical repository](https://github.com/dmlc/mxnet).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe tested our code on [MXNet@(commit 62ecb60)](https://github.com/dmlc/mxnet/tree/62ecb60).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDue to the rapid development of MXNet, it is recommended to checkout this version if you encounter any issues.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe may maintain this repository periodically if MXNet adds important feature in future release.  2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPython packages might missing: cython, opencv-python >= 3.2.0, easydict.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor Windows users, Visual Studio 2015 is needed to compile cython module.   ### Requirements: Hardware  Any NVIDIA GPUs with at least 5GB memory should be OK  ### Installation  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mClone the FCIS repository, and we'll call the directory that you cloned FCIS as ${FCIS_ROOT}. ~~~ git clone https://github.com/msracver/FCIS.git ~~~ 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor Windows users, run ``cmd .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m\\init.bat``.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor Linux user, run `sh .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/init.sh`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe scripts will build cython module automatically and create some folders.  3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mInstall MXNet:    **Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.6).**   ***Quick start***   3.1 Install MXNet and all dependencies by   ```  pip install -r requirements.txt  ```  If there is no other error message, MXNet should be installed successfully.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/external/mxnet/$(YOUR_MXNET_PACKAGE)`, and modify `MXNET_VERSION` in `.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/experiments/fcis/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThus you can switch among different versions of MXNet quickly.  ### Demo  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo run the demo with our trained model (on COCO trainval35k), please download the model manually from [OneDrive](https://1drv.ms/u/s!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAm-5JzdW2XHzhqMJZmVOEDgfde8_tg) (Chinese users can also get it from [BaiduYun](https://pan.baidu.com/s/1geOHioV) with code `tmd4`), and put it under folder `model/`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMake sure it looks like this:  ```  .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/model/fcis_coco-0000.params  ``` 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRun  ```  python .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/fcis/demo.py  ```  ### Preparation for Training & Testing  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease download VOC 2012 dataset with additional annotations from [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMove ```inst, cls, img``` folders to VOCdevit and make sure it looks like this:     Please use the train&val split in this repo, which follows the protocal of [SDS](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds/).   ```  .data/VOCdevkit/VOCSDS/img/  .data/VOCdevkit/VOCSDS/inst/  .data/VOCdevkit/VOCSDS/cls/  ```   2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease download [COCO dataset](http://mscoco.org/dataset/#download) and annotations for the 5k image [minival](https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mdl=0) subset and [val2014 minus minival (val35k)](https://dl.dropboxusercontent.com/s/s3tw5zcg7395368/instances_valminusminival2014.json.zip?\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mdl=0).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMake sure it looks like this:  ```  .data/coco/  .data/coco/annotations/instances_valminusminival2014.json  .data/coco/annotations/instances_minival2014.json  ```  3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAm-5JzdW2XHzhqMEtxf1Ciym8uZ8sg), and put it under folder `.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/model`.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mMake sure it looks like this:  ```  .\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/model/pretrained_model/resnet_v1_101-0000.params  ```  ### Usage  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAll of our experiment settings (GPU #, dataset, etc.) are kept in yaml config files at folder `.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m/experiments/fcis/cfgs`. 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTwo config files have been provided so far: FCIS@COCO with OHEM and FCIS@VOC without OHEM.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe use 8 and 4 GPUs to train models on COCO and on VOC, respectively. 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo perform experiments, run the python scripts with the corresponding config file as input.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor example, to train and test FCIS on COCO with ResNet-v1-101, use the following command     ```     python experiments/fcis/fcis_end2end_train_test.py --cfg experiments/fcis/cfgs/resnet_v1_101_coco_fcis_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/fcis/coco/` or `output/fcis/voc/`. 4.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPlease find more details in config files and in our code.  ### Misc.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCode has been tested under:  - Ubuntu 14.04 with a Maxwell Titan X GPU and Intel Xeon CPU E5-2620 v2 @ 2.10GHz - Windows Server 2012 R2 with 8 K40 GPUs and Intel Xeon CPU E5-2650 v2 @ 2.60GHz - Windows Server 2012 R2 with 4 Pascal Titan X GPUs and Intel Xeon CPU E5-2650 v4 @ 2.30GHz\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# BOOKSUM: A Collection of Datasets for Long-form Narrative Summarization Authors: [Wojciech Kryściński](https://twitter.com/iam_wkr), [Nazneen Rajani](https://twitter.com/nazneenrajani), [Divyansh Agarwal](https://twitter.com/jigsaw2212), [Caiming Xiong](https://twitter.com/caimingxiong), [Dragomir Radev](http://www.cs.yale.edu/homes/radev/)  ## Introduction The majority of available text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWhile relevant, such datasets will offer limited challenges for future generations of text summarization systems.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe address these issues by introducing BookSum, a collection of datasets for long-form narrative summarization.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOur dataset covers source documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe domain and structure of our dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTo facilitate future work, we trained and evaluated multiple extractive and abstractive summarization models as baselines for our dataset.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPaper link: https://arxiv.org/abs/2105.08209  <p align=\"center\"><img src=\"misc/book_sumv4.png\"></p>   ## Table of Contents  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Updates](#updates) 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Citation](#citation) 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Legal Note](#legal-note) 4.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[License](#license) 5.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Usage](#usage) 6.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Get Involved](#get-involved)  ## Updates #### 4/15/2021 Initial commit   ## Citation ``` @article{kryscinski2021booksum,       title={BookSum: A Collection of Datasets for Long-form Narrative Summarization},        author={Wojciech Kry{\\'s}ci{\\'n}ski and Nazneen Rajani and Divyansh Agarwal and Caiming Xiong and Dragomir Radev},       year={2021},       eprint={2105.08209},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  ## Legal Note By downloading or using the resources, including any code or scripts, shared in this code repository, you hereby agree to the following terms, and your use of the resources is conditioned on and subject to these terms. 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou may only use the scripts shared in this code repository for research purposes.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou may not use or allow others to use the scripts for any other purposes and other uses are expressly prohibited. 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou will comply with all terms and conditions, and are responsible for obtaining all rights, related to the services you access and the data you collect. 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe do not make any representations or warranties whatsoever regarding the sources from which data is collected.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFurthermore, we are not liable for any damage, loss or expense of any kind arising from or relating to your use of the resources shared in this code repository or the data collected, regardless of whether such liability is based in tort, contract or otherwise.  ## License The code is released under the **BSD-3 License** (see `LICENSE.txt` for details).   ## Usage  #### 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mChapterized Project Guteberg Data The chapterized book text from Gutenberg, for the books we use in our work, has been made available through a public GCP bucket.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt can be fetched using: ``` gsutil cp gs://sfr-books-dataset-chapters-research/all_chapterized_books.zip . ```  or downloaded directly [here](https://storage.cloud.google.com/sfr-books-dataset-chapters-research/all_chapterized_books.zip).  #### 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mData Collection Data collection scripts for the summary text are organized by the different sources that we use summaries from.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mNote: At the time of collecting the data, all links in literature_links.tsv were working for the respective sources.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mFor each data source, the file `literature_links.tsv.pruned` contains the links for the books in our dataset.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRun `get_summaries.py` to collect the summaries from the links for each source.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mAdditionally, `get_works.py` can be used to collect an exhaustive set of summaries from that source.  ``` cd scripts/data_collection/cliffnotes/ python get_summaries.py ```  #### 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mData Cleaning   1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPerform basic cleanup operations and setup the summary text for splitting and further cleaning operations     ```     cd scripts/data_cleaning_scripts/     python basic_clean.py     ```  2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUsing a mapping of which chapter summaries are separable (`alignments/chapter_summary_aligned.jsonl.aggregate_splits`), the summary text is split into different sections (eg.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mChapters 1-3 summary separated into 3 different sections - Chapter 1 summary, Chapter 2 summary, Chapter 3 summary)     ```     python split_aggregate_chaps_all_sources.py     ```  3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe main cleanup script separates out analysis/commentary/notes from the summary text, removes prefixes etc.     ```     python clean_summaries.py     ```  #### Data Alignments Generating paragraph alignments from the chapter-level-summary-alignments, is performed individually for the train/test/val splits:  Gather the data from the summaries and book chapters into a single jsonl.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe script needs to be run separately for each split as the matched file ``` cd paragraph-level-summary-alignments python gather_data.py --matched_file /path/to/chapter_summary_aligned_{train/test/val}_split.jsonl --split_paragraphs ```  Generate alignments of the paragraphs with sentences from the summary using the bi-encoder **paraphrase-distilroberta-base-v1** ``` python align_data_bi_encoder_paraphrase.py --data_path /path/to/chapter_summary_aligned_{train/test/val}_split.jsonl.gathered --stable_alignment ```  ## Troubleshooting 1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe web archive links we collect the summaries from can often be unreliable, taking a long time to load.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mOne way to fix this is to use higher sleep timeouts when one of the links throws an exception, which has been implemented in some of the scripts. 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSome links that constantly throw errors are aggregated in a file called - 'section_errors.txt'.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis is useful to inspect which links are actually unavailable and re-running the data collection scripts for those specific links. 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSome paths in the provided files might throw errors depending on where the chapterized books have been downloaded.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIt is recommended to download them in booksum root directory for the scripts to work without requiring any modifications to the paths.   ## Get Involved Please create a GitHub issue if you have any questions, suggestions, requests or bug-reports.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mWe welcome PRs!\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# MeanSum: A Model for Unsupervised Neural Multi-Document Abstractive Summarization  Corresponding paper, accepted to ICML 2019: [https://arxiv.org/abs/1810.05739](https://arxiv.org/abs/1810.05739).  ## Requirements  Main requirements: - python 3 - torch 0.4.0  Rest of python packages in ```requirements.txt```.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTested in Docker, image = ```pytorch/pytorch:0.4_cuda9_cudnn7```.  ## General setup   Execute inside ```scripts/```:  ##### Create directories that aren't part of the Git repo (checkpoints/, outputs/):  ``` bash setup_dirs.sh ```  ##### Install python packages:  ``` bash install_python_pkgs.sh ```  ##### The default parameters for Tensorboard(x?)\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mcause texts from writer.add_text() to not show up.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mUpdate by:  ``` python update_tensorboard.py ```    ## Downloading data and pretrained models  ### Data  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload Yelp data: https://www.yelp.com/dataset and place files in ```datasets/yelp_dataset/``` 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mRun script to pre-process script and create train, val, test splits:     ```     bash scripts/preprocess_data.sh     ``` 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload subword tokenizer built on Yelp and place in  ```datasets/yelp_dataset/processed/```:  [link](https://s3.us-east-2.amazonaws.com/unsup-sum/subwordenc_32000_maxrevs260_fixed.pkl)  ### Pre-trained models  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload summarization model and place in  ```stable_checkpoints/sum/mlstm/yelp/batch_size_16-notes_cycloss_honly-sum_lr_0.0005-tau_2.0/```:  [link](https://s3.us-east-2.amazonaws.com/unsup-sum/sum_e0_tot3.32_r1f0.27.pt) 2.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload language model and place in  ```stable_checkpoints/lm/mlstm/yelp/batch_size_512-lm_lr_0.001-notes_data260_fixed/```:  [link](https://s3.us-east-2.amazonaws.com/unsup-sum/lm_e24_2.88.pt) 3.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mDownload classification model and place in  ```stable_checkpoints/clf/cnn/yelp/batch_size_256-notes_data260_fixed/```:  [link](https://s3.us-east-2.amazonaws.com/unsup-sum/clf_e10_l0.6760_a0.7092.pt)   ### Reference summaries  Download from: [link](https://s3.us-east-2.amazonaws.com/unsup-sum/summaries_0-200_cleaned.csv).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mEach row contains \"Input.business_id\", \"Input.original_review_\\<num\\>\\_id\",  \"Input.original_review__\\<num\\>\\_\", \"Answer.summary\", etc.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe \"Answer.summary\" is the reference summary written by the Mechanical Turk worker.   ## Running  Testing with pretrained mode.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis will output and save the automated metrics.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mResults will be in ```outputs/eval/yelp/n_docs_8/unsup_<run_name>```  NOTE: Unlike some conventions, 'gpus' option here represents the GPU ID (the one which is visible) and NOT the number of GPUs.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mHence, for a machine with a single GPU, you will give gpus=0 ``` python train_sum.py --mode=test --gpus=0 --batch_size=16 --notes=<run_name> ```  Training summarization model (using pre-trained language model and default hyperparams).\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe automated metrics results will be in ```checkpoints/sum/mlstm/yelp/<hparams>_<additional_notes>```.: ``` python train_sum.py --batch_size=16 --gpus=0,1,2,3 --notes=<additional_notes>  ```\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1079647 processing \u001b[31m# Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites  ## About  The Forschungszentrum Jülich Machine Learning Library  It is currently being developed and maintained at the [Applied Machine Learning](https://www.fz-juelich.de/en/inm/inm-7/research-groups/applied-machine-learning-aml) group at [Forschungszentrum Juelich](https://www.fz-juelich.de/en), Germany.   ## Overview  **PrettYharmonize** is a Python package developed to address data leakage in the harmonization of biomedical datasets with site-specific variability, particularly under scenarios where class balance differs across data collection sites.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mTraditional harmonization methods like ComBat, while widely used, often struggle with data leakage, leading to compromised model performance.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mPrettYharmonize introduces a novel approach that leverages \"pretending\" target labels to mitigate this issue, preserving biologically relevant signals without inadvertently introducing leakage.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThe methodology has been validated on synthetic benchmarks and real-world data, including MRI and clinical datasets, to ensure robustness in diverse site-target-dependence scenarios.  !\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m[Workflow of PrettYharmonize](figures/PrettYharmonize_workflow.png) *Figure 1: Workflow for the PrettYharmonize data harmonization approach, outlining key stages in data processing.*  For more details, see our paper on arXiv: [Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites](https://arxiv.org/abs/2410.19643).  ## Installation  To set up the environment for PrettYharmonize, follow these steps:  1.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31m**Clone the repository:**     ```bash    git clone https://github.com/juaml/PrettYharmonize.git    cd PrettYharmonize   ## Citation ```bibtex If you use PrettYharmonize in your work, please cite the following: @article{nieto2024impact,   title={Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites},   author={Nieto, Nicol{\\'a}s and Eickhoff, Simon B and Jung, Christian and Reuter, Martin and Diers, Kersten and Kelm, Malte and Lichtenberg, Artur and Raimondo, Federico and Patil, Kaustubh R},   journal={arXiv preprint arXiv:2410.19643},   year={2024} } ```  ## Licensing  preattyharmonize is released under the AGPL v3 license:  preattyharmonize, FZJuelich AML machine learning library.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mCopyright (C) 2020, authors of preattyharmonize.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mSee the GNU Affero General Public License for more details.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mYou should have received a copy of the GNU Affero General Public License along with this program.\u001b[0m ...\n",
      "Process-1079647 processing \u001b[31mIf not, see <http://www.gnu.org/licenses/>.\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_names:\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    ref_doc = webanno_tsv_read_file(file_path)\n",
    "    predicted_doc = call_serial(ref_doc)\n",
    "    # Verify\n",
    "    if ref_doc.text != predicted_doc.text:\n",
    "        #logging.warning('content changed')\n",
    "        pass\n",
    "    if len(ref_doc.sentences) == len(predicted_doc.sentences):\n",
    "        #logging.warning('sentences changed')\n",
    "        pass\n",
    "    if len(ref_doc.tokens) == len(predicted_doc.tokens):\n",
    "        #logging.warning('tokens changed')\n",
    "        pass\n",
    "    for s1, s2 in zip(ref_doc.sentences, predicted_doc.sentences):\n",
    "        if s1 == s2:\n",
    "            #logging.warning(f'sentence changed, \\n{s1}\\n{s2}')\n",
    "            pass\n",
    "\n",
    "    for t1, t2 in zip(ref_doc.tokens, predicted_doc.tokens):\n",
    "        if t1 == t2:\n",
    "            #logging.warning(f'token changed: \\n{t1}\\n{t2}')\n",
    "            pass\n",
    "\n",
    "    logging.warning(f\"Predicted {len(predicted_doc.annotations)} annotations\")\n",
    "    prediction_path = os.path.join(output_folder, file_name)\n",
    "    with open(prediction_path, 'w') as fd:\n",
    "        fd.write(predicted_doc.tsv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c80d22-87b1-4cf4-836e-795ced878a0d",
   "metadata": {},
   "source": [
    "# Scorer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0972bd0-a799-4cf1-b9f1-e1578895855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from webanno_tsv import webanno_tsv_read_file, Document, Annotation\n",
    "from typing import List, Union\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "LABELS = [\n",
    "    'CONFERENCE',\n",
    "    'DATASET',\n",
    "    'EVALMETRIC',\n",
    "    'LICENSE',\n",
    "    'ONTOLOGY',\n",
    "    'PROGLANG',\n",
    "    'PROJECT',\n",
    "    'PUBLICATION',\n",
    "    'SOFTWARE',\n",
    "    'WORKSHOP'\n",
    "]\n",
    "\n",
    "def to_char_bio(src_path: str, ref_path: str) -> List[List[str]]:\n",
    "    ref_doc = webanno_tsv_read_file(ref_path)\n",
    "    # Parse the WebAnno TSV file\n",
    "    doc = webanno_tsv_read_file(src_path)\n",
    "    # Initialize a list to store character-level BIO tags\n",
    "    bio_tags_list = []\n",
    "    for target_label in LABELS:\n",
    "        bio_tags = ['#'] * len(ref_doc.text)  # Default to '#' for all characters\n",
    "        # Pick interested sentences and default them to 'O'\n",
    "        for annotation in ref_doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "            sentences = ref_doc.annotation_sentences(annotation)\n",
    "            for sentence in sentences:\n",
    "                tokens = ref_doc.sentence_tokens(sentence)\n",
    "                start_char, end_char = tokens[0].start, tokens[-1].end\n",
    "                bio_tags[start_char:end_char] = ['O'] * (end_char-start_char)\n",
    "\n",
    "        for annotation in doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "\n",
    "            start_token, end_token = annotation.tokens[0], annotation.tokens[-1]\n",
    "            start_char = start_token.start\n",
    "            end_char = end_token.end\n",
    "            # Sanity check\n",
    "            if ref_doc.text[start_char:end_char] != annotation.text:\n",
    "                msg = f\"ERROR: src: {src_path}, annotated '{annotation.text}', text: '{ref_doc.text[start_char:end_char]}'\"\n",
    "                print(msg)\n",
    "\n",
    "            if 'I-' in bio_tags[start_char]:\n",
    "                # Overlapping, it's annotated by another annotations, we connect them as one annotations\n",
    "                pass\n",
    "            else:\n",
    "                if bio_tags[start_char] != '#':\n",
    "                    # Assign BIO tags to characters in the entity span\n",
    "                    bio_tags[start_char] = f'B-{label}'  # Beginning of the entity\n",
    "\n",
    "            for i in range(start_char + 1, end_char):\n",
    "                if bio_tags[i] != '#':\n",
    "                    bio_tags[i] = f'I-{label}'  # Inside the entity\n",
    "\n",
    "        # Remove unannotated sentences from bio list.\n",
    "        bio_tags = [x for x in filter(lambda x: x != '#', bio_tags)]\n",
    "        bio_tags_list.append(bio_tags)\n",
    "\n",
    "    return bio_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30596d64-3c78-4cf3-bf29-523fc98a9735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe1c599-cfe9-466d-b0a3-95d0bc4394f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeb3a41d-1c03-40de-a06b-4043c89ec832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    return reduce(lambda x, y: x + y, lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5deab33-5d5e-401b-b440-d2cc989e8e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ann/fiz-ddb/notebook/readme2kg-exp/src\n",
      "WARN: 231sm_Low_Resource_KBP_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: ARM-software_keyword-transformer_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: Cardio-AI_3d-mri-domain-adaptation_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: ChopinSharp_ref-nms_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: James-Durant_fisher-information_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: MELALab_nela-gt-2019_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: allenai_aspire_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: alpiges_LinConGauss_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: anonymous-submission-22_dejavu_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: aspiaspace_earthpt_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: benedekrozemberczki_karateclub_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: claws-lab_awesome-crowd-combat-misinformation_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: daijifeng001_TA-FCN_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: dennlinger_tsar-2022-shared-task_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: dylanashley_story-distiller_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: felixxu35_hamiltoniq_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: freedomintelligence_mllm-bench_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: idt-iti_lightweight-face-detector-pruning_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: iqvianlp_llm-onto-infuse_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: juaml_juharmonize_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: lojzezust_slr_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: lucy3_words_as_gatekeepers_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: microsoft_opendatasheets-framework_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: ml-jku_vnegnn_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: mmp2_megaman_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: nokia_codesearch_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: prasunroy_air-writing_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: qiantianwen_nuscenes-qa_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: salesforce_booksum_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: snap-research_test-time-aggregation-for-cf_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: sosuperic_MeanSum_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: stanfordhci_modelsketchbook_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: tiehangd_Para_DPMM_master_readme.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: uwnetworkslab_netcov_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: vaikkunth_PrivacyFL_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: wenzhengzhang_entqa_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: zju-vipa_awesome-neural-trees_main_README.md.tsv is missing, fill 'O' list as default prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      " {\n",
      "  \"overall_accuracy\": 0.9564671580140925,\n",
      "  \"overall_macro_precision\": 0.1912934316028185,\n",
      "  \"overall_macro_recall\": 0.2,\n",
      "  \"overall_macro_f1\": 0.19554985200670627,\n",
      "  \"CONFERENCE_macro_precision\": NaN,\n",
      "  \"CONFERENCE_macro_recall\": NaN,\n",
      "  \"CONFERENCE_macro_f1\": NaN,\n",
      "  \"DATASET_macro_precision\": 0.3265321047242943,\n",
      "  \"DATASET_macro_recall\": 0.3333333333333333,\n",
      "  \"DATASET_macro_f1\": 0.329897668920167,\n",
      "  \"EVALMETRIC_macro_precision\": NaN,\n",
      "  \"EVALMETRIC_macro_recall\": NaN,\n",
      "  \"EVALMETRIC_macro_f1\": NaN,\n",
      "  \"LICENSE_macro_precision\": NaN,\n",
      "  \"LICENSE_macro_recall\": NaN,\n",
      "  \"LICENSE_macro_f1\": NaN,\n",
      "  \"ONTOLOGY_macro_precision\": NaN,\n",
      "  \"ONTOLOGY_macro_recall\": NaN,\n",
      "  \"ONTOLOGY_macro_f1\": NaN,\n",
      "  \"PROGLANG_macro_precision\": NaN,\n",
      "  \"PROGLANG_macro_recall\": NaN,\n",
      "  \"PROGLANG_macro_f1\": NaN,\n",
      "  \"PROJECT_macro_precision\": NaN,\n",
      "  \"PROJECT_macro_recall\": NaN,\n",
      "  \"PROJECT_macro_f1\": NaN,\n",
      "  \"PUBLICATION_macro_precision\": 0.3143107801600548,\n",
      "  \"PUBLICATION_macro_recall\": 0.3333333333333333,\n",
      "  \"PUBLICATION_macro_f1\": 0.32354269226109844,\n",
      "  \"SOFTWARE_macro_precision\": NaN,\n",
      "  \"SOFTWARE_macro_recall\": NaN,\n",
      "  \"SOFTWARE_macro_f1\": NaN,\n",
      "  \"WORKSHOP_macro_precision\": NaN,\n",
      "  \"WORKSHOP_macro_recall\": NaN,\n",
      "  \"WORKSHOP_macro_f1\": NaN\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/ann/anaconda3/envs/mathd2/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "print(os.getcwd())\n",
    "ref_dir = '../results/Meta-Llama-3-8B-Instruct/test_unlabeled/'\n",
    "pred_dir = '../results/Meta-Llama-3-8B-Instruct/prompt-0/'\n",
    "score_dir = '../results/scores/'\n",
    "\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "os.makedirs(score_dir, exist_ok=True)\n",
    "\n",
    "ref_file_names = sorted([fp for fp in os.listdir(ref_dir) if os.path.isfile(f'{ref_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "if len(ref_file_names) == 0:\n",
    "    raise Exception(\"ERROR: No reference files found, configuration error?\")\n",
    "\n",
    "all_ref_bio_tags_list = []\n",
    "for ref_file_name in ref_file_names:\n",
    "    src_path = os.path.join(ref_dir, ref_file_name)\n",
    "    ref_path = src_path\n",
    "    all_ref_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "\n",
    "pred_file_names = sorted([fp for fp in os.listdir(pred_dir) if os.path.isfile(f'{pred_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "all_pred_bio_tags_list = []\n",
    "for idx, ref_file_name in enumerate(ref_file_names):\n",
    "    try:\n",
    "        src_path = os.path.join(pred_dir, ref_file_name)\n",
    "        ref_path = os.path.join(ref_dir, ref_file_name)\n",
    "        all_pred_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "    except FileNotFoundError:\n",
    "        nbr_labels = len(all_ref_bio_tags_list[idx])\n",
    "        assert nbr_labels == len(LABELS), \"ERROR: reference tags doesn't have ${len(LABELS)} labels.\"\n",
    "        pred = []\n",
    "        for label_idx in range(nbr_labels):\n",
    "            pred.append(['O'] * len(all_ref_bio_tags_list[idx][label_idx]))\n",
    "\n",
    "        print(f\"WARN: {ref_file_name} is missing, fill 'O' list as default prediction\")\n",
    "        all_pred_bio_tags_list.append(pred)\n",
    "# Sanity checking\n",
    "for idx, (ref_list, pred_list) in enumerate(zip(all_ref_bio_tags_list, all_pred_bio_tags_list)):\n",
    "    for label_idx, (ref, pred) in enumerate(zip(ref_list, pred_list)):\n",
    "        assert len(ref) == len(pred), f'ERROR: {ref_file_names[idx]}, label: {LABELS[label_idx]}, reference length: {len(ref)}, prediction length: {len(pred)}'\n",
    "\n",
    "scores = {}\n",
    "################################################################################\n",
    "# Consider whole dataset\n",
    "################################################################################\n",
    "ref_bio_tags_list = flatten(flatten(all_ref_bio_tags_list))\n",
    "pred_bio_tags_list = flatten(flatten(all_pred_bio_tags_list))\n",
    "\n",
    "accuracy = accuracy_score(ref_bio_tags_list, pred_bio_tags_list)\n",
    "scores['overall_accuracy'] = accuracy\n",
    "average = 'macro'\n",
    "ref_bio_tags_list = flatten(flatten(all_ref_bio_tags_list))\n",
    "pred_bio_tags_list = flatten(flatten(all_pred_bio_tags_list))\n",
    "\n",
    "f1 = f1_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "precision = precision_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "recall = recall_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "scores[f\"overall_{average}_precision\"] = precision\n",
    "scores[f\"overall_{average}_recall\"] = recall\n",
    "scores[f\"overall_{average}_f1\"] = f1\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# For each class\n",
    "################################################################################\n",
    "label_to_ref_bio_tags_list = defaultdict(list)\n",
    "label_to_pred_bio_tags_list = defaultdict(list)\n",
    "for ref_bio_tags_list, pred_bio_tags_list in zip(all_ref_bio_tags_list, all_pred_bio_tags_list):\n",
    "    if len(ref_bio_tags_list) != len(LABELS):\n",
    "        print('ERROR: ref bio tags list')\n",
    "    if len(pred_bio_tags_list) != len(LABELS):\n",
    "        print('ERROR: pred bio tags list')\n",
    "\n",
    "    for label, ref_bio_tags, pred_bio_tags in zip(LABELS, ref_bio_tags_list, pred_bio_tags_list):\n",
    "        label_to_ref_bio_tags_list[label].extend(ref_bio_tags)\n",
    "        label_to_pred_bio_tags_list[label].extend(pred_bio_tags)\n",
    "        if len(label_to_ref_bio_tags_list[label]) != len(label_to_pred_bio_tags_list[label]):\n",
    "            print('ERROR: label_to_ref_pred_bio_tags')\n",
    "\n",
    "\n",
    "for label in label_to_ref_bio_tags_list.keys():\n",
    "    ref_bio_tags_list = label_to_ref_bio_tags_list[label]\n",
    "    pred_bio_tags_list = label_to_pred_bio_tags_list[label]\n",
    "    accuracy = accuracy_score(ref_bio_tags_list, pred_bio_tags_list)\n",
    "    f1 = f1_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "    precision = precision_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "    recall = recall_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "    scores[f\"{label}_{average}_precision\"] = precision\n",
    "    scores[f\"{label}_{average}_recall\"] = recall\n",
    "    scores[f\"{label}_{average}_f1\"] = f1\n",
    "\n",
    "print(\"Scores:\\n\", json.dumps(scores, indent=2))\n",
    "\n",
    "with open(os.path.join(score_dir, 'Meta-Llama-3-8B-Instruct-scores.json'), 'w') as fd:\n",
    "    json.dump(scores, fd, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
