{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b957bb-3508-4c00-8abc-9547198a8549",
   "metadata": {},
   "source": [
    "## Read/Write with zongxiong's webanno parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2402c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from seqeval) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64156087-5576-4ff5-994a-b47b96c01d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from webanno_tsv import webanno_tsv_read_file, webanno_tsv_write, Annotation\n",
    "from dataclasses import dataclass, replace\n",
    "import os\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9568d7b-38cb-48c7-923c-06d22b6041d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Read the WebAnno TSV file\n",
    "file_name = '231sm_Low_Resource_KBP_master_README.md.tsv'\n",
    "input_file_path = f'../data/train/{file_name}'  # Replace with the path to the provided WebAnno TSV file\n",
    "ref_doc = webanno_tsv_read_file(input_file_path)\n",
    "print('number of ref doc annotations: ', len(ref_doc.annotations))\n",
    "# Step 2: Access and modify annotations\n",
    "# For example, let's add a new annotation or modify existing ones\n",
    "# Here, we assume that the layer name is 'NER' and the field is 'label'\n",
    "# layer  = ref_doc.annotations[0].layer\n",
    "# field_name = 'value'\n",
    "print(doc.layer_defs)\n",
    "print(doc.annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a1ca4a-e9b2-4c50-a969-73ec3ac9a908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  43\n",
      "number of predicted doc annotations:  44\n",
      "Modified annotations have been written to ../results/dummy/231sm_Reasoning_In_EE_main_README.md.tsv\n",
      "number of ref doc annotations:  6\n",
      "number of predicted doc annotations:  7\n",
      "Modified annotations have been written to ../results/dummy/231sm_Low_Resource_KBP_master_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "def add_one_more_annotations_in_files(folder_path=\"../data/train\"):\n",
    "    # Iterate through files in the folder\n",
    "    # output_path = '../results/dummy'\n",
    "    output_path = '../data/test'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            exported_doc = webanno_tsv_read_file(file_path)\n",
    "            tokens_to_annotate = exported_doc.tokens[0:18]\n",
    "            # Example: Add a new annotation for a specific token\n",
    "            # Find the token you want to annotate (e.g., the first token in the first sentence)\n",
    "            new_annotation = utils.make_annotation(tokens=tokens_to_annotate, label='SOFTWARE')\n",
    "            # Add the new annotation to the document\n",
    "            predicted_doc = utils.replace_webanno_annotations(exported_doc, annotations=[*exported_doc.annotations, new_annotation])\n",
    "            # Step 3: Write the modified document to a new TSV file\n",
    "            with open(os.path.join(output_path, file_name), \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(predicted_doc.tsv())\n",
    "\n",
    "            print('number of ref doc annotations: ', len(exported_doc.annotations))\n",
    "            print('number of predicted doc annotations: ', len(predicted_doc.annotations))\n",
    "            print(f\"Modified annotations have been written to {os.path.join(output_path, file_name)}\")\n",
    "\n",
    "add_one_more_annotations_in_files(folder_path='../../data/curated_tsvs/val_unlabeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3bed42f-6506-4617-9a98-49856791531e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  43\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/231sm_Reasoning_In_EE_main_README.md.tsv\n",
      "number of ref doc annotations:  6\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/231sm_Low_Resource_KBP_master_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "def rm_annotations_in_files(folder_path=\"../data/train\"):\n",
    "    # Iterate through files in the folder\n",
    "    output_path = '../results/empty'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            exported_doc = webanno_tsv_read_file(file_path)\n",
    "            predicted_doc = replace(exported_doc, annotations=[])\n",
    "            with open(os.path.join(output_path, file_name), \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(predicted_doc.tsv())\n",
    "\n",
    "            print('number of ref doc annotations: ', len(exported_doc.annotations))\n",
    "            print('number of predicted doc annotations: ', len(predicted_doc.annotations))\n",
    "            print(f\"Modified annotations have been written to {os.path.join(output_path, file_name)}\")\n",
    "            \n",
    "rm_annotations_in_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c6f3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 44 annotations\n",
      "Annotation(tokens=[Token(sentence_idx=20, idx=98, start=9756, end=9769, text='International'), Token(sentence_idx=20, idx=99, start=9770, end=9775, text='Joint'), Token(sentence_idx=20, idx=100, start=9776, end=9786, text='Conference'), Token(sentence_idx=20, idx=101, start=9787, end=9789, text='on'), Token(sentence_idx=20, idx=102, start=9790, end=9797, text='Natural'), Token(sentence_idx=20, idx=103, start=9798, end=9806, text='Language'), Token(sentence_idx=20, idx=104, start=9807, end=9817, text='Processing')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='CONFERENCE', label_id=14)\n",
      "Predicted 0 annotations\n",
      "Predicted 7 annotations\n",
      "Annotation(tokens=[Token(sentence_idx=1, idx=105, start=475, end=482, text='TAC-KBP'), Token(sentence_idx=1, idx=106, start=482, end=483, text='-'), Token(sentence_idx=1, idx=107, start=483, end=487, text='2017'), Token(sentence_idx=1, idx=108, start=488, end=493, text='Event'), Token(sentence_idx=1, idx=109, start=494, end=499, text='Track'), Token(sentence_idx=1, idx=110, start=500, end=504, text='Data')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='DATASET', label_id=4)\n",
      "Predicted 0 annotations\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "def verify(ref_doc, predicted_doc):\n",
    "    assert ref_doc.text == predicted_doc.text, 'content changed'\n",
    "    assert len(ref_doc.sentences) == len(predicted_doc.sentences), 'sentences changed'\n",
    "    assert len(ref_doc.tokens) == len(predicted_doc.tokens), 'tokens changed'\n",
    "    for s1, s2 in zip(ref_doc.sentences, predicted_doc.sentences):\n",
    "        assert s1 == s2, f'sentence changed, \\n{s1}\\n{s2}'\n",
    "\n",
    "    for t1, t2 in zip(ref_doc.tokens, predicted_doc.tokens):\n",
    "        assert t1 == t2, f'token changed: \\n{t1}\\n{t2}'\n",
    "\n",
    "    print(f\"Predicted {len(predicted_doc.annotations)} annotations\")\n",
    "    if len(predicted_doc.annotations) > 0:\n",
    "        print(predicted_doc.annotations[-1])\n",
    "\n",
    "for file_path in os.listdir('../data/train'):\n",
    "    ref_doc = webanno_tsv_read_file(f'../data/train/{file_path}')\n",
    "    dummy_predicted_doc = webanno_tsv_read_file(f'../results/dummy/{file_path}')\n",
    "    empty_predicted_doc = webanno_tsv_read_file(f'../results/empty/{file_path}')\n",
    "    verify(ref_doc, dummy_predicted_doc)\n",
    "    verify(ref_doc, empty_predicted_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f26a1f9-5a80-454c-95c7-a2affadc14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all 10 entity types\n",
    "label_list = [\n",
    "    'CONFERENCE', 'DATASET', 'EVALMETRIC', 'LICENSE', 'ONTOLOGY', \n",
    "    'PROGLANG', 'PROJECT', 'PUBLICATION', 'SOFTWARE', 'WORKSHOP'\n",
    "]\n",
    "def dummy_whole_sent(folder_path=\"../data/\"):\n",
    "    # Define output folder\n",
    "    output_path = folder_path + \"pred/\"\n",
    "    os.makedirs(output_path, exist_ok=True)  # Create output folder if it doesn't exist\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".tsv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Read the WebAnno TSV file\n",
    "            doc = webanno_tsv_read_file(file_path)\n",
    "            layer  = doc.annotations[0].layer\n",
    "\n",
    "            # Create dummy annotations for every sentence and every label\n",
    "            new_annotations = []\n",
    "            for sentence in doc.sentences:\n",
    "                # Get the tokens in the sentence\n",
    "                sentence_tokens = doc.sentence_tokens(sentence)\n",
    "                #if not sentence_tokens:\n",
    "                #    continue  # Skip empty sentences\n",
    "\n",
    "                # Create annotations for every label\n",
    "                #for idx, lb in enumerate(label_list):\n",
    "                for lb in [\"DATASET\"]:\n",
    "                    new_anno = Annotation(\n",
    "                                    tokens=sentence_tokens,\n",
    "                                    layer=layer,  # Assuming the layer is \"NER\"\n",
    "                                    field=\"value\",  # Assuming the field is \"label\"\n",
    "                                    label=lb,\n",
    "                                    label_id=-1  # Use a dummy label ID\n",
    "                                )\n",
    "                    new_annotations.append(new_anno)\n",
    "\n",
    "            # Create a new document with the dummy annotations\n",
    "            new_doc = replace(doc, annotations=new_annotations)\n",
    "            output_file_path = output_path + file_name\n",
    "            print(f\"Predictions written to {output_file_path}\")\n",
    "            # Write the predictions to a WebAnno TSV file\n",
    "            with open(output_file_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(new_doc.tsv())\n",
    "\n",
    "            written_doc = webanno_tsv_read_file(output_file_path)\n",
    "            verify(doc, written_doc)\n",
    "            \n",
    "# Run the function\n",
    "dummy_whole_sent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228d89e-81f3-4c01-ac3d-1f01c830773b",
   "metadata": {},
   "source": [
    "## Try BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24be6bc7-a5aa-4603-8dee-92dd4749dc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "\n",
    "LABELS = [\n",
    "    'CONFERENCE',\n",
    "    'DATASET',\n",
    "    'EVALMETRIC',\n",
    "    'LICENSE',\n",
    "    'ONTOLOGY',\n",
    "    'PROGLANG',\n",
    "    'PROJECT',\n",
    "    'PUBLICATION',\n",
    "    'SOFTWARE',\n",
    "    'WORKSHOP'\n",
    "]\n",
    "\n",
    "\n",
    "def to_char_bio(src_path: str, ref_path: str) -> List[List[str]]:\n",
    "    ref_doc = webanno_tsv_read_file(ref_path)\n",
    "\n",
    "    # Parse the WebAnno TSV file\n",
    "    doc = webanno_tsv_read_file(src_path)\n",
    "    # Initialize a list to store character-level BIO tags\n",
    "    bio_tags_list = []\n",
    "    for target_label in LABELS:\n",
    "        bio_tags = ['#'] * len(doc.text)  # Default to '#' for all characters\n",
    "        # Pick interested sentences and default to 'O'\n",
    "        for annotation in ref_doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "            sentences = doc.annotation_sentences(annotation)\n",
    "            for sentence in sentences:\n",
    "                tokens = doc.sentence_tokens(sentence)\n",
    "                start_char, end_char = tokens[0].start, tokens[-1].end\n",
    "                bio_tags[start_char:end_char] = ['O'] * (end_char-start_char)\n",
    "\n",
    "        for annotation in doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "\n",
    "            start_token, end_token = annotation.tokens[0], annotation.tokens[-1]\n",
    "            start_char = start_token.start\n",
    "            end_char = end_token.end\n",
    "            # Sanity check\n",
    "            if ref_doc.text[start_char:end_char] != annotation.text:\n",
    "                msg = f\"ERROR: src: {src_path}, annotated '{annotation.text}', text: '{ref_doc.text[start_char:end_char]}'\"\n",
    "                print(msg)\n",
    "\n",
    "            # Assign BIO tags to characters in the entity span\n",
    "            if 'I-' in bio_tags[start_char]:\n",
    "                # It's inside other ENTITY, skip it\n",
    "                pass\n",
    "            else:\n",
    "                bio_tags[start_char] = f'B-{label}'  # Beginning of the entity\n",
    "\n",
    "            for i in range(start_char + 1, end_char):\n",
    "                bio_tags[i] = f'I-{label}'  # Inside the entity\n",
    "\n",
    "        # Remove unannotated sentences from bio list.\n",
    "        bio_tags = [x for x in filter(lambda x: x != '#', bio_tags)]\n",
    "        if len(bio_tags) > 0:\n",
    "            bio_tags_list.append(bio_tags)\n",
    "\n",
    "    return bio_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbccf20d-5025-46e6-8b8d-09699fcb9b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_dir = '../data/train'\n",
    "pred_dir = '../results/dummy'\n",
    "\n",
    "ref_file_names = sorted([fp for fp in os.listdir(ref_dir) if os.path.isfile(f'{ref_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "all_ref_bio_tags_list = []\n",
    "for ref_file_name in ref_file_names:\n",
    "    src_path = os.path.join(ref_dir, ref_file_name)\n",
    "    ref_path = src_path\n",
    "    all_ref_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "\n",
    "pred_file_names = sorted([fp for fp in os.listdir(pred_dir) if os.path.isfile(f'{pred_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "all_pred_bio_tags_list = []\n",
    "for idx, ref_file_name in enumerate(ref_file_names):\n",
    "    try:\n",
    "        src_path = os.path.join(pred_dir, ref_file_name)\n",
    "        ref_path = os.path.join(ref_dir, ref_file_name)\n",
    "        all_pred_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "    except FileNotFoundError:\n",
    "        nbr_labels = len(all_ref_bio_tags_list[idx])\n",
    "        pred = []\n",
    "        for label_idx in range(nbr_labels):\n",
    "            pred.append(['O'] * len(all_ref_bio_tags_list[idx][label_idx]))\n",
    "        print(f\"WARN: {ref_file_name} is missing, fill 'O' list as default prediction\")\n",
    "        all_pred_bio_tags_list.append(pred)\n",
    "\n",
    "# Sanity checking\n",
    "for ref_list, pred_list in zip(all_ref_bio_tags_list, all_pred_bio_tags_list):\n",
    "    for ref, pred in zip(ref_list, pred_list):\n",
    "        # print(len(ref), len(pred))\n",
    "        assert len(ref) == len(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
