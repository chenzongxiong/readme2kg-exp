{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b957bb-3508-4c00-8abc-9547198a8549",
   "metadata": {},
   "source": [
    "## Read/Write with zongxiong's webanno parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2402c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from seqeval) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64156087-5576-4ff5-994a-b47b96c01d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from webanno_tsv import webanno_tsv_read_file, webanno_tsv_write, Annotation\n",
    "from dataclasses import dataclass, replace\n",
    "import os\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9568d7b-38cb-48c7-923c-06d22b6041d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  40\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read the WebAnno TSV file\n",
    "file_name = '231sm_Low_Resource_KBP_master_README.md.tsv'\n",
    "input_file_path = f'../data/train/{file_name}'  # Replace with the path to the provided WebAnno TSV file\n",
    "ref_doc = webanno_tsv_read_file(input_file_path)\n",
    "print('number of ref doc annotations: ', len(ref_doc.annotations))\n",
    "# Step 2: Access and modify annotations\n",
    "# For example, let's add a new annotation or modify existing ones\n",
    "# Here, we assume that the layer name is 'NER' and the field is 'label'\n",
    "# layer  = ref_doc.annotations[0].layer\n",
    "# field_name = 'value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9a1ca4a-e9b2-4c50-a969-73ec3ac9a908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  40\n",
      "number of predicted doc annotations:  41\n",
      "Modified annotations have been written to ../results/231sm_Low_Resource_KBP_master_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "# Example: Add a new annotation for a specific token\n",
    "# Find the token you want to annotate (e.g., the first token in the first sentence)\n",
    "tokens_to_annotate = ref_doc.tokens[0:18]\n",
    "\n",
    "# Create a new annotation for the token\n",
    "new_annotation = utils.make_annotation(tokens=tokens_to_annotate, label='SOFTWARE')\n",
    "\n",
    "# Add the new annotation to the document\n",
    "predicted_doc = utils.replace_webanno_annotations(ref_doc, annotations=[*doc.annotations, new_annotation])\n",
    "print('number of ref doc annotations: ', len(ref_doc.annotations))\n",
    "print('number of predicted doc annotations: ', len(predicted_doc.annotations))\n",
    "\n",
    "# predicted_doc = utils.make_webanno_document(ref_doc.sentences, ref_doc.tokens, [*ref_doc.annotations, new_annotation])\n",
    "\n",
    "# Step 3: Write the modified document to a new TSV file\n",
    "output_file_path = f'../results/{file_name}'  # Replace with the desired output file path\n",
    "with open(output_file_path, 'w+', encoding='utf-8') as f:\n",
    "    f.write(predicted_doc.tsv())\n",
    "\n",
    "\n",
    "print(f\"Modified annotations have been written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15a2431c-ef9f-4459-a051-314b838ebe07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 41 annotations\n",
      "Annotation(tokens=[Token(sentence_idx=1, idx=1, start=0, end=1, text='#'), Token(sentence_idx=1, idx=2, start=2, end=18, text='Low_Resource_KBP'), Token(sentence_idx=1, idx=3, start=19, end=28, text='knowledge'), Token(sentence_idx=1, idx=4, start=29, end=34, text='graph'), Token(sentence_idx=1, idx=5, start=35, end=45, text='population'), Token(sentence_idx=1, idx=6, start=46, end=48, text='in'), Token(sentence_idx=1, idx=7, start=49, end=52, text='low'), Token(sentence_idx=1, idx=8, start=53, end=61, text='resource'), Token(sentence_idx=1, idx=9, start=62, end=72, text='conditions'), Token(sentence_idx=1, idx=10, start=75, end=78, text='The'), Token(sentence_idx=1, idx=11, start=79, end=83, text='file'), Token(sentence_idx=1, idx=12, start=84, end=85, text='\"'), Token(sentence_idx=1, idx=13, start=85, end=86, text='*'), Token(sentence_idx=1, idx=14, start=86, end=106, text='Few-Shot_ED.json.zip'), Token(sentence_idx=1, idx=14, start=86, end=97, text='Few-Shot_ED'), Token(sentence_idx=1, idx=15, start=106, end=107, text='*'), Token(sentence_idx=1, idx=16, start=107, end=108, text='\"'), Token(sentence_idx=1, idx=17, start=109, end=111, text='is')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='SOFTWARE', label_id=0)\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "def verify(ref_doc, predicted_doc):\n",
    "    assert ref_doc.text == predicted_doc.text, 'content changed'\n",
    "    assert len(ref_doc.sentences) == len(predicted_doc.sentences), 'sentences changed'\n",
    "    assert len(ref_doc.tokens) == len(predicted_doc.tokens), 'tokens changed'\n",
    "    for s1, s2 in zip(ref_doc.sentences, predicted_doc.sentences):\n",
    "        assert s1 == s2, f'sentence changed, \\n{s1}\\n{s2}'\n",
    "\n",
    "    for t1, t2 in zip(ref_doc.tokens, predicted_doc.tokens):\n",
    "        assert t1 == t2, f'token changed: \\n{t1}\\n{t2}'\n",
    "\n",
    "    print(f\"Predicted {len(predicted_doc.annotations)} annotations\")\n",
    "    print(predicted_doc.annotations[-1])\n",
    "\n",
    "verify(doc, predicted_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ee4ace5-b64c-4364-9b0b-a323548da9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', ['identifier', 'value'])]\n"
     ]
    }
   ],
   "source": [
    "print(doc.layer_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a58736ca-86d6-4cab-a414-8a40ad648c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation(tokens=[Token(sentence_idx=1, idx=1, start=0, end=1, text='#'), Token(sentence_idx=1, idx=2, start=2, end=18, text='Low_Resource_KBP'), Token(sentence_idx=1, idx=3, start=19, end=28, text='knowledge'), Token(sentence_idx=1, idx=4, start=29, end=34, text='graph'), Token(sentence_idx=1, idx=5, start=35, end=45, text='population'), Token(sentence_idx=1, idx=6, start=46, end=48, text='in'), Token(sentence_idx=1, idx=7, start=49, end=52, text='low'), Token(sentence_idx=1, idx=8, start=53, end=61, text='resource'), Token(sentence_idx=1, idx=9, start=62, end=72, text='conditions'), Token(sentence_idx=1, idx=10, start=75, end=78, text='The'), Token(sentence_idx=1, idx=11, start=79, end=83, text='file'), Token(sentence_idx=1, idx=12, start=84, end=85, text='\"'), Token(sentence_idx=1, idx=13, start=85, end=86, text='*'), Token(sentence_idx=1, idx=14, start=86, end=106, text='Few-Shot_ED.json.zip'), Token(sentence_idx=1, idx=14, start=86, end=97, text='Few-Shot_ED'), Token(sentence_idx=1, idx=15, start=106, end=107, text='*'), Token(sentence_idx=1, idx=16, start=107, end=108, text='\"'), Token(sentence_idx=1, idx=17, start=109, end=111, text='is'), Token(sentence_idx=1, idx=18, start=112, end=115, text='the'), Token(sentence_idx=1, idx=19, start=116, end=117, text='*'), Token(sentence_idx=1, idx=20, start=117, end=118, text='*'), Token(sentence_idx=1, idx=21, start=118, end=119, text='*'), Token(sentence_idx=1, idx=22, start=119, end=127, text='FewEvent'), Token(sentence_idx=1, idx=23, start=127, end=128, text='*'), Token(sentence_idx=1, idx=24, start=128, end=129, text='*'), Token(sentence_idx=1, idx=25, start=129, end=130, text='*'), Token(sentence_idx=1, idx=26, start=131, end=138, text='dataset'), Token(sentence_idx=1, idx=27, start=139, end=142, text='for'), Token(sentence_idx=1, idx=28, start=143, end=146, text='the'), Token(sentence_idx=1, idx=29, start=147, end=152, text='paper'), Token(sentence_idx=1, idx=30, start=153, end=161, text='accepted'), Token(sentence_idx=1, idx=31, start=162, end=164, text='by'), Token(sentence_idx=1, idx=32, start=165, end=169, text='WSDM'), Token(sentence_idx=1, idx=33, start=170, end=174, text='2020'), Token(sentence_idx=1, idx=34, start=175, end=176, text='*'), Token(sentence_idx=1, idx=35, start=176, end=177, text='*'), Token(sentence_idx=1, idx=36, start=177, end=178, text='*'), Token(sentence_idx=1, idx=37, start=178, end=179, text='['), Token(sentence_idx=1, idx=38, start=179, end=180, text='\"'), Token(sentence_idx=1, idx=39, start=180, end=193, text='Meta-Learning'), Token(sentence_idx=1, idx=40, start=194, end=198, text='with'), Token(sentence_idx=1, idx=41, start=199, end=219, text='Dynamic-Memory-Based'), Token(sentence_idx=1, idx=42, start=220, end=232, text='Prototypical'), Token(sentence_idx=1, idx=43, start=233, end=240, text='Network'), Token(sentence_idx=1, idx=44, start=241, end=244, text='for'), Token(sentence_idx=1, idx=45, start=245, end=253, text='Few-Shot'), Token(sentence_idx=1, idx=46, start=254, end=259, text='Event'), Token(sentence_idx=1, idx=47, start=260, end=269, text='Detection'), Token(sentence_idx=1, idx=48, start=269, end=270, text='\"'), Token(sentence_idx=1, idx=49, start=270, end=271, text=']'), Token(sentence_idx=1, idx=50, start=271, end=272, text='('), Token(sentence_idx=1, idx=51, start=272, end=277, text='https'), Token(sentence_idx=1, idx=52, start=277, end=278, text=':'), Token(sentence_idx=1, idx=53, start=278, end=279, text='/'), Token(sentence_idx=1, idx=54, start=279, end=280, text='/'), Token(sentence_idx=1, idx=55, start=280, end=289, text='arxiv.org'), Token(sentence_idx=1, idx=56, start=289, end=290, text='/'), Token(sentence_idx=1, idx=57, start=290, end=293, text='abs'), Token(sentence_idx=1, idx=58, start=293, end=294, text='/'), Token(sentence_idx=1, idx=59, start=294, end=304, text='1910.11621'), Token(sentence_idx=1, idx=60, start=304, end=305, text=')'), Token(sentence_idx=1, idx=61, start=305, end=306, text='*'), Token(sentence_idx=1, idx=62, start=306, end=307, text='*'), Token(sentence_idx=1, idx=63, start=307, end=308, text='*'), Token(sentence_idx=1, idx=64, start=311, end=312, text='#'), Token(sentence_idx=1, idx=65, start=312, end=313, text='#'), Token(sentence_idx=1, idx=66, start=314, end=320, text='Source'), Token(sentence_idx=1, idx=67, start=321, end=323, text='of'), Token(sentence_idx=1, idx=68, start=324, end=327, text='Raw'), Token(sentence_idx=1, idx=69, start=328, end=332, text='Data'), Token(sentence_idx=1, idx=70, start=333, end=334, text='*'), Token(sentence_idx=1, idx=71, start=335, end=337, text='We'), Token(sentence_idx=1, idx=72, start=338, end=343, text='first'), Token(sentence_idx=1, idx=73, start=344, end=349, text='scale'), Token(sentence_idx=1, idx=74, start=350, end=352, text='up'), Token(sentence_idx=1, idx=75, start=353, end=356, text='the'), Token(sentence_idx=1, idx=76, start=357, end=363, text='number'), Token(sentence_idx=1, idx=77, start=364, end=366, text='of'), Token(sentence_idx=1, idx=78, start=367, end=372, text='event'), Token(sentence_idx=1, idx=79, start=373, end=378, text='types'), Token(sentence_idx=1, idx=80, start=379, end=381, text='in'), Token(sentence_idx=1, idx=81, start=382, end=390, text='existing'), Token(sentence_idx=1, idx=82, start=391, end=399, text='datasets'), Token(sentence_idx=1, idx=83, start=399, end=400, text=','), Token(sentence_idx=1, idx=84, start=401, end=410, text='including'), Token(sentence_idx=1, idx=85, start=411, end=414, text='the'), Token(sentence_idx=1, idx=86, start=415, end=416, text='['), Token(sentence_idx=1, idx=87, start=416, end=419, text='ACE'), Token(sentence_idx=1, idx=88, start=419, end=420, text='-'), Token(sentence_idx=1, idx=89, start=420, end=424, text='2005'), Token(sentence_idx=1, idx=90, start=425, end=431, text='corpus'), Token(sentence_idx=1, idx=91, start=431, end=432, text=']'), Token(sentence_idx=1, idx=92, start=432, end=433, text='('), Token(sentence_idx=1, idx=93, start=433, end=437, text='http'), Token(sentence_idx=1, idx=94, start=437, end=438, text=':'), Token(sentence_idx=1, idx=95, start=438, end=439, text='/'), Token(sentence_idx=1, idx=96, start=439, end=440, text='/'), Token(sentence_idx=1, idx=97, start=440, end=462, text='projects.ldc.upenn.edu'), Token(sentence_idx=1, idx=98, start=462, end=463, text='/'), Token(sentence_idx=1, idx=99, start=463, end=466, text='ace'), Token(sentence_idx=1, idx=100, start=466, end=467, text='/'), Token(sentence_idx=1, idx=101, start=467, end=468, text=')'), Token(sentence_idx=1, idx=102, start=468, end=469, text=','), Token(sentence_idx=1, idx=103, start=470, end=473, text='and'), Token(sentence_idx=1, idx=104, start=474, end=475, text='['), Token(sentence_idx=1, idx=105, start=475, end=482, text='TAC-KBP'), Token(sentence_idx=1, idx=106, start=482, end=483, text='-'), Token(sentence_idx=1, idx=107, start=483, end=487, text='2017'), Token(sentence_idx=1, idx=108, start=488, end=493, text='Event'), Token(sentence_idx=1, idx=109, start=494, end=499, text='Track'), Token(sentence_idx=1, idx=110, start=500, end=504, text='Data'), Token(sentence_idx=1, idx=111, start=504, end=505, text=']'), Token(sentence_idx=1, idx=112, start=505, end=506, text='('), Token(sentence_idx=1, idx=113, start=506, end=511, text='https'), Token(sentence_idx=1, idx=114, start=511, end=512, text=':'), Token(sentence_idx=1, idx=115, start=512, end=513, text='/'), Token(sentence_idx=1, idx=116, start=513, end=514, text='/'), Token(sentence_idx=1, idx=117, start=514, end=526, text='tac.nist.gov'), Token(sentence_idx=1, idx=118, start=526, end=527, text='/'), Token(sentence_idx=1, idx=119, start=527, end=531, text='2017'), Token(sentence_idx=1, idx=120, start=531, end=532, text='/'), Token(sentence_idx=1, idx=121, start=532, end=535, text='KBP'), Token(sentence_idx=1, idx=122, start=535, end=536, text='/'), Token(sentence_idx=1, idx=123, start=536, end=541, text='Event'), Token(sentence_idx=1, idx=124, start=541, end=542, text='/'), Token(sentence_idx=1, idx=125, start=542, end=552, text='index.html'), Token(sentence_idx=1, idx=126, start=552, end=553, text=')')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='CONFERENCE', label_id=0)\n"
     ]
    }
   ],
   "source": [
    "print(doc.annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3bed42f-6506-4617-9a98-49856791531e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rm_annotations_in_files(folder_path=\"../data/\"):\n",
    "    # Iterate through files in the folder\n",
    "    output_path = folder_path+\"/test\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            exported_doc = webanno_tsv_read_file(file_path)\n",
    "            converted_doc = replace(exported_doc, annotations=[])\n",
    "            with open(os.path.join(output_path, file_name), \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(converted_doc.tsv())\n",
    "            \n",
    "rm_annotations_in_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f26a1f9-5a80-454c-95c7-a2affadc14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all 10 entity types\n",
    "label_list = [\n",
    "    'CONFERENCE', 'DATASET', 'EVALMETRIC', 'LICENSE', 'ONTOLOGY', \n",
    "    'PROGLANG', 'PROJECT', 'PUBLICATION', 'SOFTWARE', 'WORKSHOP'\n",
    "]\n",
    "def dummy_whole_sent(folder_path=\"../data/\"):\n",
    "    # Define output folder\n",
    "    output_path = folder_path + \"pred/\"\n",
    "    os.makedirs(output_path, exist_ok=True)  # Create output folder if it doesn't exist\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".tsv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Read the WebAnno TSV file\n",
    "            doc = webanno_tsv_read_file(file_path)\n",
    "            layer  = doc.annotations[0].layer\n",
    "\n",
    "            # Create dummy annotations for every sentence and every label\n",
    "            new_annotations = []\n",
    "            for sentence in doc.sentences:\n",
    "                # Get the tokens in the sentence\n",
    "                sentence_tokens = doc.sentence_tokens(sentence)\n",
    "                #if not sentence_tokens:\n",
    "                #    continue  # Skip empty sentences\n",
    "\n",
    "                # Create annotations for every label\n",
    "                #for idx, lb in enumerate(label_list):\n",
    "                for lb in [\"DATASET\"]:\n",
    "                    new_anno = Annotation(\n",
    "                                    tokens=sentence_tokens,\n",
    "                                    layer=layer,  # Assuming the layer is \"NER\"\n",
    "                                    field=\"value\",  # Assuming the field is \"label\"\n",
    "                                    label=lb,\n",
    "                                    label_id=-1  # Use a dummy label ID\n",
    "                                )\n",
    "                    new_annotations.append(new_anno)\n",
    "\n",
    "            # Create a new document with the dummy annotations\n",
    "            new_doc = replace(doc, annotations=new_annotations)\n",
    "            output_file_path = output_path + file_name\n",
    "            print(f\"Predictions written to {output_file_path}\")\n",
    "            # Write the predictions to a WebAnno TSV file\n",
    "            with open(output_file_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(new_doc.tsv())\n",
    "\n",
    "            written_doc = webanno_tsv_read_file(output_file_path)\n",
    "            verify(doc, written_doc)\n",
    "            \n",
    "# Run the function\n",
    "dummy_whole_sent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228d89e-81f3-4c01-ac3d-1f01c830773b",
   "metadata": {},
   "source": [
    "## Try BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24be6bc7-a5aa-4603-8dee-92dd4749dc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "\n",
    "LABELS = [\n",
    "    'CONFERENCE',\n",
    "    'DATASET',\n",
    "    'EVALMETRIC',\n",
    "    'LICENSE',\n",
    "    'ONTOLOGY',\n",
    "    'PROGLANG',\n",
    "    'PROJECT',\n",
    "    'PUBLICATION',\n",
    "    'SOFTWARE',\n",
    "    'WORKSHOP'\n",
    "]\n",
    "\n",
    "\n",
    "def to_char_bio(src_path: str, ref_path: str) -> List[List[str]]:\n",
    "    ref_doc = webanno_tsv_read_file(ref_path)\n",
    "\n",
    "    # Parse the WebAnno TSV file\n",
    "    doc = webanno_tsv_read_file(src_path)\n",
    "    # Initialize a list to store character-level BIO tags\n",
    "    bio_tags_list = []\n",
    "    for target_label in LABELS:\n",
    "        bio_tags = ['#'] * len(doc.text)  # Default to '#' for all characters\n",
    "        # Pick interested sentences and default to 'O'\n",
    "        for annotation in ref_doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "            sentences = doc.annotation_sentences(annotation)\n",
    "            for sentence in sentences:\n",
    "                tokens = doc.sentence_tokens(sentence)\n",
    "                start_char, end_char = tokens[0].start, tokens[-1].end\n",
    "                bio_tags[start_char:end_char] = ['O'] * (end_char-start_char)\n",
    "\n",
    "        for annotation in doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "\n",
    "            start_token, end_token = annotation.tokens[0], annotation.tokens[-1]\n",
    "            start_char = start_token.start\n",
    "            end_char = end_token.end\n",
    "            # Sanity check\n",
    "            if ref_doc.text[start_char:end_char] != annotation.text:\n",
    "                msg = f\"ERROR: src: {src_path}, annotated '{annotation.text}', text: '{ref_doc.text[start_char:end_char]}'\"\n",
    "                print(msg)\n",
    "\n",
    "            # Assign BIO tags to characters in the entity span\n",
    "            if 'I-' in bio_tags[start_char]:\n",
    "                # It's inside other ENTITY, skip it\n",
    "                pass\n",
    "            else:\n",
    "                bio_tags[start_char] = f'B-{label}'  # Beginning of the entity\n",
    "\n",
    "            for i in range(start_char + 1, end_char):\n",
    "                bio_tags[i] = f'I-{label}'  # Inside the entity\n",
    "\n",
    "        # Remove unannotated sentences from bio list.\n",
    "        bio_tags = [x for x in filter(lambda x: x != '#', bio_tags)]\n",
    "        if len(bio_tags) > 0:\n",
    "            bio_tags_list.append(bio_tags)\n",
    "\n",
    "    return bio_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dbccf20d-5025-46e6-8b8d-09699fcb9b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_dir = '../data/'\n",
    "pred_dir = '../results/'\n",
    "\n",
    "ref_file_names = sorted([fp for fp in os.listdir(ref_dir) if os.path.isfile(f'{ref_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "all_ref_bio_tags_list = []\n",
    "for ref_file_name in ref_file_names:\n",
    "    src_path = os.path.join(ref_dir, ref_file_name)\n",
    "    ref_path = src_path\n",
    "    all_ref_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "\n",
    "pred_file_names = sorted([fp for fp in os.listdir(pred_dir) if os.path.isfile(f'{pred_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "all_pred_bio_tags_list = []\n",
    "for idx, ref_file_name in enumerate(ref_file_names):\n",
    "    try:\n",
    "        src_path = os.path.join(pred_dir, ref_file_name)\n",
    "        ref_path = os.path.join(ref_dir, ref_file_name)\n",
    "        all_pred_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "    except FileNotFoundError:\n",
    "        nbr_labels = len(all_ref_bio_tags_list[idx])\n",
    "        pred = []\n",
    "        for label_idx in range(nbr_labels):\n",
    "            pred.append(['O'] * len(all_ref_bio_tags_list[idx][label_idx]))\n",
    "        print(f\"WARN: {ref_file_name} is missing, fill 'O' list as default prediction\")\n",
    "        all_pred_bio_tags_list.append(pred)\n",
    "\n",
    "# Sanity checking\n",
    "for ref_list, pred_list in zip(all_ref_bio_tags_list, all_pred_bio_tags_list):\n",
    "    for ref, pred in zip(ref_list, pred_list):\n",
    "        # print(len(ref), len(pred))\n",
    "        assert len(ref) == len(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
