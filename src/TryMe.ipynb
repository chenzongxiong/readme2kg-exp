{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b957bb-3508-4c00-8abc-9547198a8549",
   "metadata": {},
   "source": [
    "## Read/Write with zongxiong's webanno parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef159661-31dc-4a34-8b0b-12f762e22a97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: webanno_tsv 0.1.0\n",
      "Uninstalling webanno_tsv-0.1.0:\n",
      "  Would remove:\n",
      "    /opt/conda/lib/python3.10/site-packages/webanno_tsv-0.1.0.dist-info/*\n",
      "    /opt/conda/lib/python3.10/site-packages/webanno_tsv/*\n",
      "Proceed (Y/n)?   Successfully uninstalled webanno_tsv-0.1.0\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes | pip uninstall webanno_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64156087-5576-4ff5-994a-b47b96c01d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from webanno_tsv import webanno_tsv_read_file, webanno_tsv_write, Annotation\n",
    "from dataclasses import dataclass, replace\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9568d7b-38cb-48c7-923c-06d22b6041d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Read the WebAnno TSV file\n",
    "file_name = '231sm_Low_Resource_KBP_master_README.md.tsv'\n",
    "input_file_path = f'../data/{file_name}'  # Replace with the path to the provided WebAnno TSV file\n",
    "doc = webanno_tsv_read_file(input_file_path)\n",
    "\n",
    "# Step 2: Access and modify annotations\n",
    "# For example, let's add a new annotation or modify existing ones\n",
    "# Here, we assume that the layer name is 'NER' and the field is 'label'\n",
    "layer  = doc.annotations[0].layer\n",
    "field_name = 'value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a1ca4a-e9b2-4c50-a969-73ec3ac9a908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified annotations have been written to ../data/pred/231sm_Low_Resource_KBP_master_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "# Example: Add a new annotation for a specific token\n",
    "# Find the token you want to annotate (e.g., the first token in the first sentence)\n",
    "tokens_to_annotate = doc.tokens[0:18]\n",
    "\n",
    "# Create a new annotation for the token\n",
    "new_annotation = Annotation(\n",
    "    tokens=tokens_to_annotate,\n",
    "    layer=layer,\n",
    "    field=field_name,\n",
    "    label='SOFTWARE',  # Replace with the desired NER label\n",
    "    label_id=1  # Assign a unique label ID\n",
    ")\n",
    "\n",
    "# Add the new annotation to the document\n",
    "doc = replace(doc, annotations=[*doc.annotations, new_annotation])\n",
    "\n",
    "# Step 3: Write the modified document to a new TSV file\n",
    "output_file_path = f'../data/pred/{file_name}'  # Replace with the desired output file path\n",
    "with open(output_file_path, 'w+', encoding='utf-8') as f:\n",
    "    f.write(doc.tsv())\n",
    "\n",
    "print(f\"Modified annotations have been written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ee4ace5-b64c-4364-9b0b-a323548da9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity',\n",
       "  ['identifier', 'value'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.layer_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a58736ca-86d6-4cab-a414-8a40ad648c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Annotation(tokens=[Token(sentence_idx=1, idx=14, start=86, end=97, text='Few-Shot_ED')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='DATASET', label_id=-1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3bed42f-6506-4617-9a98-49856791531e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rm_annotations_in_files(folder_path=\"../data/\"):\n",
    "    # Iterate through files in the folder\n",
    "    output_path = folder_path+\"/unlabelled\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            exported_doc = webanno_tsv_read_file(file_path)\n",
    "            converted_doc = replace(exported_doc, annotations=[])\n",
    "            with open(os.path.join(output_path, file_name), \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(converted_doc.tsv())\n",
    "            \n",
    "rm_annotations_in_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a2431c-ef9f-4459-a051-314b838ebe07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify\n",
    "def verify(ref_doc, predicted_doc):\n",
    "    assert ref_doc.text == predicted_doc.text, 'content changed'\n",
    "    assert len(ref_doc.sentences) == len(predicted_doc.sentences), 'sentences changed'\n",
    "    assert len(ref_doc.tokens) == len(predicted_doc.tokens), 'tokens changed'\n",
    "    for s1, s2 in zip(ref_doc.sentences, predicted_doc.sentences):\n",
    "        assert s1 == s2, f'sentence changed, \\n{s1}\\n{s2}'\n",
    "\n",
    "    for t1, t2 in zip(ref_doc.tokens, predicted_doc.tokens):\n",
    "        assert t1 == t2, f'token changed: \\n{t1}\\n{t2}'\n",
    "\n",
    "    print(f\"Predicted {len(predicted_doc.annotations)} annotations\")\n",
    "    #print(predicted_doc.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f26a1f9-5a80-454c-95c7-a2affadc14ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions written to ../data/pred/231sm_Reasoning_In_EE_main_README.md.tsv\n",
      "Predicted 20 annotations\n",
      "Predictions written to ../data/pred/231sm_Low_Resource_KBP_master_README.md.tsv\n",
      "Predicted 4 annotations\n"
     ]
    }
   ],
   "source": [
    "# List of all 10 entity types\n",
    "label_list = [\n",
    "    'CONFERENCE', 'DATASET', 'EVALMETRIC', 'LICENSE', 'ONTOLOGY', \n",
    "    'PROGLANG', 'PROJECT', 'PUBLICATION', 'SOFTWARE', 'WORKSHOP'\n",
    "]\n",
    "def dummy_whole_sent(folder_path=\"../data/\"):\n",
    "    # Define output folder\n",
    "    output_path = folder_path + \"pred/\"\n",
    "    os.makedirs(output_path, exist_ok=True)  # Create output folder if it doesn't exist\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".tsv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Read the WebAnno TSV file\n",
    "            doc = webanno_tsv_read_file(file_path)\n",
    "            layer  = doc.annotations[0].layer\n",
    "\n",
    "            # Create dummy annotations for every sentence and every label\n",
    "            new_annotations = []\n",
    "            for sentence in doc.sentences:\n",
    "                # Get the tokens in the sentence\n",
    "                sentence_tokens = doc.sentence_tokens(sentence)\n",
    "                #if not sentence_tokens:\n",
    "                #    continue  # Skip empty sentences\n",
    "\n",
    "                # Create annotations for every label\n",
    "                #for idx, lb in enumerate(label_list):\n",
    "                for lb in [\"DATASET\"]:\n",
    "                    new_anno = Annotation(\n",
    "                                    tokens=sentence_tokens,\n",
    "                                    layer=layer,  # Assuming the layer is \"NER\"\n",
    "                                    field=\"value\",  # Assuming the field is \"label\"\n",
    "                                    label=lb,\n",
    "                                    label_id=-1  # Use a dummy label ID\n",
    "                                )\n",
    "                    new_annotations.append(new_anno)\n",
    "\n",
    "            # Create a new document with the dummy annotations\n",
    "            new_doc = replace(doc, annotations=new_annotations)\n",
    "            output_file_path = output_path + file_name\n",
    "            print(f\"Predictions written to {output_file_path}\")\n",
    "            # Write the predictions to a WebAnno TSV file\n",
    "            with open(output_file_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(new_doc.tsv())\n",
    "\n",
    "            written_doc = webanno_tsv_read_file(output_file_path)\n",
    "            verify(doc, written_doc)\n",
    "            \n",
    "# Run the function\n",
    "dummy_whole_sent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228d89e-81f3-4c01-ac3d-1f01c830773b",
   "metadata": {},
   "source": [
    "## Try BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24be6bc7-a5aa-4603-8dee-92dd4749dc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "\n",
    "LABELS = [\n",
    "    'CONFERENCE',\n",
    "    'DATASET',\n",
    "    'EVALMETRIC',\n",
    "    'LICENSE',\n",
    "    'ONTOLOGY',\n",
    "    'PROGLANG',\n",
    "    'PROJECT',\n",
    "    'PUBLICATION',\n",
    "    'SOFTWARE',\n",
    "    'WORKSHOP'\n",
    "]\n",
    "\n",
    "\n",
    "def to_char_bio(src_path: str, ref_path: str) -> List[List[str]]:\n",
    "    ref_doc = webanno_tsv_read_file(ref_path)\n",
    "\n",
    "    # Parse the WebAnno TSV file\n",
    "    doc = webanno_tsv_read_file(src_path)\n",
    "    # Initialize a list to store character-level BIO tags\n",
    "    bio_tags_list = []\n",
    "    for target_label in LABELS:\n",
    "        bio_tags = ['#'] * len(doc.text)  # Default to '#' for all characters\n",
    "        # Pick interested sentences and default to 'O'\n",
    "        for annotation in ref_doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "            sentences = doc.annotation_sentences(annotation)\n",
    "            for sentence in sentences:\n",
    "                tokens = doc.sentence_tokens(sentence)\n",
    "                start_char, end_char = tokens[0].start, tokens[-1].end\n",
    "                bio_tags[start_char:end_char] = ['O'] * (end_char-start_char)\n",
    "\n",
    "        for annotation in doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "\n",
    "            start_token, end_token = annotation.tokens[0], annotation.tokens[-1]\n",
    "            start_char = start_token.start\n",
    "            end_char = end_token.end\n",
    "            # Sanity check\n",
    "            if ref_doc.text[start_char:end_char] != annotation.text:\n",
    "                msg = f\"ERROR: src: {src_path}, annotated '{annotation.text}', text: '{ref_doc.text[start_char:end_char]}'\"\n",
    "                print(msg)\n",
    "\n",
    "            # Assign BIO tags to characters in the entity span\n",
    "            if 'I-' in bio_tags[start_char]:\n",
    "                # It's inside other ENTITY, skip it\n",
    "                pass\n",
    "            else:\n",
    "                bio_tags[start_char] = f'B-{label}'  # Beginning of the entity\n",
    "\n",
    "            for i in range(start_char + 1, end_char):\n",
    "                bio_tags[i] = f'I-{label}'  # Inside the entity\n",
    "\n",
    "        # Remove unannotated sentences from bio list.\n",
    "        bio_tags = [x for x in filter(lambda x: x != '#', bio_tags)]\n",
    "        if len(bio_tags) > 0:\n",
    "            bio_tags_list.append(bio_tags)\n",
    "\n",
    "    return bio_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbccf20d-5025-46e6-8b8d-09699fcb9b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553 553\n",
      "553 1466\n",
      "553 553\n",
      "1392 1392\n",
      "7459 10070\n",
      "3509 3509\n",
      "1641 1641\n",
      "1392 1392\n",
      "1256 1256\n"
     ]
    }
   ],
   "source": [
    "ref_dir = '../data/'\n",
    "pred_dir = '../data/pred/'\n",
    "\n",
    "ref_file_names = sorted([fp for fp in os.listdir(ref_dir) if os.path.isfile(f'{ref_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "all_ref_bio_tags_list = []\n",
    "for ref_file_name in ref_file_names:\n",
    "    src_path = os.path.join(ref_dir, ref_file_name)\n",
    "    ref_path = src_path\n",
    "    all_ref_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "\n",
    "pred_file_names = sorted([fp for fp in os.listdir(pred_dir) if os.path.isfile(f'{pred_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "all_pred_bio_tags_list = []\n",
    "for idx, ref_file_name in enumerate(ref_file_names):\n",
    "    try:\n",
    "        src_path = os.path.join(pred_dir, ref_file_name)\n",
    "        ref_path = os.path.join(ref_dir, ref_file_name)\n",
    "        all_pred_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "    except FileNotFoundError:\n",
    "        nbr_labels = len(all_ref_bio_tags_list[idx])\n",
    "        pred = []\n",
    "        for label_idx in range(nbr_labels):\n",
    "            pred.append(['O'] * len(all_ref_bio_tags_list[idx][label_idx]))\n",
    "        print(f\"WARN: {ref_file_name} is missing, fill 'O' list as default prediction\")\n",
    "        all_pred_bio_tags_list.append(pred)\n",
    "\n",
    "# Sanity checking\n",
    "for ref_list, pred_list in zip(all_ref_bio_tags_list, all_pred_bio_tags_list):\n",
    "    for ref, pred in zip(ref_list, pred_list):\n",
    "        print(len(ref), len(pred))\n",
    "        #assert len(ref) == len(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
