{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b957bb-3508-4c00-8abc-9547198a8549",
   "metadata": {},
   "source": [
    "# Read/Write with zongxiong's webanno parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2402c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from seqeval) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/readme/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64156087-5576-4ff5-994a-b47b96c01d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from webanno_tsv import webanno_tsv_read_file, webanno_tsv_write, Annotation\n",
    "from dataclasses import dataclass, replace\n",
    "import os\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de617cf-e8a3-4608-8337-2ba134308986",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9568d7b-38cb-48c7-923c-06d22b6041d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  6\n"
     ]
    }
   ],
   "source": [
    "# Read the WebAnno TSV file\n",
    "file_name = '231sm_Low_Resource_KBP_master_README.md.tsv'\n",
    "input_file_path = f'../data/train/{file_name}'  # Replace with the path to the provided WebAnno TSV file\n",
    "ref_doc = webanno_tsv_read_file(input_file_path)\n",
    "print('number of ref doc annotations: ', len(ref_doc.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4339a150-bed3-4aef-a013-f710b772011b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Annotation(tokens=[Token(sentence_idx=1, idx=14, start=86, end=97, text='Few-Shot_ED')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='DATASET', label_id=-1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the first annoatation\n",
    "ref_doc.annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aeb917b-0d34-42f9-8695-5a1298b551d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(idx=1, text='# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the first sentence\n",
    "ref_doc.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27114957-e271-4a18-996b-071aaa971994",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(sentence_idx=1, idx=1, start=0, end=1, text='#'),\n",
       " Token(sentence_idx=1, idx=2, start=2, end=18, text='Low_Resource_KBP'),\n",
       " Token(sentence_idx=1, idx=3, start=19, end=28, text='knowledge')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the first 3 tokens\n",
    "ref_doc.tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7413024-0e10-4e7b-adac-3015be91bafe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KBP'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the document by start, end positions\n",
    "ref_doc.text[15:18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a139ab06-3249-4675-848a-e598329d85e3",
   "metadata": {},
   "source": [
    "### Get all labeled sentences of each entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "572a021a-ae48-4e42-a539-c2da37740f97",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled sentences of CONFERENCE:\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n",
      "Labeled sentences of DATASET:\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n",
      "Labeled sentences of PUBLICATION:\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n"
     ]
    }
   ],
   "source": [
    "# List of all 10 entity types\n",
    "label_list = [\n",
    "    'CONFERENCE', 'DATASET', 'EVALMETRIC', 'LICENSE', 'ONTOLOGY', \n",
    "    'PROGLANG', 'PROJECT', 'PUBLICATION', 'SOFTWARE', 'WORKSHOP'\n",
    "]\n",
    "annotations_by_label = {lb:[] for lb in label_list}\n",
    "\n",
    "for anno in ref_doc.annotations:\n",
    "    annotations_by_label[anno.label].extend(ref_doc.annotation_sentences(annotation=anno))\n",
    "\n",
    "for lb in label_list:\n",
    "    la = annotations_by_label[lb]\n",
    "    if len(la) > 0:\n",
    "        print(f\"Labeled sentences of {lb}:\")\n",
    "        for sent in la:\n",
    "            print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed1232-99c5-44ef-84fe-b858f4d4a5c4",
   "metadata": {},
   "source": [
    "## Write a WebAnno TSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab77d34-2776-45f4-b885-a0b7d5d0c148",
   "metadata": {
    "tags": []
   },
   "source": [
    "### write to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a1ca4a-e9b2-4c50-a969-73ec3ac9a908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  1\n",
      "Modified annotations have been written to ../results/dummy/daijifeng001_TA-FCN_master_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  1\n",
      "Modified annotations have been written to ../results/dummy/conversationai_unhealthy-conversations_main_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  1\n",
      "Modified annotations have been written to ../results/dummy/OpenBioLink_ITO_master_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  1\n",
      "Modified annotations have been written to ../results/dummy/poloclub_diffusion-explainer_main_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "def add_one_more_annotations_in_files(folder_path=\"../data/val/\"): # data in\n",
    "    # Iterate through files in the folder\n",
    "    output_path = '../results/dummy'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            exported_doc = webanno_tsv_read_file(file_path)\n",
    "            tokens_to_annotate = exported_doc.tokens[0:18]\n",
    "            # Example: Add a new annotation for a specific token\n",
    "            # Find the token you want to annotate (e.g., the first token in the first sentence)\n",
    "            new_annotation = utils.make_annotation(tokens=tokens_to_annotate, label='SOFTWARE')\n",
    "            # Add the new annotation to the document\n",
    "            predicted_doc = utils.replace_webanno_annotations(exported_doc, annotations=[*exported_doc.annotations, new_annotation])\n",
    "            # Step 3: Write the modified document to a new TSV file\n",
    "            with open(os.path.join(output_path, file_name), \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(predicted_doc.tsv())\n",
    "\n",
    "            print('number of ref doc annotations: ', len(exported_doc.annotations))\n",
    "            print('number of predicted doc annotations: ', len(predicted_doc.annotations))\n",
    "            print(f\"Modified annotations have been written to {os.path.join(output_path, file_name)}\")\n",
    "\n",
    "add_one_more_annotations_in_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c440c376-a3b0-4de4-afe0-f3ad6033c00e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  43\n",
      "number of predicted doc annotations:  44\n",
      "Modified annotations have been written to ../results/dummy/231sm_Reasoning_In_EE_main_README.md.tsv\n",
      "number of ref doc annotations:  6\n",
      "number of predicted doc annotations:  7\n",
      "Modified annotations have been written to ../results/dummy/231sm_Low_Resource_KBP_master_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "add_one_more_annotations_in_files(\"../data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71752b3a-2ce5-4d5a-ba18-e37c2b9f91a9",
   "metadata": {},
   "source": [
    "### remove all labeled and write to new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3bed42f-6506-4617-9a98-49856791531e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  43\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/231sm_Reasoning_In_EE_main_README.md.tsv\n",
      "number of ref doc annotations:  6\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/231sm_Low_Resource_KBP_master_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "def rm_annotations_in_files(folder_path=\"../data/train\"):\n",
    "    # Iterate through files in the folder\n",
    "    output_path = '../results/empty'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            exported_doc = webanno_tsv_read_file(file_path)\n",
    "            predicted_doc = replace(exported_doc, annotations=[])\n",
    "            with open(os.path.join(output_path, file_name), \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(predicted_doc.tsv())\n",
    "\n",
    "            print('number of ref doc annotations: ', len(exported_doc.annotations))\n",
    "            print('number of predicted doc annotations: ', len(predicted_doc.annotations))\n",
    "            print(f\"Modified annotations have been written to {os.path.join(output_path, file_name)}\")\n",
    "            \n",
    "rm_annotations_in_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b491ab-c623-475e-8d7f-88cbdfba576f",
   "metadata": {},
   "source": [
    "## Sanitary Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c6f3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 44 annotations\n",
      "Annotation(tokens=[Token(sentence_idx=20, idx=98, start=9756, end=9769, text='International'), Token(sentence_idx=20, idx=99, start=9770, end=9775, text='Joint'), Token(sentence_idx=20, idx=100, start=9776, end=9786, text='Conference'), Token(sentence_idx=20, idx=101, start=9787, end=9789, text='on'), Token(sentence_idx=20, idx=102, start=9790, end=9797, text='Natural'), Token(sentence_idx=20, idx=103, start=9798, end=9806, text='Language'), Token(sentence_idx=20, idx=104, start=9807, end=9817, text='Processing')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='CONFERENCE', label_id=14)\n",
      "Predicted 0 annotations\n",
      "Predicted 7 annotations\n",
      "Annotation(tokens=[Token(sentence_idx=1, idx=105, start=475, end=482, text='TAC-KBP'), Token(sentence_idx=1, idx=106, start=482, end=483, text='-'), Token(sentence_idx=1, idx=107, start=483, end=487, text='2017'), Token(sentence_idx=1, idx=108, start=488, end=493, text='Event'), Token(sentence_idx=1, idx=109, start=494, end=499, text='Track'), Token(sentence_idx=1, idx=110, start=500, end=504, text='Data')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='DATASET', label_id=4)\n",
      "Predicted 0 annotations\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "def verify(ref_doc, predicted_doc):\n",
    "    assert ref_doc.text == predicted_doc.text, 'content changed'\n",
    "    assert len(ref_doc.sentences) == len(predicted_doc.sentences), 'sentences changed'\n",
    "    assert len(ref_doc.tokens) == len(predicted_doc.tokens), 'tokens changed'\n",
    "    for s1, s2 in zip(ref_doc.sentences, predicted_doc.sentences):\n",
    "        assert s1 == s2, f'sentence changed, \\n{s1}\\n{s2}'\n",
    "\n",
    "    for t1, t2 in zip(ref_doc.tokens, predicted_doc.tokens):\n",
    "        assert t1 == t2, f'token changed: \\n{t1}\\n{t2}'\n",
    "\n",
    "    print(f\"Predicted {len(predicted_doc.annotations)} annotations\")\n",
    "    if len(predicted_doc.annotations) > 0:\n",
    "        print(predicted_doc.annotations[-1])\n",
    "\n",
    "for file_path in os.listdir('../data/train'):\n",
    "    ref_doc = webanno_tsv_read_file(f'../data/train/{file_path}')\n",
    "    dummy_predicted_doc = webanno_tsv_read_file(f'../results/dummy/{file_path}')\n",
    "    empty_predicted_doc = webanno_tsv_read_file(f'../results/empty/{file_path}')\n",
    "    verify(ref_doc, dummy_predicted_doc)\n",
    "    verify(ref_doc, empty_predicted_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228d89e-81f3-4c01-ac3d-1f01c830773b",
   "metadata": {},
   "source": [
    "## Try BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24be6bc7-a5aa-4603-8dee-92dd4749dc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "\n",
    "LABELS = [\n",
    "    'CONFERENCE',\n",
    "    'DATASET',\n",
    "    'EVALMETRIC',\n",
    "    'LICENSE',\n",
    "    'ONTOLOGY',\n",
    "    'PROGLANG',\n",
    "    'PROJECT',\n",
    "    'PUBLICATION',\n",
    "    'SOFTWARE',\n",
    "    'WORKSHOP'\n",
    "]\n",
    "\n",
    "\n",
    "def to_char_bio(src_path: str, ref_path: str) -> List[List[str]]:\n",
    "    ref_doc = webanno_tsv_read_file(ref_path)\n",
    "\n",
    "    # Parse the WebAnno TSV file\n",
    "    doc = webanno_tsv_read_file(src_path)\n",
    "    # Initialize a list to store character-level BIO tags\n",
    "    bio_tags_list = []\n",
    "    for target_label in LABELS:\n",
    "        bio_tags = ['#'] * len(doc.text)  # Default to '#' for all characters\n",
    "        # Pick interested sentences and default to 'O'\n",
    "        for annotation in ref_doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "            sentences = doc.annotation_sentences(annotation)\n",
    "            for sentence in sentences:\n",
    "                tokens = doc.sentence_tokens(sentence)\n",
    "                start_char, end_char = tokens[0].start, tokens[-1].end\n",
    "                bio_tags[start_char:end_char] = ['O'] * (end_char-start_char)\n",
    "\n",
    "        for annotation in doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "\n",
    "            start_token, end_token = annotation.tokens[0], annotation.tokens[-1]\n",
    "            start_char = start_token.start\n",
    "            end_char = end_token.end\n",
    "            # Sanity check\n",
    "            if ref_doc.text[start_char:end_char] != annotation.text:\n",
    "                msg = f\"ERROR: src: {src_path}, annotated '{annotation.text}', text: '{ref_doc.text[start_char:end_char]}'\"\n",
    "                print(msg)\n",
    "\n",
    "            # Assign BIO tags to characters in the entity span\n",
    "            if 'I-' in bio_tags[start_char]:\n",
    "                # It's inside other ENTITY, skip it\n",
    "                pass\n",
    "            else:\n",
    "                bio_tags[start_char] = f'B-{label}'  # Beginning of the entity\n",
    "\n",
    "            for i in range(start_char + 1, end_char):\n",
    "                bio_tags[i] = f'I-{label}'  # Inside the entity\n",
    "\n",
    "        # Remove unannotated sentences from bio list.\n",
    "        bio_tags = [x for x in filter(lambda x: x != '#', bio_tags)]\n",
    "        if len(bio_tags) > 0:\n",
    "            bio_tags_list.append(bio_tags)\n",
    "\n",
    "    return bio_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbccf20d-5025-46e6-8b8d-09699fcb9b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_dir = '../data/train'\n",
    "pred_dir = '../results/dummy'\n",
    "\n",
    "ref_file_names = sorted([fp for fp in os.listdir(ref_dir) if os.path.isfile(f'{ref_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "all_ref_bio_tags_list = []\n",
    "for ref_file_name in ref_file_names:\n",
    "    src_path = os.path.join(ref_dir, ref_file_name)\n",
    "    ref_path = src_path\n",
    "    all_ref_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "\n",
    "pred_file_names = sorted([fp for fp in os.listdir(pred_dir) if os.path.isfile(f'{pred_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "all_pred_bio_tags_list = []\n",
    "for idx, ref_file_name in enumerate(ref_file_names):\n",
    "    try:\n",
    "        src_path = os.path.join(pred_dir, ref_file_name)\n",
    "        ref_path = os.path.join(ref_dir, ref_file_name)\n",
    "        all_pred_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "    except FileNotFoundError:\n",
    "        nbr_labels = len(all_ref_bio_tags_list[idx])\n",
    "        pred = []\n",
    "        for label_idx in range(nbr_labels):\n",
    "            pred.append(['O'] * len(all_ref_bio_tags_list[idx][label_idx]))\n",
    "        print(f\"WARN: {ref_file_name} is missing, fill 'O' list as default prediction\")\n",
    "        all_pred_bio_tags_list.append(pred)\n",
    "\n",
    "# Sanity checking\n",
    "for ref_list, pred_list in zip(all_ref_bio_tags_list, all_pred_bio_tags_list):\n",
    "    for ref, pred in zip(ref_list, pred_list):\n",
    "        # print(len(ref), len(pred))\n",
    "        assert len(ref) == len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ee4ad-3d9d-4b45-804b-eb51fa5290f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
