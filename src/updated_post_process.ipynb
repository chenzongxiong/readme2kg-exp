{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802af48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "import glob\n",
    "import cleaner\n",
    "from typing import List, Optional\n",
    "from collections import defaultdict\n",
    "from webanno_tsv import webanno_tsv_read_file, Token\n",
    "import utils\n",
    "from predictor import LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e50cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotation_labels_if_possible(predicted_text):\n",
    "    label_to_text_list = defaultdict(list)\n",
    "    cumulative_adjustment = 0\n",
    "    # We need to process matches in the order they appear in the text\n",
    "    all_matches = []\n",
    "    \n",
    "    # First collect all matches across all labels\n",
    "    for label in LABELS:\n",
    "        regex = f'<{label}>(.*?)</{label}>'\n",
    "        matches = re.finditer(regex, predicted_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        for m in matches:\n",
    "            all_matches.append({\n",
    "                'label': label,\n",
    "                'match': m,\n",
    "                'start_tag_len': len(label) + 2,    # <LABEL>\n",
    "                'end_tag_len': len(label) + 3         # </LABEL>\n",
    "            })\n",
    "    \n",
    "    # Sort matches by their start position in the original text\n",
    "    all_matches.sort(key=lambda x: x['match'].start())\n",
    "    \n",
    "    # Process matches in order\n",
    "    for item in all_matches:\n",
    "        m = item['match']\n",
    "        label = item['label']\n",
    "        start_tag_len = item['start_tag_len']\n",
    "        end_tag_len = item['end_tag_len']\n",
    "        \n",
    "        # Calculate positions adjusted for previously removed tags\n",
    "        text_start = m.start(1) - cumulative_adjustment\n",
    "        text_end = m.end(1) - cumulative_adjustment\n",
    "        \n",
    "        label_to_text_list[label].append({\n",
    "            'text': m.group(1),\n",
    "            'start': text_start - start_tag_len,  # adjust for this match's opening tag\n",
    "            'end': text_end - start_tag_len       # text_end is already after opening tag\n",
    "        })\n",
    "        \n",
    "        # Update cumulative adjustment for future matches\n",
    "        cumulative_adjustment += start_tag_len + end_tag_len\n",
    "    \n",
    "    return label_to_text_list\n",
    "\n",
    "\n",
    "def post_process(predicted_text, tokens):\n",
    "    cleaned_text = cleaner.Cleaner(predicted_text).clean()\n",
    "    label_to_text_list = extract_annotation_labels_if_possible(cleaned_text)\n",
    "    return label_to_text_list\n",
    "\n",
    "\n",
    "\n",
    "def make_span_tokens(tokens: List['Token'], start_char: int, end_char: int) -> Optional[List['Token']]:\n",
    "    \"\"\"\n",
    "    Extract tokens within a character range, handling partial overlaps accurately.\n",
    "    \n",
    "    Args:\n",
    "        tokens (List[Token]): List of tokens from original text\n",
    "        start_char (int): Start character position (inclusive)\n",
    "        end_char (int): End character position (exclusive)\n",
    "    \n",
    "    Returns:\n",
    "        Optional[List[Token]]: Matching tokens with partial overlaps adjusted, or None\n",
    "    \"\"\"\n",
    "    span_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Skip tokens completely before the range\n",
    "        if token.end <= start_char:\n",
    "            continue\n",
    "        # Skip tokens completely after the range\n",
    "        if token.start >= end_char:\n",
    "            break  # Subsequent tokens have larger positions\n",
    "        \n",
    "        # Handle fully contained tokens\n",
    "        if token.start >= start_char and token.end <= end_char:\n",
    "            span_tokens.append(token)\n",
    "            continue\n",
    "        \n",
    "        # Handle partially overlapping tokens\n",
    "        overlap_start = max(token.start, start_char)\n",
    "        overlap_end = min(token.end, end_char)\n",
    "        \n",
    "        # Extract overlapping text segment\n",
    "        overlap_text = token.text[\n",
    "            (overlap_start - token.start) : \n",
    "            (overlap_end - token.start)\n",
    "        ]\n",
    "        \n",
    "        if overlap_text:\n",
    "            partial_token = Token(\n",
    "                sentence_idx=token.sentence_idx,\n",
    "                idx=f\"{token.idx}.partial\",  # Mark partial tokens\n",
    "                start=overlap_start,\n",
    "                end=overlap_end,\n",
    "                text=overlap_text\n",
    "            )\n",
    "            span_tokens.append(partial_token)\n",
    "    \n",
    "    return span_tokens if span_tokens else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791a59f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load and build full text\n",
    "# org_path = f'../data/test_labeled/ARM-software_keyword-transformer_master_README.md.tsv'\n",
    "# doc = webanno_tsv_read_file(org_path)\n",
    "# full_text = doc.text  # This is key: get the original document full text\n",
    "# annotations = []\n",
    "\n",
    "\n",
    "# cursor = 0\n",
    "\n",
    "# for sentence in doc.sentences:\n",
    "#     tokens = doc.sentence_tokens(sentence)\n",
    "#     original_text = sentence.text\n",
    "#     sid = hashlib.sha256(original_text.encode()).hexdigest()[:8]\n",
    "#     path = f'../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv'\n",
    "\n",
    "#     with open(f'{path}/{sid}.txt', 'r') as fd:\n",
    "#         predicted_text = fd.read()\n",
    "\n",
    "#     label_to_text_list = post_process(predicted_text, tokens)\n",
    "    \n",
    "#     sentence_offset = full_text.find(sentence.text, cursor)\n",
    "#     cursor = sentence_offset + len(sentence.text)\n",
    "    \n",
    "#     span_tokens_to_label_list = []\n",
    "#     for label, text_list in label_to_text_list.items():\n",
    "#         for text in text_list:\n",
    "#             span_tokens = make_span_tokens(\n",
    "#                 tokens,\n",
    "#                 start_char=text['start'] + sentence_offset,\n",
    "#                 end_char=text['end'] + sentence_offset\n",
    "#             )\n",
    "#             span_tokens_to_label_list.append({\n",
    "#                 'span_tokens': span_tokens,\n",
    "#                 'label': label\n",
    "#             })\n",
    "\n",
    "#     print(span_tokens_to_label_list)\n",
    "#     for span_tokens_to_label in span_tokens_to_label_list:\n",
    "#         span_tokens = span_tokens_to_label['span_tokens']\n",
    "#         label = span_tokens_to_label['label']\n",
    "#         if span_tokens is None:\n",
    "#             continue\n",
    "#         annotation = utils.make_annotation(tokens=span_tokens, label=label)\n",
    "#         annotations.append(annotation)\n",
    "# predicted_doc = utils.replace_webanno_annotations(doc, annotations=annotations)\n",
    "# # Verify\n",
    "# if doc.text != predicted_doc.text:\n",
    "#     #logging.warning('content changed')\n",
    "#     pass\n",
    "# if len(doc.sentences) == len(predicted_doc.sentences):\n",
    "#     #logging.warning('sentences changed')\n",
    "#     pass\n",
    "# if len(doc.tokens) == len(predicted_doc.tokens):\n",
    "#     #logging.warning('tokens changed')\n",
    "#     pass\n",
    "# for s1, s2 in zip(doc.sentences, predicted_doc.sentences):\n",
    "#     if s1 == s2:\n",
    "#         #logging.warning(f'sentence changed, \\n{s1}\\n{s2}')\n",
    "#         pass\n",
    "\n",
    "# for t1, t2 in zip(doc.tokens, predicted_doc.tokens):\n",
    "#     if t1 == t2:\n",
    "#         #logging.warning(f'token changed: \\n{t1}\\n{t2}')\n",
    "#         pass\n",
    "\n",
    "# with open(\"ARM-software_keyword-transformer_master_README.md.tsv\", 'w') as fd:\n",
    "#     fd.write(predicted_doc.tsv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57d22e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../data/test_labeled\\231sm_Low_Resource_KBP_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/231sm_Low_Resource_KBP_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\allenai_aspire_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/allenai_aspire_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\alpiges_LinConGauss_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/alpiges_LinConGauss_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\anonymous-submission-22_dejavu_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/anonymous-submission-22_dejavu_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\ARM-software_keyword-transformer_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/ARM-software_keyword-transformer_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\aspiaspace_earthpt_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/aspiaspace_earthpt_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\benedekrozemberczki_karateclub_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/benedekrozemberczki_karateclub_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\Cardio-AI_3d-mri-domain-adaptation_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/Cardio-AI_3d-mri-domain-adaptation_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\ChopinSharp_ref-nms_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/ChopinSharp_ref-nms_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\claws-lab_awesome-crowd-combat-misinformation_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/claws-lab_awesome-crowd-combat-misinformation_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\daijifeng001_TA-FCN_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/daijifeng001_TA-FCN_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\dennlinger_tsar-2022-shared-task_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/dennlinger_tsar-2022-shared-task_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\dylanashley_story-distiller_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/dylanashley_story-distiller_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\felixxu35_hamiltoniq_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/felixxu35_hamiltoniq_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\freedomintelligence_mllm-bench_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/freedomintelligence_mllm-bench_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\idt-iti_lightweight-face-detector-pruning_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/idt-iti_lightweight-face-detector-pruning_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\iqvianlp_llm-onto-infuse_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/iqvianlp_llm-onto-infuse_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\James-Durant_fisher-information_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/James-Durant_fisher-information_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\juaml_juharmonize_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/juaml_juharmonize_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\lojzezust_slr_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/lojzezust_slr_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\lucy3_words_as_gatekeepers_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/lucy3_words_as_gatekeepers_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\MELALab_nela-gt-2019_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/MELALab_nela-gt-2019_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\microsoft_opendatasheets-framework_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/microsoft_opendatasheets-framework_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\ml-jku_vnegnn_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/ml-jku_vnegnn_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\mmp2_megaman_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/mmp2_megaman_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\nokia_codesearch_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/nokia_codesearch_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\prasunroy_air-writing_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/prasunroy_air-writing_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\qiantianwen_nuscenes-qa_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/qiantianwen_nuscenes-qa_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\salesforce_booksum_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/salesforce_booksum_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\snap-research_test-time-aggregation-for-cf_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/snap-research_test-time-aggregation-for-cf_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\sosuperic_MeanSum_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/sosuperic_MeanSum_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\stanfordhci_modelsketchbook_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/stanfordhci_modelsketchbook_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\tiehangd_Para_DPMM_master_readme.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/tiehangd_Para_DPMM_master_readme.md.tsv\n",
      "Processing file: ../data/test_labeled\\uwnetworkslab_netcov_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/uwnetworkslab_netcov_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\vaikkunth_PrivacyFL_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/vaikkunth_PrivacyFL_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\wenzhengzhang_entqa_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/wenzhengzhang_entqa_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\zju-vipa_awesome-neural-trees_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/zju-vipa_awesome-neural-trees_main_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "ref_dir = '../data/test_labeled'\n",
    "tsv_files = glob.glob(os.path.join(ref_dir, '*.tsv'))\n",
    "\n",
    "for org_path in tsv_files:\n",
    "    print(f\"Processing file: {org_path}\")\n",
    "    doc = webanno_tsv_read_file(org_path)\n",
    "    full_text = doc.text\n",
    "    annotations = []\n",
    "    cursor = 0\n",
    "\n",
    "    file_base = os.path.splitext(os.path.basename(org_path))[0]\n",
    "    pred_base_path = f'../results/deepseek-chat/prompt-0/zzz_{file_base}.tsv'\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        tokens = doc.sentence_tokens(sentence)\n",
    "        original_text = sentence.text\n",
    "        sid = hashlib.sha256(original_text.encode()).hexdigest()[:8]\n",
    "\n",
    "        pred_path = os.path.join(pred_base_path, f'{sid}.txt')\n",
    "        if not os.path.exists(pred_path):\n",
    "            print(f\"Prediction not found: {pred_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(pred_path, 'r') as fd:\n",
    "            predicted_text = fd.read()\n",
    "\n",
    "        offset = full_text.find(original_text, cursor)\n",
    "        if offset == -1:\n",
    "            print(f\"Error: sentence not found in full_text: {original_text[:30]}...\")\n",
    "            continue\n",
    "        cursor = offset + len(original_text)\n",
    "\n",
    "        label_to_text_list = post_process(predicted_text,tokens)\n",
    "\n",
    "        span_tokens_to_label_list = []\n",
    "        for label, text_list in label_to_text_list.items():\n",
    "            for text in text_list:\n",
    "                start_abs = text['start'] + offset\n",
    "                end_abs = text['end'] + offset\n",
    "                span_tokens = make_span_tokens(tokens, start_abs, end_abs)\n",
    "                span_tokens_to_label_list.append({\n",
    "                    'span_tokens': span_tokens,\n",
    "                    'label': label\n",
    "                })\n",
    "\n",
    "        for span_tokens_to_label in span_tokens_to_label_list:\n",
    "            span_tokens = span_tokens_to_label['span_tokens']\n",
    "            label = span_tokens_to_label['label']\n",
    "            if span_tokens is None:\n",
    "                continue\n",
    "            annotation = utils.make_annotation(tokens=span_tokens, label=label)\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    predicted_doc = utils.replace_webanno_annotations(doc, annotations=annotations)\n",
    "\n",
    "    output_path = f\"../results/deepseek-chat/test_unlabeled_up/{file_base}.tsv\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w') as fd:\n",
    "        fd.write(predicted_doc.tsv())\n",
    "    print(f\"Saved predicted file to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
