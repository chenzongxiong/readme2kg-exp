{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "802af48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "import glob\n",
    "import cleaner\n",
    "from collections import defaultdict\n",
    "from webanno_tsv import webanno_tsv_read_file, Token\n",
    "from typing import List, Optional\n",
    "import utils\n",
    "from predictor import LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f1a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated matc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e50cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotation_labels_if_possible(predicted_text, original_text, offset=0):\n",
    "    label_to_text_list = defaultdict(list)\n",
    "    used_spans = set()\n",
    "\n",
    "    all_labels = \"|\".join(LABELS)\n",
    "    tag_pattern = re.compile(\n",
    "        fr'<({all_labels})>\\s*(.*?)\\s*</\\1>',\n",
    "        flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    matches = list(re.finditer(tag_pattern, predicted_text))\n",
    "    for match in matches:\n",
    "        original_label = match.group(1)\n",
    "        label = original_label.upper()\n",
    "        content = match.group(2).strip()\n",
    "\n",
    "        pattern = re.escape(content)\n",
    "        content_pattern = re.compile(pattern)\n",
    "\n",
    "        found = False\n",
    "        for found_match in content_pattern.finditer(original_text):\n",
    "            span = (found_match.start() + offset, found_match.end() + offset)\n",
    "\n",
    "            if any(s < span[1] and e > span[0] for s, e in used_spans):\n",
    "                continue\n",
    "\n",
    "            label_to_text_list[label].append({\n",
    "                \"text\": content,\n",
    "                \"start\": span[0],\n",
    "                \"end\": span[1]\n",
    "            })\n",
    "            used_spans.add(span)\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "        if not found:\n",
    "            print(f\"Warning: '{content}' not found in original_text.\")\n",
    "\n",
    "    return label_to_text_list\n",
    "\n",
    "\n",
    "def post_process(predicted_text, original_text, tokens, offset):\n",
    "    cleaned_text = cleaner.Cleaner(predicted_text).clean()\n",
    "    label_to_text_list = extract_annotation_labels_if_possible(cleaned_text, original_text, offset)\n",
    "    return label_to_text_list\n",
    "\n",
    "\n",
    "def make_span_tokens(tokens: List[Token], start_char: int, end_char: int) -> Optional[List[Token]]:\n",
    "    span_tokens = []\n",
    "    for token in tokens:\n",
    "        # Skip tokens completely outside the range\n",
    "        if token.end <= start_char:\n",
    "            continue\n",
    "        if token.start >= end_char:\n",
    "            continue\n",
    "\n",
    "        # If token is fully inside the span, keep it\n",
    "        if token.start >= start_char and token.end <= end_char:\n",
    "            span_tokens.append(token)\n",
    "        else:\n",
    "            # Handle left boundary token\n",
    "            if token.start < start_char < token.end:\n",
    "                sliced_text = token.text[start_char - token.start:]\n",
    "                span_tokens.append(Token(\n",
    "                    idx=f\"{token.idx}.L\",\n",
    "                    sentence_idx=token.sentence_idx,\n",
    "                    start=start_char,\n",
    "                    end=token.end,\n",
    "                    text=sliced_text\n",
    "                ))\n",
    "            # Handle right boundary token\n",
    "            elif token.start < end_char < token.end:\n",
    "                sliced_text = token.text[:end_char - token.start]\n",
    "                span_tokens.append(Token(\n",
    "                    idx=f\"{token.idx}.R\",\n",
    "                    sentence_idx=token.sentence_idx,\n",
    "                    start=token.start,\n",
    "                    end=end_char,\n",
    "                    text=sliced_text\n",
    "                ))\n",
    "            # Handle token fully covering the span (rare but possible)\n",
    "            elif token.start < start_char and token.end > end_char:\n",
    "                sliced_text = token.text[start_char - token.start : end_char - token.start]\n",
    "                span_tokens.append(Token(\n",
    "                    idx=f\"{token.idx}.M\",\n",
    "                    sentence_idx=token.sentence_idx,\n",
    "                    start=start_char,\n",
    "                    end=end_char,\n",
    "                    text=sliced_text\n",
    "                ))\n",
    "\n",
    "    return span_tokens if span_tokens else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9ad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'Keyword Transformer', 'start': 2, 'end': 21}], 'PUBLICATION': [{'text': 'Keyword Transformer: A Self-Attention Model for Keyword Spotting', 'start': 163, 'end': 227}], 'CONFERENCE': [{'text': 'Interspeech 2021', 'start': 277, 'end': 293}]})\n",
      "[{'span_tokens': [Token(sentence_idx=1, idx=2, start=2, end=9, text='Keyword'), Token(sentence_idx=1, idx=3, start=10, end=21, text='Transformer')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=1, idx=39, start=163, end=170, text='Keyword'), Token(sentence_idx=1, idx=40, start=171, end=182, text='Transformer'), Token(sentence_idx=1, idx=41, start=182, end=183, text=':'), Token(sentence_idx=1, idx=42, start=184, end=185, text='A'), Token(sentence_idx=1, idx=43, start=186, end=200, text='Self-Attention'), Token(sentence_idx=1, idx=44, start=201, end=206, text='Model'), Token(sentence_idx=1, idx=45, start=207, end=210, text='for'), Token(sentence_idx=1, idx=46, start=211, end=218, text='Keyword'), Token(sentence_idx=1, idx=47, start=219, end=227, text='Spotting')], 'label': 'PUBLICATION'}, {'span_tokens': [Token(sentence_idx=1, idx=63, start=277, end=288, text='Interspeech'), Token(sentence_idx=1, idx=64, start=289, end=293, text='2021')], 'label': 'CONFERENCE'}]\n",
      "Warning: 'Interspeech' not found in original_text.\n",
      "defaultdict(<class 'list'>, {'PUBLICATION': [{'text': 'Keyword Transformer: A Self-Attention Model for Keyword Spotting', 'start': 464, 'end': 528}]})\n",
      "[{'span_tokens': [Token(sentence_idx=2, idx=42, start=464, end=471, text='Keyword'), Token(sentence_idx=2, idx=43, start=472, end=483, text='Transformer'), Token(sentence_idx=2, idx=44, start=483, end=484, text=':'), Token(sentence_idx=2, idx=45, start=485, end=486, text='A'), Token(sentence_idx=2, idx=46, start=487, end=501, text='Self-Attention'), Token(sentence_idx=2, idx=47, start=502, end=507, text='Model'), Token(sentence_idx=2, idx=48, start=508, end=511, text='for'), Token(sentence_idx=2, idx=49, start=512, end=519, text='Keyword'), Token(sentence_idx=2, idx=50, start=520, end=528, text='Spotting')], 'label': 'PUBLICATION'}]\n",
      "defaultdict(<class 'list'>, {'CONFERENCE': [{'text': 'Interspeech 2021', 'start': 564, 'end': 580}], 'DATASET': [{'text': 'Google Speech Commands', 'start': 674, 'end': 696}]})\n",
      "[{'span_tokens': [Token(sentence_idx=3, idx=1, start=564, end=575, text='Interspeech'), Token(sentence_idx=3, idx=2, start=576, end=580, text='2021')], 'label': 'CONFERENCE'}, {'span_tokens': [Token(sentence_idx=3, idx=35, start=674, end=680, text='Google'), Token(sentence_idx=3, idx=36, start=681, end=687, text='Speech'), Token(sentence_idx=3, idx=37, start=688, end=696, text='Commands')], 'label': 'DATASET'}]\n",
      "defaultdict(<class 'list'>, {'DATASET': [{'text': 'V2', 'start': 780, 'end': 782}]})\n",
      "[{'span_tokens': [Token(sentence_idx=4, idx=6, start=780, end=782, text='V2')], 'label': 'DATASET'}]\n",
      "defaultdict(<class 'list'>, {'DATASET': [{'text': 'v0.02', 'start': 926, 'end': 931}]})\n",
      "[{'span_tokens': [Token(sentence_idx=5, idx='2.L', start=926, end=931, text='v0.02')], 'label': 'DATASET'}]\n",
      "defaultdict(<class 'list'>, {})\n",
      "[]\n",
      "defaultdict(<class 'list'>, {})\n",
      "[]\n",
      "defaultdict(<class 'list'>, {'DATASET': [{'text': 'speech_commands_v0.01', 'start': 1082, 'end': 1103}]})\n",
      "[{'span_tokens': [Token(sentence_idx=8, idx=30, start=1082, end=1103, text='speech_commands_v0.01')], 'label': 'DATASET'}]\n",
      "defaultdict(<class 'list'>, {})\n",
      "[]\n",
      "defaultdict(<class 'list'>, {})\n",
      "[]\n",
      "defaultdict(<class 'list'>, {})\n",
      "[]\n",
      "defaultdict(<class 'list'>, {'PROGLANG': [{'text': 'python3', 'start': 1357, 'end': 1364}]})\n",
      "[{'span_tokens': [Token(sentence_idx=12, idx=37, start=1357, end=1364, text='python3')], 'label': 'PROGLANG'}]\n",
      "defaultdict(<class 'list'>, {})\n",
      "[]\n",
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'Tensorflow 2.4.0rc1', 'start': 1495, 'end': 1514}, {'text': 'CUDA 11', 'start': 1520, 'end': 1527}]})\n",
      "[{'span_tokens': [Token(sentence_idx=14, idx=29, start=1495, end=1505, text='Tensorflow'), Token(sentence_idx=14, idx=30, start=1506, end=1514, text='2.4.0rc1')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=14, idx=32, start=1520, end=1524, text='CUDA'), Token(sentence_idx=14, idx=33, start=1525, end=1527, text='11')], 'label': 'SOFTWARE'}]\n",
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'Tensorflow', 'start': 1563, 'end': 1573}]})\n",
      "[{'span_tokens': [Token(sentence_idx=15, idx=10, start=1563, end=1573, text='Tensorflow')], 'label': 'SOFTWARE'}]\n",
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'Tensorflow', 'start': 1646, 'end': 1656}]})\n",
      "[{'span_tokens': [Token(sentence_idx=16, idx=6, start=1646, end=1656, text='Tensorflow')], 'label': 'SOFTWARE'}]\n",
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'Keyword-Transformer', 'start': 1890, 'end': 1909}]})\n",
      "[{'span_tokens': [Token(sentence_idx=17, idx=34, start=1890, end=1909, text='Keyword-Transformer')], 'label': 'SOFTWARE'}]\n",
      "defaultdict(<class 'list'>, {})\n",
      "[]\n",
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'KWT-3', 'start': 2800, 'end': 2805}], 'DATASET': [{'text': 'Speech Commands V2', 'start': 2822, 'end': 2840}]})\n",
      "[{'span_tokens': [Token(sentence_idx=19, idx=121, start=2800, end=2803, text='KWT'), Token(sentence_idx=19, idx=122, start=2803, end=2804, text='-'), Token(sentence_idx=19, idx=123, start=2804, end=2805, text='3')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=19, idx=127, start=2822, end=2828, text='Speech'), Token(sentence_idx=19, idx=128, start=2829, end=2837, text='Commands'), Token(sentence_idx=19, idx=129, start=2838, end=2840, text='V2')], 'label': 'DATASET'}]\n",
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'KWT', 'start': 3021, 'end': 3024}, {'text': 'Att-MH-RNN', 'start': 3555, 'end': 3565}, {'text': 'DeIT', 'start': 3596, 'end': 3600}]})\n",
      "[{'span_tokens': [Token(sentence_idx=20, idx=5, start=3021, end=3024, text='KWT')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=20, idx=132, start=3555, end=3565, text='Att-MH-RNN')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=20, idx=141, start=3596, end=3600, text='DeIT')], 'label': 'SOFTWARE'}]\n",
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'KWT-3', 'start': 3655, 'end': 3660}, {'text': 'KWT-3', 'start': 3811, 'end': 3816}, {'text': 'KWT-2', 'start': 3818, 'end': 3823}, {'text': 'KWT-1', 'start': 3828, 'end': 3833}]})\n",
      "[{'span_tokens': [Token(sentence_idx=21, idx=3, start=3655, end=3658, text='KWT'), Token(sentence_idx=21, idx=4, start=3658, end=3659, text='-'), Token(sentence_idx=21, idx=5, start=3659, end=3660, text='3')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=21, idx=35, start=3811, end=3814, text='KWT'), Token(sentence_idx=21, idx=36, start=3814, end=3815, text='-'), Token(sentence_idx=21, idx=37, start=3815, end=3816, text='3')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=21, idx=39, start=3818, end=3821, text='KWT'), Token(sentence_idx=21, idx=40, start=3821, end=3822, text='-'), Token(sentence_idx=21, idx=41, start=3822, end=3823, text='2')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=21, idx=43, start=3828, end=3831, text='KWT'), Token(sentence_idx=21, idx=44, start=3831, end=3832, text='-'), Token(sentence_idx=21, idx=45, start=3832, end=3833, text='1')], 'label': 'SOFTWARE'}]\n",
      "defaultdict(<class 'list'>, {})\n",
      "[]\n",
      "defaultdict(<class 'list'>, {'DATASET': [{'text': 'Google Speech Commands v2', 'start': 4226, 'end': 4251}], 'SOFTWARE': [{'text': 'KWS streaming work', 'start': 4354, 'end': 4372}], 'PROJECT': [{'text': 'Google Research', 'start': 4455, 'end': 4470}]})\n",
      "[{'span_tokens': [Token(sentence_idx=23, idx=151, start=4226, end=4232, text='Google'), Token(sentence_idx=23, idx=152, start=4233, end=4239, text='Speech'), Token(sentence_idx=23, idx=153, start=4240, end=4248, text='Commands'), Token(sentence_idx=23, idx=154, start=4249, end=4251, text='v2')], 'label': 'DATASET'}, {'span_tokens': [Token(sentence_idx=23, idx=179, start=4354, end=4357, text='KWS'), Token(sentence_idx=23, idx=180, start=4358, end=4367, text='streaming'), Token(sentence_idx=23, idx=181, start=4368, end=4372, text='work')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=23, idx=201, start=4455, end=4461, text='Google'), Token(sentence_idx=23, idx=202, start=4462, end=4470, text='Research')], 'label': 'PROJECT'}]\n",
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'kws_streaming', 'start': 4562, 'end': 4575}]})\n",
      "[{'span_tokens': [Token(sentence_idx=24, idx=20, start=4562, end=4575, text='kws_streaming')], 'label': 'SOFTWARE'}]\n",
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'DeiT', 'start': 4631, 'end': 4635}, {'text': 'deit', 'start': 4673, 'end': 4677}]})\n",
      "[{'span_tokens': [Token(sentence_idx=25, idx=8, start=4631, end=4635, text='DeiT')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=25, idx=19, start=4673, end=4677, text='deit')], 'label': 'SOFTWARE'}]\n",
      "defaultdict(<class 'list'>, {})\n",
      "[]\n",
      "defaultdict(<class 'list'>, {'LICENSE': [{'text': 'Apache 2.0', 'start': 4856, 'end': 4866}]})\n",
      "[{'span_tokens': [Token(sentence_idx=27, idx=27, start=4856, end=4862, text='Apache'), Token(sentence_idx=27, idx=28, start=4863, end=4866, text='2.0')], 'label': 'LICENSE'}]\n",
      "defaultdict(<class 'list'>, {'SOFTWARE': [{'text': 'KWS streaming', 'start': 4931, 'end': 4944}, {'text': 'kws_streaming', 'start': 5020, 'end': 5033}], 'PROJECT': [{'text': 'google-research', 'start': 4976, 'end': 4991}, {'text': 'google-research', 'start': 4992, 'end': 5007}, {'text': 'Google Research', 'start': 5038, 'end': 5053}]})\n",
      "[{'span_tokens': [Token(sentence_idx=28, idx=9, start=4931, end=4934, text='KWS'), Token(sentence_idx=28, idx=10, start=4935, end=4944, text='streaming')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=28, idx=28, start=5020, end=5033, text='kws_streaming')], 'label': 'SOFTWARE'}, {'span_tokens': [Token(sentence_idx=28, idx=20, start=4976, end=4991, text='google-research')], 'label': 'PROJECT'}, {'span_tokens': [Token(sentence_idx=28, idx=22, start=4992, end=5007, text='google-research')], 'label': 'PROJECT'}, {'span_tokens': [Token(sentence_idx=28, idx=31, start=5038, end=5044, text='Google'), Token(sentence_idx=28, idx=32, start=5045, end=5053, text='Research')], 'label': 'PROJECT'}]\n",
      "defaultdict(<class 'list'>, {'LICENSE': [{'text': 'Apache 2.0', 'start': 5089, 'end': 5099}]})\n",
      "[{'span_tokens': [Token(sentence_idx=29, idx=7, start=5089, end=5095, text='Apache'), Token(sentence_idx=29, idx=8, start=5096, end=5099, text='2.0')], 'label': 'LICENSE'}]\n"
     ]
    }
   ],
   "source": [
    "# ## Exampleï¼š\n",
    "\n",
    "\n",
    "# # Load and build full text\n",
    "# org_path = f'../data/test_labeled/ARM-software_keyword-transformer_master_README.md.tsv'\n",
    "# doc = webanno_tsv_read_file(org_path)\n",
    "# full_text = doc.text  # This is key: get the original document full text\n",
    "# annotations = []\n",
    "\n",
    "\n",
    "# # Start scanning from the beginning\n",
    "# cursor = 0\n",
    "\n",
    "# for sentence in doc.sentences:\n",
    "#     tokens = doc.sentence_tokens(sentence)\n",
    "#     original_text = sentence.text\n",
    "#     sid = hashlib.sha256(original_text.encode()).hexdigest()[:8]\n",
    "#     path = f'../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv'\n",
    "\n",
    "#     with open(f'{path}/{sid}.txt', 'r') as fd:\n",
    "#         predicted_text = fd.read()\n",
    "\n",
    "#     # Find true position of this sentence in the original text\n",
    "#     offset = full_text.find(original_text, cursor)\n",
    "#     if offset == -1:\n",
    "#         print(f\"Error: sentence not found in full_text\")\n",
    "#         continue\n",
    "\n",
    "#     # Update cursor for next search to avoid matching same sentence again\n",
    "#     cursor = offset + len(original_text)\n",
    "\n",
    "#     label_to_text_list = post_process(predicted_text, original_text, tokens, offset)\n",
    "#     print(label_to_text_list)\n",
    "#     span_tokens_to_label_list = []\n",
    "#     for label, text_list in label_to_text_list.items():\n",
    "#         for text in text_list:\n",
    "#             span_tokens_to_label_list.append({\n",
    "#                 'span_tokens': make_span_tokens(tokens, text['start'], text['end']), # The problem is here: make_span_tokens func \n",
    "#                 'label': label\n",
    "#             })\n",
    "#     print(span_tokens_to_label_list)\n",
    "#     for span_tokens_to_label in span_tokens_to_label_list:\n",
    "#         span_tokens = span_tokens_to_label['span_tokens']\n",
    "#         label = span_tokens_to_label['label']\n",
    "#         if span_tokens is None:\n",
    "#             continue\n",
    "#         annotation = utils.make_annotation(tokens=span_tokens, label=label)\n",
    "#         annotations.append(annotation)\n",
    "# predicted_doc = utils.replace_webanno_annotations(doc, annotations=annotations)\n",
    "# # Verify\n",
    "# if doc.text != predicted_doc.text:\n",
    "#     #logging.warning('content changed')\n",
    "#     pass\n",
    "# if len(doc.sentences) == len(predicted_doc.sentences):\n",
    "#     #logging.warning('sentences changed')\n",
    "#     pass\n",
    "# if len(doc.tokens) == len(predicted_doc.tokens):\n",
    "#     #logging.warning('tokens changed')\n",
    "#     pass\n",
    "# for s1, s2 in zip(doc.sentences, predicted_doc.sentences):\n",
    "#     if s1 == s2:\n",
    "#         #logging.warning(f'sentence changed, \\n{s1}\\n{s2}')\n",
    "#         pass\n",
    "\n",
    "# for t1, t2 in zip(doc.tokens, predicted_doc.tokens):\n",
    "#     if t1 == t2:\n",
    "#         #logging.warning(f'token changed: \\n{t1}\\n{t2}')\n",
    "#         pass\n",
    "\n",
    "# with open(\"ARM-software_keyword-transformer_master_README.md.tsv\", 'w') as fd:\n",
    "#     fd.write(predicted_doc.tsv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0123607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../data/test_labeled\\231sm_Low_Resource_KBP_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/231sm_Low_Resource_KBP_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\allenai_aspire_main_README.md.tsv\n",
      "Warning: 'Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity' not found in original_text.\n",
      "Warning: 'RELISH-DB' not found in original_text.\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/allenai_aspire_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\alpiges_LinConGauss_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/alpiges_LinConGauss_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\anonymous-submission-22_dejavu_master_README.md.tsv\n",
      "Warning: 'Proceedings of the 2022 30th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering' not found in original_text.\n",
      "Warning: 'ESEC/FSE 2022' not found in original_text.\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/anonymous-submission-22_dejavu_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\ARM-software_keyword-transformer_master_README.md.tsv\n",
      "Warning: 'Interspeech' not found in original_text.\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/ARM-software_keyword-transformer_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\aspiaspace_earthpt_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/aspiaspace_earthpt_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\benedekrozemberczki_karateclub_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/benedekrozemberczki_karateclub_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\Cardio-AI_3d-mri-domain-adaptation_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/Cardio-AI_3d-mri-domain-adaptation_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\ChopinSharp_ref-nms_main_README.md.tsv\n",
      "Warning: 'refcoco' not found in original_text.\n",
      "Warning: 'refcoco' not found in original_text.\n",
      "Warning: 'refcoco' not found in original_text.\n",
      "Warning: 'refcoco' not found in original_text.\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/ChopinSharp_ref-nms_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\claws-lab_awesome-crowd-combat-misinformation_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/claws-lab_awesome-crowd-combat-misinformation_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\daijifeng001_TA-FCN_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/daijifeng001_TA-FCN_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\dennlinger_tsar-2022-shared-task_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/dennlinger_tsar-2022-shared-task_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\dylanashley_story-distiller_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/dylanashley_story-distiller_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\felixxu35_hamiltoniq_main_README.md.tsv\n",
      "Warning: 'Python' not found in original_text.\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/felixxu35_hamiltoniq_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\freedomintelligence_mllm-bench_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/freedomintelligence_mllm-bench_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\idt-iti_lightweight-face-detector-pruning_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/idt-iti_lightweight-face-detector-pruning_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\iqvianlp_llm-onto-infuse_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/iqvianlp_llm-onto-infuse_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\James-Durant_fisher-information_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/James-Durant_fisher-information_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\juaml_juharmonize_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/juaml_juharmonize_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\lojzezust_slr_master_README.md.tsv\n",
      "Warning: 'International Semantic Web Conference 2019' not found in original_text.\n",
      "Warning: 'workshop on ontologies' not found in original_text.\n",
      "Warning: 'Intelligence Task Ontology' not found in original_text.\n",
      "Warning: 'Precision' not found in original_text.\n",
      "Warning: 'Recall' not found in original_text.\n",
      "Warning: 'Maules Creek' not found in original_text.\n",
      "Warning: 'Protege' not found in original_text.\n",
      "Warning: 'International Semantic Web Conference 2019' not found in original_text.\n",
      "Warning: 'workshop on ontologies' not found in original_text.\n",
      "Warning: 'Intelligence Task Ontology' not found in original_text.\n",
      "Warning: 'Precision' not found in original_text.\n",
      "Warning: 'Recall' not found in original_text.\n",
      "Warning: 'Maules Creek' not found in original_text.\n",
      "Warning: 'Protege' not found in original_text.\n",
      "Warning: 'International Semantic Web Conference 2019' not found in original_text.\n",
      "Warning: 'workshop on ontologies' not found in original_text.\n",
      "Warning: 'Intelligence Task Ontology' not found in original_text.\n",
      "Warning: 'Precision' not found in original_text.\n",
      "Warning: 'Recall' not found in original_text.\n",
      "Warning: 'Maules Creek' not found in original_text.\n",
      "Warning: 'Protege' not found in original_text.\n",
      "Warning: 'International Semantic Web Conference 2019' not found in original_text.\n",
      "Warning: 'workshop on ontologies' not found in original_text.\n",
      "Warning: 'Intelligence Task Ontology' not found in original_text.\n",
      "Warning: 'Precision' not found in original_text.\n",
      "Warning: 'Recall' not found in original_text.\n",
      "Warning: 'Maules Creek' not found in original_text.\n",
      "Warning: 'Protege' not found in original_text.\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/lojzezust_slr_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\lucy3_words_as_gatekeepers_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/lucy3_words_as_gatekeepers_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\MELALab_nela-gt-2019_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/MELALab_nela-gt-2019_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\microsoft_opendatasheets-framework_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/microsoft_opendatasheets-framework_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\ml-jku_vnegnn_master_README.md.tsv\n",
      "Warning: 'Medical Cognitive Computing Center (MC3)' not found in original_text.\n",
      "Warning: 'INCONTROL-RL' not found in original_text.\n",
      "Warning: 'EPILEPSIA' not found in original_text.\n",
      "Warning: 'INTEGRATE' not found in original_text.\n",
      "Warning: 'Software Competence Center Hagenberg GmbH' not found in original_text.\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/ml-jku_vnegnn_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\mmp2_megaman_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/mmp2_megaman_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\nokia_codesearch_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/nokia_codesearch_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\prasunroy_air-writing_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/prasunroy_air-writing_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\qiantianwen_nuscenes-qa_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/qiantianwen_nuscenes-qa_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\salesforce_booksum_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/salesforce_booksum_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\snap-research_test-time-aggregation-for-cf_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/snap-research_test-time-aggregation-for-cf_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\sosuperic_MeanSum_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/sosuperic_MeanSum_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\stanfordhci_modelsketchbook_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/stanfordhci_modelsketchbook_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\tiehangd_Para_DPMM_master_readme.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/tiehangd_Para_DPMM_master_readme.md.tsv\n",
      "Processing file: ../data/test_labeled\\uwnetworkslab_netcov_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/uwnetworkslab_netcov_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\vaikkunth_PrivacyFL_master_README.md.tsv\n",
      "Warning: 'Software' not found in original_text.\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/vaikkunth_PrivacyFL_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\wenzhengzhang_entqa_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/wenzhengzhang_entqa_main_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\n",
      "Processing file: ../data/test_labeled\\zju-vipa_awesome-neural-trees_main_README.md.tsv\n",
      "Saved predicted file to: ../results/deepseek-chat/test_unlabeled_up/zju-vipa_awesome-neural-trees_main_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Traverse all .tsv files in the input folder\n",
    "ref_dir = '../data/test_labeled'\n",
    "tsv_files = glob.glob(os.path.join(ref_dir, '*.tsv'))\n",
    "\n",
    "for org_path in tsv_files:\n",
    "    print(f\"Processing file: {org_path}\")\n",
    "    doc = webanno_tsv_read_file(org_path)\n",
    "    full_text = doc.text\n",
    "    annotations = []\n",
    "    cursor = 0\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        tokens = doc.sentence_tokens(sentence)\n",
    "        original_text = sentence.text\n",
    "        sid = hashlib.sha256(original_text.encode()).hexdigest()[:8]\n",
    "\n",
    "        # Assuming each tsv has a corresponding folder in results with the same name (excluding extension)\n",
    "        file_base = os.path.splitext(os.path.basename(org_path))[0]\n",
    "        src_path = f'../results/deepseek-chat/prompt-0/zzz_{file_base}.tsv'\n",
    "\n",
    "        pred_path = os.path.join(src_path, f'{sid}.txt')\n",
    "        if not os.path.exists(pred_path):\n",
    "            print(f\"Prediction not found: {pred_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(pred_path, 'r') as fd:\n",
    "            predicted_text = fd.read()\n",
    "\n",
    "        offset = full_text.find(original_text, cursor)\n",
    "        if offset == -1:\n",
    "            print(f\"Error: sentence not found in full_text: {original_text[:30]}...\")\n",
    "            continue\n",
    "        cursor = offset + len(original_text)\n",
    "\n",
    "        label_to_text_list = post_process(predicted_text, original_text, tokens, offset)\n",
    "\n",
    "        span_tokens_to_label_list = []\n",
    "        for label, text_list in label_to_text_list.items():\n",
    "            for text in text_list:\n",
    "                span_tokens = make_span_tokens(tokens, text['start'], text['end'])\n",
    "                span_tokens_to_label_list.append({\n",
    "                    'span_tokens': span_tokens,\n",
    "                    'label': label\n",
    "                })\n",
    "\n",
    "        for span_tokens_to_label in span_tokens_to_label_list:\n",
    "            span_tokens = span_tokens_to_label['span_tokens']\n",
    "            label = span_tokens_to_label['label']\n",
    "            if span_tokens is None:\n",
    "                continue\n",
    "            annotation = utils.make_annotation(tokens=span_tokens, label=label)\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    predicted_doc = utils.replace_webanno_annotations(doc, annotations=annotations)\n",
    "\n",
    "    # Optional verification\n",
    "    if doc.text != predicted_doc.text:\n",
    "        pass\n",
    "    if len(doc.sentences) != len(predicted_doc.sentences):\n",
    "        pass\n",
    "    if len(doc.tokens) != len(predicted_doc.tokens):\n",
    "        pass\n",
    "\n",
    "    for s1, s2 in zip(doc.sentences, predicted_doc.sentences):\n",
    "        if s1 != s2:\n",
    "            pass\n",
    "    for t1, t2 in zip(doc.tokens, predicted_doc.tokens):\n",
    "        if t1 != t2:\n",
    "            pass\n",
    "\n",
    "    output_path = f\"../results/deepseek-chat/test_unlabeled_up/{file_base}.tsv\"\n",
    "    with open(output_path, 'w') as fd:\n",
    "        fd.write(predicted_doc.tsv())\n",
    "    print(f\"Saved predicted file to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae09fe",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662cf4df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
