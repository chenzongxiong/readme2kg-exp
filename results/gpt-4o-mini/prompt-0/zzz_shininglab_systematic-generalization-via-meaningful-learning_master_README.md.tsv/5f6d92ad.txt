```markdown
Please find the example pipeline shown below.  ### Models + LSTM - <PROGLANG>lstm_luong_wmt_en_de</PROGLANG> + <PROGLANG>Transformer</PROGLANG> - <PROGLANG>transformer_iwslt_de_en</PROGLANG> + <PROGLANG>Dynamic Conv.</PROGLANG> - <PROGLANG>lightconv_iwslt_de_en</PROGLANG>  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c <DATASET>iwslt14.tokenized.de-en/code</DATASET> <DATASET>iwslt14.tokenized.de-en/iwslt14.vocab.en</DATASET> <DATASET>iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe</DATASET> ```  ### Preprocessing ``` TEXT=examples/translation/<DATASET>iwslt14.tokenized.de-en</DATASET> fairseq-preprocess --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/<DATASET>iwslt14.tokenized.de-en</DATASET> \     --workers 20 ```  ### Training <PROGLANG>LSTM</PROGLANG> ``` fairseq-train \     data-bin/<DATASET>iwslt14.tokenized.de-en</DATASET> \     -s en -t de \     --arch <PROGLANG>lstm_luong_wmt_en_de</PROGLANG> --share-decoder-input-output-embed \     --optimizer <PROGLANG>adam</PROGLANG> --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler <PROGLANG>inverse_sqrt</PROGLANG> --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion <EVALMETRIC>label_smoothed_cross_entropy</EVALMETRIC> --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` <PROGLANG>Transformer</PROGLANG> ``` fairseq-train \     data-bin/<DATASET>iwslt14.tokenized.de-en</DATASET> \     -s en -t de \     --arch <PROGLANG>transformer_iwslt_de_en</PROGLANG> --share-decoder-input-output-embed \     --optimizer <PROGLANG>adam</PROGLANG> --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler <PROGLANG>inverse_sqrt</PROGLANG> --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion <EVALMETRIC>label_smoothed_cross_entropy</EVALMETRIC> --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` <PROGLANG>Dynamic Conv.</PROGLANG> ``` fairseq-train \     data-bin/<DATASET>iwslt14.tokenized.de-en</DATASET> \     -s en -t de \     --arch <PROGLANG>lightconv_iwslt_de_en</PROGLANG> \     --optimizer <PROGLANG>adam</PROGLANG> --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler <PROGLANG>inverse_sqrt</PROGLANG> --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion <EVALMETRIC>label_smoothed_cross_entropy</EVALMETRIC> --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation <EVALMETRIC>BLEU</EVALMETRIC> ``` fairseq-generate data-bin/<DATASET>iwslt14.tokenized.de-en</DATASET> \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring <EVALMETRIC>bleu</EVALMETRIC> --remove-bpe --cpu >bleu.log 2>&1 & ``` <EVALMETRIC>ScareBLEU</EVALMETRIC> ``` fairseq-generate data-bin/<DATASET>iwslt14.tokenized.de-en</DATASET> \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring <EVALMETRIC>sacrebleu</EVALMETRIC> --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = "Revisit Systematic Generalization via Meaningful Learning",     author = "Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan",     booktitle = "<CONFERENCE>Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</CONFERENCE>",     month = dec,     year = "2022",     address = "Abu Dhabi, United Arab Emirates (Hybrid)",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2022.blackboxnlp-1.6",     pages = "62--79",     abstract = "Humans can systematically generalize to novel compositions of existing concepts.
```