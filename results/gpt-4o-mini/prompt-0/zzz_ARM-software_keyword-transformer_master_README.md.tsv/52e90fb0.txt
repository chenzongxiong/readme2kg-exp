The model-specific arguments for KWT are:  ```shell --num_layers 12 \ #number of sequential transformer encoders --heads 3 \ #number of attentions heads --d_model 192 \ #embedding dimension --mlp_dim 768 \ #mlp-dimension --dropout1 0. \ #dropout in mlp/multi-head attention blocks --attention_type 'time' \ #attention type: 'time', 'freq', 'both' or 'patch' --patch_size '1,40' \ #spectrogram patch_size, if patch attention is used --prenorm False \ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (<SOFTWARE>Att-MH-RNN</SOFTWARE>), similar to the approach in [<CONFERENCE>DeIT</CONFERENCE>](https://github.com/facebookresearch/deit).