Then, run the following commands to evaluate on <EVALMETRIC>REC</EVALMETRIC> and <EVALMETRIC>RES</EVALMETRIC> task: ``` # Evaluate <EVALMETRIC>REC</EVALMETRIC> performance python tools/extract_mrcn_ref_feats.py --dataset <DATASET>refcoco</DATASET> --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset <DATASET>refcoco</DATASET> --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate <EVALMETRIC>RES</EVALMETRIC> performance python tools/run_propose_to_mask.py --dataset <DATASET>refcoco</DATASET> --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset <DATASET>refcoco</DATASET> --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **<SOFTWARE>MAttNet</SOFTWARE>-style detection file** (note the <SOFTWARE>MattNet</SOFTWARE>-style detection files can be directly used to evaluate downstream <EVALMETRIC>REG</EVALMETRIC> task performance).