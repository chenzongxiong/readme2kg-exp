Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<DATASET>_<split_by>.txt` - selected GloVe feature: `std_glove_<DATASET>_<split_by>.npy` - referring expression database: `std_refdb_<DATASET>_<split_by>.json` - critical objects database: `std_ctxdb_<DATASET>_<split_by>.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset <DATASET>refcoco</DATASET> --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset <DATASET>refcoco</DATASET> --split-by unc ```  We use tensorboard to monitor the training process.