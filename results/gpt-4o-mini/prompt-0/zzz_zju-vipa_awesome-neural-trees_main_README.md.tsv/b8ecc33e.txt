```markdown
abstract_id=4210199)   - ***Bigot NDTs for knowledge distillation***   - **"<PUBLICATION>Tree-Like Decision Distillation</PUBLICATION>"**, <CONFERENCE>CVPR</CONFERENCE>, 2021     - Jie Song *et al.* *(layer-wise dissect the decision process of a DNN)*     - [[Paper]](https://openaccess.thecvf.com/content/<CONFERENCE>CVPR2021</CONFERENCE>/html/Song_Tree-Like_Decision_Distillation_CVPR_2021_paper.html)   - **"<PUBLICATION>Tree-Like Branching Network for Multi-class Classification</PUBLICATION>"**, <CONFERENCE>LNNS</CONFERENCE>, 2021     - Mengqi Xue, Jie Song, Li Sun, Mingli Song *(mine the underlying category relationships from a trained teacher network and determines the appropriate layers on which specialized branches grow)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-93247-3_18)   - **"<PUBLICATION>Distilling a Neural Network Into a Soft Decision Tree</PUBLICATION>"**, <CONFERENCE>AI*IA</CONFERENCE>, 2017     - Nicholas Frosst, Geoffrey Hinton *(use a trained NN to provide soft targets for training a fuzzy NDT)*     - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)   - **"<PUBLICATION>TNT: An Interpretable Tree-Network-Tree Learning Framework using Knowledge Distillation</PUBLICATION>"**, <PUBLICATION>Entropy</PUBLICATION>, 2020     - Jiawei Li *et al.* *(transfer knowledge between tree models and DNNs)*     - [[Paper]](https://www.mdpi.com/1099-4300/22/11/1203)   - **"<PUBLICATION>KDExplainer: A Task-oriented Attention Model for Explaining Knowledge Distillation</PUBLICATION>"**, <PUBLICATION>arXiv</PUBLICATION>, 2021     - Mengqi Xue *et al.*     - [[Paper]](https://arxiv.org/abs/2105.04181)  #### 3.2 Expert NDTs (NDTs without Class Hierarchies) NDTs without class hierarchies restrain themselves little and perform arbitrary predictions at the leaves.
```