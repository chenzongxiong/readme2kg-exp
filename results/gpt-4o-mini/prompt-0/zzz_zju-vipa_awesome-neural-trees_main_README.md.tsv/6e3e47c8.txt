```markdown
- ***<PUBLICATION>Hierarchical Mixtures of Experts (HME)</PUBLICATION>***   - **"<PUBLICATION>Hierarchical Mixtures of Experts and the EM Algorithm</PUBLICATION>", <PUBLICATION>Neural computation</PUBLICATION>, 1994**      - <PERSON>Michael I Jordan</PERSON>, <PERSON>Robert A Jacobs</PERSON> *(the original HME, a tree-structured model for regression and classification.)*     - [[<PUBLICATION>Paper</PUBLICATION>]](https://ieeexplore.ieee.org/abstract/document/6796382)   - **"<PUBLICATION>Classification using hierarchical mixtures of experts</PUBLICATION>", <PUBLICATION>NNSP</PUBLICATION>, 1994**     - <PERSON>Steve R Waterhouse</PERSON>, <PERSON>Anthony J Robinson</PERSON> *(each leaf expert is non-linear and performs multi-way classification)*     - [[<PUBLICATION>Paper</PUBLICATION>]](https://ieeexplore.ieee.org/abstract/document/366050)   - **"<PUBLICATION>Bayesian Hierarchical Mixtures of Experts</PUBLICATION>", <PUBLICATION>arXiv</PUBLICATION>, 2012**     - <PERSON>Christopher M Bishop</PERSON>, <PERSON>Naonori Ueda</PERSON>, <PERSON>Steve Waterhouse</PERSON> *(bayesian treatments of the HME model to prevent the severe overfitting caused by maximum likelihood)*     - [[<PUBLICATION>Paper</PUBLICATION>]](https://arxiv.org/abs/1212.2447)     - ***<PUBLICATION>Generalized HMEs in advanced frameworks</PUBLICATION>***   - **"<PUBLICATION>Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization</PUBLICATION>", <CONFERENCE>CVPR</CONFERENCE>, 2020**     - <PERSON>Ruyi Ji</PERSON> *et al.* *(incorporate convolutional operations along edges and use attention transformer modules to capture discriminative features)*     - [[<PUBLICATION>Paper</PUBLICATION>]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ji_Attention_Convolutional_Binary_Neural_Tree_for_Fine-Grained_Visual_Categorization_CVPR_2020_paper.html) [[<SOFTWARE>Code</SOFTWARE>]](https://isrc.iscas.ac.cn/gitlab/research/acnet)   - **"<PUBLICATION>NDT: Neual Decision Tree Towards Fully Functioned Neural Graph</PUBLICATION>", <PUBLICATION>arXiv</PUBLICATION>, 2017**     - <PERSON>Han Xiao</PERSON> *(reformulate the non-differentiable information gain in the form of Dirac symbol and approximate it as a continuous function)*     - [[<PUBLICATION>Paper</PUBLICATION>]](https://arxiv.org/abs/1712.05934)   - **"<PUBLICATION>Decision Forests, Convolutional Networks and the Models in-Between</PUBLICATION>", <PUBLICATION>arXiv</PUBLICATION>, 2016**     - <PERSON>Yani Ioannou</PERSON> *et al.* *(hybrid model between decision forests and convolutional networks)*     - [[<PUBLICATION>Paper</PUBLICATION>]](https://arxiv.org/abs/1603.01250)   - **"<PUBLICATION>Deep Neural Decision Trees</PUBLICATION>", <PUBLICATION>arXiv</PUBLICATION>, 2018**     - <PERSON>Yongxin Yang</PERSON>, <PERSON>Irene Garcia Morillo</PERSON>, <PERSON>Timothy M Hospedales</PERSON> *(bin each feature of the input instance and determine the leaf node it will arrive)*     - [[<PUBLICATION>Paper</PUBLICATION>]](https://arxiv.org/abs/1806.06988) [[<SOFTWARE>Code</SOFTWARE>]](https://github.com/wOOL/DNDT)   - **"<PUBLICATION>ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder</PUBLICATION>", <CONFERENCE>ICML</CONFERENCE>, 2022**     - <PERSON>Sangwon Kim</PERSON>, <PERSON>Jaeyeal Nam</PERSON>, <PERSON>Byoung Chul Ko</PERSON> *(transformer version of ProtoTree with expert leaves)*     - [[<PUBLICATION>Paper</PUBLICATION>]](https://proceedings.mlr.press/v162/kim22g.html) [[<SOFTWARE>Code</SOFTWARE>]](https://github.com/jumpsnack/ViT-NeT)  - ***<PUBLICATION>Expert NDTs with architecture search phase</PUBLICATION>***   - **"<PUBLICATION>Adaptive Neural Trees</PUBLICATION>", <CONFERENCE>ICML</CONFERENCE>, 2019**     - <PERSON>Ryutaro Tanno</PERSON> *et al.* *(greedily choosing the best option between going deeper and splitting the input space)*     - [[<PUBLICATION>Paper</PUBLICATION>]](http://proceedings.mlr.press/v97/tanno19a.html?
```