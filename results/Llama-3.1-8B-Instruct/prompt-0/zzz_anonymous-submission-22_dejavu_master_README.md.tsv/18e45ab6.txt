Run experiments in the shell of the Docker container following the usage table as follows.
### Usage |Algorithm|Usage| |---|---| |<SOFTWARE>DejaVu</SOFTWARE>|Run for dataset A1: `python exp/run_GAT_node_classification.py -H=4 -L=8 -fe=GRU -bal=True --data_dir=data/A1`| |<PUBLICATION>JSS'20</PUBLICATION>|Run for dataset A1: `python exp/DejaVu/run_JSS20.py --data_dir=data/A1`| |<DATASET>iSQUAD</DATASET>|Run for dataset A1: `python exp/DejaVu/run_iSQ.py --data_dir=data/A1`| |<SOFTWARE>Decision Tree</SOFTWARE>|Run for dataset A1: `python exp/run_DT_node_classification.py --data_dir=data/A1`| |<SOFTWARE>RandomWalk@Metric</SOFTWARE>|Run for dataset A1: `python exp/DejaVu/run_random_walk_single_metric.py --data_dir=data/A1 --window_size 60 10 --score_aggregation_method=min`| |<SOFTWARE>RandomWalk@FI</SOFTWARE>|Run for dataset A1: `python exp/DejaVu/run_random_walk_failure_instance.py --data_dir=data/A1 --window_size 60 10 --anomaly_score_aggregation_method=min --corr_aggregation_method=max`| |<SOFTWARE>Global interpretation</SOFTWARE>|Run `notebooks/explain.py` as a jupyter notebook with `jupytext`| |<SOFTWARE>Local interpretation</SOFTWARE>|`DejaVu/explanability/similar_faults.py`|  The commands would print a `one-line summary` in the end, including the following fields: `A@1`, `A@2`, `A@3`, `A@5`, `MAR`, `Time`, `Epoch`, `Valid Epoch`, `output_dir`, `val_loss`, `val_MAR`, `val_A@1`, `command`, `git_commit_url`, which are the desrired results.