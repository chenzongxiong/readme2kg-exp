# <PROJECT>MLLM-Bench</PROJECT> <PROJECT>MLLM-Bench</PROJECT>: Evaluating Multimodal LLMs with Per-sample Criteria <center> ![M2M-100](https://github.com/mlml-bench/mlml-bench/blob/main/docs/m2m100.png)<br/>  This benchmark focuses on <CONFERENCE>CVPR 2023</CONFERENCE> workshop, which aims to address the challenges of multimodal learning and model evaluation. The benchmark includes several <DATASET> datasets, such as <DATASET> M2M-100</DATASET> and <DATASET> M2M-100v2</DATASET>, and evaluates the performance of various <SOFTWARE> models</SOFTWARE>, including <SOFTWARE>CLIP</SOFTWARE> and <SOFTWARE> ViLBERT</SOFTWARE>. The evaluation metrics used are <EVALMETRIC>Precision</EVALMETRIC>, <EVALMETRIC>Recall</EVALMETRIC>, and <EVALMETRIC>F1-Score</EVALMETRIC>. The benchmark is released under the <LICENSE>CC BY-NC 4.0</LICENSE> license and is available at <SOFTWARE>GitHub</SOFTWARE>.