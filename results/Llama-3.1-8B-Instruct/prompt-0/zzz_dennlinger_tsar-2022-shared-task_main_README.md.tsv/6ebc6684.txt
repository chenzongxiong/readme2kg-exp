- English <DATASET>test_none</DATASET> dataset (373 instances)<br/>  /datasets/test/<DATASET>tsar2022_en_test_none.tsv</DATASET>    - Spanish <DATASET>test_none</DATASET> dataset (368 instances)<br/>  /datasets/test/<DATASET>tsar2022_es_test_none.tsv</DATASET>    - Portuguese <DATASET>test_none</DATASET> dataset (374 instances)<br/>  /datasets/test/<DATASET>tsar2022_pt_test_none.tsv</DATASET>  The *<DATASET>test_gold</DATASET>* files contain the sentences, target complex words, and gold annotations<br/>   - English <DATASET>test_gold</DATASET> dataset (373 instances)<br/>  /datasets/test/<DATASET>tsar2022_en_test_gold.tsv</DATASET>    - Spanish <DATASET>test_gold</DATASET> dataset (368 instances)<br/>  /datasets/test/<DATASET>tsar2022_es_test_gold.tsv</DATASET>    - Portuguese <DATASET>test_gold</DATASET> dataset (374 instances)<br/>  /datasets/test/<DATASET>tsar2022_pt_test_gold.tsv</DATASET>   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/<DATASET>official</DATASET>  The following 10 metrics are reported in the official results: -  <EVALMETRIC>MAP@1</EVALMETRIC>/<EVALMETRIC>Potential@1</EVALMETRIC>/<EVALMETRIC>Precision@1</EVALMETRIC> -  <EVALMETRIC>MAP@3</EVALMETRIC> -  <EVALMETRIC>MAP@5</EVALMETRIC> -  <EVALMETRIC>MAP@10</EVALMETRIC> -  <EVALMETRIC>Potential@3</EVALMETRIC> -  <EVALMETRIC>Potential@5</EVALMETRIC> -  <EVALMETRIC>Potential@10</EVALMETRIC> -  <EVALMETRIC>Accuracy@1@top_gold_1</EVALMETRIC> -  <EVALMETRIC>Accuracy@2@top_gold_1</EVALMETRIC> -  <EVALMETRIC>Accuracy@3@top_gold_1</EVALMETRIC>    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/<DATASET>extended</DATASET><br/>   The following metrics are reported in the extended results: -  <EVALMETRIC>Potential@K</EVALMETRIC>  K={1..10}  -  <EVALMETRIC>MAP@K</EVALMETRIC>  K={1..10} -  <EVALMETRIC>Precision@K</EVALMETRIC>  K={1..10}  (macro-average) -  <EVALMETRIC>Recall@K</EVALMETRIC>  K={1..10}     (macro-average) -  <EVALMETRIC>Accuracy@K@top_gold_1</EVALMETRIC>   K={1..10}     ## Evaluation Scripts   ### <SOFTWARE>tsar_eval.py</SOFTWARE>  This script evaluates the following metric:      -  <EVALMETRIC>MAP@1</EVALMETRIC>/<EVALMETRIC>Potential@1</EVALMETRIC>/<EVALMETRIC>Precision@1</EVALMETRIC>     -  <EVALMETRIC>MAP@3</EVALMETRIC>     -  <EVALMETRIC>MAP@5</EVALMETRIC>     -  <EVALMETRIC>MAP@10</EVALMETRIC>     -  <EVALMETRIC>Potential@3</EVALMETRIC>     -  <EVALMETRIC>Potential@5</EVALMETRIC>     -  <EVALMETRIC>Potential@10</EVALMETRIC>     -  <EVALMETRIC>Accuracy@1@top_gold_1</EVALMETRIC>     -  <EVALMETRIC>Accuracy@2@top_gold_1</EVALMETRIC>     -  <EVALMETRIC>Accuracy@3@top_gold_1</EVALMETRIC>          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: <SOFTWARE>tsar_eval.py</SOFTWARE> <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3.