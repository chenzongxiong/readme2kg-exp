*   abstract_id=4210199)   - ***Bigot NDTs for knowledge distillation***   - **"Tree-Like Decision Distillation"**, <CONFERENCE>CVPR 2021</CONFERENCE>     - Jie Song *et al.* *(layer-wise dissect the decision process of a DNN)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Song_Tree-Like_Decision_Distillation_CVPR_2021_paper.html)   - **"Tree-Like Branching Network for Multi-class Classification"**, <SOFTWARE>LNNS</SOFTWARE>, 2021     - Mengqi Xue, Jie Song, Li Sun, Mingli Song *(mine the underlying category relationships from a trained teacher network and determines the appropriate layers on which specialized branches grow)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-93247-3_18)   - **"Distilling a Neural Network Into a Soft Decision Tree"**, <SOFTWARE>AI*IA</SOFTWARE>, 2017     - Nicholas Frosst, Geoffrey Hinton *(use a trained NN to provide soft targets for training a fuzzy NDT)*     - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)   - **"TNT: An Interpretable Tree-Network-Tree Learning Framework using Knowledge Distillation"**, <SOFTWARE>Entropy</SOFTWARE>, 2020     - Jiawei Li *et al.* *(transfer knowledge between tree models and DNNs)*     - [[Paper]](https://www.mdpi.com/1099-4300/22/11/1203)   - **"KDExplainer: A Task-oriented Attention Model for Explaining Knowledge Distillation"**, <SOFTWARE>arXiv</SOFTWARE>, 2021     - Mengqi Xue *et al.*     - [[Paper]](https://arxiv.org/abs/2105.04181)  #### 3.2 Expert NDTs (NDTs without Class Hierarchies) NDTs without class hierarchies restrain themselves little and perform arbitrary predictions at the leaves.