* ***Hierarchical Mixtures of Experts (HME)***   - **"Hierarchical Mixtures of Experts and the EM Algorithm"**, <PUBLICATION>Neural computation</PUBLICATION>, 1994      - Michael I Jordan, Robert A Jacobs *(the original HME, a tree-structured model for regression and classification.)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6796382)   - **"Classification using hierarchical mixtures of experts"**, <PUBLICATION>NNSP</PUBLICATION>, 1994     - Steve R Waterhouse, Anthony J Robinson *(each leaf expert is non-linear and performs multi-way classification)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/366050)   - **"Bayesian Hierarchical Mixtures of Experts"**, <PUBLICATION>arXiv</PUBLICATION>, 2012     - Christopher M Bishop, Naonori Ueda, Steve Waterhouse *(bayesian treatments of the HME model to prevent the severe overfitting caused by maximum likelihood)*     - [[Paper]](https://arxiv.org/abs/1212.2447)     - ***Generalized HMEs in advanced frameworks***   - **"Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization"**, <CONFERENCE>CVPR</CONFERENCE>, 2020     - Ruyi Ji *et al.* *(incorporate convolutional operations along edges and use attention transformer modules to capture discriminative features)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ji_Attention_Convolutional_Binary_Neural_Tree_for_Fine-Grained_Visual_Categorization_CVPR_2020_paper.html) [[Code]](https://isrc.iscas.ac.cn/gitlab/research/acnet)   - **"NDT: Neual Decision Tree Towards Fully Functioned Neural Graph"**, <PUBLICATION>arXiv</PUBLICATION>, 2017     - Han Xiao *(reformulate the non-differentiable information gain in the form of Dirac symbol and approximate it as a continuous function)*     - [[Paper]](https://arxiv.org/abs/1712.05934)   - **"Decision Forests, Convolutional Networks and the Models in-Between"**, <PUBLICATION>arXiv</PUBLICATION>, 2016     - Yani Ioannou *et al.* *(hybrid model between decision forests and convolutional networks)*     - [[Paper]](https://arxiv.org/abs/1603.01250)   - **"Deep Neural Decision Trees"**, <PUBLICATION>arXiv</PUBLICATION>, 2018     - Yongxin Yang, Irene Garcia Morillo, Timothy M Hospedales *(bin each feature of the input instance and determine the leaf node it will arrive)*     - [[Paper]](https://arxiv.org/abs/1806.06988) [[Code]](https://github.com/wOOL/DNDT)   - **"ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder"**, <CONFERENCE>ICML</CONFERENCE>, 2022     - Sangwon Kim, Jaeyeal Nam, Byoung Chul Ko *(transformer version of ProtoTree with expert leaves)*     - [[Paper]](https://proceedings.mlr.press/v162/kim22g.html) [[Code]](https://github.com/jumpsnack/ViT-NeT)  - ***Expert NDTs with architecture search phase***   - **"Adaptive Neural Trees"**, <CONFERENCE>ICML</CONFERENCE>, 2019     - Ryutaro Tanno *et al.* *(greedily choosing the best option between going deeper and splitting the input space)*     - [[Paper]](http://proceedings.mlr.press/v97/tanno19a.html)