,sentence,entity_text,entity_type
0,"The MNIST dataset is included in the package but it must be unziped, on a Unix machine run the following command:  ``` unzip .",MNIST,DATASET
1,/datasets/mnist.zip -d .,mnist,DATASET
2,/datasets/mnist/train` and `.,mnist,DATASET
3,/datasets/mnist/test` needed for the experiments.,mnist,DATASET
4,"Locating and Detecting Language Model Grounding with Fakepedia  This repository contains the data and code to reproduce the results of our paper: https://arxiv.org/abs/2312.02073  Please use the following citation:  ``` @misc{monea2023glitch,       title={A Glitch in the Matrix?",Fakepedia,DATASET
5,"Locating and Detecting Language Model Grounding with Fakepedia},        author={Giovanni Monea and Maxime Peyrard and Martin Josifoski and Vishrav Chaudhary and Jason Eisner and Emre Kıcıman and Hamid Palangi and Barun Patra and Robert West},       year={2023},       eprint={2312.02073},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  > **Abstract:** Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context.",Fakepedia,DATASET
6,"We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge.",Fakepedia,DATASET
7,"We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries.",Fakepedia,DATASET
8,"We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries.",Fakepedia,DATASET
9,"We evaluate our HAIM framework by training and characterizing 14,324 independent models based on HAIM-MIMIC-MM, a multimodal clinical database (N=34,537 samples) containing 7,279 unique hospitalizations and 6,485 patients, spanning all possible input combinations of 4 data modalities (i.e., tabular, time-series, text, and images), 11 unique data sources and 12 predictive tasks.",HAIM-MIMIC-MM,DATASET
10,"Noteevents.csv are public and available for download at Physionet.org; however, other ""NOTES"" data requires pre-release direct permission from Physionet.org for download as ""discharge notes"", ""radiology notes"", ""ECG notes"" and ""ECHO notes"" are not yet publicly released for MIMIC-IV as of Sep 2022, these files are: ds_icustay.csv, ecg_icustay.csv, echo_icustay.csv, rad_icustay.csv).",MIMIC-IV,DATASET
11,"Please be advised that sufficient RAM or cluster access to parallel processing is needed to run these experiments.  ### UPDATE (Jan. 6, 2023) The radiology and the discharge notes for MIMIC-IV have been officially released on: https://physionet.org/content/mimic-iv-note/2.2/note/#files-panel  ### UPDATE (Jun. 12, 2023) For the publication, our team generated the file 'mimic-cxr-2.0.0-jpeg-txt.csv' by compiling an early-release version of participant notes and text from the images in CXR corresponding to MIMIC-IV.",MIMIC-IV,DATASET
12,"Please be advised that sufficient RAM or cluster access to parallel processing is needed to run these experiments.  ### UPDATE (Jan. 6, 2023) The radiology and the discharge notes for MIMIC-IV have been officially released on: https://physionet.org/content/mimic-iv-note/2.2/note/#files-panel  ### UPDATE (Jun. 12, 2023) For the publication, our team generated the file 'mimic-cxr-2.0.0-jpeg-txt.csv' by compiling an early-release version of participant notes and text from the images in CXR corresponding to MIMIC-IV.",mimic-iv-note,DATASET
13,"Please be advised that sufficient RAM or cluster access to parallel processing is needed to run these experiments.  ### UPDATE (Jan. 6, 2023) The radiology and the discharge notes for MIMIC-IV have been officially released on: https://physionet.org/content/mimic-iv-note/2.2/note/#files-panel  ### UPDATE (Jun. 12, 2023) For the publication, our team generated the file 'mimic-cxr-2.0.0-jpeg-txt.csv' by compiling an early-release version of participant notes and text from the images in CXR corresponding to MIMIC-IV.",MIMIC-IV,DATASET
14,"As of June 12, 2023, Physionet has not fully released these notes, but it is likely they are planning to do so as part of their full release of MIMIC-IV.",MIMIC-IV,DATASET
15,The CoNLL04 and ADE datasets (joint entity and relation extraction) in the correct format can be downloaded using https://github.com/markus-eberts/spert/blob/master/scripts/fetch_datasets.sh.,CoNLL04,DATASET
16,The CoNLL04 and ADE datasets (joint entity and relation extraction) in the correct format can be downloaded using https://github.com/markus-eberts/spert/blob/master/scripts/fetch_datasets.sh.,ADE,DATASET
17,"For example, to replicate the paper's results on CoNLL04, have the following section in the config file: ``` [conll04_final] datasets = conll04 model_name_or_path = t5-base num_train_epochs = 200 max_seq_length = 256 max_seq_length_eval = 512 train_split = train,dev per_device_train_batch_size = 8 per_device_eval_batch_size = 16 do_train = True do_eval = False do_predict = True episodes = 1-10 num_beams = 8 ``` Then run `python run.py conll04_final`.",CoNLL04,DATASET
18,"For example, to replicate the paper's results on CoNLL04, have the following section in the config file: ``` [conll04_final] datasets = conll04 model_name_or_path = t5-base num_train_epochs = 200 max_seq_length = 256 max_seq_length_eval = 512 train_split = train,dev per_device_train_batch_size = 8 per_device_eval_batch_size = 16 do_train = True do_eval = False do_predict = True episodes = 1-10 num_beams = 8 ``` Then run `python run.py conll04_final`.",conll04,DATASET
19,"For example, to replicate the paper's results on CoNLL04, have the following section in the config file: ``` [conll04_final] datasets = conll04 model_name_or_path = t5-base num_train_epochs = 200 max_seq_length = 256 max_seq_length_eval = 512 train_split = train,dev per_device_train_batch_size = 8 per_device_eval_batch_size = 16 do_train = True do_eval = False do_predict = True episodes = 1-10 num_beams = 8 ``` Then run `python run.py conll04_final`.",conll04,DATASET
20,"For example, to replicate the paper's results on CoNLL04, have the following section in the config file: ``` [conll04_final] datasets = conll04 model_name_or_path = t5-base num_train_epochs = 200 max_seq_length = 256 max_seq_length_eval = 512 train_split = train,dev per_device_train_batch_size = 8 per_device_eval_batch_size = 16 do_train = True do_eval = False do_predict = True episodes = 1-10 num_beams = 8 ``` Then run `python run.py conll04_final`.",conll04,DATASET
21,"The final weights and intermediate checkpoints are written in a directory such as `experiments/conll04_final-t5-base-ep200-len256-b8-train`, with one subdirectory per episode.",conll04,DATASET
22,"For this, set `do_train = False` or (more easily) provide the `-e` command-line argument: `python run.py conll04_final -e`.",conll04,DATASET
23,"For example, to test the multi-task model on the CoNLL04 dataset, run `python run.py multitask -e --eval_datasets conll04`.",CoNLL04,DATASET
24,"For example, to test the multi-task model on the CoNLL04 dataset, run `python run.py multitask -e --eval_datasets conll04`.",conll04,DATASET
25,"/a2t/topic_classification/) evaluated on BabelDomains (Camacho- Collados and Navigli, 2017)  dataset. - [Relation classification](.",BabelDomains,DATASET
26,"/a2t/relation_classification/) evaluated on TACRED (Zhang et al., 2017) dataset. -->  To get started with the repository consider reading the **new** [documentation](https://osainz59.github.io/Ask2Transformers)!",TACRED,DATASET
27,"-- $$\text{HiTZ/A2T\_[pretrained\_model]\_[NLI\_datasets]\_[finetune\_datasets]}$$ -->  <h3 align=""center"">HiTZ/A2T_[pretrained_model]_[NLI_datasets]_[finetune_datasets]</h3>   - `pretrained_model`: The checkpoint used for initialization.",NLI,DATASET
28,For example: RoBERTa<sub>large</sub>. - `NLI_datasets`: The NLI datasets used for pivot training,NLI,DATASET
29,- `S`: Standford Natural Language Inference (SNLI) dataset,Standford Natural Language Inference,DATASET
30,- `S`: Standford Natural Language Inference (SNLI) dataset,SNLI,DATASET
31,- `M`: Multi Natural Language Inference (MNLI) dataset,Multi Natural Language Inference,DATASET
32,- `M`: Multi Natural Language Inference (MNLI) dataset,MNLI,DATASET
33,- `F`: Fever-nli dataset,Fever-nli,DATASET
34,- `A`: Adversarial Natural Language Inference (ANLI) dataset. - `finetune_datasets`: The datasets used for fine tuning the entailment model.,Adversarial Natural Language Inference,DATASET
35,- `A`: Adversarial Natural Language Inference (ANLI) dataset. - `finetune_datasets`: The datasets used for fine tuning the entailment model.,ANLI,DATASET
36,For example: ACE-arg.,ACE-arg,DATASET
37,Some models like `HiTZ/A2T_RoBERTa_SMFA_ACE-arg` have been trained marking some information between square brackets (`'[['` and `']]'`) like the event trigger span.,ACE-arg,DATASET
38,"To train your own model, first, you will need to convert your actual dataset in some sort of NLI data, we recommend you to have a look to [tacred2mnli.py](https://github.com/osainz59/Ask2Transformers/blob/master/scripts/tacred2mnli.py) script that serves as an example",NLI,DATASET
39,"""Royalty and nobility"",         ""Sport and recreation"",         ""Textile and clothing"",         ""Transport and travel"",         ""Warfare and defense""     ],     ""preprocess_labels"": true,     ""dataset"": ""babeldomains"",     ""test_path"": ""data/babeldomains.domain.gloss.tsv"",     ""use_cuda"": true,     ""half"": true } ```  Consider reading the papers to access the results",babeldomains,DATASET
40,"In our experiments on TACRED we attain 63{\%} F1 zero-shot, 69{\%} with 16 examples per relation (17{\%} points better than the best supervised system on the same conditions), and only 4 points short to the state-of-the-art (which uses 20 times more training data).",TACRED,DATASET
41,"We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, allowing to report the best results to date on TACRED when fully trained.",TACRED,DATASET
42,"RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the ""secret"" ingredient of ChatGPT and GPT4.   ### Milestone of AI Robo-Advisor: FinGPT-Forecaster  Try the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)  The dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405  !",FinGPT/fingpt-forecaster-dow30-202305-202405,DATASET
43,"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 × RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 × A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 × A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 × RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 × RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 × NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 × A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 × A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",TFNS,DATASET
44,"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 × RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 × A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 × A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 × RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 × RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 × NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 × A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 × A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",NWGI,DATASET
45,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",fingpt-sentiment-train,DATASET
46,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",FinGPT/fingpt-sentiment-train,DATASET
47,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",fingpt-finred,DATASET
48,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",FinGPT/fingpt-finred,DATASET
49,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",FinGPT/fingpt-headline,DATASET
50,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",fingpt-ner,DATASET
51,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",FinGPT/fingpt-ner,DATASET
52,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",fingpt-fiqa_qa,DATASET
53,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",FinGPT/fingpt-fiqa_qa,DATASET
54,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",fingpt-fineval,DATASET
55,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",FinGPT/fingpt-fineval,DATASET
56,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",FPB,DATASET
57,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",financial_phrasebank,DATASET
58,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",FiQA-SA,DATASET
59,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",pauri32/fiqa-2018,DATASET
60,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",TFNS,DATASET
61,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",zeroshot/twitter-financial-news-sentiment,DATASET
62,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",NWGI,DATASET
63,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",oliverwang15/news_with_gpt_instructions,DATASET
64,"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet",PromptNet,DATASET
65,"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet",ImageNet,DATASET
66,"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet",WordNet,DATASET
67,"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet",PromptNet,DATASET
68,"Blunt](https://www.imperial.ac.uk/people/m.blunt)   *Department of Earth Science and Engineering, Imperial College London*    This is the code repository accompanying the publication:    *Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models*  [[ArXiv](http://arxiv.org/abs/1802.05622)]  ## Datasets and pre-trained models  ### Ketton Limestone Dataset We provide two pre-trained GAN models.",Ketton Limestone,DATASET
69,Due to the stochastic nature of the optimization procedure the resulting images have distinctly different features away from the conditioning data.   ### Maules Creek Dataset  We have trained a generative adversarial network on the Maules Creek alluvial aquifer training image.,Maules Creek,DATASET
70,Due to the stochastic nature of the optimization procedure the resulting images have distinctly different features away from the conditioning data.   ### Maules Creek Dataset  We have trained a generative adversarial network on the Maules Creek alluvial aquifer training image.,Maules Creek,DATASET
71,"If you choose to use the Maules Creek training image, please consider citing their originators at [trainingimages.org](www.trainingimages.org)    #### Results  !",Maules Creek,DATASET
72,[Maules Creek](figures/fig_2.png)  We have conditioned 1024 realizations of the Maules Creek alluvial aquifer model and present mean and standard deviation maps of the resulting ensemble.,Maules Creek,DATASET
73,[Maules Creek](figures/fig_2.png)  We have conditioned 1024 realizations of the Maules Creek alluvial aquifer model and present mean and standard deviation maps of the resulting ensemble.,Maules Creek,DATASET
74,"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",Caltech UCSD Birds,DATASET
75,"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",Butterfly200,DATASET
76,"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",Vegfru,DATASET
77,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2.",Butterfly200,DATASET
78,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2.",CUB_200_2011,DATASET
79,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2.",Vegfru,DATASET
80,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2.",Butterfly200,DATASET
81,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2.",CUB_200_2011,DATASET
82,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2.",Vegfru,DATASET
83,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2.",Butterfly200,DATASET
84,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2.",CUB_200_2011,DATASET
85,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2.",Vegfru,DATASET
86,"Download datasets  [Caltech UCSD Birds](http://www.vision.caltech.edu/visipedia/CUB-200.html) originally covers 200 classes of birds, and we  extend this dataset with a [four-level category hierarchy](https://www.dropbox.com/sh/kugj7vogy2no795/AABJWUxM6rXWOeNbCUPj269ua?",Caltech UCSD Birds,DATASET
87,"Download datasets  [Caltech UCSD Birds](http://www.vision.caltech.edu/visipedia/CUB-200.html) originally covers 200 classes of birds, and we  extend this dataset with a [four-level category hierarchy](https://www.dropbox.com/sh/kugj7vogy2no795/AABJWUxM6rXWOeNbCUPj269ua?",CUB-200,DATASET
88,[Butterfly 200](https://www.dropbox.com/sh/3p4x1oc5efknd69/AABwnyoH2EKi6H9Emcyd0pXCa?,Butterfly 200,DATASET
89,"Download trained models The trained models of our HSE framework and the baseline methods on the extended Caltech UCSD birds, Butterfly-200, and VegFru datasets can be downloaded from [OneDrive](https://1drv.ms/f/s!",Caltech UCSD birds,DATASET
90,"Download trained models The trained models of our HSE framework and the baseline methods on the extended Caltech UCSD birds, Butterfly-200, and VegFru datasets can be downloaded from [OneDrive](https://1drv.ms/f/s!",Butterfly-200,DATASET
91,"Download trained models The trained models of our HSE framework and the baseline methods on the extended Caltech UCSD birds, Butterfly-200, and VegFru datasets can be downloaded from [OneDrive](https://1drv.ms/f/s!",VegFru,DATASET
92,"/scripts/deploy_hse.sh [GPU_ID] [DATASET] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' ``` ## deploy baseline ``` .",CUB_200_2011,DATASET
93,"/scripts/deploy_hse.sh [GPU_ID] [DATASET] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' ``` ## deploy baseline ``` .",Butterfly200,DATASET
94,"/scripts/deploy_hse.sh [GPU_ID] [DATASET] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' ``` ## deploy baseline ``` .",Vegfru,DATASET
95,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details).",CUB_200_2011,DATASET
96,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details).",Butterfly200,DATASET
97,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details).",Vegfru,DATASET
98,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details).",CUB_200_2011,DATASET
99,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details).",Butterfly200,DATASET
100,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details).",Vegfru,DATASET
101,"Dataset is available at HuggingFace 🤗 : https://huggingface.co/datasets/greek_legal_code  *** ### Abstract  In this work, we study the task of classifying legal texts written in the Greek language.",greek_legal_code,DATASET
102,The source code is currently set up for the configuration of three clients performing secure and differentially private federated learning using logistic regresion on the MNIST dataset.,MNIST,DATASET
103,"In our example, it loads the MNIST dataset and processes it for the client agent instances.",MNIST,DATASET
104,"We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the ISBI 2012 EM segmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy datasets of cell nuclei.",ISBI 2012 EM segmentation benchmark,DATASET
105,"We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the ISBI 2012 EM segmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy datasets of cell nuclei.",BBBC010 C. elegans,DATASET
106,"We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the ISBI 2012 EM segmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy datasets of cell nuclei.",3d fluorescence microscopy,DATASET
107,"We show furthermore that our method also applies to 3d light microscopy data of drosophila neurons, which exhibit extreme cases of complex shape clusters.  ## Installation  This package requires Python 3 and PyTorch.",3d light microscopy data of drosophila neurons,DATASET
108,"The following instructions were tested on linux/ubuntu 20.04.   ``` conda create --name ppp --yes conda activate ppp conda install python=3.9 pytorch-cuda torchvision torchaudio cudatoolkit -c pytorch -c nvidia --yes git clone https://github.com/Kainmueller-Lab/PatchPerPix.git cd PatchPerPix PATH=/usr/local/cuda/bin:$PATH CUDA_ROOT=/usr/local/cuda pip install -e . ```  ## Organization - PatchPerPix: contains the code for our instance assembly pipeline to go from predictions to instances - experiments: contains the training and prediction code to generate predictions and the main script; contains one sub-folder per application/dataset   - `run_ppp.py`:  - main script to run the experiments  - command line arguments are used to select the experiment and the sub-task to be executed (training, inference etc, see below for an example)  - the parameters for the network training and the postprocessing have to be defined in a config file ([example config file](https://github.com/Kainmueller-Lab/PatchPerPix/blob/master/experiments/flylight/setups/setup01/default.toml))   - flylight: an example experiment for the FlyLight Instances Segmentation Benchmark Dataset  - setups: here the different experiment setups are placed, the python scripts should not be called manually, but will be called by the main script      - `train.py`: trains the network      - `predict_no_gp.py`: prediction after training      - `decode.py`: if ppp+dec is used, decode the predicted patch encodings to the full patches      - `default.toml`: example configuration file   - `default_train_code.toml`: example configuration file that uses ppp+dec   - `torch_loss.py`: auxiliary file for the loss computation   - `torch_model.py`: auxiliary file for the torch model definition   ## Data preparation  The code expects the data to be in the *zarr* format ([https://zarr.readthedocs.io/en/stable/](https://zarr.readthedocs.io/en/stable/)).",FlyLight Instances Segmentation Benchmark,DATASET
109,"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",nuclei3d,DATASET
110,"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",The FlyLight Instance Segmentation Datset,DATASET
111,# CaLiGraph for Semantic Reasoning Evaluation Challenge  ## SemREC'21  This repository contains code and data to use [CaLiGraph](http://caligraph.org) as a benchmark dataset in the [Semantic Reasoning Evaluation Challange](https://semrec.github.io) at the [International Semantic Web Conference 2021 (ISWC'21)](https://iswc2021.semanticweb.org).,CaLiGraph,DATASET
112,# CaLiGraph for Semantic Reasoning Evaluation Challenge  ## SemREC'21  This repository contains code and data to use [CaLiGraph](http://caligraph.org) as a benchmark dataset in the [Semantic Reasoning Evaluation Challange](https://semrec.github.io) at the [International Semantic Web Conference 2021 (ISWC'21)](https://iswc2021.semanticweb.org).,CaLiGraph,DATASET
113,The paper describing the dataset characteristics and results for well-known reasoners can be found [here](https://arxiv.org/pdf/2110.05028.pdf).  ### Datasets We use [CaLiGraph version 2.1.0](https://zenodo.org/record/5509912) as foundation for the challenge dataset.,CaLiGraph version 2.1.0,DATASET
114,"In particular, we use the files `caligraph-ontology.nt.bz2` and `caligraph-instances_types.nt.bz2` to generate our sample data.",caligraph,DATASET
115,The datasets and all potentially inferrable assertions can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).,CaLiGraph,DATASET
116,The datasets and all potentially inferrable assertions can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).,CaLiGraph,DATASET
117,The precomputed results for all the sample datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).   ## SemREC'22  For [SemREC'22](https://semrec.github.io) at the [International Semantic Web Conference 2022 (ISWC'22)](https://iswc2022.semanticweb.org) we provide [CaLiGraph](http://caligraph.org) as an additional challenge dataset for the participants of Task 2 (Systems).,CaLiGraph,DATASET
118,The precomputed results for all the sample datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).   ## SemREC'22  For [SemREC'22](https://semrec.github.io) at the [International Semantic Web Conference 2022 (ISWC'22)](https://iswc2022.semanticweb.org) we provide [CaLiGraph](http://caligraph.org) as an additional challenge dataset for the participants of Task 2 (Systems).,CaLiGraph,DATASET
119,The precomputed results for all the sample datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).   ## SemREC'22  For [SemREC'22](https://semrec.github.io) at the [International Semantic Web Conference 2022 (ISWC'22)](https://iswc2022.semanticweb.org) we provide [CaLiGraph](http://caligraph.org) as an additional challenge dataset for the participants of Task 2 (Systems).,CaLiGraph,DATASET
120,The precomputed results for all the sample datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).   ## SemREC'22  For [SemREC'22](https://semrec.github.io) at the [International Semantic Web Conference 2022 (ISWC'22)](https://iswc2022.semanticweb.org) we provide [CaLiGraph](http://caligraph.org) as an additional challenge dataset for the participants of Task 2 (Systems).,caligraph,DATASET
121,"For this purpose, we use the subsets `clg_10e4`, `clg_10e5`, and `clg_full` and split their inferrable assertions into training, validation, and test files.",clg_10e4,DATASET
122,"For this purpose, we use the subsets `clg_10e4`, `clg_10e5`, and `clg_full` and split their inferrable assertions into training, validation, and test files.",clg_10e5,DATASET
123,"For this purpose, we use the subsets `clg_10e4`, `clg_10e5`, and `clg_full` and split their inferrable assertions into training, validation, and test files.",clg_ful,DATASET
124,The split datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/SemREC-2022-Datasets/).,CaLiGraph,DATASET
125,The split datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/SemREC-2022-Datasets/).,CaLiGraph,DATASET
126,"To support the task, we collect PAVS10K, the first panoramic video dataset for audiovisual salient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting PAV-SOD, and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks.",PAVS10K,DATASET
127,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our PAVS10K.,PAVS10K,DATASET
128,"As a result, our CAV-Net outperforms all competing models and is able to estimate the aleatoric uncertainties within PAVS10K.",PAVS10K,DATASET
129,"We hope that our work could serve as a starting point for advancing SOD towards immersive media.  ------  # PAVS10K  <p align=""center"">     <img src="".",PAVS10K,DATASET
130,"/figures/fig_teaser.jpg""/> <br />     <em>      Figure 1: An example of our PAVS10K where coarse-to-fine annotations are provided, based on a guidance of fixations acquired from subjective experiments conducted by multiple (N) subjects wearing Head-Mounted Displays (HMDs) and headphones.",PAVS10K,DATASET
131,"/figures/fig_related_datasets.jpg""/> <br />     <em>      Figure 2: Summary of widely used salient object detection (SOD)/video object segmentation (VOS) datasets and PAVS10K.",PAVS10K,DATASET
132,"/figures/fig_dataset_examples.jpg""/> <br />     <em>      Figure 3: Examples of challenging attributes on equirectangular images from our PAVS10K, with instance-level ground truth and fixations as annotation guidance.",PAVS10K,DATASET
133,"/figures/fig_dataset_statistics.jpg""/> <br />     <em>      Figure 4: Statistics of the proposed PAVS10K.",PAVS10K,DATASET
134,"(c) Sound sources of PAVS10K scenes, such as musical instruments, human instances and animals.",PAVS10K,DATASET
135,> Note: The PAVS10K dataset does not own the copyright of videos.,PAVS10K,DATASET
136,"Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to PAVS10K.  ------  # Citation          @article{zhang2023pav,       title={PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={ACM Transactions on Multimedia Computing, Communications and Applications},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).",PAVS10K,DATASET
137,The image and text tensors will also have to be transferred accordingly.  ## Available Models  ```python import diht   print(diht.available_models()) ```  ## Example ImageNet-1K zero-shot evaluation  A simple image classification zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download ImageNet-1K dataset from the original website.,ImageNet-1K,DATASET
138,The image and text tensors will also have to be transferred accordingly.  ## Available Models  ```python import diht   print(diht.available_models()) ```  ## Example ImageNet-1K zero-shot evaluation  A simple image classification zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download ImageNet-1K dataset from the original website.,ImageNet-1K,DATASET
139,Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.,imagenet,DATASET
140,Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.,imagenet,DATASET
141,Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.,ImageNet1K,DATASET
142,Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.,COCO,DATASET
143,Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.,Flickr30K,DATASET
144,Json files (`coco_test.json` and `flickr30k_test.json`) can be downloaded from https://github.com/salesforce/ALBEF#download.,coco,DATASET
145,Json files (`coco_test.json` and `flickr30k_test.json`) can be downloaded from https://github.com/salesforce/ALBEF#download.,flickr30k,DATASET
146,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",COCO,DATASET
147,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",FLICKR30K,DATASET
148,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",COCO,DATASET
149,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",COCO,DATASET
150,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",Flickr30K,DATASET
151,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",Flickr30K,DATASET
152,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",ImageNet-1K,DATASET
153,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",COCO,DATASET
154,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",COCO,DATASET
155,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",Flickr30K,DATASET
156,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",Flickr30K,DATASET
157,"# HAGRID: A Human-LLM Collaborative Dataset for Generative Information-seeking with Attribution  <p align=""center""><img src=""https://github.com/project-miracl/hagrid/blob/main/assets/icon.png?",HAGRID,DATASET
158,"raw=true"" alt=""HAGRID"" width=""20%""><br> </p>  <p align=""center"">     <a href=""https://www.python.org/"">         <img alt=""Build"" src=""https://img.shields.io/badge/Made%20with-Python-1f425f.svg?",HAGRID,DATASET
159,"color=purple"">     </a>     <a href=""https://github.com/project-miracl/hagrid/blob/master/LICENSE"">         <img alt=""License"" src=""https://img.shields.io/github/license/project-miracl/hagrid"">     </a>     <a href=""https://arxiv.org/abs/2307.16883"">         <img alt=""arXiv"" src=""https://img.shields.io/badge/arXiv-2307.16883-b31b1b.svg"">     </a> </p>  HAGRID (**H**uman-in-the-loop **A**ttributable **G**enerative **R**etrieval for **I**nformation-seeking **D**ataset) is a dataset for generative information-seeking scenarios.",HAGRID,DATASET
160,"It is constructed on top of [MIRACL 🌍 🙌 🌏 ](HTTP://miracl.ai), an information retrieval dataset that consists of queries along with a set of manually labelled relevant passages (quotes).",MIRACL,DATASET
161,"It is constructed on top of [MIRACL 🌍 🙌 🌏 ](HTTP://miracl.ai), an information retrieval dataset that consists of queries along with a set of manually labelled relevant passages (quotes).",miracl,DATASET
162,(#baselines-coming-soon)   - [Contact](#contact)   - [License](#license)   - [Citation](#citation)  ## Data  HAGRID is hosted on Hugging Face 🤗 : [link](https://huggingface.co/datasets/miracl/hagrid).,HAGRID,DATASET
163,(#baselines-coming-soon)   - [Contact](#contact)   - [License](#license)   - [Citation](#citation)  ## Data  HAGRID is hosted on Hugging Face 🤗 : [link](https://huggingface.co/datasets/miracl/hagrid).,miracl/hagrid,DATASET
164,"```python import datasets hagrid = datasets.load_dataset(""miracl/hagrid"", split=""train"") print(hagrid[0]) ```  |  Split | #Q | #A       | #Informativeness  | #Attribuatability       | |:-----|:------:|:-------:|:------:|:-------:| | Train     | 1,922 | 3,214 | 3,214 | 754 | | Dev     | 716 | 1,318 | 1,157 | 826 |   ## Baselines (Coming soon!)",miracl/hagrid,DATASET
165,"See [LICENSE](LICENSE) for details.  ## Citation If you find this dataset and repository helpful, please cite HAGRID as follows: ``` @article{hagrid,       title={{HAGRID}: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution},        author={Ehsan Kamalloo and Aref Jafari and Xinyu Zhang and Nandan Thakur and Jimmy Lin},       year={2023},       journal={arXiv:2307.16883}, } ```",HAGRID,DATASET
166,"See [LICENSE](LICENSE) for details.  ## Citation If you find this dataset and repository helpful, please cite HAGRID as follows: ``` @article{hagrid,       title={{HAGRID}: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution},        author={Ehsan Kamalloo and Aref Jafari and Xinyu Zhang and Nandan Thakur and Jimmy Lin},       year={2023},       journal={arXiv:2307.16883}, } ```",HAGRID,DATASET
167,"# Run the Docker image docker run --gpus all -e CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES --rm --ipc host \    --mount type=bind,source=.,target=/code/ \    --mount type=bind,source=/dev/shm,target=/dev/shm \    borealtc-gpu python3 main.py ```  ## Dataset  The `data` directory contains two different datasets:  * the `BorealTC` dataset, our publicly available dataset * the `Vulpi` dataset, from the 2021 [paper](https://doi.org/10.1016/j.jterra.2020.12.002) of Vulpi et al.",BorealTC,DATASET
168,"# Run the Docker image docker run --gpus all -e CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES --rm --ipc host \    --mount type=bind,source=.,target=/code/ \    --mount type=bind,source=/dev/shm,target=/dev/shm \    borealtc-gpu python3 main.py ```  ## Dataset  The `data` directory contains two different datasets:  * the `BorealTC` dataset, our publicly available dataset * the `Vulpi` dataset, from the 2021 [paper](https://doi.org/10.1016/j.jterra.2020.12.002) of Vulpi et al.",Vulpi,DATASET
169,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba.",Vulpi,DATASET
170,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba.",vulpi,DATASET
171,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba.",vulpi,DATASET
172,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba.",BorealTC,DATASET
173,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba.",borealtc,DATASET
174,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba.",borealtc,DATASET
175,Download an event-based dataset:     - [The Public Event dataset](https://rpg.ifi.uzh.ch/davis_data.html)     - [The MVSEC dataset](https://daniilidis-group.github.io/mvsec/) 2.,Public Event,DATASET
176,Download an event-based dataset:     - [The Public Event dataset](https://rpg.ifi.uzh.ch/davis_data.html)     - [The MVSEC dataset](https://daniilidis-group.github.io/mvsec/) 2.,MVSEC,DATASET
177,Download an event-based dataset:     - [The Public Event dataset](https://rpg.ifi.uzh.ch/davis_data.html)     - [The MVSEC dataset](https://daniilidis-group.github.io/mvsec/) 2.,mvsec,DATASET
178,You can also use a non-event-based dataset to explore the visual and inertial modes:     - [The EuRoC dataset](https://projects.asl.ethz.ch/datasets/doku.php?,EuRoC,DATASET
179,The original algorithm is designed for larger images of datasets like `EuRoC`.,EuRoC,DATASET
180,If you try to run it on small images of an event-based dataset (especially the hdr_boxes of The Public Event dataset) the backend (optimizer) throws unhandled exceptions.,Public Event,DATASET
181,"We reach SOTA results for S3DIS (74.7 mIoU  6-Fold) and on KITTI- 360 (58.3 mIoU) without requiring point colorization,  meshing, or the use of depth cameras: our full pipeline **only requires raw,  large-scale 3D point clouds and a set of images and poses**.",S3DIS,DATASET
182,"We reach SOTA results for S3DIS (74.7 mIoU  6-Fold) and on KITTI- 360 (58.3 mIoU) without requiring point colorization,  meshing, or the use of depth cameras: our full pipeline **only requires raw,  large-scale 3D point clouds and a set of images and poses**.",KITTI- 360,DATASET
183,url=https://paperswithcode.com/badge/learning-multi-view-aggregation-in-the-wild/semantic-segmentation-on-s3dis)](https://paperswithcode.com/sota/semantic-segmentation-on-s3dis?,s3dis,DATASET
184,url=https://paperswithcode.com/badge/learning-multi-view-aggregation-in-the-wild/semantic-segmentation-on-s3dis)](https://paperswithcode.com/sota/semantic-segmentation-on-s3dis?,s3dis,DATASET
185,url=https://paperswithcode.com/badge/learning-multi-view-aggregation-in-the-wild/3d-semantic-segmentation-on-kitti-360)](https://paperswithcode.com/sota/3d-semantic-segmentation-on-kitti-360?,kitti-360,DATASET
186,url=https://paperswithcode.com/badge/learning-multi-view-aggregation-in-the-wild/3d-semantic-segmentation-on-kitti-360)](https://paperswithcode.com/sota/3d-semantic-segmentation-on-kitti-360?,kitti-360,DATASET
187,"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules.",S3DIS,DATASET
188,"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules.",ScanNet,DATASET
189,"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules.",KITTI360,DATASET
190,"You can also run inference from a checkpoint and visualize predictions: - `notebooks/kitti360_visualization.ipynb` (at least **350G** of memory 💾 ) - `notebooks/s3dis_visualization.ipynb` (at least **400G** of memory 💾 ) - `notebooks/scannet_visualization.ipynb` (at least **1.3T** of memory 💾 )  Notebooks to create multimodal models, get familiar with model configuration and run forward and backward passes for debugging: - `notebooks/multimodal_model.ipynb`  Notebooks to run full inference on multimodal datasets, from a model checkpoint.",kitti360,DATASET
191,"You can also run inference from a checkpoint and visualize predictions: - `notebooks/kitti360_visualization.ipynb` (at least **350G** of memory 💾 ) - `notebooks/s3dis_visualization.ipynb` (at least **400G** of memory 💾 ) - `notebooks/scannet_visualization.ipynb` (at least **1.3T** of memory 💾 )  Notebooks to create multimodal models, get familiar with model configuration and run forward and backward passes for debugging: - `notebooks/multimodal_model.ipynb`  Notebooks to run full inference on multimodal datasets, from a model checkpoint.",s3dis,DATASET
192,"You can also run inference from a checkpoint and visualize predictions: - `notebooks/kitti360_visualization.ipynb` (at least **350G** of memory 💾 ) - `notebooks/s3dis_visualization.ipynb` (at least **400G** of memory 💾 ) - `notebooks/scannet_visualization.ipynb` (at least **1.3T** of memory 💾 )  Notebooks to create multimodal models, get familiar with model configuration and run forward and backward passes for debugging: - `notebooks/multimodal_model.ipynb`  Notebooks to run full inference on multimodal datasets, from a model checkpoint.",scannet,DATASET
193,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section.",kitti360,DATASET
194,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section.",s3dis,DATASET
195,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section.",scannet,DATASET
196,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section.",kitti360,DATASET
197,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section.",s3dis,DATASET
198,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section.",scannet,DATASET
199,<br>  ## 🤖  Models | Model name                                            |   Dataset    |         mIoU          |  💾    |                                             👇                                              | |-------------------------------------------------------|:------------:|:---------------------:|:-----:|:------------------------------------------------------------------------------------------:| | Res16UNet34-L4-early                                  | S3DIS 6-Fold |         74.7          | 2.0G  | [link](https://drive.google.com/file/d/19SgU1f2Ny1du5fRL0d9L1721Gqi1AnsY/view?,S3DIS 6-Fold,DATASET
200,usp=sharing) | | Res16UNet34-PointPyramid-early-cityscapes-interpolate |  KITTI-360   | 61.7 Val / 58.3 Test  | 339M  | [link](https://drive.google.com/file/d/1ucQVJ1cdzwpW6HzthaOqTR1BwTp95vrl/view?,KITTI-360,DATASET
201,usp=sharing) | | Res16UNet34-L4-early                                  |   ScanNet    |       71.0 Val        | 341M  | [link](https://drive.google.com/file/d/1H03540psSjturqerEBJkX5B7R8s6fEba/view?,ScanNet,DATASET
202,"/illustrations/interactive_visualization_snapshot.png""> </p>   Examples of such HTML produced on S3DIS Fold 5 are zipped [here](.",S3DIS Fold 5,DATASET
203,"- For datasets, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) and [ScanNet](https://github.com/ScanNet/ScanNet)  repositories was used.",KITTI-360,DATASET
204,"- For datasets, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) and [ScanNet](https://github.com/ScanNet/ScanNet)  repositories was used.",kitti360,DATASET
205,"- For datasets, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) and [ScanNet](https://github.com/ScanNet/ScanNet)  repositories was used.",ScanNet,DATASET
206,"- For datasets, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) and [ScanNet](https://github.com/ScanNet/ScanNet)  repositories was used.",ScanNet,DATASET
207,"- For datasets, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) and [ScanNet](https://github.com/ScanNet/ScanNet)  repositories was used.",ScanNet,DATASET
208,The underlying dataset \textit{RumexLeaves} is made publicly available and is the first of its kind with keypoint-guided polyline annotations leading along the line from the lowest stem point along the leaf basal to the leaf apex.,RumexLeaves,DATASET
209,__Sources__: * [RumexLeaves Website](https://dtu-pas.github.io/RumexLeaves/) * [Publication](https://ieeexplore.ieee.org/document/10373101) * [Arxiv](https://arxiv.org/abs/2312.08805) * [Dataset](https://data.dtu.dk/articles/dataset/_strong_RumexLeaves_Dataset_introduced_by_Paper_Fine-grained_Leaf_Analysis_for_Efficient_Weeding_Robots_strong_/23659524)  ## Getting started locally 1.,RumexLeaves,DATASET
210,__Sources__: * [RumexLeaves Website](https://dtu-pas.github.io/RumexLeaves/) * [Publication](https://ieeexplore.ieee.org/document/10373101) * [Arxiv](https://arxiv.org/abs/2312.08805) * [Dataset](https://data.dtu.dk/articles/dataset/_strong_RumexLeaves_Dataset_introduced_by_Paper_Fine-grained_Leaf_Analysis_for_Efficient_Weeding_Robots_strong_/23659524)  ## Getting started locally 1.,RumexLeaves,DATASET
211,__Sources__: * [RumexLeaves Website](https://dtu-pas.github.io/RumexLeaves/) * [Publication](https://ieeexplore.ieee.org/document/10373101) * [Arxiv](https://arxiv.org/abs/2312.08805) * [Dataset](https://data.dtu.dk/articles/dataset/_strong_RumexLeaves_Dataset_introduced_by_Paper_Fine-grained_Leaf_Analysis_for_Efficient_Weeding_Robots_strong_/23659524)  ## Getting started locally 1.,RumexLeaves,DATASET
212,Visualize example images with annotations     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/visualizations/visualize_data.py     ``` 4.,rumexleaves,DATASET
213,Visualize example images with annotations     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/visualizations/visualize_data.py     ``` 4.,rumexleaves,DATASET
214,Run example inference     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/inference.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth -img data/processed/RumexLeaves/iNaturalist/4150.jpg     ``` 6.,rumexleaves,DATASET
215,Run example inference     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/inference.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth -img data/processed/RumexLeaves/iNaturalist/4150.jpg     ``` 6.,rumexleaves,DATASET
216,Run example inference     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/inference.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth -img data/processed/RumexLeaves/iNaturalist/4150.jpg     ``` 6.,RumexLeaves,DATASET
217,Run validation of final model on iNaturalist data     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/evaluate.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth     ``` 7.,iNaturalist,DATASET
218,Run validation of final model on iNaturalist data     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/evaluate.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth     ``` 7.,rumexleaves,DATASET
219,Run validation of final model on iNaturalist data     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/evaluate.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth     ``` 7.,rumexleaves,DATASET
220,Training final model from scratch     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/train.py --exp_file exp_files/train_final_model.py     ```  ## Getting started with Docker 1.,rumexleaves,DATASET
221,Training final model from scratch     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/train.py --exp_file exp_files/train_final_model.py     ```  ## Getting started with Docker 1.,rumexleaves,DATASET
222,"Currently surfaced using [WordNet](https://wordnet.princeton.edu/) synsets. ``` msb.get_similar_concepts(""your concept term"") ```  ### Compare concepts to ground truth labels This function displays the Pearson correlation coefficients between each of the selected concepts and the ground truth label column.",WordNet,DATASET
223,"Currently surfaced using [WordNet](https://wordnet.princeton.edu/) synsets. ``` msb.get_similar_concepts(""your concept term"") ```  ### Compare concepts to ground truth labels This function displays the Pearson correlation coefficients between each of the selected concepts and the ground truth label column.",wordnet,DATASET
224,"The `util/` directory contains some codes to parse specific datasets and generate queries.        ### Dataset Structure ---------------- The graphs and query/answer sets can be in any format but in order to use our train script and dataset utilities these should be in following format.        ### File Structure ---------------- Our training script assume that the data stored in the following format: ``` data/     dataset_name/         graph_{snapshot_id}.txt         graph_{snapshot_id}.txt         queries_{snapshot_id}.txt         queries_{snapshot_id}.txt ```  So for example for a a dataset named `foo` with 3 snapshots it would be like:  ``` data/     foo/         graph_1.txt         graph_2.txt         graph_3.txt         queries_1.txt         queries_2.txt         queries_3.txt         test_queries.txt         valid_queries.txt ```        ### Graph file and QueryFiles ---------------- The graph files must have the following format.  ``` node_id1 node_id2 .. .. ```  The query files must have the following format.   ``` q1_node1,q1_node2,... answer1_node1,answer1_node2,... q2_node1,q2_node2,... answer2_node1,answer2_node2 ```         ### Training the Models & Execution ---------------- Finally the models could be trained and evaluated with the following command.   ``` python3 train.py <DATASET_NAME> <MAX_VERTICES> <START_SNAPSHOT_ID> <END_SNAPSHOT_ID> <THRESHOLD> <HIDDEN_DIM_SIZE> <EPOCHS> <LEARNING_RATE> <REGULARIZATION> ```  For example: ``` python3 train.py football 200 1 4 0.4 64 100 0.001 0.00001 ```   ### Reference ----------------   ``` @inproceedings{CS-TGN, author = {Hashemi, Farnoosh and Behrouz, Ali and Rezaei Hajidehi, Milad}, title = {CS-TGN: Community Search via Temporal Graph Neural Networks}, year = {2023}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3543873.3587654}, doi = {10.1145/3543873.3587654}, booktitle = {Companion Proceedings of the Web Conference 2023}, numpages = {8}, location = {AUSTIN, TEXAS, USA}, series = {WWW '23} } ```",football,DATASET
225,[PyPI](https://img.shields.io/pypi/v/sc2-datasets?,sc2,DATASET
226,style=flat-square)](https://pypi.org/project/sc2-datasets/) [!,sc2,DATASET
227,[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.,StarCraft II,DATASET
228,[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.,SC2,DATASET
229,[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.,SC2,DATASET
230,"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796).",sc2,DATASET
231,"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796).",SC2EGSet,DATASET
232,"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796).",StarCraft II Esport Game State Dataset,DATASET
233,"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796).",SC2EGSet,DATASET
234,"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796).",StarCraft II Esport Game State Dataset,DATASET
235,Contents of this library provide PyTorch and PyTorch Lightning API for pre-processed StarCraft II dataset.  ## Installation  1.,StarCraft II,DATASET
236,Perform the following command:  ```bash $ pip install sc2_datasets ```  ## Usage  Basic example usage can be seen below.,sc2,DATASET
237,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",SC2EGSet,DATASET
238,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",sc2,DATASET
239,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",sc2,DATASET
240,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",SC2EGSet,DATASET
241,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",sc2,DATASET
242,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",sc2,DATASET
243,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",SC2EGSet,DATASET
244,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir="".",sc2,DATASET
245,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir="".",sc2,DATASET
246,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir="".",SC2EGSet,DATASET
247,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir="".",sc2,DATASET
248,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir="".",sc2,DATASET
249,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir="".",SC2EGSet,DATASET
250,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir="".",sc2,DATASET
251,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir="".",sc2,DATASET
252,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir="".",SC2EGSet,DATASET
253,"download=True,                 replaypacks=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.             )     sc2_egset_datamodule.prepare_data()     sc2_egset_datamodule.setup() ```  ## Contributing  Interested in contributing?",sc2,DATASET
254,"download=True,                 replaypacks=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.             )     sc2_egset_datamodule.prepare_data()     sc2_egset_datamodule.setup() ```  ## Contributing  Interested in contributing?",sc2,DATASET
255,"By contributing to this project, you agree to abide by its terms.  ## License  `sc2egset_dataset` was created by Andrzej Białecki.",sc2egset,DATASET
256,"It is licensed under the terms of the GNU General Public License v3.0 license.  ## Cite  ### [Dataset Description Article](https://www.researchgate.net/publication/373767449_SC2EGSet_StarCraft_II_Esport_Replay_and_Game-state_Dataset)  To cite the article that introduces [SC2ReSet](https://doi.org/10.5281/zenodo.5575796) and [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) use this:  ```bibtex @article{Białecki2023,   author   = {Bia{\l}ecki, Andrzej               and Jakubowska, Natalia               and Dobrowolski, Pawe{\l}               and Bia{\l}ecki, Piotr               and Krupi{\'{n}}ski, Leszek               and Szczap, Andrzej               and Bia{\l}ecki, Robert               and Gajewski, Jan},   title    = {SC2EGSet: StarCraft II Esport Replay and Game-state Dataset},   journal  = {Scientific Data},   year     = {2023},   month    = {Sep},   day      = {08},   volume   = {10},   number   = {1},   pages    = {600},   issn     = {2052-4463},   doi      = {10.1038/s41597-023-02510-7},   url      = {https://doi.org/10.1038/s41597-023-02510-7} } ```",SC2ReSet,DATASET
257,"It is licensed under the terms of the GNU General Public License v3.0 license.  ## Cite  ### [Dataset Description Article](https://www.researchgate.net/publication/373767449_SC2EGSet_StarCraft_II_Esport_Replay_and_Game-state_Dataset)  To cite the article that introduces [SC2ReSet](https://doi.org/10.5281/zenodo.5575796) and [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) use this:  ```bibtex @article{Białecki2023,   author   = {Bia{\l}ecki, Andrzej               and Jakubowska, Natalia               and Dobrowolski, Pawe{\l}               and Bia{\l}ecki, Piotr               and Krupi{\'{n}}ski, Leszek               and Szczap, Andrzej               and Bia{\l}ecki, Robert               and Gajewski, Jan},   title    = {SC2EGSet: StarCraft II Esport Replay and Game-state Dataset},   journal  = {Scientific Data},   year     = {2023},   month    = {Sep},   day      = {08},   volume   = {10},   number   = {1},   pages    = {600},   issn     = {2052-4463},   doi      = {10.1038/s41597-023-02510-7},   url      = {https://doi.org/10.1038/s41597-023-02510-7} } ```",SC2EGSet,DATASET
258,"It is licensed under the terms of the GNU General Public License v3.0 license.  ## Cite  ### [Dataset Description Article](https://www.researchgate.net/publication/373767449_SC2EGSet_StarCraft_II_Esport_Replay_and_Game-state_Dataset)  To cite the article that introduces [SC2ReSet](https://doi.org/10.5281/zenodo.5575796) and [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) use this:  ```bibtex @article{Białecki2023,   author   = {Bia{\l}ecki, Andrzej               and Jakubowska, Natalia               and Dobrowolski, Pawe{\l}               and Bia{\l}ecki, Piotr               and Krupi{\'{n}}ski, Leszek               and Szczap, Andrzej               and Bia{\l}ecki, Robert               and Gajewski, Jan},   title    = {SC2EGSet: StarCraft II Esport Replay and Game-state Dataset},   journal  = {Scientific Data},   year     = {2023},   month    = {Sep},   day      = {08},   volume   = {10},   number   = {1},   pages    = {600},   issn     = {2052-4463},   doi      = {10.1038/s41597-023-02510-7},   url      = {https://doi.org/10.1038/s41597-023-02510-7} } ```",SC2EGSet,DATASET
259,We provide examples to run the SLAM system in the [EuRoC dataset](https://projects.asl.ethz.ch/datasets/doku.php?,EuRoC,DATASET
260,"If no distortion coefficients are provided, they are assumed to be zero.  ### 3.2 EuRoC Example  1.",EuRoC,DATASET
261,"The specific `<TIMESTAMPS_FILE>`, `<CALIB_FILE>` and `<SETTINGS_FILE>` for the EuRoC dataset are provided in `Examples/EurocData`.  ### 3.3 Video Example It is also possible to run your own custom videos with known camera calibration.",EuRoC,DATASET
262,"The specific `<TIMESTAMPS_FILE>`, `<CALIB_FILE>` and `<SETTINGS_FILE>` for the EuRoC dataset are provided in `Examples/EurocData`.  ### 3.3 Video Example It is also possible to run your own custom videos with known camera calibration.",Euroc,DATASET
263,"<div align=""center""> <h2 align=""center"">    <b>RuleAlign Dataset</b> </h2> <div> <a target=""_blank"" href=""https://scholar.google.com.sg/citations?",RuleAlign,DATASET
264,"user=GqZfs_IAAAAJ&hl=en"">Fangyuan&#160;Yu</a><sup>1 2</sup> </div> <sup>1</sup>Temus&#160&#160&#160</span> <sup>2</sup>Stanford University</span> <br /> <sup>&#9993&#160;</sup>Corresponding author&#160;&#160;</span> <br/> <br/> <div align=""center"">     <a href=""https://arxiv.org/abs/2408.16667"" target=""_blank""> </div> </div> <h3 align=""center""> <b>:fire: Code will be released soon</b> </h3>  ## :books: RuleEval Dataset  RuleAlign is a dataset designed to evaluate rule-based alignment of language models.",RuleEval,DATASET
265,"user=GqZfs_IAAAAJ&hl=en"">Fangyuan&#160;Yu</a><sup>1 2</sup> </div> <sup>1</sup>Temus&#160&#160&#160</span> <sup>2</sup>Stanford University</span> <br /> <sup>&#9993&#160;</sup>Corresponding author&#160;&#160;</span> <br/> <br/> <div align=""center"">     <a href=""https://arxiv.org/abs/2408.16667"" target=""_blank""> </div> </div> <h3 align=""center""> <b>:fire: Code will be released soon</b> </h3>  ## :books: RuleEval Dataset  RuleAlign is a dataset designed to evaluate rule-based alignment of language models.",RuleAlign,DATASET
266,"It accompanies the paper ""Iterative Graph Reasoning"" and contains:  - 5 distinct rule-based scenarios - 200 training queries per scenario (low-data learning scenario) - 100 test queries per scenario - Weak annotations provided for each query  ## :new: Updates - [08/2024] [Arxiv paper](https://arxiv.org/abs/2408.16667) released. - [08/2024] RuleAlign dataset announced.  ## :gear: Evaluation  To run the evaluation on the RuleAlign dataset, use the following command:  ```shell python -m script.eval --model_name xxx ```  Replace `xxx` with the name of the model you want to evaluate.  ## :hugs: Citation If you find this dataset useful for your research, please kindly cite our paper:  ``` @misc{yu2024iterativegraphreasoning,       title={Iterative Graph Reasoning},        author={Fangyuan Yu},       year={2024},       eprint={2408.03615},       archivePrefix={arXiv},       primaryClass={cs.AI} } ```  ## :mailbox: Contact  For any questions or issues regarding the RuleEval dataset, please contact the corresponding author.",RuleAlign,DATASET
267,"It accompanies the paper ""Iterative Graph Reasoning"" and contains:  - 5 distinct rule-based scenarios - 200 training queries per scenario (low-data learning scenario) - 100 test queries per scenario - Weak annotations provided for each query  ## :new: Updates - [08/2024] [Arxiv paper](https://arxiv.org/abs/2408.16667) released. - [08/2024] RuleAlign dataset announced.  ## :gear: Evaluation  To run the evaluation on the RuleAlign dataset, use the following command:  ```shell python -m script.eval --model_name xxx ```  Replace `xxx` with the name of the model you want to evaluate.  ## :hugs: Citation If you find this dataset useful for your research, please kindly cite our paper:  ``` @misc{yu2024iterativegraphreasoning,       title={Iterative Graph Reasoning},        author={Fangyuan Yu},       year={2024},       eprint={2408.03615},       archivePrefix={arXiv},       primaryClass={cs.AI} } ```  ## :mailbox: Contact  For any questions or issues regarding the RuleEval dataset, please contact the corresponding author.",RuleAlign,DATASET
268,"It accompanies the paper ""Iterative Graph Reasoning"" and contains:  - 5 distinct rule-based scenarios - 200 training queries per scenario (low-data learning scenario) - 100 test queries per scenario - Weak annotations provided for each query  ## :new: Updates - [08/2024] [Arxiv paper](https://arxiv.org/abs/2408.16667) released. - [08/2024] RuleAlign dataset announced.  ## :gear: Evaluation  To run the evaluation on the RuleAlign dataset, use the following command:  ```shell python -m script.eval --model_name xxx ```  Replace `xxx` with the name of the model you want to evaluate.  ## :hugs: Citation If you find this dataset useful for your research, please kindly cite our paper:  ``` @misc{yu2024iterativegraphreasoning,       title={Iterative Graph Reasoning},        author={Fangyuan Yu},       year={2024},       eprint={2408.03615},       archivePrefix={arXiv},       primaryClass={cs.AI} } ```  ## :mailbox: Contact  For any questions or issues regarding the RuleEval dataset, please contact the corresponding author.",RuleEval,DATASET
269,"The following datasets have been used to extract the different layers: * BD Ortho for the satellite images * BD Foret v2 for vegetation data * BD Topo for buildings and roads  Important: note that the *data precision is 50cm per pixel.*  Initially, lots of classes were present in the dataset.",BD Ortho,DATASET
270,"The following datasets have been used to extract the different layers: * BD Ortho for the satellite images * BD Foret v2 for vegetation data * BD Topo for buildings and roads  Important: note that the *data precision is 50cm per pixel.*  Initially, lots of classes were present in the dataset.",BD Foret v2,DATASET
271,"The following datasets have been used to extract the different layers: * BD Ortho for the satellite images * BD Foret v2 for vegetation data * BD Topo for buildings and roads  Important: note that the *data precision is 50cm per pixel.*  Initially, lots of classes were present in the dataset.",BD Topo,DATASET
272,"usp=sharing).  ``` # single-gpu testing python tools/test.py <CONFIG_FILE> <SEG_CHECKPOINT_FILE> --eval mIoU  # multi-gpu testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --eval mIoU  # multi-gpu, multi-scale testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --aug-test --eval mIoU ```  Example on the Ampli ANR project:   ``` # Evaluate checkpoint on a single GPU python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --eval mIoU  # Display segmentation results python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --show ```  ### Training  To train with pre-trained models, run: ``` # single-gpu training python tools/train.py <CONFIG_FILE> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  # multi-gpu training tools/dist_train.sh <CONFIG_FILE> <GPU_NUM> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  ```  Example on the Ampli ANR project with the ImageNet-22K pretrained model (available [here](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)) :   ``` python tools/train.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py --options model.pretrained="".",ImageNet-22K,DATASET
273,This method holds the **1st place** entry on the [KITTI depth completion benchmark](http://www.cvlibs.net/datasets/kitti/eval_depth.php?,KITTI,DATASET
274,This method holds the **1st place** entry on the [KITTI depth completion benchmark](http://www.cvlibs.net/datasets/kitti/eval_depth.php?,kitti,DATASET
275,This is the late fusion technique used in our framework. * This method ranks first on the KITTI depth completion benchmark without using additional data or postprocessing.,KITTI,DATASET
276,The predictions of our model for the KITTI test set can be downloaded [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x).  !,KITTI,DATASET
277,(Works with Pytorch 1.1)   ## Dataset The [Kitti dataset](www.cvlibs.net/datasets/kitti/) has been used.,Kitti,DATASET
278,(Works with Pytorch 1.1)   ## Dataset The [Kitti dataset](www.cvlibs.net/datasets/kitti/) has been used.,kitti,DATASET
279,"Secondly, you'll need to unzip and download the camera images from kitti.",kitti,DATASET
280,"|--validation ```   ## Run Code To run the code:  `python main.py --data_path /path/to/data/ --lr_policy plateau`  Flags: - Set flag ""input_type"" to rgb or depth. - Set flag ""pretrained"" to true or false to use a model pretrained on Cityscapes for the global branch. - See `python main.py --help` for more information.",Cityscapes,DATASET
281,You can find the model pretrained on Cityscapes [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,Cityscapes,DATASET
282,You can find a fully trained model and its corresponding predictions for the KITTI test set [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,KITTI,DATASET
283,"and execute the following command:  `source Test/test.sh /path/to/directory_with_saved_model/ $num_samples /path/to/dataset/ /path/to/directory_with_ground_truth_for_selected_validation_files/`  (You might have to recompile the C files for testing, provided by KITTI, if your architecture is different from mine)  ## Results  Comparision with state-of-the-art:  !",KITTI,DATASET
284,"Generated with `conda env export > environment.yml`     │     ├── setup.py           <- Makes project pip installable (pip install -e .) so src can be imported     ├── src                <- Python modules with classes and functions that will be orchestrated in the notebooks.         ├── data           <- Python modules - Generators, preprocess and postprocess functions         ├── models         <- Python modules - TF.keras 2.X Model and Layer definition         ├── utils          <- Python modules - Metrics, losses, prediction, evaluation code, TF-callbacks and io-utils         └── visualization  <- Python modules - Plot functions for the evaluation and data description  ## Dataset For this work a multi-centric heterogeneous cine-SSFPs CMR TOF data set from the [German Competence Network for Congenital Heart Defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/)) was used (study identifier: NCT00266188, title: Non-invasive Imaging and Exercise Tolerance Tests in Post-repair Tetralogy of Fallot -Intervention and Course in Patients Over 8 Years Old).",multi-centric heterogeneous cine-SSFPs CMR TOF,DATASET
285,This TOF dataset constitutes one of the largest compiled data set of this pathology to date.,TOF,DATASET
286,"It achieves state-of-the-art performance on multiple datasets such as FaceWarehouse, MICC Florence and BU-3DFE.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy.",FaceWarehouse,DATASET
287,"It achieves state-of-the-art performance on multiple datasets such as FaceWarehouse, MICC Florence and BU-3DFE.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy.",MICC Florence,DATASET
288,"It achieves state-of-the-art performance on multiple datasets such as FaceWarehouse, MICC Florence and BU-3DFE.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy.",BU-3DFE,DATASET
289,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.,FaceWareHouse,DATASET
290,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.,Florence,DATASET
291,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.,BU3DFE,DATASET
292,"We conduct an experiment on AFLW_2000 dataset (NME) to evaluate the performance, as shown in the table below: <p align=""center"">  <img src=""/images/alignment.png""> </p>  |Method|[0°,30°]|[30°,60°]|[60°,90°]|Overall| |:---:|:---:|:---:|:---:|:---:| |[3DDFA 16](https://arxiv.org/abs/1511.07212)</center>|3.78|4.54|7.93|5.42| |[3DDFA+SDM 16](https://arxiv.org/abs/1511.07212)|3.43|4.24|7.17|4.94| |[Bulat et al. 17](https://arxiv.org/abs/1703.00862)|**2.47**|**3.01**|**4.31**|**3.26**| |[PRN 18](https://arxiv.org/abs/1803.07835)|2.75|3.51|4.61|3.62| |Ours|2.56|3.11|4.45|3.37|   ### ● Easy and Fast Faces are represented with Basel Face Model 2009, which is easy for further manipulations (e.g expression transfer).",AFLW_2000,DATASET
293,"(https://github.com/Juyong/3DFace) You can find a link named ""CoarseData"" in the first row of Introduction part in their repository.",CoarseData,DATASET
294,Download and unzip the Coarse_Dataset.zip.,Coarse,DATASET
295,The expression basis are constructed using [Facewarehouse](http://kunzhou.net/zjugaps/facewarehouse/) data and transferred to BFM topology.  3.,Facewarehouse,DATASET
296,Training process has been tested with this model to ensure similar results. - [Resnet50-v1](https://github.com/tensorflow/models/blob/master/research/slim/README.md) pre-trained on ImageNet from Tensorflow Slim.,ImageNet,DATASET
297,"If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={IEEE Computer Vision and Pattern Recognition Workshops},      year={2019}  } ## The face images on this page are from the public [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset released by MMLab, CUHK.",CelebA,DATASET
298,"If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={IEEE Computer Vision and Pattern Recognition Workshops},      year={2019}  } ## The face images on this page are from the public [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset released by MMLab, CUHK.",CelebA,DATASET
299,"Extract relevant features sets (`BouroujeniEtAl`, `MarrasEtAl`, `LalleConati`, and `ChenCui`) through our lab's EDM 2021 contribution on [benchmarks for feature predictive power](https://github.com/epfl-ml4ed/flipped-classroom).",BouroujeniEtAl,DATASET
300,"Extract relevant features sets (`BouroujeniEtAl`, `MarrasEtAl`, `LalleConati`, and `ChenCui`) through our lab's EDM 2021 contribution on [benchmarks for feature predictive power](https://github.com/epfl-ml4ed/flipped-classroom).",MarrasEtAl,DATASET
301,"Extract relevant features sets (`BouroujeniEtAl`, `MarrasEtAl`, `LalleConati`, and `ChenCui`) through our lab's EDM 2021 contribution on [benchmarks for feature predictive power](https://github.com/epfl-ml4ed/flipped-classroom).",LalleConati,DATASET
302,"Extract relevant features sets (`BouroujeniEtAl`, `MarrasEtAl`, `LalleConati`, and `ChenCui`) through our lab's EDM 2021 contribution on [benchmarks for feature predictive power](https://github.com/epfl-ml4ed/flipped-classroom).",ChenCui,DATASET
303,"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?",PRW,DATASET
304,"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?",PRW,DATASET
305,"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?",CUHK-SYSU,DATASET
306,"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?",json annotations,DATASET
307,"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter",PRW,DATASET
308,"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter",prw,DATASET
309,"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter",PRW,DATASET
310,"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter",CUHK-SYSU,DATASET
311,|    name  | dataset  | backbone |  mAP  | top-1 |  mAP+ | top-1+  | download| | :-------------: | :-----: | :-----: | :-------------------: | :-----: | :-----: | :------: | :-----------------: | |     PSTR | PRW    | PVTv2-B2  |   57.46  |   90.57   |58.07   |    92.03     |          [model](https://drive.google.com/file/d/1hrmyvS9f8fzflpoIlEhWQ-XDyNp_qCGq/view?,PRW,DATASET
312,usp=sharing)         | |     PSTR |  PRW   | ResNet50  |   50.03   | 88.04   | 50.64   |    89.94   |        [model](https://drive.google.com/file/d/12j71smXyc3QAyCvIPRlQCbyhSXZENBIX/view?,PRW,DATASET
313,usp=sharing)         | |     PSTR |  PRW   | ResNet50-DCN  |   51.09   | 88.33   | 51.62   |    90.13   |        [model](https://drive.google.com/file/d/111f_efZOYMFkz9i76TgqcO7a88npoJV5/view?,PRW,DATASET
314,usp=sharing)         | |     PSTR | CUHK-SYSU     | PVTv2-B2    |   95.31  |   96.28   |95.78   |    96.83      |       [model](https://drive.google.com/file/d/1vrQdZTVgJ2D6ty_XJAYmsJgziW9TZHHW/view?,CUHK-SYSU,DATASET
315,usp=sharing)         | |     PSTR | CUHK-SYSU    | ResNet50|   93.55   | 94.93   | 94.16   | 95.48   |          [model](https://drive.google.com/file/d/1U4r_WaTfODmuhslL_15u5bXdFwLLBC5m/view?,CUHK-SYSU,DATASET
316,usp=sharing)         | |     PSTR | CUHK-SYSU    | ResNet50-DCN|   94.22   | 95.28   | 94.90   | 95.97   |          [model](https://drive.google.com/file/d/1cCbpAGrldxQaRrF7FCZXqx4VaNP-C278/view?,CUHK-SYSU,DATASET
317,# BLEBeacon Dataset The BLEBeacon dataset is a collection of Bluetooth Low Energy (BLE)  advertisement packets/traces generated from BLE beacons carried by people following their daily routine inside a university building for a whole month.,BLEBeacon,DATASET
318,# BLEBeacon Dataset The BLEBeacon dataset is a collection of Bluetooth Low Energy (BLE)  advertisement packets/traces generated from BLE beacons carried by people following their daily routine inside a university building for a whole month.,BLEBeacon,DATASET
319,"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019‑03‑12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019",BLEBeacon,DATASET
320,"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019‑03‑12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019",CRAWDAD,DATASET
321,"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019‑03‑12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019",blebeacon,DATASET
322,"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019‑03‑12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019",blebeacon,DATASET
323,"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019‑03‑12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019",blebeacon,DATASET
324,> [Cite using BibTeX](https://crawdad.org/unm/blebeacon/20190312/)    # Technical Summary  ## Operation     Users carried off-the-shelf Gimbal Series 10 iBeacons that continuously transmit BLE advertisement packets.,blebeacon,DATASET
325,"[RSSI](https://github.com/dimisik/BLEBeacon-Dataset/blob/master/images/ARCH.png) * RSSI Report: all advertisement packet receptions from beacon devices are directly reported to a server with a message that contains the beacon/user ID, the packet's Received Signal Strength Indicator (RSSI), a reception timestamp, and finally the ID of the RPi that received the advertisement (Fig. 1).   !",BLEBeacon,DATASET
326,[Check](https://github.com/dimisik/BLEBeacon-Dataset/blob/master/images/check.png) * Check-In/Check-Out Report: each RPi scanner continuously manages a list of current occupants/users in its proximity.,BLEBeacon,DATASET
327,"A thirty-second period is used to ensure that the occupant exited the RPi proximity.   ## Dataset  The BLEBeacon dataset contains two files, one with the trial readings from the RSSI report operation (RSSI Report file) and the other from the Check-In/Check-Out report operation (Check-In Check-Out Report file).",BLEBeacon,DATASET
328,"# Readme  Further information and a detailed description of the sensing infrastructure setup, the real-subject trial, and the BLEBeacon dataset can be found in: D.",BLEBeacon,DATASET
329,"Devetsikiotis,  [""BLEBeacon: A Real-Subject Trial Dataset from Mobile Bluetooth Low Energy Beacons""](https://arxiv.org/abs/1802.08782), arXiv preprint arXiv:1802.08782, 2018.   ### Research Papers   Publications related to the BLEBeacon dataset:  * D.",BLEBeacon,DATASET
330,"Devetsikiotis,  [""BLEBeacon: A Real-Subject Trial Dataset from Mobile Bluetooth Low Energy Beacons""](https://arxiv.org/abs/1802.08782), arXiv preprint arXiv:1802.08782, 2018.   ### Research Papers   Publications related to the BLEBeacon dataset:  * D.",BLEBeacon,DATASET
331,# NELA-GT-2019  This repository contain examples of how to use the NELA-GT-2019 data set with Python 3.,NELA-GT-2019,DATASET
332,# NELA-GT-2019  This repository contain examples of how to use the NELA-GT-2019 data set with Python 3.,NELA-GT-2019,DATASET
333,"persistentId=doi:10.7910/DVN/O7FWPO  __For more details about this dataset, check the paper__: https://arxiv.org/abs/2003.08444  If you use this dataset in your work, please cite us as follows: <br> ``` @misc{     gruppi2020nelagt2019,     title={NELA-GT-2019: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles},     author={Maurício Gruppi and Benjamin D.",NELA-GT-2019,DATASET
334,"Horne and Sibel Adalı},     year={2020},     eprint={2003.08444},     archivePrefix={arXiv},     primaryClass={cs.CY} } ``` ## Data  Metadata|| ---|--- Dataset name|`NELA-GT-2019` Formats|`Sqlite3`,`JSON` No. of articles|`1118821` No. of sources|`261` Collection period|`2019-01-01` to `2019-12-31`  ### Fields  Each data point collected corresponds to an article and contains the fields described below.",NELA-GT-2019,DATASET
335,+ Loading data from single or multiple sources from the database   + Loading data from the database into a Pandas dataframe  Usage: ``` python3 load-sqlite3.py <path-to-database> ```  ###  load-json.py  * How to load NELA in JSON format with Python 3,NELA,DATASET
336,+ Loading a single source's JSON   + Loading a directory of NELA JSON files - **WARNING**: this consumes a lot of memory  Usage: ``` python3 load-json.py <path-to-file> ```,NELA,DATASET
337,"For image-text retrieval on Flickr30K and MSCOCO, we compute IR@1 and TR@1 for the Recall@1 on image-retrieval (IR) and text-retrieval (TR).",Flickr30K,DATASET
338,"For image-text retrieval on Flickr30K and MSCOCO, we compute IR@1 and TR@1 for the Recall@1 on image-retrieval (IR) and text-retrieval (TR).",MSCOCO,DATASET
339,"We present the results on three different model sizes in the table below, including the training times of TempNet on Nvidia A100-80G GPUs and their win rates on AlpacaEval data.",AlpacaEval,DATASET
340,"<div align=""center"">   <img src=""images/exp4.jpg"" width=""40%""/> </div>  Here, we reveal why TempNet enhances performance by comparing the performances of LLaMA2 7B Chat (with the default $\tau=0.7$) and LLaMA2 7B Chat + TempNet on the AlpacaEval dataset.",AlpacaEval,DATASET
341,The results are saved at `data/sst2/filter_train_nonewline.json` and `data/sst2/filter_val_nonewline.json`.  ``` python build_classifier_data_for_sst2.py ```  3.,sst2,DATASET
342,The results are saved at `data/sst2/filter_train_nonewline.json` and `data/sst2/filter_val_nonewline.json`.  ``` python build_classifier_data_for_sst2.py ```  3.,sst2,DATASET
343,The results are saved at `data/sst2/filter_train_nonewline.json` and `data/sst2/filter_val_nonewline.json`.  ``` python build_classifier_data_for_sst2.py ```  3.,sst2,DATASET
344,The logits are saved at `dp_finetuning/sst2/domain_classifier_output`.,sst2,DATASET
345,"**In this simple implementation, we directly select natural sentences instead of fixed-length sequences.** We found that this simple implementation is enough to achieve good performance on SST-2.   ``` cd ..",SST-2,DATASET
346,Private fine-tuning on sst-2.,sst-2,DATASET
347,"/codes/examples.py --model_name_or_path GAIR/ReasonEval-7B # Specify the model name or path here --model_size 7B # Indicate the model size of ReasonEval (7B or 34B) ```  ## Meta Evaluation ### Datasets The datasets for meta-evaluations are composed of three parts:  - **Meta-Reasoning-MATH**: This dataset is constructed as follows:    - To collect the first type of errors affecting the correctness of steps, we recruit undergraduates who have a solid mathematical background to label solutions generated by Abel and WizardMath.",Meta-Reasoning-MATH,DATASET
348,/dataset/mr-math_invalid_errors.json     {       // Instance ID.,mr-math_invalid_errors,DATASET
349,"""model_output_solution_first_error_step"": ""N/A""     }     ```    - For the second type of errors affecting the efficiency of problem solving process, as they are more rarer than the first one, we sample solutions from the test set of [PRM800K](https://github.com/openai/prm800k) directly, containing 150 samples with redundant steps and 150 samples without.",PRM800K,DATASET
350,"""model_output_solution_first_error_step"": ""N/A""     }     ```    - For the second type of errors affecting the efficiency of problem solving process, as they are more rarer than the first one, we sample solutions from the test set of [PRM800K](https://github.com/openai/prm800k) directly, containing 150 samples with redundant steps and 150 samples without.",prm800k,DATASET
351,/dataset/mr-math_redundant_errors.json     {       // Instance ID.,mr-math_redundant_errors,DATASET
352,"""generator"": ""GPT-4 (PRM800K)"",        // The solution in a step-by-step format.",PRM800K,DATASET
353,"""rating"": [1, 1, 1, 1, 1, 1, 1, 1, 1]     }     ``` - **Meta-Reasoning-GSM8K**: This dataset consists of 3000 solutions from problems in GSM8K, including variations of code solutions and backward reasoning.",Meta-Reasoning-GSM8K,DATASET
354,"""rating"": [1, 1, 1, 1, 1, 1, 1, 1, 1]     }     ``` - **Meta-Reasoning-GSM8K**: This dataset consists of 3000 solutions from problems in GSM8K, including variations of code solutions and backward reasoning.",GSM8K,DATASET
355,"For more details, see the official repository for [MR-GSM8K](https://github.com/dvlab-research/MR-GSM8K). - **Perturbation**: We introduce 6 types of errors to one of the intermediate steps in the solutions from the test set of [PRM800K](https://github.com/openai/prm800k).",MR-GSM8K,DATASET
356,"For more details, see the official repository for [MR-GSM8K](https://github.com/dvlab-research/MR-GSM8K). - **Perturbation**: We introduce 6 types of errors to one of the intermediate steps in the solutions from the test set of [PRM800K](https://github.com/openai/prm800k).",MR-GSM8K,DATASET
357,"For more details, see the official repository for [MR-GSM8K](https://github.com/dvlab-research/MR-GSM8K). - **Perturbation**: We introduce 6 types of errors to one of the intermediate steps in the solutions from the test set of [PRM800K](https://github.com/openai/prm800k).",Perturbation,DATASET
358,"For more details, see the official repository for [MR-GSM8K](https://github.com/dvlab-research/MR-GSM8K). - **Perturbation**: We introduce 6 types of errors to one of the intermediate steps in the solutions from the test set of [PRM800K](https://github.com/openai/prm800k).",PRM800K,DATASET
359,"For more details, see the official repository for [MR-GSM8K](https://github.com/dvlab-research/MR-GSM8K). - **Perturbation**: We introduce 6 types of errors to one of the intermediate steps in the solutions from the test set of [PRM800K](https://github.com/openai/prm800k).",prm800k,DATASET
360,/dataset/perturbation.json   {     // Instance ID.,perturbatio,DATASET
361,/codes/mr-gsm8k_eval.py python .,mr-gsm8k,DATASET
362,/codes/mr-math_eval.py --error_type invalid python .,mr-math,DATASET
363,"/codes/mr-math_eval.py --error_type redundant ```  ### Citation Please cite the paper if the resource in this repo or the paper is helpful to you. ``` @article{xia2024evaluating,         title={Evaluating Mathematical Reasoning Beyond Accuracy},          author={Xia, Shijie and Li, Xuefeng and Liu, Yixin and Wu, Tongshuang and Liu, Pengfei},         journal={arXiv preprint arXiv:2404.05692},         year={2024}, } ```",mr-math,DATASET
364,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",MNIST,DATASET
365,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",MNIST,DATASET
366,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Connect-4,DATASET
367,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Connect-4,DATASET
368,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Letter,DATASET
369,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Letter,DATASET
370,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Optical recognition,DATASET
371,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Optical recognition,DATASET
372,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Pendigits,DATASET
373,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Pendigits,DATASET
374,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Protein,DATASET
375,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Protein,DATASET
376,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",SenseIT,DATASET
377,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",SenseIT,DATASET
378,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",USPS,DATASET
379,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",USPS,DATASET
380,:car:  ## Updates  - \[2024.04\] - We have further improved our manuscript and code. - \[2023.12\] - We provide trained weights on SemanticKITTI and nuScenes.,SemanticKITTI,DATASET
381,:car:  ## Updates  - \[2024.04\] - We have further improved our manuscript and code. - \[2023.12\] - We provide trained weights on SemanticKITTI and nuScenes.,nuScenes,DATASET
382,/docs/DATA_PREPARE.md) for the details to prepare the <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org) and <sup>2</sup>[nuScenes](https://www.nuscenes.org) datasets.  ## :rocket: Getting Started  Please refer to [GET_STARTED.md](.,SemanticKITTI,DATASET
383,/docs/DATA_PREPARE.md) for the details to prepare the <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org) and <sup>2</sup>[nuScenes](https://www.nuscenes.org) datasets.  ## :rocket: Getting Started  Please refer to [GET_STARTED.md](.,kitti,DATASET
384,/docs/DATA_PREPARE.md) for the details to prepare the <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org) and <sup>2</sup>[nuScenes](https://www.nuscenes.org) datasets.  ## :rocket: Getting Started  Please refer to [GET_STARTED.md](.,nuScenes,DATASET
385,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup.",SemanticKITTI,DATASET
386,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup.",nuScenes,DATASET
387,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup.",ScribbleKITTI,DATASET
388,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup.",SemanticPOSS,DATASET
389,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup.",SemanticKITTI,DATASET
390,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup.",nuScenes,DATASET
391,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup.",ScribbleKITTI,DATASET
392,"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes.",SemKITTI,DATASET
393,"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes.",nuScenes,DATASET
394,"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes.",SemanticKITTI,DATASET
395,"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes.",nuScenes,DATASET
396,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SemanticKITTI,DATASET
397,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",kitti,DATASET
398,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SemanticKITTI,DATASET
399,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",kitti,DATASET
400,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",nuScenes,DATASET
401,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",nuscenes,DATASET
402,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",nuScenes,DATASET
403,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",nuscenes,DATASET
404,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",ScribbleKITTI,DATASET
405,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",scribblekitti,DATASET
406,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SemanticPOSS,DATASET
407,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",semanticposs,DATASET
408,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SemanticPOSS,DATASET
409,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SemanticKITTI,DATASET
410,"And we introduce **ProteinKG25**, a new large-scale KG dataset, promting the research on protein language pre-training.",ProteinKG25,DATASET
411,"BLAST, DeepGraphGO, we provide related links to configurate as follows:  [BLAST](https://www.ncbi.nlm.nih.gov/books/NBK569861/) / [Interproscan](https://github.com/ebi-pf-team/interproscan) / [DeepGraphGO](https://github.com/yourh/DeepGraphGO) / [GNN-PPI](https://github.com/lvguofeng/GNN_PPI)  ## Data preparation <span id=""data-preparation""></span> For pretraining OntoProtein, fine-tuning on protein-related tasks and inference, we provide acquirement approach of related data.  ### Pre-training data <span id=""pre-training-data""></span> To incorporate Gene Ontology knowledge into language models and train OntoProtein, we construct [ProteinKG25](https://zjunlp.github.io/project/ProteinKG25/), a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and protein entities.",ProteinKG25,DATASET
412,"BLAST, DeepGraphGO, we provide related links to configurate as follows:  [BLAST](https://www.ncbi.nlm.nih.gov/books/NBK569861/) / [Interproscan](https://github.com/ebi-pf-team/interproscan) / [DeepGraphGO](https://github.com/yourh/DeepGraphGO) / [GNN-PPI](https://github.com/lvguofeng/GNN_PPI)  ## Data preparation <span id=""data-preparation""></span> For pretraining OntoProtein, fine-tuning on protein-related tasks and inference, we provide acquirement approach of related data.  ### Pre-training data <span id=""pre-training-data""></span> To incorporate Gene Ontology knowledge into language models and train OntoProtein, we construct [ProteinKG25](https://zjunlp.github.io/project/ProteinKG25/), a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and protein entities.",ProteinKG25,DATASET
413,"There have two approach to acquire the pre-training data: 1) download our prepared data **ProteinKG25**, 2) generate your own pre-training data.",ProteinKG25,DATASET
414,"<div align=center><img src=""resources/img/times.png"" width=""50%"" height=""50%"" /></div>  #### Download released data  We have released our prepared data **ProteinKG25** in [Google Drive](https://drive.google.com/file/d/1iTC2-zbvYZCDhWM_wxRufCvV6vvPk8HR/view).",ProteinKG25,DATASET
415,Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<dataset>_<split_by>.txt` - selected GloVe feature: `std_glove_<dataset>_<split_by>.npy` - referring expression database: `std_refdb_<dataset>_<split_by>.json` - critical objects database: `std_ctxdb_<dataset>_<split_by>.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset refcoco --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset refcoco --split-by unc ```  We use tensorboard to monitor the training process.,refcoco,DATASET
416,Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<dataset>_<split_by>.txt` - selected GloVe feature: `std_glove_<dataset>_<split_by>.npy` - referring expression database: `std_refdb_<dataset>_<split_by>.json` - critical objects database: `std_ctxdb_<dataset>_<split_by>.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset refcoco --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset refcoco --split-by unc ```  We use tensorboard to monitor the training process.,refcoco,DATASET
417,The log file can be found in `tb` folder.  ## Evaluate Recall **Save Ref-NMS proposals:** ``` PYTHONPATH=$PWD python tools/save_ref_nms_proposals.py --dataset refcoco --split-by unc --tid <tid> --m <loss_type> ``` `<loss_type>` can be either `att_vanilla` for binary XE loss or `att_rank` for rank loss.,refcoco,DATASET
418,**Evaluate recall on referent object:** ``` PYTHONPATH=$PWD python tools/eval_proposal_hit_rate.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ``` `conf` parameter is the score threshold used to filter Ref-NMS proposals.,refcoco,DATASET
419,**Evaluate recall on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset refcoco --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.,refcoco,DATASET
420,**Evaluate recall on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset refcoco --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.,refcoco,DATASET
421,"Then, run the following commands to evaluate on REC and RES task: ``` # Evaluate REC performance python tools/extract_mrcn_ref_feats.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate RES performance python tools/run_propose_to_mask.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **MAttNet-style detection file** (note the MattNet-style detection files can be directly used to evaluate downstream REG task performance).",refcoco,DATASET
422,"Then, run the following commands to evaluate on REC and RES task: ``` # Evaluate REC performance python tools/extract_mrcn_ref_feats.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate RES performance python tools/run_propose_to_mask.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **MAttNet-style detection file** (note the MattNet-style detection files can be directly used to evaluate downstream REG task performance).",refcoco,DATASET
423,"Then, run the following commands to evaluate on REC and RES task: ``` # Evaluate REC performance python tools/extract_mrcn_ref_feats.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate RES performance python tools/run_propose_to_mask.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **MAttNet-style detection file** (note the MattNet-style detection files can be directly used to evaluate downstream REG task performance).",refcoco,DATASET
424,/cleanlab`  #### Download Datasets  --- CoNLL-2003:  - Original paper: [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition](https://arxiv.org/pdf/cs/0306050v1.pdf)  - Original dataset: [Papers with Code](https://paperswithcode.com/dataset/conll-2003),CoNLL-2003,DATASET
425,/cleanlab`  #### Download Datasets  --- CoNLL-2003:  - Original paper: [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition](https://arxiv.org/pdf/cs/0306050v1.pdf)  - Original dataset: [Papers with Code](https://paperswithcode.com/dataset/conll-2003),conll-2003,DATASET
426,"We consider the named entity recognition dataset CoNLL-2003, and use CoNLL++ as the ground truth.",CoNLL-2003,DATASET
427,"We consider the named entity recognition dataset CoNLL-2003, and use CoNLL++ as the ground truth.",CoNLL++,DATASET
428,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot-ext)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot-ext?,lasot-ext,DATASET
429,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot-ext)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot-ext?,lasot-ext,DATASET
430,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot?,lasot,DATASET
431,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot?,lasot,DATASET
432,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-tracking-on-tnl2k)](https://paperswithcode.com/sota/visual-tracking-on-tnl2k?,tnl2k,DATASET
433,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-tracking-on-tnl2k)](https://paperswithcode.com/sota/visual-tracking-on-tnl2k?,tnl2k,DATASET
434,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-trackingnet)](https://paperswithcode.com/sota/visual-object-tracking-on-trackingnet?,trackingnet,DATASET
435,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-trackingnet)](https://paperswithcode.com/sota/visual-object-tracking-on-trackingnet?,trackingnet,DATASET
436,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-youtube-vis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-youtube-vis-1?,youtube-vis,DATASET
437,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-youtube-vis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-youtube-vis-1?,youtube-vis,DATASET
438,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-ovis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-ovis-1?,ovis,DATASET
439,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-ovis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-ovis-1?,ovis,DATASET
440,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-davis)](https://paperswithcode.com/sota/referring-expression-segmentation-on-davis?,davis,DATASET
441,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-davis)](https://paperswithcode.com/sota/referring-expression-segmentation-on-davis?,davis,DATASET
442,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco?,refcoco,DATASET
443,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco?,refcoco,DATASET
444,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco-3)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco-3?,refcoco,DATASET
445,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco-3)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco-3?,refcoco,DATASET
446,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco?,refcoco,DATASET
447,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco?,refcoco,DATASET
448,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco-1)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco-1?,refcoco,DATASET
449,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco-1)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco-1?,refcoco,DATASET
450,"user=wPea4DkAAAAJ&hl=en&oi=ao)<sup>3</sup>, [John Folkesson](https://www.kth.se/profile/johnf)<sup>1</sup>, [Anna Wåhlin](https://www.gu.se/en/about/find-staff/annawahlin)<sup>4</sup>  |<sup>1</sup>KTH Royal Institute of Technology|<sup>2</sup>TU Graz|<sup>3</sup>Ocean Infinity|<sup>4</sup>University of Gothenburg|  For more information, please check out the [project website](https://luxiya01.github.io/mbes-registration-project-page/)  ### Contacts If you have any questions, feel free to contact us at: - Li Ling (liling@kth.se)  # Instructions This repository contains the implementation for the MBES Dataset class and data loader, the classical methods GICP and FPFH, as well as the code for metrics computation and evaluations.",MBES,DATASET
451,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",MNIST,DATASET
452,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",mnist,DATASET
453,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",CIFAR,DATASET
454,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",cifar,DATASET
455,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",SVHN,DATASET
456,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Food101,DATASET
457,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",food-101,DATASET
458,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Flowers102,DATASET
459,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Cervical,DATASET
460,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",CheXpert,DATASET
461,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",chexpert,DATASET
462,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Facial Expression,DATASET
463,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",fer2013,DATASET
464,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Celeb,DATASET
465,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",CelebA,DATASET
466,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Adult,DATASET
467,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",COMPAS,DATASET
468,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",FICO,DATASET
469,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Boston Housing,DATASET
470,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",German Credit,DATASET
471,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Student Admission,DATASET
472,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Student Performance,DATASET
473,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",GMSC,DATASET
474,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",GiveMeSomeCredit,DATASET
475,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Diabetes,DATASET
476,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Breast Cancer,DATASET
477,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Cora,DATASET
478,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",CORA,DATASET
479,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Bitcoin,DATASET
480,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",CIC-IDS2017,DATASET
481,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",ids-2017,DATASET
482,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",IMDB Review,DATASET
483,Download KILT wikipedia knowledge base [here](https://github.com/facebookresearch/KILT) and put it under a kb directory like /raw_kb/  \ 2.,KILT,DATASET
484,Download AIDA CoNLL datasets [here](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) and place them under a raw aida directory like /raw_aida/ \ 4.,AIDA CoNLL,DATASET
485,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",AIDA,DATASET
486,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",KILT,DATASET
487,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",AIDA,DATASET
488,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",KILT,DATASET
489,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",AIDA testb,DATASET
490,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",MSNBC,DATASET
491,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",Der,DATASET
492,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",K50,DATASET
493,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",R128,DATASET
494,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",R500,DATASET
495,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",OKE15,DATASET
496,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",OKE16,DATASET
497,"The paper is available on: - *Computer Vision Foundation (CVF)*: https://openaccess.thecvf.com/content/WACV2021/html/Muller-Budack_Ontology-Driven_Event_Type_Classification_in_Images_WACV_2021_paper.html - *arXiv*: https://arxiv.org/pdf/2011.04714.pdf  Further information can be found on the **EventKG** website: http://eventkg.l3s.uni-hannover.de/VisE   ## Content  - [Ontology-driven Event Type Classification in Images](#ontology-driven-event-type-classification-in-images)   - [Content](#content)   - [Setup](#setup)     - [Setup with Singularity (for Reproducibility)](#setup-with-singularity-for-reproducibility)     - [Setup with Virtual Environment](#setup-with-virtual-environment)     - [Setup with Docker](#setup-with-docker)   - [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)   - [Models](#models)   - [Inference](#inference)   - [Test](#test)   - [VisE-D: Visual Event Classification Dataset](#vise-d-visual-event-classification-dataset)   - [VisE-O: Visual Event Ontology](#vise-o-visual-event-ontology)   - [Benchmark Ontologies](#benchmark-ontologies)   - [Supplemental Material](#supplemental-material)   - [LICENSE](#license)   ## Setup  We provide three different ways to setup the project.",VisE-D,DATASET
498,"The paper is available on: - *Computer Vision Foundation (CVF)*: https://openaccess.thecvf.com/content/WACV2021/html/Muller-Budack_Ontology-Driven_Event_Type_Classification_in_Images_WACV_2021_paper.html - *arXiv*: https://arxiv.org/pdf/2011.04714.pdf  Further information can be found on the **EventKG** website: http://eventkg.l3s.uni-hannover.de/VisE   ## Content  - [Ontology-driven Event Type Classification in Images](#ontology-driven-event-type-classification-in-images)   - [Content](#content)   - [Setup](#setup)     - [Setup with Singularity (for Reproducibility)](#setup-with-singularity-for-reproducibility)     - [Setup with Virtual Environment](#setup-with-virtual-environment)     - [Setup with Docker](#setup-with-docker)   - [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)   - [Models](#models)   - [Inference](#inference)   - [Test](#test)   - [VisE-D: Visual Event Classification Dataset](#vise-d-visual-event-classification-dataset)   - [VisE-O: Visual Event Ontology](#vise-o-visual-event-ontology)   - [Benchmark Ontologies](#benchmark-ontologies)   - [Supplemental Material](#supplemental-material)   - [LICENSE](#license)   ## Setup  We provide three different ways to setup the project.",Visual Event Classification,DATASET
499,"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",VisE-Bing,DATASET
500,"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",VisE-Wiki,DATASET
501,"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",VisE-Bing,DATASET
502,"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",VisE-Wiki,DATASET
503,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--num_predictions <int>``` sets the number of top predictions printed on the console (default ```3```)   ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## Test  This step requires to download the test images in the *VisE-Bing* or *VisE-Wiki* dataset.",VisE-Bing,DATASET
504,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--num_predictions <int>``` sets the number of top predictions printed on the console (default ```3```)   ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## Test  This step requires to download the test images in the *VisE-Bing* or *VisE-Wiki* dataset.",VisE-Wiki,DATASET
505,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## VisE-D: Visual Event Classification Dataset  The *Visual Event Classification Dataset (VisE-D)* is available on: https://data.uni-hannover.de/de/dataset/vise  You can automatically download the dataset by following the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models).",VisE-D,DATASET
506,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## VisE-D: Visual Event Classification Dataset  The *Visual Event Classification Dataset (VisE-D)* is available on: https://data.uni-hannover.de/de/dataset/vise  You can automatically download the dataset by following the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models).",Visual Event Classification Dataset,DATASET
507,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## VisE-D: Visual Event Classification Dataset  The *Visual Event Classification Dataset (VisE-D)* is available on: https://data.uni-hannover.de/de/dataset/vise  You can automatically download the dataset by following the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models).",Visual Event Classification Dataset,DATASET
508,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## VisE-D: Visual Event Classification Dataset  The *Visual Event Classification Dataset (VisE-D)* is available on: https://data.uni-hannover.de/de/dataset/vise  You can automatically download the dataset by following the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models).",VisE-D,DATASET
509,"Different versions of the *Visual Event Ontology (VisE-O)* can be downloaded here: [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/99ce7e4d-df5b-40f6-afb4-16085dbf697d/download/vise-d.tar.gz)  Furthermore you can explore the *Ontologies* using the following links:  - **Initial Ontology** (result of Section 3.2.2): [explore](https://tibhannover.github.io/VisE/VisE-O_initial/index.html) - **Disambiguated Ontology** (result of Section 3.2.3): [explore](https://tibhannover.github.io/VisE/VisE-O_disambiguated/index.html) - **Refined Ontology** (result of Section 3.2.4): [explore](https://tibhannover.github.io/VisE/VisE-O_refined/index.html)  **USAGE:** After opening an *Ontology*, the *Leaf Event Nodes* (blue), *Branch Event Nodes* (orange), and *Root Node* (yellow) as well as their *Relations* are displayed.",vise-d,DATASET
510,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3.",Web Images for Event Recognition,DATASET
511,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3.",WIDER,DATASET
512,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3.",Social Event Dataset,DATASET
513,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3.",SocEID,DATASET
514,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3.",Rare Event Dataset,DATASET
515,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3.",RED,DATASET
516,"The resulting *Ontologies* for these datasets can be downloaded and explored here: - **WIDER Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b1c2f92b-4b69-46fc-9282-16acc7a1c9aa/download/wider.tar.gz) | [explore](https://tibhannover.github.io/VisE/WIDER/index.html) - **SocEID Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/a8373c98-32a8-408c-b8e9-51e6b1e01777/download/soceid.tar.gz) | [explore](https://tibhannover.github.io/VisE/SocEID/index.html) - **RED Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/d0f5cd8b-7c3e-4055-9810-f9cba2b69a33/download/red.tar.gz) | [explore](https://tibhannover.github.io/VisE/RED/index.html)  ## Supplemental Material  Detailed information on the sampling strategy to gather event images, statistics for the training and testing datasets presented in Section 3.3, and results using different inference strategies (Section 4.2.3) are available in the [vise_supplemental.pdf](vise_supplemental.pdf).    ## LICENSE  This work is published under the GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007.",wider,DATASET
517,"The resulting *Ontologies* for these datasets can be downloaded and explored here: - **WIDER Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b1c2f92b-4b69-46fc-9282-16acc7a1c9aa/download/wider.tar.gz) | [explore](https://tibhannover.github.io/VisE/WIDER/index.html) - **SocEID Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/a8373c98-32a8-408c-b8e9-51e6b1e01777/download/soceid.tar.gz) | [explore](https://tibhannover.github.io/VisE/SocEID/index.html) - **RED Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/d0f5cd8b-7c3e-4055-9810-f9cba2b69a33/download/red.tar.gz) | [explore](https://tibhannover.github.io/VisE/RED/index.html)  ## Supplemental Material  Detailed information on the sampling strategy to gather event images, statistics for the training and testing datasets presented in Section 3.3, and results using different inference strategies (Section 4.2.3) are available in the [vise_supplemental.pdf](vise_supplemental.pdf).    ## LICENSE  This work is published under the GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007.",soceid,DATASET
518,"The resulting *Ontologies* for these datasets can be downloaded and explored here: - **WIDER Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b1c2f92b-4b69-46fc-9282-16acc7a1c9aa/download/wider.tar.gz) | [explore](https://tibhannover.github.io/VisE/WIDER/index.html) - **SocEID Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/a8373c98-32a8-408c-b8e9-51e6b1e01777/download/soceid.tar.gz) | [explore](https://tibhannover.github.io/VisE/SocEID/index.html) - **RED Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/d0f5cd8b-7c3e-4055-9810-f9cba2b69a33/download/red.tar.gz) | [explore](https://tibhannover.github.io/VisE/RED/index.html)  ## Supplemental Material  Detailed information on the sampling strategy to gather event images, statistics for the training and testing datasets presented in Section 3.3, and results using different inference strategies (Section 4.2.3) are available in the [vise_supplemental.pdf](vise_supplemental.pdf).    ## LICENSE  This work is published under the GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007.",red,DATASET
519,"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.",MNIST,DATASET
520,"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.",mnist,DATASET
521,"We provide the train and validation splits in  ```data/MNIST_train.npy``` and ```data/MNIST_val.npy```. - a custom dataset with 200,000 gray-scale natural image patches of size 28x28 extracted from  [ImageNet](https://www.image-net.org/index.php).",MNIST,DATASET
522,"We provide the train and validation splits in  ```data/MNIST_train.npy``` and ```data/MNIST_val.npy```. - a custom dataset with 200,000 gray-scale natural image patches of size 28x28 extracted from  [ImageNet](https://www.image-net.org/index.php).",MNIST,DATASET
523,"We provide the train and validation splits in  ```data/MNIST_train.npy``` and ```data/MNIST_val.npy```. - a custom dataset with 200,000 gray-scale natural image patches of size 28x28 extracted from  [ImageNet](https://www.image-net.org/index.php).",ImageNet,DATASET
524,"We provide the train and validation splits in  ```data/MNIST_train.npy``` and ```data/MNIST_val.npy```. - a custom dataset with 200,000 gray-scale natural image patches of size 28x28 extracted from  [ImageNet](https://www.image-net.org/index.php).",image-net,DATASET
525,The script to generate it is  [build_imagenet_LCN.sh](https://github.com/kevtimova/deep-sparse/blob/main/scripts/build_ImageNet_LCN.sh).  ### Training  The scripts below can be used to train sparse autoencoders with our different setups,imagenet,DATASET
526,The script to generate it is  [build_imagenet_LCN.sh](https://github.com/kevtimova/deep-sparse/blob/main/scripts/build_ImageNet_LCN.sh).  ### Training  The scripts below can be used to train sparse autoencoders with our different setups,ImageNet,DATASET
527,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,MNIST,DATASET
528,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,MNIST,DATASET
529,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,MNIST,DATASET
530,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,MNIST,DATASET
531,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,MNIST,DATASET
532,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,MNIST,DATASET
533,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,MNIST,DATASET
534,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,ImageNet_patches,DATASET
535,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,ImageNet,DATASET
536,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,ImageNet_patches,DATASET
537,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,ImageNet,DATASET
538,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,ImageNet_patches,DATASET
539,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,ImageNet,DATASET
540,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,Imagenet_patches,DATASET
541,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,ImageNet,DATASET
542,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,MNIST,DATASET
543,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,ImageNet,DATASET
544,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,MNIST,DATASET
545,"Step 1: Given a pre-trained encoder, ```compute_codes.py``` can be used to create a dataset containing the codes  for each MNIST image.",MNIST,DATASET
546,The corpus consists of lyrics of the data from DAMP and DALI datasets and artists from Billboard (2015-2018) [4]   **Pronunciation Model**: A predefined lexicon that provides a mapping between words and their phonemic representations.,DAMP,DATASET
547,The corpus consists of lyrics of the data from DAMP and DALI datasets and artists from Billboard (2015-2018) [4]   **Pronunciation Model**: A predefined lexicon that provides a mapping between words and their phonemic representations.,DALI,DATASET
548,"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",DAMP,DATASET
549,"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",Sing!300x30x2,DATASET
550,300x30x2 data within the DAMP repository[6].,Sing!300x30x2,DATASET
551,300x30x2 data within the DAMP repository[6].,DAMP,DATASET
552,"To retrieve the data, you need to apply for authorization from https://ccrma.stanford.edu/damp/ .",damp,DATASET
553,Please define the directory where you downloaded the dataset (```path-to-damp```) as follows: ``` datadir_damp='path-to-damp' ```  * DALI:  DALI_v2.0 is used to train the polyphonic and cross-domain lyrics transcription models.,damp,DATASET
554,Please define the directory where you downloaded the dataset (```path-to-damp```) as follows: ``` datadir_damp='path-to-damp' ```  * DALI:  DALI_v2.0 is used to train the polyphonic and cross-domain lyrics transcription models.,damp,DATASET
555,Please define the directory where you downloaded the dataset (```path-to-damp```) as follows: ``` datadir_damp='path-to-damp' ```  * DALI:  DALI_v2.0 is used to train the polyphonic and cross-domain lyrics transcription models.,damp,DATASET
556,Please define the directory where you downloaded the dataset (```path-to-damp```) as follows: ``` datadir_damp='path-to-damp' ```  * DALI:  DALI_v2.0 is used to train the polyphonic and cross-domain lyrics transcription models.,DALI,DATASET
557,Please define the directory where you downloaded the dataset (```path-to-damp```) as follows: ``` datadir_damp='path-to-damp' ```  * DALI:  DALI_v2.0 is used to train the polyphonic and cross-domain lyrics transcription models.,DALI_v2.0,DATASET
558,"To retrieve the data, please refer to the relevant Github repository at:  https://github.com/gabolsgabs/DALI  According to the repository, you can download the audio files under 'Getting the audio' section.",DALI,DATASET
559,"Refer this as: ``` datadir_dali='path-to-dali' ```  * DALI-TALT:  This dataset is a subset of DALI, presented in [7] It is the largest test set used for evaluating polyphonic ALT models.",DALI-TALT,DATASET
560,"Refer this as: ``` datadir_dali='path-to-dali' ```  * DALI-TALT:  This dataset is a subset of DALI, presented in [7] It is the largest test set used for evaluating polyphonic ALT models.",DALI,DATASET
561,The data can be retrieved via the tutorial at: https://github.com/emirdemirel/DALI-TestSet4ALT . ``` datadir_dali_talt='path-to-dali-talt' ```  * Jamendo:  Jamendo(lyrics) is a benchmark evaluation set for both lyrics transcription and audio-to-lyrics alignment tasks[8].,DALI-TestSet4ALT,DATASET
562,The data can be retrieved via the tutorial at: https://github.com/emirdemirel/DALI-TestSet4ALT . ``` datadir_dali_talt='path-to-dali-talt' ```  * Jamendo:  Jamendo(lyrics) is a benchmark evaluation set for both lyrics transcription and audio-to-lyrics alignment tasks[8].,dali_talt,DATASET
563,The data can be retrieved via the tutorial at: https://github.com/emirdemirel/DALI-TestSet4ALT . ``` datadir_dali_talt='path-to-dali-talt' ```  * Jamendo:  Jamendo(lyrics) is a benchmark evaluation set for both lyrics transcription and audio-to-lyrics alignment tasks[8].,dali-talt,DATASET
564,The data can be retrieved via the tutorial at: https://github.com/emirdemirel/DALI-TestSet4ALT . ``` datadir_dali_talt='path-to-dali-talt' ```  * Jamendo:  Jamendo(lyrics) is a benchmark evaluation set for both lyrics transcription and audio-to-lyrics alignment tasks[8].,Jamendo,DATASET
565,The data can be retrieved via the tutorial at: https://github.com/emirdemirel/DALI-TestSet4ALT . ``` datadir_dali_talt='path-to-dali-talt' ```  * Jamendo:  Jamendo(lyrics) is a benchmark evaluation set for both lyrics transcription and audio-to-lyrics alignment tasks[8].,Jamendo(lyrics),DATASET
566,Data can be retrieved at https://github.com/f90/jamendolyrics .  ``` datadir_jamendo='path-to-jamendo' ```   ### B) Running the training pipeline  There are two recipes included in this repository.,jamendolyrics,DATASET
567,Data can be retrieved at https://github.com/f90/jamendolyrics .  ``` datadir_jamendo='path-to-jamendo' ```   ### B) Running the training pipeline  There are two recipes included in this repository.,jamendo,DATASET
568,Data can be retrieved at https://github.com/f90/jamendolyrics .  ``` datadir_jamendo='path-to-jamendo' ```   ### B) Running the training pipeline  There are two recipes included in this repository.,jamendo,DATASET
569,It is written to work on DAMP.,DAMP,DATASET
570,Define the absolute path to the DAMP - Sing!,DAMP,DATASET
571,"300x30x2  repository.    ```  audiopath = path-to-DAMP-dataset ``` Then, simply pass the ```$audiopath``` variable to the main recipe: ``` .",DAMP,DATASET
572,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` .",damp,DATASET
573,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` .",damp,DATASET
574,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` .",dali,DATASET
575,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` .",dali_talt,DATASET
576,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` .",dali_talt,DATASET
577,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` .",jamendo,DATASET
578,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` .",jamendo,DATASET
579,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)",damp,DATASET
580,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)",DAMP,DATASET
581,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)",dali,DATASET
582,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)",DALI,DATASET
583,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)",dali_talt,DATASET
584,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)",DALI-TALT,DATASET
585,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)",jamendo,DATASET
586,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)",jamendo,DATASET
587,/data/.  ## Run HOC + Vote Based and Rank Based method  On CIFAR-10 .  ``` sh .,CIFAR-10,DATASET
588,/test_c10_instance.sh   ```  On CIFAR-100  ``` sh .,CIFAR-100,DATASET
589,"If you just want the data, it's a few steps:  Let `ldc_ontonotes_path` be the path to your LDC download of `Ontonotes 5.0`, that is, `LDC2013T19`.",Ontonotes 5.0,DATASET
590,Mine looks like `/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/`.,ontonotes-release-5.0,DATASET
591,"Next, due to some regrettable firewalling, our script to download the train/dev/test split information fails, so you have to navigate via a browser to:        https://cemantix.org/conll/2012/download/  and manually download `conll-2012-train.v4.tar.gz`, `conll-2012-development.v4.tar.gz`, and then navigate to the `test` folder and download `conll-2012-test-key.tar.gz`.",conll-2012-train.v4,DATASET
592,"Next, due to some regrettable firewalling, our script to download the train/dev/test split information fails, so you have to navigate via a browser to:        https://cemantix.org/conll/2012/download/  and manually download `conll-2012-train.v4.tar.gz`, `conll-2012-development.v4.tar.gz`, and then navigate to the `test` folder and download `conll-2012-test-key.tar.gz`.",conll-2012-development.v4,DATASET
593,"Next, due to some regrettable firewalling, our script to download the train/dev/test split information fails, so you have to navigate via a browser to:        https://cemantix.org/conll/2012/download/  and manually download `conll-2012-train.v4.tar.gz`, `conll-2012-development.v4.tar.gz`, and then navigate to the `test` folder and download `conll-2012-test-key.tar.gz`.",conll-2012-test,DATASET
594,Place these files in the `scripts/ontonotes_scripts/` directory of this repository.,ontonotes,DATASET
595,"Now, run  ``` cd scripts/ontonotes_scripts ldc_ontonotes_path=/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/ bash prep_ontonotes_v4.sh $ldc_onotonotes_path ```  Nice.",ontonotes,DATASET
596,"Now, run  ``` cd scripts/ontonotes_scripts ldc_ontonotes_path=/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/ bash prep_ontonotes_v4.sh $ldc_onotonotes_path ```  Nice.",ontonotes-release-5.0,DATASET
597,"Now, run  ``` cd scripts/ontonotes_scripts ldc_ontonotes_path=/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/ bash prep_ontonotes_v4.sh $ldc_onotonotes_path ```  Nice.",ontonotes,DATASET
598,"Now, run  ``` cd scripts/ontonotes_scripts ldc_ontonotes_path=/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/ bash prep_ontonotes_v4.sh $ldc_onotonotes_path ```  Nice.",onotonotes,DATASET
599,"In our evaluation, human annotators chat with conversational QA models about passages from the [QuAC](https://quac.ai) development set, and after that the annotators judge the correctness of model answers.",QuAC,DATASET
600,"In our evaluation, human annotators chat with conversational QA models about passages from the [QuAC](https://quac.ai) development set, and after that the annotators judge the correctness of model answers.",quac,DATASET
601,"""context"": ""Azaria wrote and directed the 2004 short film Nobody's Perfect, ..."",         # The ID from the original QuAC dataset.",QuAC,DATASET
602,"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/).",QuAC,DATASET
603,"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/).",quac,DATASET
604,"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/).",CANARD,DATASET
605,"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).",coco,DATASET
606,"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).",coco,DATASET
607,"If you want to change only a few options, instead of re-writing a new configuration file, you can override the configuration as the follows:  ``` python <train | eval>.py --dataloader__batch_size 32 --dataloader__eval_batch_size 8 --model__eval_method matching_prob ```  See [config/parser.py](config/parser.py) for details  ## Dataset preparation  ### COCO Caption  We followed the same split provided by [VSE++](http://www.cs.toronto.edu/~faghri/vsepp/data.tar).",COCO Caption,DATASET
608,"Note that we also need `instances_<train | val>2014.json` for computing PMRP score.  ### CUB Caption  Download images (CUB-200-2011) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016).",CUB Caption,DATASET
609,"Note that we also need `instances_<train | val>2014.json` for computing PMRP score.  ### CUB Caption  Download images (CUB-200-2011) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016).",CUB-200-2011,DATASET
610,"Note that we also need `instances_<train | val>2014.json` for computing PMRP score.  ### CUB Caption  Download images (CUB-200-2011) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016).",CUB-200-2011,DATASET
611,<br> We are planning to re-implement efficient PMRP as soon as possible.  ### COCO Caption  ``` # Compute recall metrics python evaluate_recall_coco.py .,COCO Caption,DATASET
612,<br> We are planning to re-implement efficient PMRP as soon as possible.  ### COCO Caption  ``` # Compute recall metrics python evaluate_recall_coco.py .,coco,DATASET
613,/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  ``` # Compute plausible match R-Precision (PMRP) metric python extract_rankings_coco.py .,coco,DATASET
614,/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  ``` # Compute plausible match R-Precision (PMRP) metric python extract_rankings_coco.py .,coco,DATASET
615,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",coco,DATASET
616,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",coco,DATASET
617,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",coco,DATASET
618,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",coco,DATASET
619,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",ECCV Caption,DATASET
620,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",eccv-caption,DATASET
621,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",ECCV Caption,DATASET
622,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",eccv-caption,DATASET
623,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",eccv-caption,DATASET
624,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",CUB Caption,DATASET
625,"<br> Please note that, hence, the results couldn't be reproduced if you use smaller hardware than V100.  ### COCO Caption  ``` python train_coco.py .",COCO Caption,DATASET
626,"<br> Please note that, hence, the results couldn't be reproduced if you use smaller hardware than V100.  ### COCO Caption  ``` python train_coco.py .",coco,DATASET
627,"/config/coco/pcme_coco.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 46 hours in a single V100 with mixed precision training.  ### CUB Caption  We use CUB Caption dataset [(Reed, et al. 2016)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf) as a new cross-modal retrieval benchmark.",coco,DATASET
628,"/config/coco/pcme_coco.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 46 hours in a single V100 with mixed precision training.  ### CUB Caption  We use CUB Caption dataset [(Reed, et al. 2016)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf) as a new cross-modal retrieval benchmark.",coco,DATASET
629,"/config/coco/pcme_coco.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 46 hours in a single V100 with mixed precision training.  ### CUB Caption  We use CUB Caption dataset [(Reed, et al. 2016)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf) as a new cross-modal retrieval benchmark.",CUB Caption,DATASET
630,"/config/coco/pcme_coco.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 46 hours in a single V100 with mixed precision training.  ### CUB Caption  We use CUB Caption dataset [(Reed, et al. 2016)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf) as a new cross-modal retrieval benchmark.",CUB Caption,DATASET
631,"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.  #### hyperparameter search  We additionally use cross-validation splits by (Xian, et el. 2017), namely using 100 classes for training and 50 classes for validation.   ``` python train_cub.py .",cub,DATASET
632,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py .",cub,DATASET
633,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py .",cub,DATASET
634,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py .",cub,DATASET
635,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py .",cub,DATASET
636,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py .",cub,DATASET
637,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py .",cub,DATASET
638,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp.",cub,DATASET
639,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp.",ECCV Caption,DATASET
640,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp.",eccv-caption,DATASET
641,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp.",eccv_caption,DATASET
642,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp.",ECCV Caption,DATASET
643,/model_recommendation/rank_utility_privacy.csv` - separate ranking for UW and VUMC data under different synthesis paradigms,UW,DATASET
644,/model_recommendation/rank_utility_privacy.csv` - separate ranking for UW and VUMC data under different synthesis paradigms,VUMC,DATASET
645,/model_recommendation/rank_utility_privacy.csv` - comparison rank for UW synthetic data generated under combined and separate synthesis paradigms.,UW,DATASET
646,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.,UW,DATASET
647,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.,UW,DATASET
648,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.,UW,DATASET
649,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.,VUMC,DATASET
650,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.,VUMC,DATASET
651,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.,VUMC,DATASET
652,"Besides binary medical features, the UW datasets have features including race and gender.",UW,DATASET
653,"Besides binary medical features, the VUMC datasets have features including race, age, gender, and 7 continuous medical features.",VUMC,DATASET
654,# The Unhealthy Comments Corpus (UCC)   The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online.,Unhealthy Comments Corpus,DATASET
655,# The Unhealthy Comments Corpus (UCC)   The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online.,UCC,DATASET
656,# The Unhealthy Comments Corpus (UCC)   The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online.,Unhealthy Comments Corpus,DATASET
657,# The Unhealthy Comments Corpus (UCC)   The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online.,UCC,DATASET
658,"The UCC contributes further high quality data on  attributes  like  sarcasm,  hostility,  and  condescension, adding to existing datasets on these and related attributes, and provides (to the best of our knowledge) the first dataset of this scale with labels for dismissiveness,  unfair generalisations,  antagonistic behavior, and overall assessments of whether those comments fall within 'healthy' conversation.",UCC,DATASET
659,The raw comments were taken from the [comments](https://github.com/sfu-discourse-lab/SOCC#comments) corpus within the [SFU Opinion and Comments Corpus](https://github.com/sfu-discourse-lab/SOCC).   ## Baseline classification  We provide notebooks to replicate our baseline classification results on this dataset in `UnhealthyConversations.ipynb`.,SOCC,DATASET
660,The raw comments were taken from the [comments](https://github.com/sfu-discourse-lab/SOCC#comments) corpus within the [SFU Opinion and Comments Corpus](https://github.com/sfu-discourse-lab/SOCC).   ## Baseline classification  We provide notebooks to replicate our baseline classification results on this dataset in `UnhealthyConversations.ipynb`.,SFU Opinion and Comments Corpus,DATASET
661,The raw comments were taken from the [comments](https://github.com/sfu-discourse-lab/SOCC#comments) corpus within the [SFU Opinion and Comments Corpus](https://github.com/sfu-discourse-lab/SOCC).   ## Baseline classification  We provide notebooks to replicate our baseline classification results on this dataset in `UnhealthyConversations.ipynb`.,SOCC,DATASET
662,"For example, to train a single model: ``` python -m clinicaldg.scripts.train\        --algorithm ERM\        --dataset eICUSubsampleUnobs\        --es_method val\        --hparams  '{""eicu_architecture"": ""GRU"", ""eicu_subsample_g1_mean"": 0.5, ""eicu_subsample_g2_mean"": 0.05}'\        --output_dir /path/to/output ```  To sweep a range of datasets, algorithms, and hyperparameters: ``` python -m clinicaldg.scripts.sweep launch\        --output_dir=/my/sweep/output/path\        --command_launcher slurm\        --algorithms ERMID ERM IRM VREx RVP IGA CORAL MLDG GroupDRO \        --datasets CXR CXRBinary\        --n_hparams 10\        --n_trials 5\        --es_method train\        --hparams '{""cxr_augment"": 1}' ```  A detailed list of `hparams` available for each dataset can be found [here](hparams.md).",eICUSubsampleUnobs,DATASET
663,"For example, to train a single model: ``` python -m clinicaldg.scripts.train\        --algorithm ERM\        --dataset eICUSubsampleUnobs\        --es_method val\        --hparams  '{""eicu_architecture"": ""GRU"", ""eicu_subsample_g1_mean"": 0.5, ""eicu_subsample_g2_mean"": 0.05}'\        --output_dir /path/to/output ```  To sweep a range of datasets, algorithms, and hyperparameters: ``` python -m clinicaldg.scripts.sweep launch\        --output_dir=/my/sweep/output/path\        --command_launcher slurm\        --algorithms ERMID ERM IRM VREx RVP IGA CORAL MLDG GroupDRO \        --datasets CXR CXRBinary\        --n_hparams 10\        --n_trials 5\        --es_method train\        --hparams '{""cxr_augment"": 1}' ```  A detailed list of `hparams` available for each dataset can be found [here](hparams.md).",CXR,DATASET
664,"For example, to train a single model: ``` python -m clinicaldg.scripts.train\        --algorithm ERM\        --dataset eICUSubsampleUnobs\        --es_method val\        --hparams  '{""eicu_architecture"": ""GRU"", ""eicu_subsample_g1_mean"": 0.5, ""eicu_subsample_g2_mean"": 0.05}'\        --output_dir /path/to/output ```  To sweep a range of datasets, algorithms, and hyperparameters: ``` python -m clinicaldg.scripts.sweep launch\        --output_dir=/my/sweep/output/path\        --command_launcher slurm\        --algorithms ERMID ERM IRM VREx RVP IGA CORAL MLDG GroupDRO \        --datasets CXR CXRBinary\        --n_hparams 10\        --n_trials 5\        --es_method train\        --hparams '{""cxr_augment"": 1}' ```  A detailed list of `hparams` available for each dataset can be found [here](hparams.md).",CXRBinary,DATASET
665,Link datasets  COCO  Download the json file([coco_to_ytvis2019.json](https://drive.google.com/file/d/17L33_woQh7eUMemCmnFDOgPUKi-2fTW0/view?,COCO,DATASET
666,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,coco,DATASET
667,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,coco,DATASET
668,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,coco,DATASET
669,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,coco,DATASET
670,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,YTVIS 2019,DATASET
671,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,ytvis_2019,DATASET
672,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,ytvis_2019,DATASET
673,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,ytvis_2019,DATASET
674,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,ytvis_2019,DATASET
675,* Training using 4 GPUS(TESLA V100-PCIE-32GB) * Pre-training on COCO dataset ```bash python train_net.py --num-gpus 4 --config-file .,COCO,DATASET
676,/checkpoint/coco/ ``` * Fine-tuning on YTVIS2019 with pre-trained weights on COCO dataset ([r50_coco.pth](https://drive.google.com/file/d/1rb3i9MBtAjh3SJ2AWdgb3PvuPpF8Swpi/view?,coco,DATASET
677,/checkpoint/coco/ ``` * Fine-tuning on YTVIS2019 with pre-trained weights on COCO dataset ([r50_coco.pth](https://drive.google.com/file/d/1rb3i9MBtAjh3SJ2AWdgb3PvuPpF8Swpi/view?,YTVIS2019,DATASET
678,/checkpoint/coco/ ``` * Fine-tuning on YTVIS2019 with pre-trained weights on COCO dataset ([r50_coco.pth](https://drive.google.com/file/d/1rb3i9MBtAjh3SJ2AWdgb3PvuPpF8Swpi/view?,COCO,DATASET
679,/checkpoint/ytvis_2019/ MODEL.WEIGHTS path/to/pre-trained-model.pth ```  4.,ytvis_2019,DATASET
680,Evaluating on YTVIS 2019 ```bash python train_net.py --eval-only --num-gpus 1 --config-file .,YTVIS 2019,DATASET
681,"/checkpoint/ytvis_2019/ MODEL.WEIGHTS path/to/model.pth ``` ""results.json"" saved in OUTPUT_DIR/inference/  ## Model Checkpoints (YTVIS 2019) Due to the small size of YTVIS dataset, the scores may fluctuate even if retrained with the same configuration.",ytvis_2019,DATASET
682,"/checkpoint/ytvis_2019/ MODEL.WEIGHTS path/to/model.pth ``` ""results.json"" saved in OUTPUT_DIR/inference/  ## Model Checkpoints (YTVIS 2019) Due to the small size of YTVIS dataset, the scores may fluctuate even if retrained with the same configuration.",YTVIS 2019,DATASET
683,"/checkpoint/ytvis_2019/ MODEL.WEIGHTS path/to/model.pth ``` ""results.json"" saved in OUTPUT_DIR/inference/  ## Model Checkpoints (YTVIS 2019) Due to the small size of YTVIS dataset, the scores may fluctuate even if retrained with the same configuration.",YTVIS,DATASET
684,usp=sharing) |  ## Video Comparisons The overall flow of our VISOLO and the comparison of different VIS methods on the YouTube-VIS 2019 dataset are provided at https://youtu.be/j33H7vcJ2uU  ## License  VISOLO is released under the [Apache 2.0 license](LICENSE).,YouTube-VIS 2019,DATASET
685,"If using the KAIST dataset, we recommend that you place the images in the root directory following the defined list on line 60 of `train.py`.  #### Dataset Selection GUI   !",KAIST,DATASET
686,/Figures/data_selection.png)  This is highly recommended to do since the KAIST dataset is very large and our current implementation takes every 200th image.,KAIST,DATASET
687,"When using the KAIST dataset for evaluation, it is also recommended to simply add `""KAIST""` to the `saved_dirs.json` file.",KAIST,DATASET
688,"When using the KAIST dataset for evaluation, it is also recommended to simply add `""KAIST""` to the `saved_dirs.json` file.",KAIST,DATASET
689,"/Datasets/Thermal/train/640_flir_hr""     ],     ""test_dirs"": [       ""KAIST"",       "".",Thermal,DATASET
690,"/Datasets/Thermal/train/640_flir_hr""     ],     ""test_dirs"": [       ""KAIST"",       "".",KAIST,DATASET
691,"/Datasets/Thermal/test/320_axis_mr"",       "".",Thermal,DATASET
692,"/Datasets/Thermal/test/640_flir_hr"",       "".",Thermal,DATASET
693,"/Datasets/Thermal/test/Flir_test"",       "".",Thermal,DATASET
694,"/Datasets/Thermal/test/160_domo_lr""     ],     ""val_dirs"": [       "".",Thermal,DATASET
695,"/Datasets/Thermal/test/Flir_test""     ]   },   ""div2k"": {     ""train_dirs"": [       "".",Thermal,DATASET
696,"/Datasets/DIV2K/train""     ],     ""test_dirs"": [       "".",DIV2K,DATASET
697,"/Datasets/DIV2K/test"",       "".",DIV2K,DATASET
698,"/Datasets/Urban100"",       "".",Urban100,DATASET
699,"/Datasets/BSDS100"",       "".",BSDS100,DATASET
700,"/Datasets/Thermal/test/320_axis_mr"",       "".",Thermal,DATASET
701,"/Datasets/Thermal/test/640_flir_hr"",       "".",Thermal,DATASET
702,"/Datasets/Thermal/test/Flir_test"",       "".",Thermal,DATASET
703,"/Datasets/Thermal/test/160_domo_lr""     ],     ""val_dirs"": [       "".",Thermal,DATASET
704,"/Datasets/DIV2K/test""     ]   } } ```  In this JSON file, `Flir_test` refers to the [FLIR Thermal Dataset for Algorithm Training](https://www.flir.com/oem/adas/adas-dataset-form/), `320_axis_mr, 640_flir_hr, 160_domo_lr` refers to the [Thermal Image Super-Resolution Challenge](https://pbvs-workshop.github.io/datasets.html) dataset, and `KAIST` refers to the [KAIST](https://soonminhwang.github.io/rgbt-ped-detection/) dataset.",DIV2K,DATASET
705,"/Datasets/DIV2K/test""     ]   } } ```  In this JSON file, `Flir_test` refers to the [FLIR Thermal Dataset for Algorithm Training](https://www.flir.com/oem/adas/adas-dataset-form/), `320_axis_mr, 640_flir_hr, 160_domo_lr` refers to the [Thermal Image Super-Resolution Challenge](https://pbvs-workshop.github.io/datasets.html) dataset, and `KAIST` refers to the [KAIST](https://soonminhwang.github.io/rgbt-ped-detection/) dataset.",KAIST,DATASET
706,"/Datasets/DIV2K/test""     ]   } } ```  In this JSON file, `Flir_test` refers to the [FLIR Thermal Dataset for Algorithm Training](https://www.flir.com/oem/adas/adas-dataset-form/), `320_axis_mr, 640_flir_hr, 160_domo_lr` refers to the [Thermal Image Super-Resolution Challenge](https://pbvs-workshop.github.io/datasets.html) dataset, and `KAIST` refers to the [KAIST](https://soonminhwang.github.io/rgbt-ped-detection/) dataset.",KAIST,DATASET
707,)            | 100 languages |   ASSIN2        |   0.8680     |   0.8680    | | PTT5 (Carmo et al,ASSIN2,DATASET
708,)             | EN & PT       |   ASSIN2        |   0.8850     |   0.8860    | | BERTimbau Large (Souza et al,ASSIN2,DATASET
709,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?,ASSIN2,DATASET
710,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?,MNLI,DATASET
711,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?,ASSIN2,DATASET
712,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?,SQuAD,DATASET
713,usp=sharing) (Q&A) - [FaQuAD](https://colab.research.google.com/drive/1HdPjzn61genPyZfiDG5fqAwPNI4vhegw?,FaQuAD,DATASET
714,usp=sharing) (Q&A) - [MNLI](https://colab.research.google.com/drive/1Y9ZaJuN-SVo0fmwypPzcJCOetFw-A2tx?,MNLI,DATASET
715,usp=sharing) (NLI) - [ASSIN2](https://colab.research.google.com/drive/1S5zwaw8KWee8y6Vyq-XHC8Am3GmGFpv9?,ASSIN2,DATASET
716,usp=sharing) (NLI)  The datasets SQuAD and MNLI are directly downloaded from the notebooks of this repository.,SQuAD,DATASET
717,usp=sharing) (NLI)  The datasets SQuAD and MNLI are directly downloaded from the notebooks of this repository.,MNLI,DATASET
718,We also provide the FaQuAD and ASSIN2 datasets,FaQuAD,DATASET
719,We also provide the FaQuAD and ASSIN2 datasets,ASSIN2,DATASET
720,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?",SQuAD,DATASET
721,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?",FaQuAD,DATASET
722,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?",MNLI,DATASET
723,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?",ASSIN2,DATASET
724,"# [SciNLI: A Corpus for Natural Language Inference on Scientific Text](https://aclanthology.org/2022.acl-long.511.pdf) This repository contains the dataset and code for the ACL 2022 paper ""SciNLI: A Corpus for Natural Language Inference on Scientific Text.""",SciNLI,DATASET
725,"# [SciNLI: A Corpus for Natural Language Inference on Scientific Text](https://aclanthology.org/2022.acl-long.511.pdf) This repository contains the dataset and code for the ACL 2022 paper ""SciNLI: A Corpus for Natural Language Inference on Scientific Text.""",SciNLI,DATASET
726,"In this paper, we introduce SciNLI, a large dataset for NLI that captures the formality in scientific text and contains 107,412 sentence pairs extracted from scholarly papers on NLP and computational linguistics.",SciNLI,DATASET
727,Our experiments show that SciNLI is harder to classify than the existing NLI datasets.,SciNLI,DATASET
728,Our best performing model with XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23% showing that there is substantial room for improvement.  ## Dataset Description We derive [SciNLI](https://drive.google.com/drive/folders/1kjBTVBV1HlMWW5xK8V096LahsU3pULHU?,SciNLI,DATASET
729,"* 'label': corresponding label representing the semantic relation between the premise and hypothesis.      => train.jsonl, test.jsonl and dev.jsonl contain the same data as the CSV files but they are formatted in a json formal similar to SNLI and MNLI.",SNLI,DATASET
730,"* 'label': corresponding label representing the semantic relation between the premise and hypothesis.      => train.jsonl, test.jsonl and dev.jsonl contain the same data as the CSV files but they are formatted in a json formal similar to SNLI and MNLI.",MNLI,DATASET
731,"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",SciNLI,DATASET
732,"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",PASCAL VOC,DATASET
733,[optional] MATLAB (required for PASCAL VOC evaluation only)  ### Requirements: hardware  1.,PASCAL VOC,DATASET
734,/tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.,PASCAL VOC 2007,DATASET
735,"Download the training, validation, test data and VOCdevkit   ```Shell  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar  ```   2.",voc2007,DATASET
736,"Download the training, validation, test data and VOCdevkit   ```Shell  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar  ```   2.",voc2007,DATASET
737,"Download the training, validation, test data and VOCdevkit   ```Shell  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar  ```   2.",voc2007,DATASET
738,Create symlinks for the PASCAL VOC dataset   ```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5.,PASCAL VOC,DATASET
739,Create symlinks for the PASCAL VOC dataset   ```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5.,PASCAL,DATASET
740,[Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6.,PASCAL VOC 2010,DATASET
741,Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models  ### Download pre-computed Selective Search object proposals  Pre-computed selective search boxes can also be downloaded for VOC2007 and VOC2012.,VOC2007,DATASET
742,Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models  ### Download pre-computed Selective Search object proposals  Pre-computed selective search boxes can also be downloaded for VOC2007 and VOC2012.,VOC2012,DATASET
743,"/data/scripts/fetch_selective_search_data.sh ```  This will populate the `$FRCN_ROOT/data` folder with `selective_selective_data`.  ### Download pre-trained ImageNet models  Pre-trained ImageNet models can be downloaded for the three networks described in the paper: CaffeNet (model **S**), VGG_CNN_M_1024 (model **M**), and VGG16 (model **L**).",ImageNet,DATASET
744,"For example, train a VGG16 network on VOC 2007 trainval:  ```Shell .",VOC 2007,DATASET
745,MATLAB is currently required for PASCAL VOC evaluation.,PASCAL VOC,DATASET
746,"For example, test the VGG 16 network on VOC 2007 test:  ```Shell .",VOC 2007,DATASET
747,/tools/test_net.py --gpu 1 --def models/VGG16/test.prototxt \  --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel ```  Test output is written underneath `$FRCN_ROOT/output`.,voc_2007,DATASET
748,/tools/compress_net.py --def models/VGG16/test.prototxt \  --def-svd models/VGG16/compressed/test.prototxt \     --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel # Test the model you just compressed .,voc_2007,DATASET
749,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",PASCAL VOC,DATASET
750,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",voc_2007,DATASET
751,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",voc_2007,DATASET
752,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast_rcnn,DATASET
753,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",voc_2007,DATASET
754,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",voc_2007,DATASET
755,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",voc_2012,DATASET
756,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",voc_2012,DATASET
757,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",VOC07,DATASET
758,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",VOC12,DATASET
759,The models are available on the Hugging Face model hub:  - [avsolatorio/GIST-large-Embedding-v0](https://huggingface.co/avsolatorio/GIST-large-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,MEDI+MTEBcls,DATASET
760,The base model used is the [`BAAI/bge-large-en-v1.5`](https://huggingface.co/BAAI/bge-large-en-v1.5). - [avsolatorio/GIST-Embedding-v0](https://huggingface.co/avsolatorio/GIST-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,MEDI+MTEBcls,DATASET
761,The base model used is the [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5). - [avsolatorio/GIST-small-Embedding-v0](https://huggingface.co/avsolatorio/GIST-small-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,MEDI+MTEBcls,DATASET
762,The base model used is the [`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-small-en-v1.5). - [avsolatorio/GIST-all-MiniLM-L6-v2](https://huggingface.co/avsolatorio/GIST-all-MiniLM-L6-v2): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,MEDI+MTEBcls,DATASET
763,# Data  The dataset used is a compilation of the MEDI dataset and the MTEB Classification training dataset.,MEDI,DATASET
764,# Data  The dataset used is a compilation of the MEDI dataset and the MTEB Classification training dataset.,MTEB Classification,DATASET
765,"A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:  - Dataset: [avsolatorio/medi-data-mteb_avs_triplets](https://huggingface.co/datasets/avsolatorio/medi-data-mteb_avs_triplets) - Revision: 238a0499b6e6b690cc64ea56fde8461daa8341bb  The dataset contains a `task_type` key which can be used to select only the mteb classification tasks (prefixed with `mteb_`).",avsolatorio/medi-data-mteb_avs_triplets,DATASET
766,"A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:  - Dataset: [avsolatorio/medi-data-mteb_avs_triplets](https://huggingface.co/datasets/avsolatorio/medi-data-mteb_avs_triplets) - Revision: 238a0499b6e6b690cc64ea56fde8461daa8341bb  The dataset contains a `task_type` key which can be used to select only the mteb classification tasks (prefixed with `mteb_`).",avsolatorio/medi-data-mteb_avs_triplets,DATASET
767,"A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:  - Dataset: [avsolatorio/medi-data-mteb_avs_triplets](https://huggingface.co/datasets/avsolatorio/medi-data-mteb_avs_triplets) - Revision: 238a0499b6e6b690cc64ea56fde8461daa8341bb  The dataset contains a `task_type` key which can be used to select only the mteb classification tasks (prefixed with `mteb_`).",mteb,DATASET
768,"The **MEDI Dataset** is published in the following paper: [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://arxiv.org/abs/2212.09741).",MEDI,DATASET
769,"The MTEB Benchmark results of the GIST embedding model, compared with the base model, suggest that the fine-tuning dataset has perturbed the model considerably, which resulted in significant improvements in certain tasks while adversely degrading performance in some.",MTEB,DATASET
770,# Evaluation  The model was evaluated using the [MTEB Evaluation](https://huggingface.co/mteb) suite,MTEB,DATASET
771,# Evaluation  The model was evaluated using the [MTEB Evaluation](https://huggingface.co/mteb) suite,mteb,DATASET
772,"**Correction**: In our paper, we denote the dataset 'USPTO' incorrectly as 'USTPO'.",USPTO,DATASET
773,"It is also easy to build your own library, you only need to create a pickled file of a dictionary as:     {         'reactant' :[List of smiles]         'reagent' :[List of smiles]     } You can run this to generate the representation for this library: ``` python generation/build_react_lib.py --input_file $YOUR_LIB_PATH --model_dir ckpt/uni_rxn_gen.ckpt ``` Run this script with the default configuration will create a representation library using provided reactants (from ZINC subset) and reagents (from USPTO).",ZINC,DATASET
774,"It is also easy to build your own library, you only need to create a pickled file of a dictionary as:     {         'reactant' :[List of smiles]         'reagent' :[List of smiles]     } You can run this to generate the representation for this library: ``` python generation/build_react_lib.py --input_file $YOUR_LIB_PATH --model_dir ckpt/uni_rxn_gen.ckpt ``` Run this script with the default configuration will create a representation library using provided reactants (from ZINC subset) and reagents (from USPTO).",USPTO,DATASET
775,"Then you are able to run the generation process based on your library ```  #generate structure analogues  INPUT=/place/of/your/input/seed #support sdf file and raw smiles file OUTPUT=/place/to_store/your/output  python generation/generate_paths_reb.py --model_dir ckpt/uni_rxn_gen.ckpt --input_file $INPUT --react_lib_file dataset/data/react_lib_smi_rep.pkl --output_dir $OUTPUT ```  The output file is a list of chemical reaction paths, each path is formulated as: input seed --> reactants, reagents --> intermediate product --> reactants, reagents --> intermediate product --> ... --> final product  See more configuration guide by running `python generation/generate_paths_reb.py -h`    ## Train From Scratch (Optional) ### Dataset Construction  First download the USTPO_MIT dataset from (https://github.com/wengong-jin/nips17-rexgen/blob/master/USPTO/data.zip) and save it to `dataset/raw/`, also unzip it.",USPTO,DATASET
776,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",Cora,DATASET
777,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",CORA,DATASET
778,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",Citeseer,DATASET
779,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",Pubmed,DATASET
780,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",DBLP,DATASET
781,Take Cora dataset as an example:  - make a folder called **dataset** at root directory,Cora,DATASET
782,- make a folder called **cora** in **dataset** directory,cora,DATASET
783,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",Cora,DATASET
784,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",cora,DATASET
785,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",cora,DATASET
786,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",cora,DATASET
787,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",cora,DATASET
788,"The project is built upon two prominent datasets: **LIDC-IDRI** and **LUNA16**, both of which are publicly available and widely used in lung nodule research.",LIDC-IDRI,DATASET
789,"The project is built upon two prominent datasets: **LIDC-IDRI** and **LUNA16**, both of which are publicly available and widely used in lung nodule research.",LUNA16,DATASET
790,"By utilizing these datasets, the project aims to achieve a more comprehensive analysis of the performance of deep learning models in the medical imaging field.  ## Datasets The project utilizes several key datasets that have been essential in driving advancements in lung nodule detection and diagnosis:  - **LIDC-IDRI**: A large-scale dataset containing over 1,000 lung CT cases with multi-radiologist annotations.",LIDC-IDRI,DATASET
791,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",LUNA16,DATASET
792,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",LIDC-IDRI,DATASET
793,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",LUNA16,DATASET
794,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",ELCAP,DATASET
795,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",NSCLC,DATASET
796,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",ANODE09,DATASET
797,"Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% mAP for the first time.",THUMOS14,DATASET
798,"Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.56% average mAP) and the more challenging EPIC-Kitchens 100 (+13.5% average mAP over prior works).",ActivityNet 1.3,DATASET
799,"Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.56% average mAP) and the more challenging EPIC-Kitchens 100 (+13.5% average mAP over prior works).",EPIC-Kitchens 100,DATASET
800,"In addition, ActionFormer is the backbone for many winning solutions in the Ego4D Moment Queries Challenge 2022.",Ego4D Moment Queries,DATASET
801,logo=arXiv)](https://arxiv.org/abs/2404.02257) <br>  ## Changelog * 11/18/2022: We have released the [tech report](https://arxiv.org/abs/2211.09074) for our submission to the [Ego4D Moment Queries (MQ) Challenge](https://eval.ai/web/challenges/challenge-page/1626/overview).,Ego4D Moment Queries,DATASET
802,"The code repo now includes config files, pre-trained models and results on the Ego4D MQ benchmark",Ego4D MQ,DATASET
803,* 08/01/2022: Updated code repo with latest results on ActivityNet,ActivityNet,DATASET
804,"* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average mAP on THUMOS14 (>66.0%) and slightly improved results on ActivityNet and EPIC-Kitchens 100.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2.",THUMOS14,DATASET
805,"* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average mAP on THUMOS14 (>66.0%) and slightly improved results on ActivityNet and EPIC-Kitchens 100.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2.",ActivityNet,DATASET
806,"* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average mAP on THUMOS14 (>66.0%) and slightly improved results on ActivityNet and EPIC-Kitchens 100.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2.",EPIC-Kitchens 100,DATASET
807,"/libs/utils: Utility functions for training, inference, and postprocessing.  ## Installation * Follow INSTALL.md for installing necessary dependencies and compiling the code.  ## Frequently Asked Questions * See FAQ.md.   ## To Reproduce Our Results on THUMOS14 **Download Features and Annotations** * Download *thumos.tar.gz* (`md5sum 375f76ffbf7447af1035e694971ec9b2`) from [this Box link](https://uwmadison.box.com/s/glpuxadymf3gd01m1cj6g5c3bn39qbgr) or [this Google Drive link](https://drive.google.com/file/d/1zt2eoldshf99vJMDuu8jqxda55dCyhZP/view?",THUMOS14,DATASET
808,"/libs/utils: Utility functions for training, inference, and postprocessing.  ## Installation * Follow INSTALL.md for installing necessary dependencies and compiling the code.  ## Frequently Asked Questions * See FAQ.md.   ## To Reproduce Our Results on THUMOS14 **Download Features and Annotations** * Download *thumos.tar.gz* (`md5sum 375f76ffbf7447af1035e694971ec9b2`) from [this Box link](https://uwmadison.box.com/s/glpuxadymf3gd01m1cj6g5c3bn39qbgr) or [this Google Drive link](https://drive.google.com/file/d/1zt2eoldshf99vJMDuu8jqxda55dCyhZP/view?",thumos,DATASET
809,"pwd=74eh). * The file includes I3D features, action annotations in json format (similar to ActivityNet annotation format), and external classification scores.",ActivityNet,DATASET
810,/configs/thumos_i3d.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,thumos,DATASET
811,/ckpt/thumos_i3d_reproduce/logs ``` * Evaluate the trained model.,thumos,DATASET
812,/configs/thumos_i3d.yaml .,thumos,DATASET
813,"/ckpt/thumos_i3d_reproduce ``` * Training our model on THUMOS requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.",thumos,DATASET
814,"/ckpt/thumos_i3d_reproduce ``` * Training our model on THUMOS requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.",THUMOS,DATASET
815,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for THUMOS 14.,THUMOS 14,DATASET
816,│ └───pretrained/ │    └───thumos_i3d_reproduce/ │    │  └───thumos_reproduce_log.txt │    │  └───thumos_reproduce_results.txt │    │   └───,thumos,DATASET
817,│ └───pretrained/ │    └───thumos_i3d_reproduce/ │    │  └───thumos_reproduce_log.txt │    │  └───thumos_reproduce_results.txt │    │   └───,thumos,DATASET
818,│ └───pretrained/ │    └───thumos_i3d_reproduce/ │    │  └───thumos_reproduce_log.txt │    │  └───thumos_reproduce_results.txt │    │   └───,thumos,DATASET
819,/pretrained/thumos_i3d_reproduce/config.txt*. * The training log is located at *.,thumos,DATASET
820,/pretrained/thumos_i3d_reproduce/thumos_reproduce_log.txt* and also *.,thumos,DATASET
821,/pretrained/thumos_i3d_reproduce/logs*. * The pre-trained model is *.,thumos,DATASET
822,/pretrained/thumos_i3d_reproduce/epoch_034.pth.tar*. * Evaluate the pre-trained model.,thumos,DATASET
823,/configs/thumos_i3d.yaml .,thumos,DATASET
824,/pretrained/thumos_i3d_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on ActivityNet 1.3 **Download Features and Annotations** * Download *anet_1.3.tar.gz* (`md5sum c415f50120b9425ee1ede9ac3ce11203`) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?,thumos,DATASET
825,/pretrained/thumos_i3d_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on ActivityNet 1.3 **Download Features and Annotations** * Download *anet_1.3.tar.gz* (`md5sum c415f50120b9425ee1ede9ac3ce11203`) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?,ActivityNet 1.3,DATASET
826,"pwd=xuit). * The file includes TSP features, action annotations in json format (similar to ActivityNet annotation format), and external classification scores.",ActivityNet,DATASET
827,"**Details**: The features are extracted from the R(2+1)D-34 model pretrained with TSP on ActivityNet using clips of `16 frames` at a frame rate of `15 fps` and a stride of `16 frames` (*i.e.,* **non-overlapping** clips).",ActivityNet,DATASET
828,"/ckpt/anet_tsp_reproduce ``` * Training our model on ActivityNet requires ~4.6GB GPU memory, yet the inference might require over 10GB GPU memory.",ActivityNet,DATASET
829,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for ActivityNet 1.3.,ActivityNet 1.3,DATASET
830,/pretrained/anet_i3d_reproduce/ ```  * The results (mAP at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on EPIC Kitchens 100 **Download Features and Annotations** * Download *epic_kitchens.tar.gz* (`md5sum add9803756afd9a023bc9a9c547e0229`) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?,EPIC Kitchens 100,DATASET
831,/pretrained/anet_i3d_reproduce/ ```  * The results (mAP at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on EPIC Kitchens 100 **Download Features and Annotations** * Download *epic_kitchens.tar.gz* (`md5sum add9803756afd9a023bc9a9c547e0229`) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?,epic_kitchens,DATASET
832,pwd=f3tx). * The file includes SlowFast features as well as action annotations in json format (similar to ActivityNet annotation format).,SlowFast,DATASET
833,pwd=f3tx). * The file includes SlowFast features as well as action annotations in json format (similar to ActivityNet annotation format).,ActivityNet,DATASET
834,**Details**: The features are extracted from the SlowFast model pretrained on the training set of EPIC Kitchens 100 (action classification) using clips of `32 frames` at a frame rate of `30 fps` and a stride of `16 frames`.,EPIC Kitchens 100,DATASET
835,"│ └───data/ │    └───epic_kitchens/ │    │  └───annotations │    │  └───features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * On EPIC Kitchens, we train separate models for nouns and verbs. * To train our ActionFormer on verbs with SlowFast features, use ```shell python .",EPIC Kitchens,DATASET
836,"│ └───data/ │    └───epic_kitchens/ │    │  └───annotations │    │  └───features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * On EPIC Kitchens, we train separate models for nouns and verbs. * To train our ActionFormer on verbs with SlowFast features, use ```shell python .",SlowFast,DATASET
837,"/configs/epic_slowfast_verb.yaml --output reproduce ``` * To train our ActionFormer on nouns with SlowFast features, use ```shell python .",SlowFast,DATASET
838,"/ckpt/epic_slowfast_noun_reproduce ``` * Training our model on EPIC Kitchens requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.",EPIC Kitchens,DATASET
839,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for EPIC-Kitchens 100.,EPIC-Kitchens 100,DATASET
840,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,Ego4D Moment Queries,DATASET
841,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,SlowFast,DATASET
842,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,Omnivore,DATASET
843,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,Ego4D,DATASET
844,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,EgoVLP,DATASET
845,"For example, training on Omnivore and EgoVLP features will create an experiment folder under *.",Omnivore,DATASET
846,"For example, training on Omnivore and EgoVLP features will create an experiment folder under *.",EgoVLP,DATASET
847,"/ckpt/ego4d_omnivore_egovlp_reproduce ``` * Training our model on Ego4D with all three features requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.",Ego4D,DATASET
848,**[Optional] Evaluating Our Pre-trained Model**  We also provide pre-trained models for Ego4D trained with all feature combinations.,Ego4D,DATASET
849,│ └───pretrained/ │    └───ego4d_omnivore_egovlp_reproduce/ │    │   └───ego4d_omnivore_egovlp_reproduce_log.txt │    │   └───ego4d_omnivore_egovlp_reproduce_results.txt │    │   └───,ego4d_omnivore_egovlp,DATASET
850,│ └───pretrained/ │    └───ego4d_omnivore_egovlp_reproduce/ │    │   └───ego4d_omnivore_egovlp_reproduce_log.txt │    │   └───ego4d_omnivore_egovlp_reproduce_results.txt │    │   └───,ego4d_omnivore_egovlp,DATASET
851,│ └───pretrained/ │    └───ego4d_omnivore_egovlp_reproduce/ │    │   └───ego4d_omnivore_egovlp_reproduce_log.txt │    │   └───ego4d_omnivore_egovlp_reproduce_results.txt │    │   └───,ego4d_omnivore_egovlp,DATASET
852,/pretrained/ego4d_omnivore_egovlp_reproduce/config.txt*. * The training log is located at *.,ego4d_omnivore_egovlp,DATASET
853,/pretrained/ego4d_omnivore_egovlp_reproduce/ego4d_omnivore_egovlp_reproduce_log.txt* and also *.,ego4d_omnivore_egovlp,DATASET
854,/pretrained/ego4d_omnivore_egovlp_reproduce/logs*. * The pre-trained model is *.,ego4d_omnivore_egovlp,DATASET
855,/pretrained/ego4d_omnivore_egovlp_reproduce/epoch_010.pth.tar*. * Evaluate the pre-trained model.,ego4d_omnivore_egovlp,DATASET
856,"Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper. ``` @inproceedings{zhang2022actionformer,   title={ActionFormer: Localizing Moments of Actions with Transformers},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={European Conference on Computer Vision},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} } ```  If you cite our results on Ego4D, please consider citing our tech report in addition to the main paper. ``` @article{mu2022actionformerego4d,   title={Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} } ```  If you are using TSP features, please cite ``` @inproceedings{alwassel2021tsp,   title={{TSP}: Temporally-sensitive pretraining of video encoders for localization tasks},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},   pages={3173--3183},   year={2021} } ```",Ego4D,DATASET
857,Download Yelp data: https://www.yelp.com/dataset and place files in ```datasets/yelp_dataset/``` 2.,yelp_dataset,DATASET
858,Download subword tokenizer built on Yelp and place in  ```datasets/yelp_dataset/processed/```:  [link](https://s3.us-east-2.amazonaws.com/unsup-sum/subwordenc_32000_maxrevs260_fixed.pkl)  ### Pre-trained models  1.,yelp_dataset,DATASET
859,More details about the configuration parameters can be found [here](configs).  ### Datasets We include the following medical datasets formatted in the required format:  - MedQA - MedMCQA - PubMedQA - MMLU (medical subsets) - CareQA  Other datasets can be used if they are in the required format.,MedQA,DATASET
860,More details about the configuration parameters can be found [here](configs).  ### Datasets We include the following medical datasets formatted in the required format:  - MedQA - MedMCQA - PubMedQA - MMLU (medical subsets) - CareQA  Other datasets can be used if they are in the required format.,MedMCQA,DATASET
861,More details about the configuration parameters can be found [here](configs).  ### Datasets We include the following medical datasets formatted in the required format:  - MedQA - MedMCQA - PubMedQA - MMLU (medical subsets) - CareQA  Other datasets can be used if they are in the required format.,PubMedQA,DATASET
862,More details about the configuration parameters can be found [here](configs).  ### Datasets We include the following medical datasets formatted in the required format:  - MedQA - MedMCQA - PubMedQA - MMLU (medical subsets) - CareQA  Other datasets can be used if they are in the required format.,MMLU (medical subsets),DATASET
863,More details about the configuration parameters can be found [here](configs).  ### Datasets We include the following medical datasets formatted in the required format:  - MedQA - MedMCQA - PubMedQA - MMLU (medical subsets) - CareQA  Other datasets can be used if they are in the required format.,CareQA,DATASET
864,"/reachable_db.h5` db.generate(data, overwrite=True)  # Pull reachable set for first point in dataset x = data.iloc[0] reachable_set = db[x] print(reachable_set) # should return the following output: ##    age  marital_status  years_since_last_default  job_type_a  job_type_b  job_type_c ## 0  32.0             1.0                       5.0         0.0         1.0         0.0 ## 1  32.0             1.0                       5.0         0.0         0.0         1.0 ## 2  32.0             1.0                       5.0         1.0         0.0         0.0 ## 3  33.0             1.0                       6.0         0.0         0.0         1.0 ## 4  33.0             1.0                       6.0         0.0         1.0         0.0 ## 5  33.0             1.0                       6.0         1.0         0.0         0.0 ``` Given a classifier `clf` with a predict method, you can test if a point has recourse as `np.any(clf.predict(reachable_set.X))`  For more examples, check out [this script](https://github.com/ustunb/reachml/blob/main/research/iclr2024/scripts/setup_dataset_actionset_fico.py) which sets up the action set for the FICO dataset.  ### Resources and Citation  For more about recourse verification, check out our paper ICLR 2024 spotlight paper: [Prediction without Preclusion](https://openreview.net/forum?",FICO,DATASET
865,"# BOOKSUM: A Collection of Datasets for Long-form Narrative Summarization Authors: [Wojciech Kryściński](https://twitter.com/iam_wkr), [Nazneen Rajani](https://twitter.com/nazneenrajani), [Divyansh Agarwal](https://twitter.com/jigsaw2212), [Caiming Xiong](https://twitter.com/caimingxiong), [Dragomir Radev](http://www.cs.yale.edu/homes/radev/)  ## Introduction The majority of available text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases.",BOOKSUM,DATASET
866,"We address these issues by introducing BookSum, a collection of datasets for long-form narrative summarization.",BookSum,DATASET
867,"[Get Involved](#get-involved)  ## Updates #### 4/15/2021 Initial commit   ## Citation ``` @article{kryscinski2021booksum,       title={BookSum: A Collection of Datasets for Long-form Narrative Summarization},        author={Wojciech Kry{\'s}ci{\'n}ski and Nazneen Rajani and Divyansh Agarwal and Caiming Xiong and Dragomir Radev},       year={2021},       eprint={2105.08209},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  ## Legal Note By downloading or using the resources, including any code or scripts, shared in this code repository, you hereby agree to the following terms, and your use of the resources is conditioned on and subject to these terms. 1.",BookSum,DATASET
868,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",sop,DATASET
869,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",cub,DATASET
870,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",CUB200-2011,DATASET
871,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",cars,DATASET
872,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",CARS196,DATASET
873,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",sop,DATASET
874,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",Standford Online Porducts,DATASET
875,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",inshop,DATASET
876,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",In-Shop cloths retireval,DATASET
877,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",vid,DATASET
878,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",PKU Vehicle id,DATASET
879,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,CUB200-2011,DATASET
880,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,CUB-200,DATASET
881,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,CARS196,DATASET
882,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,cars,DATASET
883,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,Stanford Online Products,DATASET
884,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,In-shop Clothes Retrieval Benchmark,DATASET
885,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,PKU VehicleID,DATASET
886,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,pku-vehicleid,DATASET
887,"`<$datadir/cars196>`, pass `$datadir` as input to `--dataset-dir`, by default.",cars196,DATASET
888,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",CUB200-2011,DATASET
889,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",cub-200-2011,DATASET
890,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",Cars196,DATASET
891,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",cars,DATASET
892,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",cars196,DATASET
893,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",Stanford Online Products,DATASET
894,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",sop,DATASET
895,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",In-shop Clothes,DATASET
896,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",PKU Vehicle id,DATASET
897,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",CUB200,DATASET
898,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",Cars196,DATASET
899,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",In-Shop Clothes,DATASET
900,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",Online Products,DATASET
901,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",VID,DATASET
902,"Some task categories are synthetically generated through the Alpaca paradigm, others are manually labeled more classic NLP tasks (Xsum, SST2, Squad, CONLL) and are used as target tasks in most of the paper's experiments.",Xsum,DATASET
903,"Some task categories are synthetically generated through the Alpaca paradigm, others are manually labeled more classic NLP tasks (Xsum, SST2, Squad, CONLL) and are used as target tasks in most of the paper's experiments.",SST2,DATASET
904,"Some task categories are synthetically generated through the Alpaca paradigm, others are manually labeled more classic NLP tasks (Xsum, SST2, Squad, CONLL) and are used as target tasks in most of the paper's experiments.",Squad,DATASET
905,"Some task categories are synthetically generated through the Alpaca paradigm, others are manually labeled more classic NLP tasks (Xsum, SST2, Squad, CONLL) and are used as target tasks in most of the paper's experiments.",CONLL,DATASET
906,"Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lower EDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%) accuracy for CIFAR10 (TinyImagenet) datasets.",CIFAR10 (TinyImagenet),DATASET
907,"We thoroughly evaluate our method on SemanticKITTI and SemanticPOSS datasets, showing that it can significantly outperform the baseline.",SemanticKITTI,DATASET
908,"We thoroughly evaluate our method on SemanticKITTI and SemanticPOSS datasets, showing that it can significantly outperform the baseline.",SemanticPOSS,DATASET
909,"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",SemanticKITTI,DATASET
910,"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",semantic-kitti,DATASET
911,"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",SemanticPOSS,DATASET
912,"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",semanticposs,DATASET
913,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589.",SemanticPOSS,DATASET
914,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589.",SemanticKITTI,DATASET
915,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589.",SemanticPOSS,DATASET
916,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589.",SemanticKITTI,DATASET
917,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589.",SemanticPOSS,DATASET
918,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589.",SemanticKITTI,DATASET
919,EC-GAN is compared to the shared architecture method on SVHN at different dataset sizes.,SVHN,DATASET
920,"For the following example, assume to work with the dataset provided in the folder `test_data`: `wordnet31.gz`.",wordnet31,DATASET
921,"/test_data/wordnet31.gz -S -P -O   This script will produce the following files: `wordnet31.subjects_vocab`, `wordnet31.predicates_vocab` and `wordnet31.objects_vocab`.  2.",wordnet31,DATASET
922,"/test_data/wordnet31.gz -S -P -O   This script will produce the following files: `wordnet31.subjects_vocab`, `wordnet31.predicates_vocab` and `wordnet31.objects_vocab`.  2.",wordnet31,DATASET
923,"/test_data/wordnet31.gz -S -P -O   This script will produce the following files: `wordnet31.subjects_vocab`, `wordnet31.predicates_vocab` and `wordnet31.objects_vocab`.  2.",wordnet31,DATASET
924,"/test_data/wordnet31.gz -S -P -O   This script will produce the following files: `wordnet31.subjects_vocab`, `wordnet31.predicates_vocab` and `wordnet31.objects_vocab`.  2.",wordnet31,DATASET
925,"/test_data/wordnet31.gz   This script will map the dataset to integer triples,  producing the file `wordnet31.mapped.unsorted`.  3.",wordnet31,DATASET
926,"/test_data/wordnet31.gz   This script will map the dataset to integer triples,  producing the file `wordnet31.mapped.unsorted`.  3.",wordnet31,DATASET
927,Sort the file `wordnet31.mapped.unsorted` materializing the needed permutations.,wordnet31,DATASET
928,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4.",wordnet31,DATASET
929,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4.",wordnet31,DATASET
930,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4.",wordnet31,DATASET
931,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4.",wordnet31,DATASET
932,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4.",wordnet31,DATASET
933,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4.",wordnet31,DATASET
934,python3 build_stats.py wordnet31.mapped.sorted   This script will create the file `wordnet31.mapped.sorted.stats`.,wordnet31,DATASET
935,python3 build_stats.py wordnet31.mapped.sorted   This script will create the file `wordnet31.mapped.sorted.stats`.,wordnet31,DATASET
936,/test_data/wordnet31.gz  to prepare the `wordnet31` collection for indexing.,wordnet31,DATASET
937,/test_data/wordnet31.gz  to prepare the `wordnet31` collection for indexing.,wordnet31,DATASET
938,"/test_data/wordnet31.mapped.sorted -o wordnet31.pef_3t.bin  will build a 3T index (see Section 3.1 of [1]), compressed with partitioned Elias-Fano (PEF), that is serialized to the binary file `wordnet31.pef_3t.bin`.",wordnet31,DATASET
939,"/test_data/wordnet31.mapped.sorted -o wordnet31.pef_3t.bin  will build a 3T index (see Section 3.1 of [1]), compressed with partitioned Elias-Fano (PEF), that is serialized to the binary file `wordnet31.pef_3t.bin`.",wordnet31,DATASET
940,"/test_data/wordnet31.mapped.sorted -o wordnet31.pef_3t.bin  will build a 3T index (see Section 3.1 of [1]), compressed with partitioned Elias-Fano (PEF), that is serialized to the binary file `wordnet31.pef_3t.bin`.",wordnet31,DATASET
941,/test_data/wordnet31.mapped.unsorted > ../..,wordnet31,DATASET
942,/test_data/wordnet31.mapped.unsorted.queries.5000  that will create a querylog with 5000 triples selected at random.,wordnet31,DATASET
943,/queries pef_3t 1 wordnet31.pef_3t.bin -q ..,wordnet31,DATASET
944,/test_data/wordnet31.mapped.unsorted.queries.5000 -n 5000 -w 1  will execute 5000 SP?,wordnet31,DATASET
945,"/statistics pef_2tp wordnet31.pef_2tp.bin  Testing <a name=""testing""></a> -------  Run the script `test/check_everything.py` from within the `.",wordnet31,DATASET
946,/test_data/wordnet31.mapped.sorted . wordnet  This script will check every triple selection pattern for all the different types of indexes.,wordnet31,DATASET
947,"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg .",ImageNet-LT,DATASET
948,"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg .",iNaturalist 2018,DATASET
949,"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg .",Places-LT,DATASET
950,"Extensive experiments on the COCO dataset show that Team DETR achieves remarkable gains, especially for small and large objects.  ## Framework  !",COCO,DATASET
951,We strongly recommend you use `pytorch >= 1.11.0` for its less GPU memory consumption.  ### Dataset  [COCO2017](https://cocodataset.org/) is used to validate our method.,COCO2017,DATASET
952,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)",Adult Census,DATASET
953,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)",adult,DATASET
954,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)",Bank Marketing,DATASET
955,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)",Bank%2BMarketing,DATASET
956,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)",Credit Card Default,DATASET
957,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)",Dutch Census,DATASET
958,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)",COMPAS,DATASET
959,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)",compas,DATASET
960,"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   🍎  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021",OntoEvent,DATASET
961,"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   🍎  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021",OntoEvent,DATASET
962,"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   🍎  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021",OntoEvent,DATASET
963,🤗  The implementations are based on [Huggingface's Transformers](https://github.com/huggingface/transformers) and remanagement is referred to [MAVEN's baselines](https://github.com/THU-KEG/MAVEN-dataset/) & [DeepKE](https://github.com/zjunlp/DeepKE),MAVEN,DATASET
964,"It models the relationship between event types through ontology embedding: it can transfer knowledge of high-resource event types to low-resource ones, and the unseen event type can establish connection with seen ones via event ontology.   ## Project Structure The structure of data and code is as follows:   ```shell Reasoning_In_EE ├── README.md ├── OntoED   # model │   ├── README.md │   ├── data_utils.py  # for data processing │   ├── ontoed.py   # main model │   ├── run_ontoed.py  # for model running │   └── run_ontoed.sh  # bash file for model running ├── OntoEvent  # data │   ├── README.md │   ├── __init__.py │   ├── event_dict_data_on_doc.json.zip  # raw full ED data │   ├── event_dict_train_data.json   # ED data for training │   ├── event_dict_test_data.json   # ED data for testing │   ├── event_dict_valid_data.json   # ED data for validation │   └── event_relation.json     # event-event relation data └── baselines  # baseline models     ├── DMCNN     │   ├── README.md     │   ├── convert.py   # for data processing     │   ├── data    # data     │   │   └── labels.json     │   ├── dmcnn.config  # configure training & testing     │   ├── eval.sh    # bash file for model evaluation     │   ├── formatter     │   │   ├── DmcnnFormatter.py # runtime data processing     │   │   └── __init__.py     │   ├── main.py    # project entrance     │   ├── model     │   │   ├── Dmcnn.py  # main model     │   │   └── __init__.py     │   ├── raw     │   │   └── 100.utf8  # word vector     │   ├── reader     │   │   ├── MavenReader.py # runtime data reader     │   │   └── __init__.py     │   ├── requirements.txt # requirements     │   ├── train.sh   # bash file for model training     │   └── utils     │       ├── __init__.py     │       ├── configparser_hook.py     │       ├── evaluation.py     │       ├── global_variables.py     │       ├── initializer.py     │       └── runner.py     ├── JMEE     │   ├── README.md     │   ├── data    # to store data file     │   ├── enet     │   │   ├── __init__.py     │   │   ├── consts.py  # configurable parameters     │   │   ├── corpus     │   │   │   ├── Corpus.py # dataset class     │   │   │   ├── Data.py     │   │   │   ├── Sentence.py     │   │   │   └── __init__.py     │   │   ├── models   # modules of JMEE     │   │   │   ├── DynamicLSTM.py     │   │   │   ├── EmbeddingLayer.py     │   │   │   ├── GCN.py     │   │   │   ├── HighWay.py     │   │   │   ├── SelfAttention.py     │   │   │   ├── __init__.py     │   │   │   ├── ee.py     │   │   │   └── model.py # main model     │   │   ├── run     │   │   │   ├── __init__.py     │   │   │   └── ee     │   │   │       ├── __init__.py     │   │   │       └── runner.py # runner class     │   │   ├── testing.py  # evaluation     │   │   ├── training.py  # training     │   │   └── util.py     │   ├── eval.sh    # bash file for model evaluation     │   ├── requirements.txt # requirements     │   └── train.sh   # bash file for model training     ├── README.md     ├── eq1.png     ├── eq2.png     ├── jointEE-NN     │   ├── README.md     │   ├── data     │   │   └── fistDoc.nnData4.txt # data format sample     │   ├── evaluateJEE.py   # model evaluation     │   ├── jeeModels.py   # main model     │   ├── jee_processData.py  # data process     │   └── jointEE.py    # project entrance     └── stanford.zip   # cleaned dataset for baseline models ```  ## Requirements  - python==3.6.9  - torch==1.8.0 (lower may also be OK)  - transformers==2.8.0  - sklearn==0.20.2   ## Usage   **1.",OntoEvent,DATASET
965,- 'LABEL\_PATH' and 'RELATION\_PATH' means the path for [event\_dict\_train_data.json](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent/event_dict_train_data.json) and [event_relation.json](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent/event_relation.json) respectively.   **3.,OntoEvent,DATASET
966,- 'LABEL\_PATH' and 'RELATION\_PATH' means the path for [event\_dict\_train_data.json](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent/event_dict_train_data.json) and [event_relation.json](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent/event_relation.json) respectively.   **3.,OntoEvent,DATASET
967,"/run_ontoed.sh ('--do_train', '--do_eval', '--evaluate_during_training', '--do_test' is necessarily input in 'run_ontoed.sh')  Or you can run run_ontoed.py with manual parameter input (parameters can be copied from 'run_ontoed.sh')  python run_ontoed.py --para...  ```   ## How about the Dataset [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent)  is proposed for ED and also annotated with correlations among events.",OntoEvent,DATASET
968,"/run_ontoed.sh ('--do_train', '--do_eval', '--evaluate_during_training', '--do_test' is necessarily input in 'run_ontoed.sh')  Or you can run run_ontoed.py with manual parameter input (parameters can be copied from 'run_ontoed.sh')  python run_ontoed.py --para...  ```   ## How about the Dataset [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent)  is proposed for ED and also annotated with correlations among events.",OntoEvent,DATASET
969,"Please refer to [OntoEvent](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) for details.   ### Statistics The statistics of OntoEvent are shown below, and the detailed data schema can be referred to our paper.",OntoEvent,DATASET
970,"Please refer to [OntoEvent](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) for details.   ### Statistics The statistics of OntoEvent are shown below, and the detailed data schema can be referred to our paper.",OntoEvent,DATASET
971,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format.",ACE 2005,DATASET
972,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format.",TAC KBP 2017,DATASET
973,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format.",FewEvent,DATASET
974,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format.",MAVEN,DATASET
975,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format.",OntoEvent,DATASET
976,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format.",OntoEvent,DATASET
977,"🍒 For each *event instance* in [```event_dict_data_on_doc.json```](https://github.com/231sm/Reasoning_In_EE/blob/main/OntoEvent/event_dict_data_on_doc.json.zip), the data format is as below:  ``` {     'doc_id': '...',      'doc_title': 'XXX',      'sent_id': ,      'event_mention': '......',      'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],      'trigger': '...',      'trigger_pos': [, ],      'event_type': '' } ``` 🍒 For each *event relation* in [```event_relation.json```](https://github.com/231sm/Reasoning_In_EE/blob/main/OntoEvent/event_relation.json), we list the *event instance pair*, and the data format is as below:  ``` 'EVENT_RELATION_1': [      [         {             'doc_id': '...',              'doc_title': 'XXX',              'sent_id': ,              'event_mention': '......',              'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],              'trigger': '...',              'trigger_pos': [, ],              'event_type': ''         },          {             'doc_id': '...',              'doc_title': 'XXX',              'sent_id': ,              'event_mention': '......',              'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],              'trigger': '...',              'trigger_pos': [, ],              'event_type': ''         }     ],      ... ] ``` 🍒 Especially for ""COSUPER"", ""SUBSUPER"" and ""SUPERSUB"", we list the *event type pair*, and the data format is as below:  ``` ""COSUPER"": [     [""Conflict.Attack"", ""Conflict.Protest""],      [""Conflict.Attack"", ""Conflict.Sending""],      ... ] ```   ## How to Cite 📋  Thank you very much for your interest in our work.",OntoEvent,DATASET
978,"🍒 For each *event instance* in [```event_dict_data_on_doc.json```](https://github.com/231sm/Reasoning_In_EE/blob/main/OntoEvent/event_dict_data_on_doc.json.zip), the data format is as below:  ``` {     'doc_id': '...',      'doc_title': 'XXX',      'sent_id': ,      'event_mention': '......',      'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],      'trigger': '...',      'trigger_pos': [, ],      'event_type': '' } ``` 🍒 For each *event relation* in [```event_relation.json```](https://github.com/231sm/Reasoning_In_EE/blob/main/OntoEvent/event_relation.json), we list the *event instance pair*, and the data format is as below:  ``` 'EVENT_RELATION_1': [      [         {             'doc_id': '...',              'doc_title': 'XXX',              'sent_id': ,              'event_mention': '......',              'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],              'trigger': '...',              'trigger_pos': [, ],              'event_type': ''         },          {             'doc_id': '...',              'doc_title': 'XXX',              'sent_id': ,              'event_mention': '......',              'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],              'trigger': '...',              'trigger_pos': [, ],              'event_type': ''         }     ],      ... ] ``` 🍒 Especially for ""COSUPER"", ""SUBSUPER"" and ""SUPERSUB"", we list the *event type pair*, and the data format is as below:  ``` ""COSUPER"": [     [""Conflict.Attack"", ""Conflict.Protest""],      [""Conflict.Attack"", ""Conflict.Sending""],      ... ] ```   ## How to Cite 📋  Thank you very much for your interest in our work.",OntoEvent,DATASET
979,"# Low_Resource_KBP knowledge graph population in low resource conditions   The file ""*Few-Shot_ED.json.zip*"" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[""Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection""](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)",Few-Shot_ED,DATASET
980,"# Low_Resource_KBP knowledge graph population in low resource conditions   The file ""*Few-Shot_ED.json.zip*"" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[""Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection""](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)",FewEvent,DATASET
981,"# Low_Resource_KBP knowledge graph population in low resource conditions   The file ""*Few-Shot_ED.json.zip*"" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[""Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection""](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)",ACE-2005 corpus,DATASET
982,"# Low_Resource_KBP knowledge graph population in low resource conditions   The file ""*Few-Shot_ED.json.zip*"" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[""Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection""](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)",TAC-KBP-2017 Event Track Data,DATASET
983,"Download the VisDial v1.0 dialog json files from [here][3] and keep it under `$PROJECT_ROOT/data` directory, for default arguments to work effectively.  2.",VisDial v1.0,DATASET
984,Get the word counts for VisDial v1.0 train split [here][4].,VisDial v1.0,DATASET
985,"[batra-mlp-lab][6] provides pre-extracted image features of VisDial v1.0 images, using a Faster-RCNN pre-trained on Visual Genome.",VisDial v1.0,DATASET
986,"[batra-mlp-lab][6] provides pre-extracted image features of VisDial v1.0 images, using a Faster-RCNN pre-trained on Visual Genome.",Visual Genome,DATASET
987,"If you wish to extract your own image features, skip this step and download VisDial v1.0 images from [here][3] instead.",VisDial v1.0,DATASET
988,Prepare the [MSCOCO][11] and [Flickr][3] images.  2.,MSCOCO,DATASET
989,Prepare the [MSCOCO][11] and [Flickr][3] images.  2.,Flickr,DATASET
990,/data/extract_features_detectron.py --image-root /path/to/MSCOCO/train2014/ /path/to/MSCOCO/val2014/ --save-path /path/to/feature --split train # Bottom-up features of 36 proposals from images of train split. python .,MSCOCO,DATASET
991,/data/extract_features_detectron.py --image-root /path/to/MSCOCO/train2014/ /path/to/MSCOCO/val2014/ --save-path /path/to/feature --split train # Bottom-up features of 36 proposals from images of train split. python .,MSCOCO,DATASET
992,/data/extract_features_detectron.py --image-root /path/to/Flickr/VisualDialog_val2018 --save-path /path/to/feature --split val # Bottom-up features of 36 proposals from images of val split. python .,Flickr,DATASET
993,/data/extract_features_detectron.py --image-root /path/to/Flickr/VisualDialog_test2018 --save-path /path/to/feature --split test # Bottom-up features of 36 proposals from images of test split. ```  Initializing GloVe Word Embeddings -------------- Simply run  ```shell python data/init_glove.py ```   Training --------  Train the model provided in this repository as:  ```shell python train.py --config-yml configs/rva.yml --gpu-ids 0 # provide more ids for multi-GPU execution other args... ```  ### Saving model checkpoints  This script will save model checkpoints at every epoch as per path specified by `--save-dirpath`.,Flickr,DATASET
994,"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",AFHQ,DATASET
995,"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",afhq,DATASET
996,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ...",AnimalFaces,DATASET
997,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ...",AFHQ,DATASET
998,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ...",AnimalFaces,DATASET
999,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ...",afhq,DATASET
1000,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ...",animal_faces,DATASET
1001,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ...",ffhq,DATASET
1002,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ...",lsun_car,DATASET
1003,__Train on local__ ``` Supervised python main.py --gpu $GPU_TO_USE --p_semi 1.0 --dataset animal_faces --data_path='..,animal_faces,DATASET
1004,/data'  Semi-supervised python main.py --gpu $GPU_TO_USE --p_semi 0.5 --dataset animal_faces --data_path='..,animal_faces,DATASET
1005,/data'  Unsupervised python main.py --gpu $GPU_TO_USE --p_semi 0.0 --dataset animal_faces --data_path='..,animal_faces,DATASET
1006,/data' ```  __Test on local__ ``` python main.py --gpu $GPU_TO_USE --validation --load_model $DIR_TO_LOAD --dataset animal_faces ```  __Monitoring__ ``` tensorboard --logdir=$DIR/events --port=$PORT ```  __Actual example__ ``` Train python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..,animal_faces,DATASET
1007,/data' ```  __Test on local__ ``` python main.py --gpu $GPU_TO_USE --validation --load_model $DIR_TO_LOAD --dataset animal_faces ```  __Monitoring__ ``` tensorboard --logdir=$DIR/events --port=$PORT ```  __Actual example__ ``` Train python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..,animal_faces,DATASET
1008,/data' --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..,animal_faces,DATASET
1009,/data' --p_semi 0.2 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --data_path '..,afhq_cat,DATASET
1010,/data' --p_semi 0.0 python main.py --gpu 1 --dataset animal_faces --data_path '..,animal_faces,DATASET
1011,"/data' --p_semi 1.0 python main.py --gpu 0,1 --dataset summer2winter --output_k 2 --data_path '..",summer2winter,DATASET
1012,/data' --p_semi 0.0 --img_size 256 --batch_size 16 --ddp  Test python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..,animal_faces,DATASET
1013,/data' --validation --load_model GAN_20190101_101010 python main.py --gpu 1 --dataset afhq_cat --output_k 10 --data_path '..,afhq_cat,DATASET
1014,/data' --validation --load_model GAN_20190101_101010 python main.py --gpu 2 --dataset summer2winter --output_k 2 --data_path '..,summer2winter,DATASET
1015,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",animal_faces,DATASET
1016,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",AFHQ,DATASET
1017,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",afhq_cat,DATASET
1018,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",afhq_cat,DATASET
1019,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",AFHQ,DATASET
1020,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",afhq_dog,DATASET
1021,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",afhq,DATASET
1022,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",afhq_dog,DATASET
1023,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",AFHQ,DATASET
1024,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",afhq,DATASET
1025,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",afhq,DATASET
1026,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",afhq_wild,DATASET
1027,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",AnimalFaces,DATASET
1028,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",animal_faces,DATASET
1029,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",animal_faces,DATASET
1030,"* For more classes on AnimalFaces, change the list at main.py#L227 then, set args.output_k to len(args.att_to_use)     * ex) args.att_to_use = \[i for i in range(100)\] then, run: python main.py --output_k 100 ...  ### Arguments * batch_size, img_size, data_path and p_semi are frequently speified. * Please refer ""help"" of the arguments in main.py.  ### Code Structure * main.py     * Execute main.py to run the codes",AnimalFaces,DATASET
1031,[afhq_cat](.,afhq_cat,DATASET
1032,[afhq_dog](.,afhq_dog,DATASET
1033,[afhq_wild](.,afhq_wild],DATASET
1034,"**Evaluation**：We will run the submitted learning method on poisoned CIFAR-10 datasets by 10 backdoor attacks used in our paper, then test the Attack Sucess Rate (ASR) and Clean Accuracy (CA) of the final model.",CIFAR-10,DATASET
1035,"| #     |           Paper            |    Venue     | Poisoned data | Architecture | Attack | ASR (%)| CA (%)| | ----- | :------------------------: | :----------: | :------------: | :----------: | :---------: | :-----------: | :----------: | | **1** | **[ABL]()** | NeurIPS 2021 |  *available* |    WRN-16-1    |   BadNets   |     3.04     |    86.11      | | **2** |                            |              |                |              |             |               |              | | **3** |                            |              |                |              |             |               |              | | **4** |                            |              |                |              |             |               |              | | **5** |                            |              |                |              |             |               |              | | **6** |                            |              |                |              |             |               |              | | **7** |                            |              |                |              |             |               |              | | **8** |                            |              |                |              |             |               |              |  ------  ## Verifying the unlearning effect of ABL with 1% isolated data:  ### An example with a pretrained model WRN-16-1, CIFAR-10, GridTrigger, target label 0, weights: `.",CIFAR-10,DATASET
1036,Run the following command to verify the unlearning effect:  ```bash $ python quick_unlearning_demo.py  ``` The training logs are shown below. 1% isolation = 500 images from poisoned CIFAR-10.,CIFAR-10,DATASET
1037,We present the setup for the WikiSQL experiments,WikiSQL,DATASET
1038,Download data from [WikiSQL](https://github.com/salesforce/WikiSQL).   ``` $ cd wikisql_data $ wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 $ tar -xvjf data.tar.bz2 ``` 2.,WikiSQL,DATASET
1039,Download data from [WikiSQL](https://github.com/salesforce/WikiSQL).   ``` $ cd wikisql_data $ wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 $ tar -xvjf data.tar.bz2 ``` 2.,WikiSQL,DATASET
1040,Download data from [WikiSQL](https://github.com/salesforce/WikiSQL).   ``` $ cd wikisql_data $ wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 $ tar -xvjf data.tar.bz2 ``` 2.,wikisql,DATASET
1041,Download data from [WikiSQL](https://github.com/salesforce/WikiSQL).   ``` $ cd wikisql_data $ wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 $ tar -xvjf data.tar.bz2 ``` 2.,WikiSQL,DATASET
1042,Put the [lib directory](https://github.com/salesforce/WikiSQL/tree/master/lib) under `wikisql_data/scripts/` 3.,WikiSQL,DATASET
1043,Put the [lib directory](https://github.com/salesforce/WikiSQL/tree/master/lib) under `wikisql_data/scripts/` 3.,wikisql,DATASET
1044,Run annotation using Stanza and preproces the dataset ``` $ cd wikisql_data/scripts/ $ python annotate.py $ python prepare.py ```  4.,wikisql,DATASET
1045,"AryzSDJYB5TxnF31OCt_4to7uY2t), where the ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat`` are the files that can be directly used in training.",wikisql,DATASET
1046,"AryzSDJYB5TxnF31OCt_4to7uY2t), where the ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat`` are the files that can be directly used in training.",wikisql,DATASET
1047,"AryzSDJYB5TxnF31OCt_4to7uY2t), where the ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat`` are the files that can be directly used in training.",wikisql,DATASET
1048,Note: the version 2 dataset matches the v1.1 release of [WikiSQL](https://github.com/salesforce/WikiSQL).,WikiSQL,DATASET
1049,Note: the version 2 dataset matches the v1.1 release of [WikiSQL](https://github.com/salesforce/WikiSQL).,WikiSQL,DATASET
1050,"The preprocessing script ``wikisql_data/scripts/prepare_v2.py`` (python3 required) processes WikiSQL v1.1 raw data and table files to generate ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat``.   ## Training Meta + Sum loss training ``` $ OUTDIR=output/meta_sum $ mkdir $OUTDIR $ python run.py --input-dir .",wikisql,DATASET
1051,"The preprocessing script ``wikisql_data/scripts/prepare_v2.py`` (python3 required) processes WikiSQL v1.1 raw data and table files to generate ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat``.   ## Training Meta + Sum loss training ``` $ OUTDIR=output/meta_sum $ mkdir $OUTDIR $ python run.py --input-dir .",WikiSQL v1.1,DATASET
1052,"The preprocessing script ``wikisql_data/scripts/prepare_v2.py`` (python3 required) processes WikiSQL v1.1 raw data and table files to generate ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat``.   ## Training Meta + Sum loss training ``` $ OUTDIR=output/meta_sum $ mkdir $OUTDIR $ python run.py --input-dir .",wikisql,DATASET
1053,"The preprocessing script ``wikisql_data/scripts/prepare_v2.py`` (python3 required) processes WikiSQL v1.1 raw data and table files to generate ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat``.   ## Training Meta + Sum loss training ``` $ OUTDIR=output/meta_sum $ mkdir $OUTDIR $ python run.py --input-dir .",wikisql,DATASET
1054,# SciTweets  <!,SciTweets,DATASET
1055,"-- Short Introduction what this repo is about --> This repository contains a dataset, annotation framework and code for the work *""SciTweets - A Dataset and Annotation Framework for Detecting Scientific Online Discourse""* published at **CIKM2022**.",SciTweets,DATASET
1056,"-- *TODO*: refer to our work before being published (e.g arxiv preprint)  *TODO 2*: give examples of scientific online discourse here, e.g ""The **SciTweets** dataset consists of ..."" -->  __Table of contents:__ - [Contents of the Repository](#contents-of-the-repository)   - [Directory Structure](#directory-structure)   - [Statistics](#statistics) - [Pretrained Model](#pretrained-model) - [Publication](#publication) - [Licensing](#licensing) - [Contact](#credits) - [Acknowledgment](#acknowledgment)  ## Contents of the Repository  ### Directory Structure =======================<br/> This repository contains the following directories and files:  1.",SciTweets,DATASET
1057,**annotations.tsv** the annotated SciTweets dataset 2.,SciTweets,DATASET
1058,"""SciTweets-A Dataset and Annotation Framework for Detecting Scientific Online Discourse.""",SciTweets,DATASET
1059,"Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 2022, [download](https://arxiv.org/abs/2206.07360).*  ```bib @inproceedings{hafid2022scitweets,   title={SciTweets-A Dataset and Annotation Framework for Detecting Scientific Online Discourse},   author={Hafid, Salim and Schellhammer, Sebastian and Bringay, Sandra and Todorov, Konstantin and Dietze, Stefan},   booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},   pages={3988--3992},   year={2022} } ```  ## Licensing This dataset is published under CC BY 4.0 license.",SciTweets,DATASET
1060,"- `2023.09.04`  Our NuScenes-QA dataset v1.0 released.  ## :hourglass_flowing_sand: To Do  - [x] Release question & anwswer data - [x] Release visual feature - [x] Release training and testing code  ## :running: Getting Started  ### Data Preparation  We have released our question-answer annotations, please download it from [HERE](https://drive.google.com/drive/folders/1jIkICT23wZWZYPrWCa0x-ubjpClSzOuU?",NuScenes-QA,DATASET
1061,"As an alternative, you can also download the origin nuScenes dataset from [HERE](https://www.nuscenes.org/download), and extract the object-level features refer to this [LINK](https://mmdetection3d.readthedocs.io/en/v0.16.0/datasets/nuscenes_det.html) with different backbones.",nuScenes,DATASET
1062,"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",NuScenes-QA,DATASET
1063,"For now, we copy a dummy sample dataset that is available at: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData    For the current example, we should choose the sample dataset [ip_dataset.csv](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv).",ip_dataset.csv,DATASET
1064,"For now, we copy a dummy sample dataset that is available at: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData    For the current example, we should choose the sample dataset [ip_dataset.csv](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv).",ip_dataset.csv,DATASET
1065,"We should copy this Comma-Sparated Values (CSV) file into the correct path, in this case at target/data (the data sub-directory must be created):    ```bash  cd target/  mkdir data  cp /home/user/ML-Quadrat/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv data/  ```  Now, in order to run the generated application / IoT service:    ```bash  java -jar SmartPingPongCfg-1.0.0-jar-with-dependencies.jar  ```  After running the service, you will see the output in the terminal.",ip_dataset.csv,DATASET
1066,Here is an example of single GPU non-distributed training SimVP+gSTA on Moving MNIST dataset.,Moving MNIST,DATASET
1067,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?,BAIR Robot Pushing,DATASET
1068,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?,Human3.6M,DATASET
1069,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?,KTH Action,DATASET
1070,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?,KittiCaltech Pedestrian,DATASET
1071,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?,Kinetics-400,DATASET
1072,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?,kinetics,DATASET
1073,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?,Moving MNIST,DATASET
1074,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?,mmnist,DATASET
1075,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?,Moving FMNIST,DATASET
1076,"pwd=kjfk)] [[config](configs/mfmnist)]     - [x] [TaxiBJ](https://arxiv.org/abs/1610.00081) (AAAI'2017) [[download](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)] [[config](configs/taxibj)]     - [x] [WeatherBench](https://arxiv.org/abs/2002.00469) (ArXiv'2020) [[download](https://github.com/pangeo-data/WeatherBench)] [[config](configs/weather)]      </details>  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Visualization  We present visualization examples of ConvLSTM below.",mfmnist,DATASET
1077,"pwd=kjfk)] [[config](configs/mfmnist)]     - [x] [TaxiBJ](https://arxiv.org/abs/1610.00081) (AAAI'2017) [[download](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)] [[config](configs/taxibj)]     - [x] [WeatherBench](https://arxiv.org/abs/2002.00469) (ArXiv'2020) [[download](https://github.com/pangeo-data/WeatherBench)] [[config](configs/weather)]      </details>  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Visualization  We present visualization examples of ConvLSTM below.",TaxiBJ,DATASET
1078,"pwd=kjfk)] [[config](configs/mfmnist)]     - [x] [TaxiBJ](https://arxiv.org/abs/1610.00081) (AAAI'2017) [[download](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)] [[config](configs/taxibj)]     - [x] [WeatherBench](https://arxiv.org/abs/2002.00469) (ArXiv'2020) [[download](https://github.com/pangeo-data/WeatherBench)] [[config](configs/weather)]      </details>  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Visualization  We present visualization examples of ConvLSTM below.",TaxiBJ,DATASET
1079,"pwd=kjfk)] [[config](configs/mfmnist)]     - [x] [TaxiBJ](https://arxiv.org/abs/1610.00081) (AAAI'2017) [[download](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)] [[config](configs/taxibj)]     - [x] [WeatherBench](https://arxiv.org/abs/2002.00469) (ArXiv'2020) [[download](https://github.com/pangeo-data/WeatherBench)] [[config](configs/weather)]      </details>  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Visualization  We present visualization examples of ConvLSTM below.",WeatherBench,DATASET
1080,"pwd=kjfk)] [[config](configs/mfmnist)]     - [x] [TaxiBJ](https://arxiv.org/abs/1610.00081) (AAAI'2017) [[download](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)] [[config](configs/taxibj)]     - [x] [WeatherBench](https://arxiv.org/abs/2002.00469) (ArXiv'2020) [[download](https://github.com/pangeo-data/WeatherBench)] [[config](configs/weather)]      </details>  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Visualization  We present visualization examples of ConvLSTM below.",WeatherBench,DATASET
1081,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Moving MNIST,DATASET
1082,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Moving FMNIST,DATASET
1083,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",mmnist,DATASET
1084,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Moving MNIST-CIFAR,DATASET
1085,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",KittiCaltech,DATASET
1086,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",moving_mnist,DATASET
1087,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",kitticaltech,DATASET
1088,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",KTH,DATASET
1089,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Human 3.6M,DATASET
1090,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Traffic - in flow,DATASET
1091,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Traffic - out flow,DATASET
1092,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Weather - Temperature,DATASET
1093,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Weather - Humidity,DATASET
1094,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Weather - Latitude Wind,DATASET
1095,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Weather - Cloud Cover,DATASET
1096,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",BAIR Robot Pushing,DATASET
1097,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Kinetics-400,DATASET
1098,Framework for Robustness Validation of Machine Comprehension Systems  This repository contains dataset for robustness validation of machine comprehension systems trained on SQuAD dataset.,SQuAD,DATASET
1099,# NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts  This repository contains the NUBes corpus and other related material.,NUBes,DATASET
1100,# NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts  This repository contains the NUBes corpus and other related material.,NUBes,DATASET
1101,[NUBes and IULA+](#nubes-and-iula)     * [Size and Composition](#size-and-composition)     * [Data Protection](#data-protection)     * [Related Publications](#related-publications) 1.,NUBes,DATASET
1102,[NUBes and IULA+](#nubes-and-iula)     * [Size and Composition](#size-and-composition)     * [Data Protection](#data-protection)     * [Related Publications](#related-publications) 1.,IULA+,DATASET
1103,[NUBes and IULA+](#nubes-and-iula)     * [Size and Composition](#size-and-composition)     * [Data Protection](#data-protection)     * [Related Publications](#related-publications) 1.,nubes,DATASET
1104,[NUBes and IULA+](#nubes-and-iula)     * [Size and Composition](#size-and-composition)     * [Data Protection](#data-protection)     * [Related Publications](#related-publications) 1.,iula,DATASET
1105,"[Contact](#contact)   ## NUBes and IULA+  The NUBes corpus (from ""Negation and Uncertainty annotations in Biomedical texts in Spanish"") consists of sentences obtained from anonymised health records and annotated with negation and uncertainty phenomena.",NUBes,DATASET
1106,"[Contact](#contact)   ## NUBes and IULA+  The NUBes corpus (from ""Negation and Uncertainty annotations in Biomedical texts in Spanish"") consists of sentences obtained from anonymised health records and annotated with negation and uncertainty phenomena.",IULA+,DATASET
1107,"[Contact](#contact)   ## NUBes and IULA+  The NUBes corpus (from ""Negation and Uncertainty annotations in Biomedical texts in Spanish"") consists of sentences obtained from anonymised health records and annotated with negation and uncertainty phenomena.",NUBes,DATASET
1108,"[Contact](#contact)   ## NUBes and IULA+  The NUBes corpus (from ""Negation and Uncertainty annotations in Biomedical texts in Spanish"") consists of sentences obtained from anonymised health records and annotated with negation and uncertainty phenomena.",Negation and Uncertainty annotations in Biomedical texts in Spanish,DATASET
1109,IULA+ is a new version of the IULA-SCRC corpus (accessible [here](http://eines.iula.upf.edu/brat/\#/NegationOnCR_IULA)).,IULA+,DATASET
1110,IULA+ is a new version of the IULA-SCRC corpus (accessible [here](http://eines.iula.upf.edu/brat/\#/NegationOnCR_IULA)).,IULA-SCRC,DATASET
1111,IULA+ is a new version of the IULA-SCRC corpus (accessible [here](http://eines.iula.upf.edu/brat/\#/NegationOnCR_IULA)).,iula,DATASET
1112,IULA+ is a new version of the IULA-SCRC corpus (accessible [here](http://eines.iula.upf.edu/brat/\#/NegationOnCR_IULA)).,IULA,DATASET
1113,"More specifically, it consists of the same texts but annotated with NUBes' guidelines.",NUBes,DATASET
1114,/NUBes-guias-de-anotacion.pdf) (in Spanish). * NUBes and IULA+ are distributed in [BRAT standoff format](https://brat.nlplab.org/standoff.html).    ### Size and Composition  * NUBes is divided into 10 samples of approximately 3K sentences each. * The first sample ([SAMPLE-001](.,NUBes,DATASET
1115,/NUBes-guias-de-anotacion.pdf) (in Spanish). * NUBes and IULA+ are distributed in [BRAT standoff format](https://brat.nlplab.org/standoff.html).    ### Size and Composition  * NUBes is divided into 10 samples of approximately 3K sentences each. * The first sample ([SAMPLE-001](.,NUBes,DATASET
1116,/NUBes-guias-de-anotacion.pdf) (in Spanish). * NUBes and IULA+ are distributed in [BRAT standoff format](https://brat.nlplab.org/standoff.html).    ### Size and Composition  * NUBes is divided into 10 samples of approximately 3K sentences each. * The first sample ([SAMPLE-001](.,IULA+,DATASET
1117,/NUBes-guias-de-anotacion.pdf) (in Spanish). * NUBes and IULA+ are distributed in [BRAT standoff format](https://brat.nlplab.org/standoff.html).    ### Size and Composition  * NUBes is divided into 10 samples of approximately 3K sentences each. * The first sample ([SAMPLE-001](.,NUBes,DATASET
1118,"/NUBes/SAMPLE-001)) has been annotated following a process involving 2 annotators and   a referee, while the rest have been annotated by one person only.",NUBes,DATASET
1119,"/NUBes/SAMPLE-001/sample-001.traum.chico.txt)   contains sentences of the specialty ""traumatology"" and the section ""chief complaint"".",NUBes,DATASET
1120,"You can consult the sizes of NUBes and IULA+ in the following table: <table>     <thead>     <th></th>     <th>NUBes</th>     <th>IULA+</th>     </thead>     <tbody>     <tr>     <td colspan=""3""><b>overall stats</b></td>     </tr>     <tr>     <td>sentences</td>     <td>29,682</td>     <td>3,363</td>     </tr>     <tr>     <td>tokens</td>     <td>518,068</td>     <td>38,208</td>     </tr>     <tr>     <td>vocabulary size</td>     <td>31,698</td>     <td>8,651</td>     </tr>     <tr>     <td colspan=""3""><b>negation</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>7,567</td>     <td>1,022<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.25 +/- 0.66</td>     <td>1.20 +/- 0.59</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>0</td>     <td>0</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>4.01 +/- 3.59</td>     <td>3.13 +/- 2.67</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>219</td>     <td>24</td>     </tr>     <tr>     <td colspan=""3""><b>uncertainty</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>2,219</td>     <td>178<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.12 +/- 0.38</td>     <td>1.12 +/- 0.38</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>95</td>     <td>20</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>5.27 +/- 4.97</td>     <td>4.75 +/- 3.96</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>123</td>     <td>7</td>     </tr>     </tbody> </table>  <small><sup>*</sup> These numbers do not match those in the paper's Table 1; the correct counts are shown here.",NUBes,DATASET
1121,"You can consult the sizes of NUBes and IULA+ in the following table: <table>     <thead>     <th></th>     <th>NUBes</th>     <th>IULA+</th>     </thead>     <tbody>     <tr>     <td colspan=""3""><b>overall stats</b></td>     </tr>     <tr>     <td>sentences</td>     <td>29,682</td>     <td>3,363</td>     </tr>     <tr>     <td>tokens</td>     <td>518,068</td>     <td>38,208</td>     </tr>     <tr>     <td>vocabulary size</td>     <td>31,698</td>     <td>8,651</td>     </tr>     <tr>     <td colspan=""3""><b>negation</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>7,567</td>     <td>1,022<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.25 +/- 0.66</td>     <td>1.20 +/- 0.59</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>0</td>     <td>0</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>4.01 +/- 3.59</td>     <td>3.13 +/- 2.67</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>219</td>     <td>24</td>     </tr>     <tr>     <td colspan=""3""><b>uncertainty</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>2,219</td>     <td>178<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.12 +/- 0.38</td>     <td>1.12 +/- 0.38</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>95</td>     <td>20</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>5.27 +/- 4.97</td>     <td>4.75 +/- 3.96</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>123</td>     <td>7</td>     </tr>     </tbody> </table>  <small><sup>*</sup> These numbers do not match those in the paper's Table 1; the correct counts are shown here.",IULA+,DATASET
1122,"You can consult the sizes of NUBes and IULA+ in the following table: <table>     <thead>     <th></th>     <th>NUBes</th>     <th>IULA+</th>     </thead>     <tbody>     <tr>     <td colspan=""3""><b>overall stats</b></td>     </tr>     <tr>     <td>sentences</td>     <td>29,682</td>     <td>3,363</td>     </tr>     <tr>     <td>tokens</td>     <td>518,068</td>     <td>38,208</td>     </tr>     <tr>     <td>vocabulary size</td>     <td>31,698</td>     <td>8,651</td>     </tr>     <tr>     <td colspan=""3""><b>negation</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>7,567</td>     <td>1,022<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.25 +/- 0.66</td>     <td>1.20 +/- 0.59</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>0</td>     <td>0</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>4.01 +/- 3.59</td>     <td>3.13 +/- 2.67</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>219</td>     <td>24</td>     </tr>     <tr>     <td colspan=""3""><b>uncertainty</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>2,219</td>     <td>178<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.12 +/- 0.38</td>     <td>1.12 +/- 0.38</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>95</td>     <td>20</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>5.27 +/- 4.97</td>     <td>4.75 +/- 3.96</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>123</td>     <td>7</td>     </tr>     </tbody> </table>  <small><sup>*</sup> These numbers do not match those in the paper's Table 1; the correct counts are shown here.",NUBes,DATASET
1123,"You can consult the sizes of NUBes and IULA+ in the following table: <table>     <thead>     <th></th>     <th>NUBes</th>     <th>IULA+</th>     </thead>     <tbody>     <tr>     <td colspan=""3""><b>overall stats</b></td>     </tr>     <tr>     <td>sentences</td>     <td>29,682</td>     <td>3,363</td>     </tr>     <tr>     <td>tokens</td>     <td>518,068</td>     <td>38,208</td>     </tr>     <tr>     <td>vocabulary size</td>     <td>31,698</td>     <td>8,651</td>     </tr>     <tr>     <td colspan=""3""><b>negation</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>7,567</td>     <td>1,022<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.25 +/- 0.66</td>     <td>1.20 +/- 0.59</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>0</td>     <td>0</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>4.01 +/- 3.59</td>     <td>3.13 +/- 2.67</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>219</td>     <td>24</td>     </tr>     <tr>     <td colspan=""3""><b>uncertainty</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>2,219</td>     <td>178<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.12 +/- 0.38</td>     <td>1.12 +/- 0.38</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>95</td>     <td>20</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>5.27 +/- 4.97</td>     <td>4.75 +/- 3.96</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>123</td>     <td>7</td>     </tr>     </tbody> </table>  <small><sup>*</sup> These numbers do not match those in the paper's Table 1; the correct counts are shown here.",IULA+,DATASET
1124,"</small>  ### Data Protection  All sensitive information (e.g., people names, healthcare facilities, dates, and so on) in NUBes have been subsituted with fake similar data.",NUBes,DATASET
1125,"What is more, sentences that belong to the same health record are scattered across different samples.   ### Related Publications  To know more about NUBes, read our article ""NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts"" [[pdf](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.708.pdf)].",NUBes,DATASET
1126,"What is more, sentences that belong to the same health record are scattered across different samples.   ### Related Publications  To know more about NUBes, read our article ""NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts"" [[pdf](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.708.pdf)].",NUBes,DATASET
1127,"To know more about IULA-SCRC, read the article ""Annotation of negation in the IULA Spanish Clinical Record Corpus"" by Montserrat Marimon, Jorge Vivaldi and Núria Bel [[pdf](https://www.aclweb.org/anthology/W17-1807.pdf)].",IULA-SCRC,DATASET
1128,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](.",NUBes,DATASET
1129,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](.",IULA+,DATASET
1130,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](.",NUBes,DATASET
1131,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](.",IULA+,DATASET
1132,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](.",IULA-SCRC,DATASET
1133,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](.",NUBes,DATASET
1134,"/NUBes), [IULA+](.",NUBes,DATASET
1135,"/NUBes), [IULA+](.",IULA+,DATASET
1136,"/IULA+), [NUBes experiment splits](.",IULA+,DATASET
1137,"/IULA+), [NUBes experiment splits](.",NUBes,DATASET
1138,/LREC2020/data) and [NUBes annotation guidelines](.,NUBes,DATASET
1139,/NUBes-guias-de-anotacion.pdf) are licensed under the Creative Commons Attribution-ShareAlike 3.0 Spain  License.,NUBes,DATASET
1140,Run experiments This is one example of reproducing results for Movielens-1M with BPR.,Movielens-1M,DATASET
1141,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Amazon-Book,DATASET
1142,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Yelp-2018,DATASET
1143,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Gowalla,DATASET
1144,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,MovieLens-1M,DATASET
1145,# UI understanding datasets for UIBert  This directory contains two datasets that are used in the downstream tasks for evaluating [UIBert: Learning Generic Multimodal Representations for UI Understanding](https://arxiv.org/abs/2107.13731): app similar UI component retrieval data (AppSim) and referring expression component retrieval data (RefExp).,AppSim,DATASET
1146,# UI understanding datasets for UIBert  This directory contains two datasets that are used in the downstream tasks for evaluating [UIBert: Learning Generic Multimodal Representations for UI Understanding](https://arxiv.org/abs/2107.13731): app similar UI component retrieval data (AppSim) and referring expression component retrieval data (RefExp).,RefExp,DATASET
1147,"Both datasets are extended from the public Rico dataset, which contains 72k mobile app UI data.",Rico,DATASET
1148,"In AppSim, each datapoint contains two UIs of similar category and the     annotation of two semantically similar UI elements on them, such as a “Menu”     buttons that appear on two UIs. 2.",AppSim,DATASET
1149,"In RefExp, each datapoint contains a UI and a referring expression of a UI     element on it, such as “Red button on the top”.",RefExp,DATASET
1150,"We use unique image ids in both datasets to represent the UI and they can be used to retrieve the original UI data in [Rico](https://interactionmining.org/rico).  ## Data format  Both data are saved as TFRecords, which makes it easy to be loaded in Tensorflow.  1.",Rico,DATASET
1151,"We use unique image ids in both datasets to represent the UI and they can be used to retrieve the original UI data in [Rico](https://interactionmining.org/rico).  ## Data format  Both data are saved as TFRecords, which makes it easy to be loaded in Tensorflow.  1.",rico,DATASET
1152,AppSim: Each TFRecord contains unique image ids of an anchor UI and a search     UI.,AppSim,DATASET
1153,RefExp: Each TFRecord has the following features.,RefExp,DATASET
1154,-   `image/ref_exp/label`: int64_list[1].,ref_exp,DATASET
1155,-   `image/ref_exp/text`: bytes_list[1].,ref_exp,DATASET
1156,"Run experiments in the shell of the Docker container following the usage table as follows.   ### Usage |Algorithm|Usage| |---|---| |DejaVu|Run for dataset A1: `python exp/run_GAT_node_classification.py -H=4 -L=8 -fe=GRU -bal=True --data_dir=data/A1`| |JSS'20|Run for dataset A1: `python exp/DejaVu/run_JSS20.py --data_dir=data/A1`| |iSQUAD|Run for dataset A1: `python exp/DejaVu/run_iSQ.py --data_dir=data/A1`| |Decision Tree|Run for dataset A1: `python exp/run_DT_node_classification.py --data_dir=data/A1`| |RandomWalk@Metric|Run for dataset A1: `python exp/DejaVu/run_random_walk_single_metric.py --data_dir=data/A1 --window_size 60 10 --score_aggregation_method=min`| |RandomWalk@FI|Run for dataset A1: `python exp/DejaVu/run_random_walk_failure_instance.py --data_dir=data/A1 --window_size 60 10 --anomaly_score_aggregation_method=min --corr_aggregation_method=max`| |Global interpretation|Run `notebooks/explain.py` as a jupyter notebook with `jupytext`| |Local interpretation|`DejaVu/explanability/similar_faults.py`|  The commands would print a `one-line summary` in the end, including the following fields: `A@1`, `A@2`, `A@3`, `A@5`, `MAR`, `Time`, `Epoch`, `Valid Epoch`, `output_dir`, `val_loss`, `val_MAR`, `val_A@1`, `command`, `git_commit_url`, which are the desrired results.",iSQUAD,DATASET
1157,"Interspeech 2021},   pages={4249--4253},   doi={10.21437/Interspeech.2021-1286} } ```  ## Setup  ### Download Google Speech Commands  There are two versions of the dataset, V1 and V2.",Google Speech Commands,DATASET
1158,"To download and extract dataset V2, run:  ```shell wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz mkdir data2 mv .",speech_commands_v0.02,DATASET
1159,/speech_commands_v0.02.tar.gz .,speech_commands_v0.02,DATASET
1160,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .,speech_commands_v0.02,DATASET
1161,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .,wget,DATASET
1162,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .,speech_commands_v0.01,DATASET
1163,/speech_commands_v0.01.tar.gz .,speech_commands_v0.01,DATASET
1164,/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .,speech_commands_v0.01,DATASET
1165,"There are three variants of the Keyword-Transformer model:  * **Time-domain attention**: each time-window is treated as a patch, self-attention is computed between time-windows * **Frequency-domain attention**: each frequency is treated as a patch self-attention is computed between frequencies * **Combination of both**: The signal is fed into both a time- and a frequency-domain transformer and the outputs are combined * **Patch-wise attention**: Similar to the vision transformer, it extracts rectangular patches from the spectrogram, so attention happens both in the time and frequency domain simultaneously.  ## Training a model from scratch To train KWT-3 from scratch on Speech Commands V2, run    ```shell sh train.sh ```  Please note that the train directory (given by the argument  `--train_dir`) cannot exist prior to start script.",Speech Commands V2,DATASET
1166,"|Model name|embedding dim|mlp-dim|heads|depth|#params|V2-12 accuracy|pre-trained| |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:| |KWT-1|64|128|1|12|607K|97.7|[here](models_data_v2_12_labels/kwt1)| |KWT-2|128|256|2|12|2.4M|98.2|[here](models_data_v2_12_labels/kwt2)| |KWT-3|192|768|3|12|5.5M|98.7|[here](models_data_v2_12_labels/kwt3)|  To perform inference on Google Speech Commands v2 with 12 labels, run  ```shell sh eval.sh ```  ## Acknowledgements  The code heavily borrows from the [KWS streaming work](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.",Google Speech Commands v2,DATASET
1167,# HaluEval: A Hallucination Evaluation Benchmark for LLMs  This is the repo for our paper: [HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2305.11747).,HaluEval,DATASET
1168,# HaluEval: A Hallucination Evaluation Benchmark for LLMs  This is the repo for our paper: [HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2305.11747).,HaluEval,DATASET
1169,"The repo contains:  - The [35K data](#data-release) used for evaluating the LLM. - The code for [generating the data](#data-generation-process). - The code for [evaluating the model](#evaluation). - The code for [analyzing the model](#analysis).  ## Overview  HaluEval includes 5,000 general user queries with ChatGPT responses and  30,000 task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization.",HaluEval,DATASET
1170,"Furthermore, for the task-specific examples in HaluEval, we design an automatic approach to generate hallucinated samples.",HaluEval,DATASET
1171,"First, based on existing task datasets (e.g., HotpotQA) as seed data, we design task-specific instructions for ChatGPT to generate hallucinated samples in two methods, i.e., one-pass and conversational.",HotpotQA,DATASET
1172,"<a href=""https://github.com/RUCAIBox/HaluEval"" target=""_blank""><img src=""assets/pipeline.png"" alt=""HaluEval"" style=""width: 90%; min-width: 300px; display: block; margin: auto;""></a>  ## Data Release  The directory [`data`](.",HaluEval,DATASET
1173,"<a href=""https://github.com/RUCAIBox/HaluEval"" target=""_blank""><img src=""assets/pipeline.png"" alt=""HaluEval"" style=""width: 90%; min-width: 300px; display: block; margin: auto;""></a>  ## Data Release  The directory [`data`](.",HaluEval,DATASET
1174,/data/qa_data.json): 10K hallucinated samples for QA based on [HotpotQA](https://hotpotqa.github.io/) as seed data.,HotpotQA,DATASET
1175,/data/qa_data.json): 10K hallucinated samples for QA based on [HotpotQA](https://hotpotqa.github.io/) as seed data.,hotpotqa,DATASET
1176,"For each sample dictionary, the fields `knowledge`, `question`, and `right_answer` refer to the knowledge from Wikipedia, question text, and ground-truth answer collected from HotpotQA.",HotpotQA,DATASET
1177,/data/dialogue_data.json): 10K hallucinated samples for dialogue based on [OpenDialKG](https://github.com/facebookresearch/opendialkg) as seed data.,OpenDialKG,DATASET
1178,/data/dialogue_data.json): 10K hallucinated samples for dialogue based on [OpenDialKG](https://github.com/facebookresearch/opendialkg) as seed data.,opendialkg,DATASET
1179,"For each sample dictionary, the fields `knowledge`, `dialogue_history`, and `right_response` refer to the knowledge from Wikipedia, dialogue history, and ground-truth response collected from OpenDialKG.",OpenDialKG,DATASET
1180,/data/summarization_data.json): 10K hallucinated samples for summarization based on [CNN/Daily Mail](https://github.com/abisee/cnn-dailymail) as seed data.,CNN/Daily Mail,DATASET
1181,/data/summarization_data.json): 10K hallucinated samples for summarization based on [CNN/Daily Mail](https://github.com/abisee/cnn-dailymail) as seed data.,cnn-dailymail,DATASET
1182,"For each sample dictionary, the fields `document` and `right_summary` refer to the document and ground-truth summary collected from CNN/Daily Mail.",CNN/Daily Mail,DATASET
1183,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy",HotpotQA,DATASET
1184,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy",OpenDialKG,DATASET
1185,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy",CNN/Daily Mail,DATASET
1186,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy",hotpot/hotpot_train_v1.1,DATASET
1187,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy",opendialkg,DATASET
1188,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy",opendialkg,DATASET
1189,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy",ccdv/cnn_dailymail,DATASET
1190,"- `seed_data`: the downloaded training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail",HotpotQA,DATASET
1191,"- `seed_data`: the downloaded training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail",OpenDialKG,DATASET
1192,"- `seed_data`: the downloaded training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail",CNN/Daily Mail,DATASET
1193,"(one-pass and conversational in our paper) ``` python generate.py --seed_data hotpot_train_v1.1.json --task qa --strategy one-turn ```  - Finally, we select the most plausible and difficult hallucinated sample from these two sampling methods.",hotpot_train_v1.1,DATASET
1194,/evaluation/qa/qa_gpt-3.5-turbo_result.json --category all ```  ## License  HaluEval uses [MIT License](.,HaluEval,DATASET
1195,"/LICENSE).  ## Reference  Please cite the repo if you use the data or code in this repo.  ``` @misc{HaluEval,   author = {Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen },   title = {HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},   year = {2023},   journal={arXiv preprint arXiv:2305.11747},   url={https://arxiv.org/abs/2305.11747} } ```",HaluEval,DATASET
1196,"/LICENSE).  ## Reference  Please cite the repo if you use the data or code in this repo.  ``` @misc{HaluEval,   author = {Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen },   title = {HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},   year = {2023},   journal={arXiv preprint arXiv:2305.11747},   url={https://arxiv.org/abs/2305.11747} } ```",HaluEval,DATASET
1197,# Goldfinch: GOogLe image-search Dataset for FINe grained CHallenges  Goldfinch is a dataset for fine-grained recognition challenges.,Goldfinch,DATASET
1198,# Goldfinch: GOogLe image-search Dataset for FINe grained CHallenges  Goldfinch is a dataset for fine-grained recognition challenges.,Goldfinch,DATASET
1199,Goldfinch was published along with our [ECCV'16](http://www.eccv2016.org/) paper [The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition](https://arxiv.org/abs/1511.06789).,Goldfinch,DATASET
1200,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity.",cub,DATASET
1201,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity.",CUB,DATASET
1202,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity.",CUB,DATASET
1203,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity.",birdsnap,DATASET
1204,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity.",Birdsnap,DATASET
1205,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity.",Birdsnap,DATASET
1206,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity.",fgvc,DATASET
1207,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity.",FGVC-Aircraft,DATASET
1208,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity.",FGVC,DATASET
1209,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity.",FGVC,DATASET
1210,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files.",Stanford Dogs,DATASET
1211,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files.",Stanford Dogs,DATASET
1212,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files.",L-Bird,DATASET
1213,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files.",L-Butterfly,DATASET
1214,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files.",L-Dog,DATASET
1215,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files.",L-Aircraft,DATASET
1216,"id=1535 ... ```  ### flickr_{aircraft,bird,lepidoptera}.txt: For each image in the Flickr testing set: ``` freebase id (or category name for aircraft),urls ``` E.g. ``` ""northrop f 20 tigershark"",http://farm8.staticflickr.com/7151/6387816433_fbf6240392_b.jpg ""transall c 160"",http://farm1.staticflickr.com/292/19707583438_e199092d07_c.jpg ""mikoyan mig 29"",http://farm3.staticflickr.com/2831/9358339349_271a6f28cd_c.jpg ... ```  ---- ## Active learning files  ### sdogs_active_learning_iteration{1,2}.txt:  Images mined from Yahoo Flickr Creative Commons 100M dataset.",Flickr testing set,DATASET
1217,"id=1535 ... ```  ### flickr_{aircraft,bird,lepidoptera}.txt: For each image in the Flickr testing set: ``` freebase id (or category name for aircraft),urls ``` E.g. ``` ""northrop f 20 tigershark"",http://farm8.staticflickr.com/7151/6387816433_fbf6240392_b.jpg ""transall c 160"",http://farm1.staticflickr.com/292/19707583438_e199092d07_c.jpg ""mikoyan mig 29"",http://farm3.staticflickr.com/2831/9358339349_271a6f28cd_c.jpg ... ```  ---- ## Active learning files  ### sdogs_active_learning_iteration{1,2}.txt:  Images mined from Yahoo Flickr Creative Commons 100M dataset.",Yahoo Flickr Creative Commons 100M,DATASET
1218,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",SVAMP,DATASET
1219,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",SVAMP,DATASET
1220,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",GSM8K,DATASET
1221,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",gsm8k,DATASET
1222,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",MAWPS,DATASET
1223,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",mawps,DATASET
1224,"Therefore, if you are just looking to explore the method and don't need to re-create everything from scratch, we recommend that you skip this step,   If you would like to reproduce the offline dataset with the llama2 model, you need to follow these steps:   ```  git clone git@github.com:facebookresearch/llama.git  ``` and then move ```Prompt-OIRL/llama_exps/llama_step1_gen_offline.py``` to the ```llama``` folder  then run the following command   ``` torchrun --nproc_per_node 1 llama_step1_gen_offline.py \     --ckpt_dir llama-2-7b-chat/ \     --tokenizer_path tokenizer.model \     --max_seq_len 512 --max_batch_size 8 --prompt_idx 0 --dataset_eval gsm8k  ```  #### Step 2.",gsm8k,DATASET
1225,"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures.",XOR,DATASET
1226,"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures.",Spiral,DATASET
1227,"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures.",Circle,DATASET
1228,"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",Security,DATASET
1229,"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",Intrusion Detection,DATASET
1230,"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",ids-2017,DATASET
1231,"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",CICIDS2017,DATASET
1232,"</li></p> <p align=""justify""> Tor-nonTor Dataset (<a href=""https://www.unb.ca/cic/datasets/tor.html"">ISCXTor2016</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",Tor-nonTor,DATASET
1233,"</li></p> <p align=""justify""> Tor-nonTor Dataset (<a href=""https://www.unb.ca/cic/datasets/tor.html"">ISCXTor2016</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",tor,DATASET
1234,"</li></p> <p align=""justify""> Tor-nonTor Dataset (<a href=""https://www.unb.ca/cic/datasets/tor.html"">ISCXTor2016</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",ISCXTor2016,DATASET
1235,"</p> <p align=""justify""> DDoS Attach Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2018.html"">CSE-CIC-IDS2018</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",DDoS Attach,DATASET
1236,"</p> <p align=""justify""> DDoS Attach Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2018.html"">CSE-CIC-IDS2018</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",ids-2018,DATASET
1237,"</p> <p align=""justify""> DDoS Attach Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2018.html"">CSE-CIC-IDS2018</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",CSE-CIC-IDS2018,DATASET
1238,"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>].",NSL version of KDD Security,DATASET
1239,"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>].",nsl,DATASET
1240,"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>].",NSL-KDD,DATASET
1241,"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>].",NSL_KDD_Security,DATASET
1242,"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>].",DDoS Evaluation,DATASET
1243,"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>].",ddos-2019,DATASET
1244,"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>].",CICDDoS2019,DATASET
1245,"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>].",CICDDoS2019_Security,DATASET
1246,"</p> <p align=""justify""> CIC DoS Dataset (<a href=""https://www.unb.ca/cic/datasets/dos-dataset.html"">CICDoS2017</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",CIC DoS,DATASET
1247,"</p> <p align=""justify""> CIC DoS Dataset (<a href=""https://www.unb.ca/cic/datasets/dos-dataset.html"">CICDoS2017</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",dos,DATASET
1248,"</p> <p align=""justify""> CIC DoS Dataset (<a href=""https://www.unb.ca/cic/datasets/dos-dataset.html"">CICDoS2017</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",CICDoS2017,DATASET
1249,"</p>  <p align=""justify""> VPN-nonVPN Dataset (<a href=""https://www.unb.ca/cic/datasets/vpn.html"">ISCXVPN2016</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",VPN-nonVPN,DATASET
1250,"</p>  <p align=""justify""> VPN-nonVPN Dataset (<a href=""https://www.unb.ca/cic/datasets/vpn.html"">ISCXVPN2016</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",vpn,DATASET
1251,"</p>  <p align=""justify""> VPN-nonVPN Dataset (<a href=""https://www.unb.ca/cic/datasets/vpn.html"">ISCXVPN2016</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",ISCXVPN2016,DATASET
1252,"</p>  <p align=""justify""> Botnet Dataset (<a href=""https://www.unb.ca/cic/datasets/botnet.html"">BotNet2014</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",Botnet,DATASET
1253,"</p>  <p align=""justify""> Botnet Dataset (<a href=""https://www.unb.ca/cic/datasets/botnet.html"">BotNet2014</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",BotNet2014,DATASET
1254,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",MLBENCH,DATASET
1255,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",XOR,DATASET
1256,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",XOR,DATASET
1257,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",Spiral,DATASET
1258,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",Spiral,DATASET
1259,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",Circle,DATASET
1260,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",Circle,DATASET
1261,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",Smiley,DATASET
1262,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",Smiley,DATASET
1263,"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",GTSRB,DATASET
1264,"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",German Traffic Sign Recognition Benchmark,DATASET
1265,"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",German_Traffic_Sign_Recognition,DATASET
1266,"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",GTSR,DATASET
1267,"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",GTSRB,DATASET
1268,"</li>   <li><a href = ""https://www.cs.toronto.edu/~kriz/cifar.html""><b>CIFAR 10/100 Datasets</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_MATLAB/CIFAR_10_Dataset/README.md"">MATLAB implementation</a>]</li>   <li>More datasets will be tested.",cifar,DATASET
1269,"</li>   <li><a href = ""https://www.cs.toronto.edu/~kriz/cifar.html""><b>CIFAR 10/100 Datasets</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_MATLAB/CIFAR_10_Dataset/README.md"">MATLAB implementation</a>]</li>   <li>More datasets will be tested.",CIFAR 10,DATASET
1270,"</li>   <li><a href = ""https://www.cs.toronto.edu/~kriz/cifar.html""><b>CIFAR 10/100 Datasets</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_MATLAB/CIFAR_10_Dataset/README.md"">MATLAB implementation</a>]</li>   <li>More datasets will be tested.",CIFAR_10,DATASET
1271,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",NarrativeQA,DATASET
1272,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",Qasper,DATASET
1273,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",MultiFieldQA,DATASET
1274,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",HotpotQA,DATASET
1275,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",2WikiMQA,DATASET
1276,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",Musique,DATASET
1277,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",GovReport,DATASET
1278,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",QMSum,DATASET
1279,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",MultiNews,DATASET
1280,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",TREC,DATASET
1281,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",TriviaQA,DATASET
1282,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",SAMSum,DATASET
1283,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",PassageRetrieval,DATASET
1284,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",LCC,DATASET
1285,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",RepoBench-P,DATASET
1286,"However, the LongBench datasets would require some extra preparations.",LongBench,DATASET
1287,"Here is an example for InfLLM on PaulGraham Passkey; the results can be found under `<output_folder_dir>`.  ``` compress_ratio=$1 # 2x, 4x, 6x, 8x output_dir_root=$2  task=""paulgraham_passkey"" dataset=""20480words_10x10x3_7digits"" model=""mistral-7b-instruct-v0.2_infinity_model_len"" method=""infllm""  python pipeline/inf_stream_llm/main.py \ --exp_desc ${task}_${dataset}_${model}_${method}_${compress_ratio} \ # this argument is purely cosmetic.",20480words_10x10x3_7digits,DATASET
1288,Manual Download  ImageNet is a TFDS Dataset but it needs to be downloaded manually.,ImageNet,DATASET
1289,Please check the [instructions](https://www.tensorflow.org/datasets/catalog/imagenet2012).,imagenet2012,DATASET
1290,"Example  Let's take an example learner (returns always zeros) that we will ""train"" on the DEBUG stream made of Pascal VOC 2007 and Coil100 datasets.",Pascal VOC 2007,DATASET
1291,"Example  Let's take an example learner (returns always zeros) that we will ""train"" on the DEBUG stream made of Pascal VOC 2007 and Coil100 datasets.",Coil100,DATASET
1292,Pascal VOC 2007 is a TFDS dataset so it will be automatically downloaded when needed.,Pascal VOC 2007,DATASET
1293,First we download Coil100 dataset:  ```bash .,Coil100,DATASET
1294,"/build_dataset.sh -e -b -s debug ```  Note that since the DEBUG stream only downloads Coil100, we could also have used `-d coil100` instead of `-s debug`.",Coil100,DATASET
1295,"/build_dataset.sh -e -b -s debug ```  Note that since the DEBUG stream only downloads Coil100, we could also have used `-d coil100` instead of `-s debug`.",coil100,DATASET
1296,"For example on ImageNet, run the configuration `configs/pretrain_imagenet.py`.",ImageNet,DATASET
1297,"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model.",Boston Housing,DATASET
1298,"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model.",boston,DATASET
1299,"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model.",Iris,DATASET
1300,"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model.",iris,DATASET
1301,"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification"".",CADEC,DATASET
1302,"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification"".",ICHI,DATASET
1303,"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification"".",Annotated CADEC,DATASET
1304,"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification"".",CADEC,DATASET
1305,"The annotated files can be found at **CADEC-Annotations/**  ### ICHI, a multi-class medical forum question classification dataset We obtain the ICHI data from rakshajalan/ECIR-2018 [Github codebase](https://github.com/rakshajalan/ECIR-2018/tree/master/ECIR-18_medical_question_classification/Dataset), and maintain the same train-test data split.  ## Citing MedBERT If you use the labeled CADEC dataset or use MedBERT model, please cite the paper:   ``` @inproceedings{10.1145/3459637.3482128, author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy}, title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification}, year = {2021}, isbn = {9781450384469}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459637.3482128}, doi = {10.1145/3459637.3482128}, abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers.",CADEC,DATASET
1306,"The annotated files can be found at **CADEC-Annotations/**  ### ICHI, a multi-class medical forum question classification dataset We obtain the ICHI data from rakshajalan/ECIR-2018 [Github codebase](https://github.com/rakshajalan/ECIR-2018/tree/master/ECIR-18_medical_question_classification/Dataset), and maintain the same train-test data split.  ## Citing MedBERT If you use the labeled CADEC dataset or use MedBERT model, please cite the paper:   ``` @inproceedings{10.1145/3459637.3482128, author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy}, title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification}, year = {2021}, isbn = {9781450384469}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459637.3482128}, doi = {10.1145/3459637.3482128}, abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers.",ICHI,DATASET
1307,"The annotated files can be found at **CADEC-Annotations/**  ### ICHI, a multi-class medical forum question classification dataset We obtain the ICHI data from rakshajalan/ECIR-2018 [Github codebase](https://github.com/rakshajalan/ECIR-2018/tree/master/ECIR-18_medical_question_classification/Dataset), and maintain the same train-test data split.  ## Citing MedBERT If you use the labeled CADEC dataset or use MedBERT model, please cite the paper:   ``` @inproceedings{10.1145/3459637.3482128, author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy}, title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification}, year = {2021}, isbn = {9781450384469}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459637.3482128}, doi = {10.1145/3459637.3482128}, abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers.",ICHI,DATASET
1308,"The annotated files can be found at **CADEC-Annotations/**  ### ICHI, a multi-class medical forum question classification dataset We obtain the ICHI data from rakshajalan/ECIR-2018 [Github codebase](https://github.com/rakshajalan/ECIR-2018/tree/master/ECIR-18_medical_question_classification/Dataset), and maintain the same train-test data split.  ## Citing MedBERT If you use the labeled CADEC dataset or use MedBERT model, please cite the paper:   ``` @inproceedings{10.1145/3459637.3482128, author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy}, title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification}, year = {2021}, isbn = {9781450384469}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459637.3482128}, doi = {10.1145/3459637.3482128}, abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers.",CADEC,DATASET
1309,We provide the bash script in the folder `bash_scripts` for the experiments on the *CUB* dataset.,CUB,DATASET
1310,"# AltEntities (Alternative Entities) dataset: Resolving Indirect Referring Expressions for Entity Selection  ## COPYRIGHT NOTICE  This is the work of *Mohammad Javad Hosseini*, *Filip Radlinski*, *Silvia Pareti*, and *Annie Louis* from Google LLC, made available under the CC-BY SA 3.0 License.",AltEntities,DATASET
1311,"# AltEntities (Alternative Entities) dataset: Resolving Indirect Referring Expressions for Entity Selection  ## COPYRIGHT NOTICE  This is the work of *Mohammad Javad Hosseini*, *Filip Radlinski*, *Silvia Pareti*, and *Annie Louis* from Google LLC, made available under the CC-BY SA 3.0 License.",Alternative Entities,DATASET
1312,A full copy of the license can be found at https://creativecommons.org/licenses/by-sa/3.0/  ## The Dataset  AltEntities is a dataset of English-language **alternative questions with their answers**.,AltEntities,DATASET
1313,"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",Visual Genome,DATASET
1314,/experiments/scripts/fetch_images.sh ``` This will populate the `DrillDown/data` folder with `vg/VG_100K` and `vg/VG_100K_2`,VG_100K,DATASET
1315,/experiments/scripts/fetch_images.sh ``` This will populate the `DrillDown/data` folder with `vg/VG_100K` and `vg/VG_100K_2`,VG_100K_2,DATASET
1316,"# HaloQuest  Welcome to the repository of our ECCV 2024 paper, [**HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning**](https://arxiv.org/abs/2407.15680).",HaloQuest,DATASET
1317,"# HaloQuest  Welcome to the repository of our ECCV 2024 paper, [**HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning**](https://arxiv.org/abs/2407.15680).",HaloQuest,DATASET
1318,This repository contains a [colab](https://github.com/google/haloquest/blob/main/HaloQuest_Colab.ipynb) that shows how to load the HaloQuest data and how to use the Auto-Eval system.,haloquest,DATASET
1319,This repository contains a [colab](https://github.com/google/haloquest/blob/main/HaloQuest_Colab.ipynb) that shows how to load the HaloQuest data and how to use the Auto-Eval system.,HaloQuest,DATASET
1320,This repository contains a [colab](https://github.com/google/haloquest/blob/main/HaloQuest_Colab.ipynb) that shows how to load the HaloQuest data and how to use the Auto-Eval system.,HaloQuest,DATASET
1321,Code to reproduce the experiments in the paper is available [here](https://github.com/ZhecanJamesWang/HaloQuest).,HaloQuest,DATASET
1322,[Unofficial Project Page](https://haloquest.github.io/)  !,haloquest,DATASET
1323,/example-image.png)  ## Updates - 2024.07.22: Our paper is on arxiv  ## Dataset Description  ### Summary  **HaloQuest** is a novel visual question answering (VQA) dataset that focuses on multimodal hallucination in vision-language models (VLMs).,HaloQuest,DATASET
1324,"It contains **7,748** examples with a combination of real and synthetically generated images, annotated with **questions** and **answers** designed to trigger and evaluate hallucinations.  ### Supported Tasks  **HaloQuest** supports tasks related to **hallucination detection and reduction** in VLMs, providing a challenging benchmark for **Visual Question Answering**.",HaloQuest,DATASET
1325,"The dataset is useful for both evaluation and fine-tuning purposes, aiming to advance multimodal reasoning.  ## Dataset Details  ### Data Collection HaloQuest includes a mix of real images from the Open Images dataset and synthetic images generated using Midjourney.",HaloQuest,DATASET
1326,"The dataset is useful for both evaluation and fine-tuning purposes, aiming to advance multimodal reasoning.  ## Dataset Details  ### Data Collection HaloQuest includes a mix of real images from the Open Images dataset and synthetic images generated using Midjourney.",Open Images,DATASET
1327,"ZW did some work while at Google DeepMind.)  ## Citing this work  ```latex @inproceedings{wang2024haloquest,   title={HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning},   author={Zhecan Wang and Garrett Bingham and Adams Wei Yu and Quoc V.",HaloQuest,DATASET
1328,You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode  Image URLs are from the [Open Images Dataset v7](https://storage.googleapis.com/openimages/web/factsfigures_v7.html#publications) and [Midjourney Showcase](https://www.midjourney.com/showcase).,Open Images Dataset v7,DATASET
1329,You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode  Image URLs are from the [Open Images Dataset v7](https://storage.googleapis.com/openimages/web/factsfigures_v7.html#publications) and [Midjourney Showcase](https://www.midjourney.com/showcase).,openimages,DATASET
1330,">You can find the Sachs dataset in the repo, at this [link](https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/main/examples/SachsData.csv).",Sachs,DATASET
1331,"For example, with the Sachs dataset, we can define the following clusters: ```python clusters=[     ['$PKC$','$P38$','$Jnk$'],     ['$Erk$','$Akt$','$PKA$'],     ['$Raf$','$Mek$'],     ['$Plcg$','$PIP2$','$PIP3$'] ] graph_discovery = CHD.GraphDiscovery.from_dataframe(df,clusters=clusters) ```  #### After creating a `GraphDiscovery` object, for the second run  If you already have a `GraphDiscovery` object named `graph_discovery` on which you have run the algorithm, you can choose the clusters based on the results and apply them to get a new `GraphDiscovery` object: ```python graph_discovery2=graph_discovery.prepare_new_graph_with_clusters(clusters) ```  ### Using clusters Once the clusters have been defined, you can use the `GraphDiscovery` object as usual.",Sachs,DATASET
1332,"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",MOT17,DATASET
1333,"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",crowdhuman,DATASET
1334,"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",MOTSynth,DATASET
1335,"The data directory is structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |-- MOT17 │   ├── train │   ├── test |-- MOTSynth |   ├── videos │   ├── annotations ```  Then, we need to convert the all dataset to COCO format.",crowdhuman,DATASET
1336,"The data directory is structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |-- MOT17 │   ├── train │   ├── test |-- MOTSynth |   ├── videos │   ├── annotations ```  Then, we need to convert the all dataset to COCO format.",MOT17,DATASET
1337,"The data directory is structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |-- MOT17 │   ├── train │   ├── test |-- MOTSynth |   ├── videos │   ├── annotations ```  Then, we need to convert the all dataset to COCO format.",MOTSynth,DATASET
1338,We provide scripts to do this:  ```shell # crowdhuman python .,crowdhuman,DATASET
1339,/data/crowdhuman/annotations  # MOT17 python .,MOT17,DATASET
1340,/data/MOT17/ -o .,MOT17,DATASET
1341,/data/MOT17/annotations --split-train --convert-det  # MOTSynth python .,MOT17,DATASET
1342,/data/MOT17/annotations --split-train --convert-det  # MOTSynth python .,MOTSynth,DATASET
1343,/data/MOTSynth/video --out_dir_path .,MOTSynth,DATASET
1344,/data/MOTSynth/train/ python .,MOTSynth,DATASET
1345,/data/MOTSynth/all_cocoformat.json ```  The processed dataset will be structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |   ├── annotations │   │   ├── crowdhuman_train.json │   │   ├── crowdhuman_val.json |-- MOT17 │   ├── train │   │   ├── MOT17-02-DPM │   │   ├── ... │   ├── test │   ├── annotations │   │   ├── half-train_cocoformat.json │   │   ├── ... |-- MOTSynth |   ├── videos │   ├── annotations │   ├── train │   │   ├── 000 │   │   │   ├── img1 │   │   │   │   ├── 000001.jpg │   │   │   │   ├── ... │   │   ├── ... │   ├── all_cocoformat.json ``` ### Pretrained models  We use CLIP pretrained ViT models.,crowdhuman,DATASET
1346,/data/MOTSynth/all_cocoformat.json ```  The processed dataset will be structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |   ├── annotations │   │   ├── crowdhuman_train.json │   │   ├── crowdhuman_val.json |-- MOT17 │   ├── train │   │   ├── MOT17-02-DPM │   │   ├── ... │   ├── test │   ├── annotations │   │   ├── half-train_cocoformat.json │   │   ├── ... |-- MOTSynth |   ├── videos │   ├── annotations │   ├── train │   │   ├── 000 │   │   │   ├── img1 │   │   │   │   ├── 000001.jpg │   │   │   │   ├── ... │   │   ├── ... │   ├── all_cocoformat.json ``` ### Pretrained models  We use CLIP pretrained ViT models.,MOT17,DATASET
1347,/data/MOTSynth/all_cocoformat.json ```  The processed dataset will be structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |   ├── annotations │   │   ├── crowdhuman_train.json │   │   ├── crowdhuman_val.json |-- MOT17 │   ├── train │   │   ├── MOT17-02-DPM │   │   ├── ... │   ├── test │   ├── annotations │   │   ├── half-train_cocoformat.json │   │   ├── ... |-- MOTSynth |   ├── videos │   ├── annotations │   ├── train │   │   ├── 000 │   │   │   ├── img1 │   │   │   │   ├── 000001.jpg │   │   │   │   ├── ... │   │   ├── ... │   ├── all_cocoformat.json ``` ### Pretrained models  We use CLIP pretrained ViT models.,MOT17,DATASET
1348,/data/MOTSynth/all_cocoformat.json ```  The processed dataset will be structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |   ├── annotations │   │   ├── crowdhuman_train.json │   │   ├── crowdhuman_val.json |-- MOT17 │   ├── train │   │   ├── MOT17-02-DPM │   │   ├── ... │   ├── test │   ├── annotations │   │   ├── half-train_cocoformat.json │   │   ├── ... |-- MOTSynth |   ├── videos │   ├── annotations │   ├── train │   │   ├── 000 │   │   │   ├── img1 │   │   │   │   ├── 000001.jpg │   │   │   │   ├── ... │   │   ├── ... │   ├── all_cocoformat.json ``` ### Pretrained models  We use CLIP pretrained ViT models.,MOTSynth,DATASET
1349,/pretrain/ViT-B-16.pt ``` Evaluation on MOT17 half validation set ```shell bash .,MOT17,DATASET
1350,"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP).",MOT17,DATASET
1351,"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP).",MOT17,DATASET
1352,"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP).",MOT17,DATASET
1353,"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP).",crowdhuman,DATASET
1354,"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP).",MOTSynth,DATASET
1355,"/aspire-biencoder-compsci-spec-full'  # Load hyperparameters from disk. with codecs.open(os.path.join(model_path, 'run_info.json'), 'r') as fp:     hparams = json.load(fp)     model_hparams = hparams['all_hparams']  # Initialize the tokenizer and model. aspire_tok = AutoTokenizer.from_pretrained(model_hparams['base-pt-layer']) aspire_bienc = AspireBiEnc(model_hparams)  # Load model parameters from disk. model_fname = os.path.join(model_path, 'model_cur_best.pt') aspire_bienc.load_state_dict(torch.load(model_fname))  # Encode example input. title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific ""         ""Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching ""            ""fine-grained aspects of texts."" d = [title + aspire_tok.sep_token + abstract]  inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) clsrep = aspire_bienc.forward(inputs)  ```   #### Evaluation Datasets <a name=""evaldata""></a>  The paper uses the following evaluation datasets:  - RELISH was created in [Brown et al. 2019](https://academic.oup.com/database/article/doi/10.1093/database/baz085/5608006?",RELISH,DATASET
1356,- TRECCOVID presents an ad-hoc search dataset.,TRECCOVID,DATASET
1357,"The versions of the dataset used may be accessed here: [query topics](https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml), [relevance annotations](https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt), and the metadata for papers is obtained from the [CORD-19](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html) dataset in the [2021-06-21](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-06-21/metadata.csv) release.",CORD-19,DATASET
1358,"The function `get_qbe_pools` in `pre_proc_treccovid.py`, converts the dataset in its original form to the reformulated form, TRECCOVID-RF, used in the paper.",treccovid,DATASET
1359,"The function `get_qbe_pools` in `pre_proc_treccovid.py`, converts the dataset in its original form to the reformulated form, TRECCOVID-RF, used in the paper.",TRECCOVID,DATASET
1360,Dataset splits are created in `pre_proc_treccovid.py`,treccovid,DATASET
1361,- SciDocs is obtained from: [link](https://github.com/allenai/scidocs).,SciDocs,DATASET
1362,- SciDocs is obtained from: [link](https://github.com/allenai/scidocs).,scidocs,DATASET
1363,- CSFCube is obtained from: [link](https://github.com/iesl/CSFCube).,CSFCube,DATASET
1364,- CSFCube is obtained from: [link](https://github.com/iesl/CSFCube).,CSFCube,DATASET
1365,"Complete evaluation datasets used in the paper can be downloaded here: [`datasets/datasets.md`](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name=""repocontents""></a>      ├── bin     ├── config     │             └── models_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid_models         │             │             ├── disent_models.py         │             │             ├── pair_distances.py         │             │             └── sentsim_models.py         │             ├── main_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre_process             ├── extract_entities.py             ├── pp_settings.py             ├── pre_proc_cocits.py             ├── pre_proc_gorc.py             ├── pre_proc_relish.py             ├── pre_proc_scidocs.py             ├── pre_proc_treccovid.py             ├── pp_gen_nearest.py             └── pre_proc_buildreps.py   **The repository is organized broadly as:**  `src/pre_process/`: Scripts to 1) generate gather and filter co-citations data from the [S2ORC](https://github.com/allenai/s2orc) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets.",S2ORC,DATASET
1366,"Complete evaluation datasets used in the paper can be downloaded here: [`datasets/datasets.md`](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name=""repocontents""></a>      ├── bin     ├── config     │             └── models_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid_models         │             │             ├── disent_models.py         │             │             ├── pair_distances.py         │             │             └── sentsim_models.py         │             ├── main_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre_process             ├── extract_entities.py             ├── pp_settings.py             ├── pre_proc_cocits.py             ├── pre_proc_gorc.py             ├── pre_proc_relish.py             ├── pre_proc_scidocs.py             ├── pre_proc_treccovid.py             ├── pp_gen_nearest.py             └── pre_proc_buildreps.py   **The repository is organized broadly as:**  `src/pre_process/`: Scripts to 1) generate gather and filter co-citations data from the [S2ORC](https://github.com/allenai/s2orc) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets.",s2orc,DATASET
1367,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.",RELISH,DATASET
1368,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.",TRECCOVID,DATASET
1369,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.",CSFCube,DATASET
1370,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.",SciDocs,DATASET
1371,"`src/pre_process/pre_proc_gorc.py:` Code to gather full text articles from the [S2ORC](https://github.com/allenai/s2orc) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers).",S2ORC,DATASET
1372,"`src/pre_process/pre_proc_gorc.py:` Code to gather full text articles from the [S2ORC](https://github.com/allenai/s2orc) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers).",s2orc,DATASET
1373,This code assumes the 2019-09-28 release of S2ORC.,S2ORC,DATASET
1374,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",relish,DATASET
1375,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",scidocs,DATASET
1376,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",treccovid,DATASET
1377,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",RELISH,DATASET
1378,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",TRECCOVID,DATASET
1379,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",SciDocs,DATASET
1380,CSFCube data format matches the assumed format.,CSFCube,DATASET
1381,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",Semantic Scholar Open Research Corpus,DATASET
1382,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",s2orc,DATASET
1383,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",S2ORC,DATASET
1384,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",RELISH,DATASET
1385,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",TRECCOVID,DATASET
1386,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",SciDocs,DATASET
1387,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",CSFCube,DATASET
1388,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts.",BlackScholes,DATASET
1389,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts.",Heston,DATASET
1390,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts.",Ornstein-Uhlenbeck,DATASET
1391,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts.",OrnsteinUhlenbeck,DATASET
1392,For training: uncomment the code below the model params and run the file.   ## Heston dataset without Feller condition Models for the Heston dataset without Feller condition were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training for Heston without Feller # ========================================================================== ``` in the main part of the file parallel_train.py.,Heston,DATASET
1393,For training: uncomment the code below the model params and run the file.   ## Heston dataset without Feller condition Models for the Heston dataset without Feller condition were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training for Heston without Feller # ========================================================================== ``` in the main part of the file parallel_train.py.,Heston,DATASET
1394,"For performing/evaluating the cross-validation use the function-call: ``` get_training_overview(     params_extract_desc=('dataset', 'network_size', 'dropout_rate',                          'hidden_size', 'data_index'),     val_test_params_extract=((""max"", ""epoch"", ""epoch"", ""epochs_trained""),                              (""min"", ""eval_metric"",                               ""eval_metric"", ""eval_metric_min""),                              (""min"", ""test_metric"",                               ""test_metric"", ""test_metric_min""),                              (""min"", ""eval_metric"",                               ""test_metric"", ""test_metric_evaluation_min""),                              (""min"", ""eval_loss"",                               ""test_metric"", ""test_metric_eval_loss_min""),                              ) )  get_climate_cross_validation(early_stop_after_epoch=100) ```  in extras.py.   ## Training on Physionet Dataset The Physionet dataset that was used by  [Latent ODEs for Irregularly-Sampled Time Series](https://arxiv.org/abs/1907.03907) is downloaded and saved automatically when training for the first time.",Physionet,DATASET
1395,"For performing/evaluating the cross-validation use the function-call: ``` get_training_overview(     params_extract_desc=('dataset', 'network_size', 'dropout_rate',                          'hidden_size', 'data_index'),     val_test_params_extract=((""max"", ""epoch"", ""epoch"", ""epochs_trained""),                              (""min"", ""eval_metric"",                               ""eval_metric"", ""eval_metric_min""),                              (""min"", ""test_metric"",                               ""test_metric"", ""test_metric_min""),                              (""min"", ""eval_metric"",                               ""test_metric"", ""test_metric_evaluation_min""),                              (""min"", ""eval_loss"",                               ""test_metric"", ""test_metric_eval_loss_min""),                              ) )  get_climate_cross_validation(early_stop_after_epoch=100) ```  in extras.py.   ## Training on Physionet Dataset The Physionet dataset that was used by  [Latent ODEs for Irregularly-Sampled Time Series](https://arxiv.org/abs/1907.03907) is downloaded and saved automatically when training for the first time.",Physionet,DATASET
1396,Models for validation on the Physionet dataset were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training on physionet dataset # ========================================================================== ``` in the main part of the file parallel_train.py.,Physionet,DATASET
1397,Models for validation on the Physionet dataset were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training on physionet dataset # ========================================================================== ``` in the main part of the file parallel_train.py.,physionet,DATASET
1398,"Locating and Detecting Language Model Grounding with Fakepedia  This repository contains the data and code to reproduce the results of our paper: https://arxiv.org/abs/2312.02073  Please use the following citation:  ``` @misc{monea2023glitch,       title={A Glitch in the Matrix?",Fakepedia,DATASET
1399,"Locating and Detecting Language Model Grounding with Fakepedia},        author={Giovanni Monea and Maxime Peyrard and Martin Josifoski and Vishrav Chaudhary and Jason Eisner and Emre Kıcıman and Hamid Palangi and Barun Patra and Robert West},       year={2023},       eprint={2312.02073},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  > **Abstract:** Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context.",Fakepedia,DATASET
1400,"We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge.",Fakepedia,DATASET
1401,"We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries.",Fakepedia,DATASET
1402,"We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries.",Fakepedia,DATASET
1403,"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",MRQA,DATASET
1404,"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",mrqa,DATASET
1405,"To support the task, we collect PAVS10K, the first panoramic video dataset for audiovisual salient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting PAV-SOD, and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks.",PAVS10K,DATASET
1406,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our PAVS10K.,PAVS10K,DATASET
1407,"As a result, our CAV-Net outperforms all competing models and is able to estimate the aleatoric uncertainties within PAVS10K.",PAVS10K,DATASET
1408,"We hope that our work could serve as a starting point for advancing SOD towards immersive media.  ------  # PAVS10K  <p align=""center"">     <img src="".",PAVS10K,DATASET
1409,"/figures/fig_teaser.jpg""/> <br />     <em>      Figure 1: An example of our PAVS10K where coarse-to-fine annotations are provided, based on a guidance of fixations acquired from subjective experiments conducted by multiple (N) subjects wearing Head-Mounted Displays (HMDs) and headphones.",PAVS10K,DATASET
1410,"/figures/fig_related_datasets.jpg""/> <br />     <em>      Figure 2: Summary of widely used salient object detection (SOD)/video object segmentation (VOS) datasets and PAVS10K.",PAVS10K,DATASET
1411,"/figures/fig_dataset_examples.jpg""/> <br />     <em>      Figure 3: Examples of challenging attributes on equirectangular images from our PAVS10K, with instance-level ground truth and fixations as annotation guidance.",PAVS10K,DATASET
1412,"/figures/fig_dataset_statistics.jpg""/> <br />     <em>      Figure 4: Statistics of the proposed PAVS10K.",PAVS10K,DATASET
1413,"(c) Sound sources of PAVS10K scenes, such as musical instruments, human instances and animals.",PAVS10K,DATASET
1414,> Note: The PAVS10K dataset does not own the copyright of videos.,PAVS10K,DATASET
1415,"Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to PAVS10K.  ------  # Citation          @article{zhang2023pav,       title={PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={ACM Transactions on Multimedia Computing, Communications and Applications},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).",PAVS10K,DATASET
1416,"# HAGRID: A Human-LLM Collaborative Dataset for Generative Information-seeking with Attribution  <p align=""center""><img src=""https://github.com/project-miracl/hagrid/blob/main/assets/icon.png?",HAGRID,DATASET
1417,"raw=true"" alt=""HAGRID"" width=""20%""><br> </p>  <p align=""center"">     <a href=""https://www.python.org/"">         <img alt=""Build"" src=""https://img.shields.io/badge/Made%20with-Python-1f425f.svg?",HAGRID,DATASET
1418,"color=purple"">     </a>     <a href=""https://github.com/project-miracl/hagrid/blob/master/LICENSE"">         <img alt=""License"" src=""https://img.shields.io/github/license/project-miracl/hagrid"">     </a>     <a href=""https://arxiv.org/abs/2307.16883"">         <img alt=""arXiv"" src=""https://img.shields.io/badge/arXiv-2307.16883-b31b1b.svg"">     </a> </p>  HAGRID (**H**uman-in-the-loop **A**ttributable **G**enerative **R**etrieval for **I**nformation-seeking **D**ataset) is a dataset for generative information-seeking scenarios.",HAGRID,DATASET
1419,"It is constructed on top of [MIRACL 🌍 🙌 🌏 ](HTTP://miracl.ai), an information retrieval dataset that consists of queries along with a set of manually labelled relevant passages (quotes).",MIRACL,DATASET
1420,"It is constructed on top of [MIRACL 🌍 🙌 🌏 ](HTTP://miracl.ai), an information retrieval dataset that consists of queries along with a set of manually labelled relevant passages (quotes).",miracl,DATASET
1421,(#baselines-coming-soon)   - [Contact](#contact)   - [License](#license)   - [Citation](#citation)  ## Data  HAGRID is hosted on Hugging Face 🤗 : [link](https://huggingface.co/datasets/miracl/hagrid).,HAGRID,DATASET
1422,(#baselines-coming-soon)   - [Contact](#contact)   - [License](#license)   - [Citation](#citation)  ## Data  HAGRID is hosted on Hugging Face 🤗 : [link](https://huggingface.co/datasets/miracl/hagrid).,miracl/hagrid,DATASET
1423,"```python import datasets hagrid = datasets.load_dataset(""miracl/hagrid"", split=""train"") print(hagrid[0]) ```  |  Split | #Q | #A       | #Informativeness  | #Attribuatability       | |:-----|:------:|:-------:|:------:|:-------:| | Train     | 1,922 | 3,214 | 3,214 | 754 | | Dev     | 716 | 1,318 | 1,157 | 826 |   ## Baselines (Coming soon!)",miracl/hagrid,DATASET
1424,"See [LICENSE](LICENSE) for details.  ## Citation If you find this dataset and repository helpful, please cite HAGRID as follows: ``` @article{hagrid,       title={{HAGRID}: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution},        author={Ehsan Kamalloo and Aref Jafari and Xinyu Zhang and Nandan Thakur and Jimmy Lin},       year={2023},       journal={arXiv:2307.16883}, } ```",HAGRID,DATASET
1425,"See [LICENSE](LICENSE) for details.  ## Citation If you find this dataset and repository helpful, please cite HAGRID as follows: ``` @article{hagrid,       title={{HAGRID}: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution},        author={Ehsan Kamalloo and Aref Jafari and Xinyu Zhang and Nandan Thakur and Jimmy Lin},       year={2023},       journal={arXiv:2307.16883}, } ```",HAGRID,DATASET
1426,"SRD [Train](https://drive.google.com/file/d/1W8vBRJYDG9imMgr9I2XaA13tlFIEHOjS/view)|[BaiduPan](https://pan.baidu.com/s/1mj3BoRQ), [Test](https://drive.google.com/file/d/1GTi4BmQ0SJ7diDMmf-b7x2VismmXtfTo/view).",SRD,DATASET
1427,AISTD|ISTD+ [[link]](https://github.com/cvlab-stonybrook/SID)  3.,AISTD|ISTD+,DATASET
1428,ISTD [[link]](https://drive.google.com/file/d/1I0qw-65KBA6np8vIZzO6oeiOvcDBttAY/view)  4.,ISTD,DATASET
1429,USR: Unpaired Shadow Removal Dataset [[link]](https://drive.google.com/file/d/1PPAX0W4eyfn1cUrb2aBefnbrmhB1htoJ/view)  5.,USR: Unpaired Shadow Removal Dataset,DATASET
1430,LRSS: Soft Shadow Dataset [[link]](http://visual.cs.ucl.ac.uk/pubs/softshadows/)<br>    The LRSS dataset contains 134 shadow images (62 pairs of shadow and shadow-free images).,LRSS: Soft Shadow Dataset,DATASET
1431,LRSS: Soft Shadow Dataset [[link]](http://visual.cs.ucl.ac.uk/pubs/softshadows/)<br>    The LRSS dataset contains 134 shadow images (62 pairs of shadow and shadow-free images).,softshadows,DATASET
1432,LRSS: Soft Shadow Dataset [[link]](http://visual.cs.ucl.ac.uk/pubs/softshadows/)<br>    The LRSS dataset contains 134 shadow images (62 pairs of shadow and shadow-free images).,LRSS,DATASET
1433,"<br>    For shadow-free training images, 28 from LRSS and 72 randomly selected from the USR dataset.",LRSS,DATASET
1434,"<br>    For shadow-free training images, 28 from LRSS and 72 randomly selected from the USR dataset.",USR,DATASET
1435,pwd=28bv) | | AISTD/ISTD+ | [[Dropbox]](https://www.dropbox.com/scl/fi/k3suqb1ikis4mm6ok6ky4/AISTD_params_0500000.pt?,ISTD/ISTD+,DATASET
1436,pwd=3waf) | | ISTD | [[Dropbox]](https://www.dropbox.com/scl/fi/jgdcftwxpvnwxegawbrqx/ISTD_params_0600000.pt?,ISTD,DATASET
1437,pwd=hh4n) | | USR  | [[Dropbox]](https://www.dropbox.com/scl/fi/kcj4m88ha3razxqx1c7m6/USR_params_0600000.pt?,USR,DATASET
1438,pwd=e0a8)  |`results/USR/model/`| [[Dropbox]](https://www.dropbox.com/s/ybmwxtmo7cdljyz/DC-ShadowNet_USR.zip?,USR,DATASET
1439,pwd=e0a8)  |`results/USR/model/`| [[Dropbox]](https://www.dropbox.com/s/ybmwxtmo7cdljyz/DC-ShadowNet_USR.zip?,USR,DATASET
1440,pwd=u7ec) | | LRSS  | - | - | -| [[Dropbox]](https://www.dropbox.com/s/wi6g12gr1z0xsqi/DC-ShadowNet_Soft.zip?,LRSS,DATASET
1441,Download the pre-trained SRD model [[Dropbox]](https://www.dropbox.com/scl/fi/icj273vu98w1l9zzwjxt7/SRD_params_0500000.pt?,SRD,DATASET
1442,"pwd=zhd2), put in `results/SRD/model/` 2.",SRD,DATASET
1443,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1444,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",/dataset/SRD/testA/,DATASET
1445,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1446,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1447,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",AISTD,DATASET
1448,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",USR,DATASET
1449,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1450,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",AISTD,DATASET
1451,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",AISTD,DATASET
1452,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",ISTD,DATASET
1453,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",ISTD,DATASET
1454,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",USR,DATASET
1455,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",USR,DATASET
1456,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1457,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1458,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",AISTD,DATASET
1459,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",AISTD,DATASET
1460,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",ISTD,DATASET
1461,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",ISTD,DATASET
1462,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",USR,DATASET
1463,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",USR,DATASET
1464,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",DC-ShadowNet-Hard-and-Soft-Shadow-Remova,DATASET
1465,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1466,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1467,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1468,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1469,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1470,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1471,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",SRD,DATASET
1472,pwd=srdc)| SRD testC [[Dropbox]](https://www.dropbox.com/scl/fo/2ordu4qggheead9osdfo5/AA-Tqfo2hmTUJhVC-qKS9B0?,SRD,DATASET
1473,pwd=srdc) | ISTD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/ym4tt6999nqik4aovnmjd/AIPo7LNX9aUmR2X0NQo8_sI?,ISTD,DATASET
1474,rlkey=fyby4kexn1g009xhb5bkph32e&dl=0) [[BaiduPan(code:istd)]](https://pan.baidu.com/s/1aSPXH6DVpGGc_RfDP7mADQ?,istd,DATASET
1475,pwd=istd)| ISTD testC [[Dropbox]](https://www.dropbox.com/scl/fo/cqlu31fsixc48kelb96vs/ANQ6GrWGvGBclT3ThY6ltGA?,ISTD,DATASET
1476,rlkey=30yg1v0orktptc92tvsettdrv&st=5xpq3fji&dl=0)[[BaiduPan(code:istd)]](https://pan.baidu.com/s/1TmrPgDHwEaP95cyEppfCjQ?,istd,DATASET
1477,pwd=istd) | USR trainC [[Dropbox]](https://www.dropbox.com/scl/fo/61437etlg9m4i8b0rhwgx/AMEeRUwkzCMNUAWWEMVxr90?,USR,DATASET
1478,pwd=usrc)| USR testC [[Dropbox]](https://www.dropbox.com/scl/fo/8mk01goqz1mllmzwnbgk8/AEi2_Igy7ZM93-IgnU_mmXU?,USR,DATASET
1479,pwd=usrc)  | LRSS trainC [[Dropbox]](https://www.dropbox.com/scl/fo/s3qcyjp754qsnu90853wq/AHZan-_POf1KAQ6UpbyT6b8?,LRSS,DATASET
1480,rlkey=0ghgo6m4fegm6gml0p611f28e&dl=0) [[BaiduPan(code:lrss)]](https://pan.baidu.com/s/1hjIheKDy3vGChoF0WlW-LQ?,lrss,DATASET
1481,pwd=lrss)| LRSS testC [[Dropbox]](https://www.dropbox.com/scl/fo/laaymorcfz70ja93y6hwm/ALeXqQp4_f7BKH5PESinWYc?,LRSS,DATASET
1482,rlkey=kpqb5dxeneiroskj9wpfh6rob&st=v1mvl266&dl=0) [[BaiduPan(code:lrss)]](https://pan.baidu.com/s/1SRikZuabEgu43vqqlCIh9A?,lrss,DATASET
1483,"/_Shadow-Free_Chromaticity_python): [inputs](./0_Shadow-Free_Chromaticity_python/input/) and [results](./0_Shadow-Free_Chromaticity_python/sfchroma/) ``` cd 0_Shadow-Free_Chromaticity_python python physics_all.py ```  <p align=""left"">   <img width=450"" src=""teaser/chromaticity.png""> </p>   ## Train with Shadow-Robust Feature Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_pecp_loss True ```  Get the following Figure 5 in the main paper, VGG feature visualization [results](.",SRD,DATASET
1484,"/_Shadow-Free_Chromaticity_python): [inputs](./0_Shadow-Free_Chromaticity_python/input/) and [results](./0_Shadow-Free_Chromaticity_python/sfchroma/) ``` cd 0_Shadow-Free_Chromaticity_python python physics_all.py ```  <p align=""left"">   <img width=450"" src=""teaser/chromaticity.png""> </p>   ## Train with Shadow-Robust Feature Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_pecp_loss True ```  Get the following Figure 5 in the main paper, VGG feature visualization [results](.",SRD,DATASET
1485,SRD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/de9223d55519777e1621b84d5f0067e3989c11dd/evaluation/demo_srd_release.m#L22) and the dataset in `evaluation/demo_srd_release.m` and then run it. ``` demo_srd_release.m ``` Get the following Table 1 in the main paper on the SRD (size: 256x256):  | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **4.66** | 7.70 | 3.39 | | Input Image | N/A | 13.77 | 37.40 | 3.96 |  For SRD (size: 640x840): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **6.57** | **9.84** | **5.52** |  ### 2.,SRD,DATASET
1486,SRD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/de9223d55519777e1621b84d5f0067e3989c11dd/evaluation/demo_srd_release.m#L22) and the dataset in `evaluation/demo_srd_release.m` and then run it. ``` demo_srd_release.m ``` Get the following Table 1 in the main paper on the SRD (size: 256x256):  | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **4.66** | 7.70 | 3.39 | | Input Image | N/A | 13.77 | 37.40 | 3.96 |  For SRD (size: 640x840): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **6.57** | **9.84** | **5.52** |  ### 2.,SRD,DATASET
1487,SRD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/de9223d55519777e1621b84d5f0067e3989c11dd/evaluation/demo_srd_release.m#L22) and the dataset in `evaluation/demo_srd_release.m` and then run it. ``` demo_srd_release.m ``` Get the following Table 1 in the main paper on the SRD (size: 256x256):  | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **4.66** | 7.70 | 3.39 | | Input Image | N/A | 13.77 | 37.40 | 3.96 |  For SRD (size: 640x840): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **6.57** | **9.84** | **5.52** |  ### 2.,SRD,DATASET
1488,AISTD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/eefcc9ee23842847a40f40610c129ecba82f5d21/evaluation/demo_aistd_release.m#L21) and the dataset in `evaluation/demo_aistd_release.m` and then run it. ``` demo_aistd_release.m ```  Get the following Table 2 in the main paper on the AISTD (size: 256x256): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **4.7** | **10.6** | 3.7 |  For AISTD (size: 480x640): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **6.33** | **11.37** | **5.38** |  ### 3.,AISTD,DATASET
1489,AISTD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/eefcc9ee23842847a40f40610c129ecba82f5d21/evaluation/demo_aistd_release.m#L21) and the dataset in `evaluation/demo_aistd_release.m` and then run it. ``` demo_aistd_release.m ```  Get the following Table 2 in the main paper on the AISTD (size: 256x256): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **4.7** | **10.6** | 3.7 |  For AISTD (size: 480x640): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **6.33** | **11.37** | **5.38** |  ### 3.,AISTD,DATASET
1490,AISTD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/eefcc9ee23842847a40f40610c129ecba82f5d21/evaluation/demo_aistd_release.m#L21) and the dataset in `evaluation/demo_aistd_release.m` and then run it. ``` demo_aistd_release.m ```  Get the following Table 2 in the main paper on the AISTD (size: 256x256): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **4.7** | **10.6** | 3.7 |  For AISTD (size: 480x640): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **6.33** | **11.37** | **5.38** |  ### 3.,AISTD,DATASET
1491,"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them.",LRSS,DATASET
1492,"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them.",lrss,DATASET
1493,"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them.",lrss,DATASET
1494,"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them.",LRSS,DATASET
1495,"It achieves state-of-the-art performance on multiple datasets such as FaceWarehouse, MICC Florence and BU-3DFE.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy.",FaceWarehouse,DATASET
1496,"It achieves state-of-the-art performance on multiple datasets such as FaceWarehouse, MICC Florence and BU-3DFE.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy.",MICC Florence,DATASET
1497,"It achieves state-of-the-art performance on multiple datasets such as FaceWarehouse, MICC Florence and BU-3DFE.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy.",BU-3DFE,DATASET
1498,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.,FaceWareHouse,DATASET
1499,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.,Florence,DATASET
1500,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.,BU3DFE,DATASET
1501,"We conduct an experiment on AFLW_2000 dataset (NME) to evaluate the performance, as shown in the table below: <p align=""center"">  <img src=""/images/alignment.png""> </p>  |Method|[0°,30°]|[30°,60°]|[60°,90°]|Overall| |:---:|:---:|:---:|:---:|:---:| |[3DDFA 16](https://arxiv.org/abs/1511.07212)</center>|3.78|4.54|7.93|5.42| |[3DDFA+SDM 16](https://arxiv.org/abs/1511.07212)|3.43|4.24|7.17|4.94| |[Bulat et al. 17](https://arxiv.org/abs/1703.00862)|**2.47**|**3.01**|**4.31**|**3.26**| |[PRN 18](https://arxiv.org/abs/1803.07835)|2.75|3.51|4.61|3.62| |Ours|2.56|3.11|4.45|3.37|   ### ● Easy and Fast Faces are represented with Basel Face Model 2009, which is easy for further manipulations (e.g expression transfer).",AFLW_2000,DATASET
1502,"(https://github.com/Juyong/3DFace) You can find a link named ""CoarseData"" in the first row of Introduction part in their repository.",CoarseData,DATASET
1503,Download and unzip the Coarse_Dataset.zip.,Coarse,DATASET
1504,The expression basis are constructed using [Facewarehouse](http://kunzhou.net/zjugaps/facewarehouse/) data and transferred to BFM topology.  3.,Facewarehouse,DATASET
1505,Training process has been tested with this model to ensure similar results. - [Resnet50-v1](https://github.com/tensorflow/models/blob/master/research/slim/README.md) pre-trained on ImageNet from Tensorflow Slim.,ImageNet,DATASET
1506,"If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={IEEE Computer Vision and Pattern Recognition Workshops},      year={2019}  } ## The face images on this page are from the public [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset released by MMLab, CUHK.",CelebA,DATASET
1507,"If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={IEEE Computer Vision and Pattern Recognition Workshops},      year={2019}  } ## The face images on this page are from the public [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset released by MMLab, CUHK.",CelebA,DATASET
1508,/tools/dist_test.sh configs/EvPSNet_unc_mutigpu.py ${CHECKPOINT_FILE} ${GPU_NUM} --eval panoptic ```  ## Additional Notes:    * tool/cityscapes_inference.py: saves predictions in the official cityscapes panoptic format,cityscapes,DATASET
1509,"label=🤗 %20Datasets&message=BSARD&color=FF9900)](https://huggingface.co/datasets/maastrichtlawtech/bsard)  # A Statutory Article Retrieval Dataset in French  This repository contains the code for reproducing the experimental results presented in the ACL 2022 paper [""A Statutory Article Retrieval Dataset in French""](https://aclanthology.org/2022.acl-long.468/) by [Antoine Louis](https:/antoinelouis.co/work/) and [Jerry Spanakis](https://dke.maastrichtuniversity.nl/jerry.spanakis/).",BSARD,DATASET
1510,"To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles.",Belgian Statutory Article Retrieval Dataset,DATASET
1511,"To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles.",BSARD,DATASET
1512,"Using BSARD, we benchmark several state-of-the-art retrieval approaches, including lexical and dense architectures, both in zero-shot and supervised setups.",BSARD,DATASET
1513,"By the specificity of the domain and addressed task, BSARD presents a unique challenge problem for future research on legal information retrieval.  ## Documentation  Detailed documentation on the dataset and how to reproduce the main experimental results can be found [here](docs/README.md).  ## Citation  For attribution in academic contexts, please cite this work as:  ```latex @inproceedings{louis2022statutory,   title = {A Statutory Article Retrieval Dataset in French},   author = {Louis, Antoine and Spanakis, Gerasimos},   booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},   month = may,   year = {2022},   address = {Dublin, Ireland},   publisher = {Association for Computational Linguistics},   url = {https://aclanthology.org/2022.acl-long.468},   pages = {6789--6803}, } ```  ## License  This repository is MIT-licensed.",BSARD,DATASET
1514,)            | 100 languages |   ASSIN2        |   0.8680     |   0.8680    | | PTT5 (Carmo et al,ASSIN2,DATASET
1515,)             | EN & PT       |   ASSIN2        |   0.8850     |   0.8860    | | BERTimbau Large (Souza et al,ASSIN2,DATASET
1516,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?,ASSIN2,DATASET
1517,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?,MNLI,DATASET
1518,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?,ASSIN2,DATASET
1519,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?,SQuAD,DATASET
1520,usp=sharing) (Q&A) - [FaQuAD](https://colab.research.google.com/drive/1HdPjzn61genPyZfiDG5fqAwPNI4vhegw?,FaQuAD,DATASET
1521,usp=sharing) (Q&A) - [MNLI](https://colab.research.google.com/drive/1Y9ZaJuN-SVo0fmwypPzcJCOetFw-A2tx?,MNLI,DATASET
1522,usp=sharing) (NLI) - [ASSIN2](https://colab.research.google.com/drive/1S5zwaw8KWee8y6Vyq-XHC8Am3GmGFpv9?,ASSIN2,DATASET
1523,usp=sharing) (NLI)  The datasets SQuAD and MNLI are directly downloaded from the notebooks of this repository.,SQuAD,DATASET
1524,usp=sharing) (NLI)  The datasets SQuAD and MNLI are directly downloaded from the notebooks of this repository.,MNLI,DATASET
1525,We also provide the FaQuAD and ASSIN2 datasets,FaQuAD,DATASET
1526,We also provide the FaQuAD and ASSIN2 datasets,ASSIN2,DATASET
1527,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?",SQuAD,DATASET
1528,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?",FaQuAD,DATASET
1529,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?",MNLI,DATASET
1530,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?",ASSIN2,DATASET
1531,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",Cora,DATASET
1532,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",CORA,DATASET
1533,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",Citeseer,DATASET
1534,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",Pubmed,DATASET
1535,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",DBLP,DATASET
1536,Take Cora dataset as an example:  - make a folder called **dataset** at root directory,Cora,DATASET
1537,- make a folder called **cora** in **dataset** directory,cora,DATASET
1538,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",Cora,DATASET
1539,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",cora,DATASET
1540,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",cora,DATASET
1541,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",cora,DATASET
1542,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",cora,DATASET
1543,Framework for Robustness Validation of Machine Comprehension Systems  This repository contains dataset for robustness validation of machine comprehension systems trained on SQuAD dataset.,SQuAD,DATASET
1544,"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",Visual Genome,DATASET
1545,/experiments/scripts/fetch_images.sh ``` This will populate the `DrillDown/data` folder with `vg/VG_100K` and `vg/VG_100K_2`,VG_100K,DATASET
1546,/experiments/scripts/fetch_images.sh ``` This will populate the `DrillDown/data` folder with `vg/VG_100K` and `vg/VG_100K_2`,VG_100K_2,DATASET
1547,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts.",BlackScholes,DATASET
1548,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts.",Heston,DATASET
1549,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts.",Ornstein-Uhlenbeck,DATASET
1550,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts.",OrnsteinUhlenbeck,DATASET
1551,For training: uncomment the code below the model params and run the file.   ## Heston dataset without Feller condition Models for the Heston dataset without Feller condition were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training for Heston without Feller # ========================================================================== ``` in the main part of the file parallel_train.py.,Heston,DATASET
1552,For training: uncomment the code below the model params and run the file.   ## Heston dataset without Feller condition Models for the Heston dataset without Feller condition were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training for Heston without Feller # ========================================================================== ``` in the main part of the file parallel_train.py.,Heston,DATASET
1553,"For performing/evaluating the cross-validation use the function-call: ``` get_training_overview(     params_extract_desc=('dataset', 'network_size', 'dropout_rate',                          'hidden_size', 'data_index'),     val_test_params_extract=((""max"", ""epoch"", ""epoch"", ""epochs_trained""),                              (""min"", ""eval_metric"",                               ""eval_metric"", ""eval_metric_min""),                              (""min"", ""test_metric"",                               ""test_metric"", ""test_metric_min""),                              (""min"", ""eval_metric"",                               ""test_metric"", ""test_metric_evaluation_min""),                              (""min"", ""eval_loss"",                               ""test_metric"", ""test_metric_eval_loss_min""),                              ) )  get_climate_cross_validation(early_stop_after_epoch=100) ```  in extras.py.   ## Training on Physionet Dataset The Physionet dataset that was used by  [Latent ODEs for Irregularly-Sampled Time Series](https://arxiv.org/abs/1907.03907) is downloaded and saved automatically when training for the first time.",Physionet,DATASET
1554,"For performing/evaluating the cross-validation use the function-call: ``` get_training_overview(     params_extract_desc=('dataset', 'network_size', 'dropout_rate',                          'hidden_size', 'data_index'),     val_test_params_extract=((""max"", ""epoch"", ""epoch"", ""epochs_trained""),                              (""min"", ""eval_metric"",                               ""eval_metric"", ""eval_metric_min""),                              (""min"", ""test_metric"",                               ""test_metric"", ""test_metric_min""),                              (""min"", ""eval_metric"",                               ""test_metric"", ""test_metric_evaluation_min""),                              (""min"", ""eval_loss"",                               ""test_metric"", ""test_metric_eval_loss_min""),                              ) )  get_climate_cross_validation(early_stop_after_epoch=100) ```  in extras.py.   ## Training on Physionet Dataset The Physionet dataset that was used by  [Latent ODEs for Irregularly-Sampled Time Series](https://arxiv.org/abs/1907.03907) is downloaded and saved automatically when training for the first time.",Physionet,DATASET
1555,Models for validation on the Physionet dataset were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training on physionet dataset # ========================================================================== ``` in the main part of the file parallel_train.py.,Physionet,DATASET
1556,Models for validation on the Physionet dataset were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training on physionet dataset # ========================================================================== ``` in the main part of the file parallel_train.py.,physionet,DATASET
1557,The source code is currently set up for the configuration of three clients performing secure and differentially private federated learning using logistic regresion on the MNIST dataset.,MNIST,DATASET
1558,"In our example, it loads the MNIST dataset and processes it for the client agent instances.",MNIST,DATASET
1559,"Currently surfaced using [WordNet](https://wordnet.princeton.edu/) synsets. ``` msb.get_similar_concepts(""your concept term"") ```  ### Compare concepts to ground truth labels This function displays the Pearson correlation coefficients between each of the selected concepts and the ground truth label column.",WordNet,DATASET
1560,"Currently surfaced using [WordNet](https://wordnet.princeton.edu/) synsets. ``` msb.get_similar_concepts(""your concept term"") ```  ### Compare concepts to ground truth labels This function displays the Pearson correlation coefficients between each of the selected concepts and the ground truth label column.",wordnet,DATASET
1561,This method holds the **1st place** entry on the [KITTI depth completion benchmark](http://www.cvlibs.net/datasets/kitti/eval_depth.php?,KITTI,DATASET
1562,This method holds the **1st place** entry on the [KITTI depth completion benchmark](http://www.cvlibs.net/datasets/kitti/eval_depth.php?,kitti,DATASET
1563,This is the late fusion technique used in our framework. * This method ranks first on the KITTI depth completion benchmark without using additional data or postprocessing.,KITTI,DATASET
1564,The predictions of our model for the KITTI test set can be downloaded [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x).  !,KITTI,DATASET
1565,(Works with Pytorch 1.1)   ## Dataset The [Kitti dataset](www.cvlibs.net/datasets/kitti/) has been used.,Kitti,DATASET
1566,(Works with Pytorch 1.1)   ## Dataset The [Kitti dataset](www.cvlibs.net/datasets/kitti/) has been used.,kitti,DATASET
1567,"Secondly, you'll need to unzip and download the camera images from kitti.",kitti,DATASET
1568,"|--validation ```   ## Run Code To run the code:  `python main.py --data_path /path/to/data/ --lr_policy plateau`  Flags: - Set flag ""input_type"" to rgb or depth. - Set flag ""pretrained"" to true or false to use a model pretrained on Cityscapes for the global branch. - See `python main.py --help` for more information.",Cityscapes,DATASET
1569,You can find the model pretrained on Cityscapes [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,Cityscapes,DATASET
1570,You can find a fully trained model and its corresponding predictions for the KITTI test set [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,KITTI,DATASET
1571,"and execute the following command:  `source Test/test.sh /path/to/directory_with_saved_model/ $num_samples /path/to/dataset/ /path/to/directory_with_ground_truth_for_selected_validation_files/`  (You might have to recompile the C files for testing, provided by KITTI, if your architecture is different from mine)  ## Results  Comparision with state-of-the-art:  !",KITTI,DATASET
1572,"Generated with `conda env export > environment.yml`     │     ├── setup.py           <- Makes project pip installable (pip install -e .) so src can be imported     ├── src                <- Python modules with classes and functions that will be orchestrated in the notebooks.         ├── data           <- Python modules - Generators, preprocess and postprocess functions         ├── models         <- Python modules - TF.keras 2.X Model and Layer definition         ├── utils          <- Python modules - Metrics, losses, prediction, evaluation code, TF-callbacks and io-utils         └── visualization  <- Python modules - Plot functions for the evaluation and data description  ## Dataset For this work a multi-centric heterogeneous cine-SSFPs CMR TOF data set from the [German Competence Network for Congenital Heart Defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/)) was used (study identifier: NCT00266188, title: Non-invasive Imaging and Exercise Tolerance Tests in Post-repair Tetralogy of Fallot -Intervention and Course in Patients Over 8 Years Old).",multi-centric heterogeneous cine-SSFPs CMR TOF,DATASET
1573,This TOF dataset constitutes one of the largest compiled data set of this pathology to date.,TOF,DATASET
1574,# NELA-GT-2019  This repository contain examples of how to use the NELA-GT-2019 data set with Python 3.,NELA-GT-2019,DATASET
1575,# NELA-GT-2019  This repository contain examples of how to use the NELA-GT-2019 data set with Python 3.,NELA-GT-2019,DATASET
1576,"persistentId=doi:10.7910/DVN/O7FWPO  __For more details about this dataset, check the paper__: https://arxiv.org/abs/2003.08444  If you use this dataset in your work, please cite us as follows: <br> ``` @misc{     gruppi2020nelagt2019,     title={NELA-GT-2019: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles},     author={Maurício Gruppi and Benjamin D.",NELA-GT-2019,DATASET
1577,"Horne and Sibel Adalı},     year={2020},     eprint={2003.08444},     archivePrefix={arXiv},     primaryClass={cs.CY} } ``` ## Data  Metadata|| ---|--- Dataset name|`NELA-GT-2019` Formats|`Sqlite3`,`JSON` No. of articles|`1118821` No. of sources|`261` Collection period|`2019-01-01` to `2019-12-31`  ### Fields  Each data point collected corresponds to an article and contains the fields described below.",NELA-GT-2019,DATASET
1578,+ Loading data from single or multiple sources from the database   + Loading data from the database into a Pandas dataframe  Usage: ``` python3 load-sqlite3.py <path-to-database> ```  ###  load-json.py  * How to load NELA in JSON format with Python 3,NELA,DATASET
1579,+ Loading a single source's JSON   + Loading a directory of NELA JSON files - **WARNING**: this consumes a lot of memory  Usage: ``` python3 load-json.py <path-to-file> ```,NELA,DATASET
1580,"## Fully Convolutional Instance-aware Semantic Segmentation  The major contributors of this repository include [Haozhi Qi](https://github.com/Oh233), [Yi Li](https://github.com/liyi14), [Guodong Zhang](https://github.com/gd-zhang), [Haochen Zhang](https://github.com/Braininvat), [Jifeng Dai](https://github.com/daijifeng001), and [Yichen Wei](https://github.com/YichenWei).  ### Introduction  **FCIS** is a fully convolutional end-to-end solution for instance segmentation, which won the first place in COCO segmentation challenge 2016.",COCO,DATASET
1581,Visual results on the first 5k images from COCO test set of our ***COCO 2016 challenge entry***: [OneDrive](https://onedrive.live.com/?,COCO,DATASET
1582,Visual results on the first 5k images from COCO test set of our ***COCO 2016 challenge entry***: [OneDrive](https://onedrive.live.com/?,COCO,DATASET
1583,Slides in [ImageNet ILSVRC and COCO workshop 2016](http://image-net.org/challenges/ilsvrc+coco2016): [OneDrive](https://onedrive.live.com/?,COCO,DATASET
1584,* We trained our model based on the ImageNet pre-trained [ResNet-v1-101](https://github.com/KaimingHe/deep-residual-networks) using a [model converter](https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef846/tools/caffe_converter).,ImageNet,DATASET
1585,"To run the demo with our trained model (on COCO trainval35k), please download the model manually from [OneDrive](https://1drv.ms/u/s!",COCO trainval35k,DATASET
1586,Please download VOC 2012 dataset with additional annotations from [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html).,VOC 2012,DATASET
1587,Please download VOC 2012 dataset with additional annotations from [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html).,SBD,DATASET
1588,Please download [COCO dataset](http://mscoco.org/dataset/#download) and annotations for the 5k image [minival](https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?,COCO,DATASET
1589,Please download [COCO dataset](http://mscoco.org/dataset/#download) and annotations for the 5k image [minival](https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?,minival,DATASET
1590,Please download [COCO dataset](http://mscoco.org/dataset/#download) and annotations for the 5k image [minival](https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?,minival,DATASET
1591,dl=0) subset and [val2014 minus minival (val35k)](https://dl.dropboxusercontent.com/s/s3tw5zcg7395368/instances_valminusminival2014.json.zip?,val2014 minus minival (val35k),DATASET
1592,Make sure it looks like this:  ```  .data/coco/  .data/coco/annotations/instances_valminusminival2014.json  .data/coco/annotations/instances_minival2014.json  ```  3.,valminusminival2014,DATASET
1593,Make sure it looks like this:  ```  .data/coco/  .data/coco/annotations/instances_valminusminival2014.json  .data/coco/annotations/instances_minival2014.json  ```  3.,minival2014,DATASET
1594,Two config files have been provided so far: FCIS@COCO with OHEM and FCIS@VOC without OHEM.,COCO,DATASET
1595,Two config files have been provided so far: FCIS@COCO with OHEM and FCIS@VOC without OHEM.,VOC,DATASET
1596,"We use 8 and 4 GPUs to train models on COCO and on VOC, respectively. 3.",COCO,DATASET
1597,"We use 8 and 4 GPUs to train models on COCO and on VOC, respectively. 3.",VOC,DATASET
1598,"For example, to train and test FCIS on COCO with ResNet-v1-101, use the following command     ```     python experiments/fcis/fcis_end2end_train_test.py --cfg experiments/fcis/cfgs/resnet_v1_101_coco_fcis_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/fcis/coco/` or `output/fcis/voc/`. 4.",COCO,DATASET
1599,- `logs/fos_senses/es-True_res-0.0/` includes senses and their npmi scores in each field  ### Discipline-specific word types   - `logs/type_npmi/fos_set-False_lemma-True/` includes lists of word types in each subfield and their npmi scores  ## Code pipeline  **Data filtering**  Information on accessing S2ORC can be found [here](https://github.com/allenai/s2orc),S2ORC,DATASET
1600,- `logs/fos_senses/es-True_res-0.0/` includes senses and their npmi scores in each field  ### Discipline-specific word types   - `logs/type_npmi/fos_set-False_lemma-True/` includes lists of word types in each subfield and their npmi scores  ## Code pipeline  **Data filtering**  Information on accessing S2ORC can be found [here](https://github.com/allenai/s2orc),s2orc,DATASET
1601,This generates `s2orc_fos.json`  **Word type pipeline**  In the `type_jargon` folder:   FOS - `word_counts_per_fos.py`: count words per field of study.,s2orc,DATASET
1602,"Run `bash prepare_sense_input.sh 2>&1 | tee temp.log` to do the next three scripts:   - `write_mask_preds/wsi_vocab.py`: determine vocabulary of words to perform WSI - `val_data_process/process_wiktionary.py`: get wiktionary definitions for vocabulary words - `write_mask_preds/wsi_preprocessing.py`: input preparation, also copy vocab file into output folder  Then, run the following script on S2ORC and Wikipedia:   - `write_mask_preds/write_mask_preds.py`: write replacements   We recommend splitting input files into numbered parts and running the script on ranges of file numbers.",S2ORC,DATASET
1603,Usage example for S2ORC:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset s2orc --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  Usage example for Wikipedia:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset wikipedia --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  In the `WSIatScale` folder run `wsi_pipeline.sh` to do WSI pipeline for the entire vocab.,S2ORC,DATASET
1604,Usage example for S2ORC:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset s2orc --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  Usage example for Wikipedia:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset wikipedia --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  In the `WSIatScale` folder run `wsi_pipeline.sh` to do WSI pipeline for the entire vocab.,s2orc,DATASET
1605,"- `create_inverted_index.py`: create inverted index  ``` python create_inverted_index.py --replacements_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --vocab_path /home/lucyl/language-map-of-science/logs/sense_vocab/wsi_vocab_set_98_50.txt --outdir /home/lucyl/language-map-of-science/logs/inverted_index --input_ids_path /home/lucyl/language-map-of-science/data/input_paper_ids/journal_analysis.txt ```  - `cluster_reps_per_token.py`: cluster the reps  Lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  After clustering for the whole dataset, use `Wiktionary Validation.ipynb` notebook to get FOS to words json.",s2orc,DATASET
1606,"- `create_inverted_index.py`: create inverted index  ``` python create_inverted_index.py --replacements_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --vocab_path /home/lucyl/language-map-of-science/logs/sense_vocab/wsi_vocab_set_98_50.txt --outdir /home/lucyl/language-map-of-science/logs/inverted_index --input_ids_path /home/lucyl/language-map-of-science/data/input_paper_ids/journal_analysis.txt ```  - `cluster_reps_per_token.py`: cluster the reps  Lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  After clustering for the whole dataset, use `Wiktionary Validation.ipynb` notebook to get FOS to words json.",s2orc,DATASET
1607,"Cluster only wiktionary words, lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.0 ```  Can check the coverage of words that appear in FOS in `Wiktionary Validation.ipynb`",s2orc,DATASET
1608,"- `assign_clusters_to_tokens.py`: assign everyone to a cluster  Lemmatized, specifying resolution:   ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_lemmed --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  Assign only wiktionary words, lemmatized, specifying resolution:  ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_eval --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.5 ```  Sense NPMI - run `get_documentID_maps()` in `get_docID_to_group.py` - `word_sense.py`, for journals, fos, and wiktionary evaluation - `Wiktionary Validation.ipynb` is the notebook that contains Wiktionary evaluation results.",s2orc,DATASET
1609,"- `assign_clusters_to_tokens.py`: assign everyone to a cluster  Lemmatized, specifying resolution:   ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_lemmed --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  Assign only wiktionary words, lemmatized, specifying resolution:  ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_eval --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.5 ```  Sense NPMI - run `get_documentID_maps()` in `get_docID_to_group.py` - `word_sense.py`, for journals, fos, and wiktionary evaluation - `Wiktionary Validation.ipynb` is the notebook that contains Wiktionary evaluation results.",s2orc,DATASET
1610,"- `get_discipline_specific.py`: get discipline specific journals and their papers, for the audience design experiment - `jargonyness_per_paper.py`: calculate amount of jargon per abstract  Example usage:  ``` python jargonyness_per_paper.py --cutoff 0.1 --exp_name general_specific  python jargonyness_per_paper.py --cutoff 0.1 --exp_name regression_sample ```  - `expected_max_npmi.py`: expected max NPMI over token positions in abstract, for audience design experiment - `Paper Jargon Rate.ipynb`: audience design plots - `get_paper_time_and_place.py`: get FOS and year of potential papers that may cite the papers in regression study - `General Dataset Statistics`: get data used for regression  - `regression_variables.py`: get some of the simpler regression variables - `citations_per_journal.py`: for calculating the average number of citations per journal, a regression variable - `Paper Success Regression.ipynb`: notebook that runs regressions - `get_fos_citation_matrix.py`: for calculating similarity among disciplines, part of interdisciplinarity regression  **Citation graph**  Future work may want to run analysis on the S2ORC citation graph.",S2ORC,DATASET
1611,"The below script supports the conversion of S2ORC data to a `graph-tool` network, where nodes are papers labeled with paper ID",S2ORC,DATASET
1612,Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<dataset>_<split_by>.txt` - selected GloVe feature: `std_glove_<dataset>_<split_by>.npy` - referring expression database: `std_refdb_<dataset>_<split_by>.json` - critical objects database: `std_ctxdb_<dataset>_<split_by>.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset refcoco --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset refcoco --split-by unc ```  We use tensorboard to monitor the training process.,refcoco,DATASET
1613,Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<dataset>_<split_by>.txt` - selected GloVe feature: `std_glove_<dataset>_<split_by>.npy` - referring expression database: `std_refdb_<dataset>_<split_by>.json` - critical objects database: `std_ctxdb_<dataset>_<split_by>.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset refcoco --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset refcoco --split-by unc ```  We use tensorboard to monitor the training process.,refcoco,DATASET
1614,The log file can be found in `tb` folder.  ## Evaluate Recall **Save Ref-NMS proposals:** ``` PYTHONPATH=$PWD python tools/save_ref_nms_proposals.py --dataset refcoco --split-by unc --tid <tid> --m <loss_type> ``` `<loss_type>` can be either `att_vanilla` for binary XE loss or `att_rank` for rank loss.,refcoco,DATASET
1615,**Evaluate recall on referent object:** ``` PYTHONPATH=$PWD python tools/eval_proposal_hit_rate.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ``` `conf` parameter is the score threshold used to filter Ref-NMS proposals.,refcoco,DATASET
1616,**Evaluate recall on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset refcoco --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.,refcoco,DATASET
1617,**Evaluate recall on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset refcoco --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.,refcoco,DATASET
1618,"Then, run the following commands to evaluate on REC and RES task: ``` # Evaluate REC performance python tools/extract_mrcn_ref_feats.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate RES performance python tools/run_propose_to_mask.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **MAttNet-style detection file** (note the MattNet-style detection files can be directly used to evaluate downstream REG task performance).",refcoco,DATASET
1619,"Then, run the following commands to evaluate on REC and RES task: ``` # Evaluate REC performance python tools/extract_mrcn_ref_feats.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate RES performance python tools/run_propose_to_mask.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **MAttNet-style detection file** (note the MattNet-style detection files can be directly used to evaluate downstream REG task performance).",refcoco,DATASET
1620,"Then, run the following commands to evaluate on REC and RES task: ``` # Evaluate REC performance python tools/extract_mrcn_ref_feats.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate RES performance python tools/run_propose_to_mask.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **MAttNet-style detection file** (note the MattNet-style detection files can be directly used to evaluate downstream REG task performance).",refcoco,DATASET
1621,"The processed datasets will be released soon, because of the large size of the datasets (graph with esm embeddings), we will upload the pipeline to process the raw data.  ### Raw Datasets  #### SC-PDB  <https://github.com/jivankandel/PUResNet/blob/main/scpdb_subset.zip>  #### COACH420/HOLO4K  <https://github.com/rdk/p2rank-datasets>  These two datasets do not include ligands in a pre-extracted format.",SC-PDB,DATASET
1622,"The processed datasets will be released soon, because of the large size of the datasets (graph with esm embeddings), we will upload the pipeline to process the raw data.  ### Raw Datasets  #### SC-PDB  <https://github.com/jivankandel/PUResNet/blob/main/scpdb_subset.zip>  #### COACH420/HOLO4K  <https://github.com/rdk/p2rank-datasets>  These two datasets do not include ligands in a pre-extracted format.",scpdb,DATASET
1623,"The processed datasets will be released soon, because of the large size of the datasets (graph with esm embeddings), we will upload the pipeline to process the raw data.  ### Raw Datasets  #### SC-PDB  <https://github.com/jivankandel/PUResNet/blob/main/scpdb_subset.zip>  #### COACH420/HOLO4K  <https://github.com/rdk/p2rank-datasets>  These two datasets do not include ligands in a pre-extracted format.",COACH420,DATASET
1624,"The processed datasets will be released soon, because of the large size of the datasets (graph with esm embeddings), we will upload the pipeline to process the raw data.  ### Raw Datasets  #### SC-PDB  <https://github.com/jivankandel/PUResNet/blob/main/scpdb_subset.zip>  #### COACH420/HOLO4K  <https://github.com/rdk/p2rank-datasets>  These two datasets do not include ligands in a pre-extracted format.",HOLO4K,DATASET
1625,"Instead, the MOAD database contains the ligand information, according to the criteria of *relevant ligand* as specified in the MOAD database.",MOAD,DATASET
1626,"Instead, the MOAD database contains the ligand information, according to the criteria of *relevant ligand* as specified in the MOAD database.",MOAD,DATASET
1627,"Ligand Information: <http://www.bindingmoad.org/files/csv/every_bind.csv>  #### PDBBind  <http://www.pdbbind.org.cn/download/PDBbind_v2020_other_PL.tar.gz>  ## Replicate results  If you want to replicated the results from the paper, put the downloaded datasets in the specific folders",PDBBind,DATASET
1628,"Ligand Information: <http://www.bindingmoad.org/files/csv/every_bind.csv>  #### PDBBind  <http://www.pdbbind.org.cn/download/PDBbind_v2020_other_PL.tar.gz>  ## Replicate results  If you want to replicated the results from the paper, put the downloaded datasets in the specific folders",PDBbind_v2020,DATASET
1629,"- `bindingsite_test_data/coach420/raw`  - `bindingsite_test_data/holo4k/raw`  - `bindingsite_test_data/holo4k_split_chain/raw`    This dataset is the same as the holo4k dataset, but the chains are split into separate files.",coach420,DATASET
1630,"- `bindingsite_test_data/coach420/raw`  - `bindingsite_test_data/holo4k/raw`  - `bindingsite_test_data/holo4k_split_chain/raw`    This dataset is the same as the holo4k dataset, but the chains are split into separate files.",holo4k,DATASET
1631,"- `bindingsite_test_data/coach420/raw`  - `bindingsite_test_data/holo4k/raw`  - `bindingsite_test_data/holo4k_split_chain/raw`    This dataset is the same as the holo4k dataset, but the chains are split into separate files.",holo4k,DATASET
1632,"- `bindingsite_test_data/coach420/raw`  - `bindingsite_test_data/holo4k/raw`  - `bindingsite_test_data/holo4k_split_chain/raw`    This dataset is the same as the holo4k dataset, but the chains are split into separate files.",holo4k,DATASET
1633,- `bindingsite_test_data/pdbbind2020/raw`  The holo4k and the coach420 dataset were created by extracting the relevant ligans from the MOAD database.,pdbbind2020,DATASET
1634,- `bindingsite_test_data/pdbbind2020/raw`  The holo4k and the coach420 dataset were created by extracting the relevant ligans from the MOAD database.,holo4k,DATASET
1635,- `bindingsite_test_data/pdbbind2020/raw`  The holo4k and the coach420 dataset were created by extracting the relevant ligans from the MOAD database.,coach420,DATASET
1636,- `bindingsite_test_data/pdbbind2020/raw`  The holo4k and the coach420 dataset were created by extracting the relevant ligans from the MOAD database.,MOAD,DATASET
1637,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,scpdb,DATASET
1638,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,scpdb,DATASET
1639,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,scpdb,DATASET
1640,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,scpdb,DATASET
1641,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,coach420,DATASET
1642,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,coach420,DATASET
1643,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,holo4k,DATASET
1644,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,holo4k,DATASET
1645,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,holo4k,DATASET
1646,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,holo4k,DATASET
1647,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,pdbbind2020,DATASET
1648,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,pdbbind2020,DATASET
1649,"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",coach420,DATASET
1650,"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",coach420,DATASET
1651,"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",pdbbind2020,DATASET
1652,"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",holo4k,DATASET
1653,"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",holo4k,DATASET
1654,Download KILT wikipedia knowledge base [here](https://github.com/facebookresearch/KILT) and put it under a kb directory like /raw_kb/  \ 2.,KILT,DATASET
1655,Download AIDA CoNLL datasets [here](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) and place them under a raw aida directory like /raw_aida/ \ 4.,AIDA CoNLL,DATASET
1656,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",AIDA,DATASET
1657,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",KILT,DATASET
1658,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",AIDA,DATASET
1659,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",KILT,DATASET
1660,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",AIDA testb,DATASET
1661,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",MSNBC,DATASET
1662,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",Der,DATASET
1663,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",K50,DATASET
1664,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",R128,DATASET
1665,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",R500,DATASET
1666,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",OKE15,DATASET
1667,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",OKE16,DATASET
1668,Download Yelp data: https://www.yelp.com/dataset and place files in ```datasets/yelp_dataset/``` 2.,yelp_dataset,DATASET
1669,Download subword tokenizer built on Yelp and place in  ```datasets/yelp_dataset/processed/```:  [link](https://s3.us-east-2.amazonaws.com/unsup-sum/subwordenc_32000_maxrevs260_fixed.pkl)  ### Pre-trained models  1.,yelp_dataset,DATASET
1670,"# BOOKSUM: A Collection of Datasets for Long-form Narrative Summarization Authors: [Wojciech Kryściński](https://twitter.com/iam_wkr), [Nazneen Rajani](https://twitter.com/nazneenrajani), [Divyansh Agarwal](https://twitter.com/jigsaw2212), [Caiming Xiong](https://twitter.com/caimingxiong), [Dragomir Radev](http://www.cs.yale.edu/homes/radev/)  ## Introduction The majority of available text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases.",BOOKSUM,DATASET
1671,"We address these issues by introducing BookSum, a collection of datasets for long-form narrative summarization.",BookSum,DATASET
1672,"[Get Involved](#get-involved)  ## Updates #### 4/15/2021 Initial commit   ## Citation ``` @article{kryscinski2021booksum,       title={BookSum: A Collection of Datasets for Long-form Narrative Summarization},        author={Wojciech Kry{\'s}ci{\'n}ski and Nazneen Rajani and Divyansh Agarwal and Caiming Xiong and Dragomir Radev},       year={2021},       eprint={2105.08209},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  ## Legal Note By downloading or using the resources, including any code or scripts, shared in this code repository, you hereby agree to the following terms, and your use of the resources is conditioned on and subject to these terms. 1.",BookSum,DATASET
1673,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",sop,DATASET
1674,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",cub,DATASET
1675,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",CUB200-2011,DATASET
1676,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",cars,DATASET
1677,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",CARS196,DATASET
1678,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",sop,DATASET
1679,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",Standford Online Porducts,DATASET
1680,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",inshop,DATASET
1681,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",In-Shop cloths retireval,DATASET
1682,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",vid,DATASET
1683,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",PKU Vehicle id,DATASET
1684,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,CUB200-2011,DATASET
1685,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,CUB-200,DATASET
1686,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,CARS196,DATASET
1687,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,cars,DATASET
1688,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,Stanford Online Products,DATASET
1689,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,In-shop Clothes Retrieval Benchmark,DATASET
1690,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,PKU VehicleID,DATASET
1691,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,pku-vehicleid,DATASET
1692,"`<$datadir/cars196>`, pass `$datadir` as input to `--dataset-dir`, by default.",cars196,DATASET
1693,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",CUB200-2011,DATASET
1694,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",cub-200-2011,DATASET
1695,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",Cars196,DATASET
1696,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",cars,DATASET
1697,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",cars196,DATASET
1698,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",Stanford Online Products,DATASET
1699,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...",sop,DATASET
1700,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",In-shop Clothes,DATASET
1701,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",PKU Vehicle id,DATASET
1702,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",CUB200,DATASET
1703,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",Cars196,DATASET
1704,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",In-Shop Clothes,DATASET
1705,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",Online Products,DATASET
1706,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",VID,DATASET
1707,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g.",BIOSSES,DATASET
1708,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g.",biosses,DATASET
1709,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g.",mteb/sts12-sts,DATASET
1710,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g.",mteb/sts13-sts,DATASET
1711,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g.",mteb/sts14-sts,DATASET
1712,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g.",mteb/sts15-sts,DATASET
1713,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g.",mteb/sts16-sts,DATASET
1714,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).",BIOSSES,DATASET
1715,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).",biosses,DATASET
1716,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).",mteb/sts12-sts,DATASET
1717,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).",mteb/sts13-sts,DATASET
1718,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).",mteb/sts14-sts,DATASET
1719,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).",mteb/sts15-sts,DATASET
1720,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).",mteb/sts16-sts,DATASET
1721,"# Low_Resource_KBP knowledge graph population in low resource conditions   The file ""*Few-Shot_ED.json.zip*"" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[""Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection""](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)",FewEvent,DATASET
1722,"# Low_Resource_KBP knowledge graph population in low resource conditions   The file ""*Few-Shot_ED.json.zip*"" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[""Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection""](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)",ACE-2005 corpus,DATASET
1723,"# Low_Resource_KBP knowledge graph population in low resource conditions   The file ""*Few-Shot_ED.json.zip*"" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[""Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection""](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)",TAC-KBP-2017 Event Track Data,DATASET
1724,"- `2023.09.04`  Our NuScenes-QA dataset v1.0 released.  ## :hourglass_flowing_sand: To Do  - [x] Release question & anwswer data - [x] Release visual feature - [x] Release training and testing code  ## :running: Getting Started  ### Data Preparation  We have released our question-answer annotations, please download it from [HERE](https://drive.google.com/drive/folders/1jIkICT23wZWZYPrWCa0x-ubjpClSzOuU?",NuScenes-QA,DATASET
1725,"As an alternative, you can also download the origin nuScenes dataset from [HERE](https://www.nuscenes.org/download), and extract the object-level features refer to this [LINK](https://mmdetection3d.readthedocs.io/en/v0.16.0/datasets/nuscenes_det.html) with different backbones.",nuScenes,DATASET
1726,"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",NuScenes-QA,DATASET
1727,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",so-ds-feb20,DATASET
1728,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",so-ds-feb20,DATASET
1729,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",staqc-py,DATASET
1730,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",staqc-py-cleaned,DATASET
1731,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",so-ds-feb20,DATASET
1732,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",so-python-question-titles-feb20,DATASET
1733,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",so-duplicates-pacsv1-train,DATASET
1734,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",staqc-py-cleaned,DATASET
1735,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",so-duplicates-pacs-train,DATASET
1736,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",so-ds-feb20,DATASET
1737,"The staqc-py-cleaned, conala-curated, and codesearchnet collections are derived from existing datasets.",staqc-py-cleaned,DATASET
1738,"The staqc-py-cleaned, conala-curated, and codesearchnet collections are derived from existing datasets.",conala-curated,DATASET
1739,"For staqc-py and conala-curated we did some additional processing, for the codesearchnet collections we merely load the original dataset in a format that is consistent with our code.",staqc-py,DATASET
1740,"For staqc-py and conala-curated we did some additional processing, for the codesearchnet collections we merely load the original dataset in a format that is consistent with our code.",conala-curated,DATASET
1741,| name                                          | description                                                                                                                  | |-----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------| | so-ds-feb20                                   | Mined from Python Stack Overflow posts related to data science.,so-ds-feb20,DATASET
1742,"Stack Overflow dumps can be found here: https://archive.org/details/stackexchange, [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                             | | staqc-py-cleaned                     | Derived from the Python StaQC snippets (additional cleaning was done as decribed in the paper).",staqc-py-cleaned,DATASET
1743,"See https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset, [LICENSE](https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset/blob/master/LICENSE.txt)                               | | conala-curated                                | Derived from the curated snippets of the CoNaLa benchmark.",StackOverflow,DATASET
1744,"See https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset, [LICENSE](https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset/blob/master/LICENSE.txt)                               | | conala-curated                                | Derived from the curated snippets of the CoNaLa benchmark.",conala-curated,DATASET
1745,"See https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset, [LICENSE](https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset/blob/master/LICENSE.txt)                               | | conala-curated                                | Derived from the curated snippets of the CoNaLa benchmark.",CoNaLa,DATASET
1746,"See https://conala-corpus.github.io/ , [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                                                         | | codesearchnet-{language}-{train\|valid\|test} | The CodeSearchNet snippet collections used for training/MRR validation/MRR testing.",conala,DATASET
1747,"See https://conala-corpus.github.io/ , [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                                                         | | codesearchnet-{language}-{train\|valid\|test} | The CodeSearchNet snippet collections used for training/MRR validation/MRR testing.",codesearchnet,DATASET
1748,"See https://conala-corpus.github.io/ , [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                                                         | | codesearchnet-{language}-{train\|valid\|test} | The CodeSearchNet snippet collections used for training/MRR validation/MRR testing.",CodeSearchNet,DATASET
1749,See https://github.com/github/CodeSearchNet.,CodeSearchNet,DATASET
1750,| | codesearchnet-{language}                      | The CodeSearchNet snippet collections used for the weights and biases benchmark.,codesearchnet,DATASET
1751,| | codesearchnet-{language}                      | The CodeSearchNet snippet collections used for the weights and biases benchmark.,CodeSearchNet,DATASET
1752,See https://github.com/github/CodeSearchNet.,CodeSearchNet,DATASET
1753,"**Note**: not all of these snippets have descriptions |  ### Evaluation data Evaluation datasets link queries to relevant snippets in one of the above snippet collections.   #### Example: load an evaluation dataset ```python from codesearch.data import load_eval_dataset queries, query2ids = load_eval_dataset(""so-ds-feb20-valid"") ```  #### Available evaluation datasets | name                           | description                                                                     | |--------------------------------|---------------------------------------------------------------------------------| | so-ds-feb20-{valid\|test}      | Queries paired to relevant snippets in the so-ds-feb20 snippet collection",so-ds-feb20,DATASET
1754,"**Note**: not all of these snippets have descriptions |  ### Evaluation data Evaluation datasets link queries to relevant snippets in one of the above snippet collections.   #### Example: load an evaluation dataset ```python from codesearch.data import load_eval_dataset queries, query2ids = load_eval_dataset(""so-ds-feb20-valid"") ```  #### Available evaluation datasets | name                           | description                                                                     | |--------------------------------|---------------------------------------------------------------------------------| | so-ds-feb20-{valid\|test}      | Queries paired to relevant snippets in the so-ds-feb20 snippet collection",so-ds-feb20,DATASET
1755,| | staqc-py-cleaned-{valid\|test} | Queries paired to relevant snippets in the staqc-py-cleaned snippet collection. | | conala-curated-0.5-test        | Queries paired to relevant snippets in the CoNaLa benchmark                     |   It is also possible to load a snippet collection as evaluation data.,staqc-py-cleaned,DATASET
1756,| | staqc-py-cleaned-{valid\|test} | Queries paired to relevant snippets in the staqc-py-cleaned snippet collection. | | conala-curated-0.5-test        | Queries paired to relevant snippets in the CoNaLa benchmark                     |   It is also possible to load a snippet collection as evaluation data.,staqc-py-cleaned,DATASET
1757,| | staqc-py-cleaned-{valid\|test} | Queries paired to relevant snippets in the staqc-py-cleaned snippet collection. | | conala-curated-0.5-test        | Queries paired to relevant snippets in the CoNaLa benchmark                     |   It is also possible to load a snippet collection as evaluation data.,conala-curated-0.5-test,DATASET
1758,| | staqc-py-cleaned-{valid\|test} | Queries paired to relevant snippets in the staqc-py-cleaned snippet collection. | | conala-curated-0.5-test        | Queries paired to relevant snippets in the CoNaLa benchmark                     |   It is also possible to load a snippet collection as evaluation data.,CoNaLa,DATASET
1759,"To download and load the title pairs from Stack Overflow duplicate posts run:  ```python from codesearch.data import load_train_dataset duplicate_records = load_train_dataset(""so-duplicates-pacs-train"") ```  These duplicate records have been filtered to ensure that there is no overlap with the `so-ds-feb20` and `staqc-py` evaluation datasets.",so-ds-feb20,DATASET
1760,"To download and load the title pairs from Stack Overflow duplicate posts run:  ```python from codesearch.data import load_train_dataset duplicate_records = load_train_dataset(""so-duplicates-pacs-train"") ```  These duplicate records have been filtered to ensure that there is no overlap with the `so-ds-feb20` and `staqc-py` evaluation datasets.",staqc-py,DATASET
1761,"To download a text file with Stack Overflow post titles tagged with Python (used for the TNBOW baseline) run:   ```python from codesearch.data import load_train_dataset filename = load_train_dataset(""so-python-question-titles-feb20"") ```  ## Demo notebook   You can run the demo notebook `nbs/demo/demo.ipynb` to quickly try out any of the pretrained models on one of the snippet collections.  ## Benchmark on PACS  To replicate the results of our paper or evaluate your own model on the PACS benchmark, have a look at `nbs/evaluate.ipynb` and `codesearch/benchmark.ipynb`.",PACS,DATASET
1762,"To download a text file with Stack Overflow post titles tagged with Python (used for the TNBOW baseline) run:   ```python from codesearch.data import load_train_dataset filename = load_train_dataset(""so-python-question-titles-feb20"") ```  ## Demo notebook   You can run the demo notebook `nbs/demo/demo.ipynb` to quickly try out any of the pretrained models on one of the snippet collections.  ## Benchmark on PACS  To replicate the results of our paper or evaluate your own model on the PACS benchmark, have a look at `nbs/evaluate.ipynb` and `codesearch/benchmark.ipynb`.",PACS,DATASET
1763,"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```",PACS,DATASET
1764,"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```",pacs,DATASET
1765,"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```",pacs,DATASET
1766,Run experiments This is one example of reproducing results for Movielens-1M with BPR.,Movielens-1M,DATASET
1767,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Amazon-Book,DATASET
1768,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Yelp-2018,DATASET
1769,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Gowalla,DATASET
1770,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,MovieLens-1M,DATASET
1771,"Run experiments in the shell of the Docker container following the usage table as follows.   ### Usage |Algorithm|Usage| |---|---| |DejaVu|Run for dataset A1: `python exp/run_GAT_node_classification.py -H=4 -L=8 -fe=GRU -bal=True --data_dir=data/A1`| |JSS'20|Run for dataset A1: `python exp/DejaVu/run_JSS20.py --data_dir=data/A1`| |iSQUAD|Run for dataset A1: `python exp/DejaVu/run_iSQ.py --data_dir=data/A1`| |Decision Tree|Run for dataset A1: `python exp/run_DT_node_classification.py --data_dir=data/A1`| |RandomWalk@Metric|Run for dataset A1: `python exp/DejaVu/run_random_walk_single_metric.py --data_dir=data/A1 --window_size 60 10 --score_aggregation_method=min`| |RandomWalk@FI|Run for dataset A1: `python exp/DejaVu/run_random_walk_failure_instance.py --data_dir=data/A1 --window_size 60 10 --anomaly_score_aggregation_method=min --corr_aggregation_method=max`| |Global interpretation|Run `notebooks/explain.py` as a jupyter notebook with `jupytext`| |Local interpretation|`DejaVu/explanability/similar_faults.py`|  The commands would print a `one-line summary` in the end, including the following fields: `A@1`, `A@2`, `A@3`, `A@5`, `MAR`, `Time`, `Epoch`, `Valid Epoch`, `output_dir`, `val_loss`, `val_MAR`, `val_A@1`, `command`, `git_commit_url`, which are the desrired results.",iSQUAD,DATASET
1772,"Interspeech 2021},   pages={4249--4253},   doi={10.21437/Interspeech.2021-1286} } ```  ## Setup  ### Download Google Speech Commands  There are two versions of the dataset, V1 and V2.",Google Speech Commands,DATASET
1773,"To download and extract dataset V2, run:  ```shell wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz mkdir data2 mv .",speech_commands_v0.02,DATASET
1774,/speech_commands_v0.02.tar.gz .,speech_commands_v0.02,DATASET
1775,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .,speech_commands_v0.02,DATASET
1776,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .,wget,DATASET
1777,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .,speech_commands_v0.01,DATASET
1778,/speech_commands_v0.01.tar.gz .,speech_commands_v0.01,DATASET
1779,/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .,speech_commands_v0.01,DATASET
1780,"There are three variants of the Keyword-Transformer model:  * **Time-domain attention**: each time-window is treated as a patch, self-attention is computed between time-windows * **Frequency-domain attention**: each frequency is treated as a patch self-attention is computed between frequencies * **Combination of both**: The signal is fed into both a time- and a frequency-domain transformer and the outputs are combined * **Patch-wise attention**: Similar to the vision transformer, it extracts rectangular patches from the spectrogram, so attention happens both in the time and frequency domain simultaneously.  ## Training a model from scratch To train KWT-3 from scratch on Speech Commands V2, run    ```shell sh train.sh ```  Please note that the train directory (given by the argument  `--train_dir`) cannot exist prior to start script.",Speech Commands V2,DATASET
1781,"|Model name|embedding dim|mlp-dim|heads|depth|#params|V2-12 accuracy|pre-trained| |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:| |KWT-1|64|128|1|12|607K|97.7|[here](models_data_v2_12_labels/kwt1)| |KWT-2|128|256|2|12|2.4M|98.2|[here](models_data_v2_12_labels/kwt2)| |KWT-3|192|768|3|12|5.5M|98.7|[here](models_data_v2_12_labels/kwt3)|  To perform inference on Google Speech Commands v2 with 12 labels, run  ```shell sh eval.sh ```  ## Acknowledgements  The code heavily borrows from the [KWS streaming work](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.",Google Speech Commands v2,DATASET
1782,"The naming convention we follow for the pruned models is straightforward; for instance, `ERES10_FPGM` refers to the EResFD model pruned with 10% sparsity using the FPGM technique. - `torchscript/`: All the required files for android deployment of the EResFD model (and its pruned versions) using the torchscript framework.  ## Prerequisites  Before running the pruning scripts, the user needs to prepare the necessary dataset:  ### WIDER FACE Dataset  The models are trained and evaluated using the WIDER FACE dataset.",WIDER FACE,DATASET
1783,Download the WIDER FACE dataset from [here](https://shuoyang1213.me/WIDERFACE/). 2.,WIDER FACE,DATASET
1784,Download the WIDER FACE dataset from [here](https://shuoyang1213.me/WIDERFACE/). 2.,WIDERFACE,DATASET
1785,Download the [MaSTr1325 dataset](https://box.vicos.si/borja/viamaro/index.html) and corresponding [weak annotations](https://github.com/lojzezust/SLR/releases/download/weights_v2/mastr_slr.zip).,MaSTr1325,DATASET
1786,All models are trained on the MaSTr1325 dataset using SLR and weak annotations and evaluated on the [MODS benchmark](https://github.com/bborja/mods_evaluation).,MaSTr1325,DATASET
1787,"/aspire-biencoder-compsci-spec-full'  # Load hyperparameters from disk. with codecs.open(os.path.join(model_path, 'run_info.json'), 'r') as fp:     hparams = json.load(fp)     model_hparams = hparams['all_hparams']  # Initialize the tokenizer and model. aspire_tok = AutoTokenizer.from_pretrained(model_hparams['base-pt-layer']) aspire_bienc = AspireBiEnc(model_hparams)  # Load model parameters from disk. model_fname = os.path.join(model_path, 'model_cur_best.pt') aspire_bienc.load_state_dict(torch.load(model_fname))  # Encode example input. title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific ""         ""Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching ""            ""fine-grained aspects of texts."" d = [title + aspire_tok.sep_token + abstract]  inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) clsrep = aspire_bienc.forward(inputs)  ```   #### Evaluation Datasets <a name=""evaldata""></a>  The paper uses the following evaluation datasets:  - RELISH was created in [Brown et al. 2019](https://academic.oup.com/database/article/doi/10.1093/database/baz085/5608006?",RELISH,DATASET
1788,- TRECCOVID presents an ad-hoc search dataset.,TRECCOVID,DATASET
1789,"The versions of the dataset used may be accessed here: [query topics](https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml), [relevance annotations](https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt), and the metadata for papers is obtained from the [CORD-19](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html) dataset in the [2021-06-21](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-06-21/metadata.csv) release.",CORD-19,DATASET
1790,"The function `get_qbe_pools` in `pre_proc_treccovid.py`, converts the dataset in its original form to the reformulated form, TRECCOVID-RF, used in the paper.",treccovid,DATASET
1791,"The function `get_qbe_pools` in `pre_proc_treccovid.py`, converts the dataset in its original form to the reformulated form, TRECCOVID-RF, used in the paper.",TRECCOVID,DATASET
1792,Dataset splits are created in `pre_proc_treccovid.py`,treccovid,DATASET
1793,- SciDocs is obtained from: [link](https://github.com/allenai/scidocs).,SciDocs,DATASET
1794,- SciDocs is obtained from: [link](https://github.com/allenai/scidocs).,scidocs,DATASET
1795,- CSFCube is obtained from: [link](https://github.com/iesl/CSFCube).,CSFCube,DATASET
1796,- CSFCube is obtained from: [link](https://github.com/iesl/CSFCube).,CSFCube,DATASET
1797,"Complete evaluation datasets used in the paper can be downloaded here: [`datasets/datasets.md`](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name=""repocontents""></a>      ├── bin     ├── config     │             └── models_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid_models         │             │             ├── disent_models.py         │             │             ├── pair_distances.py         │             │             └── sentsim_models.py         │             ├── main_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre_process             ├── extract_entities.py             ├── pp_settings.py             ├── pre_proc_cocits.py             ├── pre_proc_gorc.py             ├── pre_proc_relish.py             ├── pre_proc_scidocs.py             ├── pre_proc_treccovid.py             ├── pp_gen_nearest.py             └── pre_proc_buildreps.py   **The repository is organized broadly as:**  `src/pre_process/`: Scripts to 1) generate gather and filter co-citations data from the [S2ORC](https://github.com/allenai/s2orc) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets.",S2ORC,DATASET
1798,"Complete evaluation datasets used in the paper can be downloaded here: [`datasets/datasets.md`](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name=""repocontents""></a>      ├── bin     ├── config     │             └── models_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid_models         │             │             ├── disent_models.py         │             │             ├── pair_distances.py         │             │             └── sentsim_models.py         │             ├── main_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre_process             ├── extract_entities.py             ├── pp_settings.py             ├── pre_proc_cocits.py             ├── pre_proc_gorc.py             ├── pre_proc_relish.py             ├── pre_proc_scidocs.py             ├── pre_proc_treccovid.py             ├── pp_gen_nearest.py             └── pre_proc_buildreps.py   **The repository is organized broadly as:**  `src/pre_process/`: Scripts to 1) generate gather and filter co-citations data from the [S2ORC](https://github.com/allenai/s2orc) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets.",s2orc,DATASET
1799,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.",RELISH,DATASET
1800,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.",TRECCOVID,DATASET
1801,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.",CSFCube,DATASET
1802,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.",SciDocs,DATASET
1803,"`src/pre_process/pre_proc_gorc.py:` Code to gather full text articles from the [S2ORC](https://github.com/allenai/s2orc) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers).",S2ORC,DATASET
1804,"`src/pre_process/pre_proc_gorc.py:` Code to gather full text articles from the [S2ORC](https://github.com/allenai/s2orc) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers).",s2orc,DATASET
1805,This code assumes the 2019-09-28 release of S2ORC.,S2ORC,DATASET
1806,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",relish,DATASET
1807,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",scidocs,DATASET
1808,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",treccovid,DATASET
1809,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",RELISH,DATASET
1810,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",TRECCOVID,DATASET
1811,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.",SciDocs,DATASET
1812,CSFCube data format matches the assumed format.,CSFCube,DATASET
1813,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",Semantic Scholar Open Research Corpus,DATASET
1814,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",s2orc,DATASET
1815,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",S2ORC,DATASET
1816,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",RELISH,DATASET
1817,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",TRECCOVID,DATASET
1818,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",SciDocs,DATASET
1819,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.",CSFCube,DATASET
