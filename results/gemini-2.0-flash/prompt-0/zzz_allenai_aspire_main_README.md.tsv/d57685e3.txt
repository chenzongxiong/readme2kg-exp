Complete evaluation `<DATASET>datasets</DATASET>` used in the paper can be downloaded here: [`datasets/datasets.md`](https://github.com/allenai/aspire/blob/main/datasets/datasets.md) ### Repository Contents <a name="repocontents"></a> ├── bin ├── config │ └── models_config │ ├── s2orcbiomed │ ├── s2orccompsci │ └── s2orcscidocs ├── scripts └── src ├── evaluation │ ├── utils │ │ ├── datasets.py │ │ ├── metrics.py │ │ ├── models.py │ │ └── utils.py │ └── evaluate.py ├── learning │ ├── facetid_models │ │ ├── disent_models.py │ │ ├── pair_distances.py │ │ └── sentsim_models.py │ ├── main_fsim.py │ ├── batchers.py │ └── trainer.py └── pre_process ├── extract_entities.py ├── pp_settings.py ├── pre_proc_cocits.py ├── pre_proc_gorc.py ├── pre_proc_relish.py ├── pre_proc_scidocs.py ├── pre_proc_treccovid.py ├── pp_gen_nearest.py └── pre_proc_buildreps.py **The repository is organized broadly as:** `src/pre_process/`: Scripts to 1) generate gather and filter co-citations data from the `<DATASET>S2ORC</DATASET>` corpus 2) generate training examples with co-citation data 3) pre-process the evaluation `<DATASET>datasets</DATASET>` into apt formats for use with models 4) extract NER entities from `<DATASET>datasets</DATASET>`.
