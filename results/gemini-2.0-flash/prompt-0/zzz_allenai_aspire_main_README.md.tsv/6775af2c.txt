View example usage and sample document matches here: [`examples/demo-contextualsentence-multim.ipynb`](https://github.com/allenai/aspire/blob/main/examples/demo-contextualsentence-multim.ipynb)  ##### `<PROJECT>SPECTER-CoCite</PROJECT>`  The `<PROJECT>SPECTER-CoCite</PROJECT>` bi-encoder model can be used via the `transformers` library as:  ```<PROGLANG>python</PROGLANG> from transformers import AutoModel, AutoTokenizer aspire_bienc = AutoModel.from_pretrained('allenai/aspire-biencoder-compsci-spec') aspire_tok = AutoTokenizer.from_pretrained('allenai/aspire-biencoder-compsci-spec') title = "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific "         "Document Similarity" abstract = "We present a new scientific document similarity model based on matching "            "fine-grained aspects of texts." d=[title + aspire_tok.sep_token + abstract] inputs = aspire_tok(d, padding=True, truncation=True, return_tensors="pt", max_length=512) result = aspire_bienc(**inputs) clsrep = result.last_hidden_state[:, 0, :] ```  However, note that the Hugging Face models don't have a set of additional scalar-mix parameters to compute a learned weighted sum of the representations from different layers of the transformer encoder.
