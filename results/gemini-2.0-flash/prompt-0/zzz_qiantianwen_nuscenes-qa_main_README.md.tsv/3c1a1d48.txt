The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash <PROGLANG>python</PROGLANG> >= 3.5 CUDA >= 9.0 <PROGLANG>PyTorch</PROGLANG> >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash <PROGLANG>python</PROGLANG>3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash <PROGLANG>python</PROGLANG>3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [<SOFTWARE>MMDetection3D</SOFTWARE>](https://github.com/open-mmlab/mmdetection3d) and [<SOFTWARE>OpenVQA</SOFTWARE>](https://github.com/MILVLG/openvqa) for open sourcing their methods.
