Please find the example pipeline shown below. ### Models + LSTM - lstm_luong_wmt_en_de + Transformer - transformer_iwslt_de_en + Dynamic Conv. - lightconv_iwslt_de_en ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ``` ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \ --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \ --destdir data-bin/iwslt14.tokenized.de-en \ --workers 20 ``` ### Training LSTM ``` fairseq-train \ data-bin/iwslt14.tokenized.de-en \ -s en -t de \ --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \ --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \ --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \ --dropout 0.2 --weight-decay 0.0 \ --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \ --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \ --max-tokens 32768 \ --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \ data-bin/iwslt14.tokenized.de-en \ -s en -t de \ --arch transformer_iwslt_de_en --share-decoder-input-output-embed \ --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \ --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \ --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \ --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \ --max-tokens 32768 \ --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \ data-bin/iwslt14.tokenized.de-en \ -s en -t de \ --arch lightconv_iwslt_de_en \ --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \ --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \ --dropout 0.1 --weight-decay 0.0 \ --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \ --max-tokens 32768 \ --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` ### Evaluation BLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \ --path checkpoints/checkpoint_best.pt \ -s en -t de \ --batch-size 128 --beam 5 --lenpen 0.6 \ --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \ --path checkpoints/checkpoint_best.pt \ -s en -t de \ --batch-size 128 --beam 5 --lenpen 0.6 \ --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ``` ## Authors * **Ning Shi** - mrshininnnnn@gmail.com ## BibTex ``` @inproceedings{shi-etal-2022-revisit, title = "Revisit Systematic Generalization via Meaningful Learning", author = "Shi, Ning and Wang, Boxin and Wang, Wei and Liu, Xiangyu and Lin, Zhouhan", booktitle = "Proceedings of the Fifth BlackboxNLP <WORKSHOP>Workshop</WORKSHOP> on Analyzing and Interpreting Neural Networks for NLP", month = dec, year = "2022", address = "Abu Dhabi, United Arab Emirates (Hybrid)", publisher = "Association for Computational Linguistics", url = "https://aclanthology.org/2022.blackboxnlp-1.6", pages = "62--79", abstract = "Humans can systematically generalize to novel compositions of existing concepts.
