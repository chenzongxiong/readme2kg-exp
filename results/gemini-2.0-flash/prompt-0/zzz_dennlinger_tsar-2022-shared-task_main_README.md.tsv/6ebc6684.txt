- English test_none <DATASET>dataset</DATASET> (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none <DATASET>dataset</DATASET> (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none <DATASET>dataset</DATASET> (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold <DATASET>dataset</DATASET> (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold <DATASET>dataset</DATASET> (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold <DATASET>dataset</DATASET> (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 <EVALMETRIC>metrics</EVALMETRIC> are reported in the official results: -  <EVALMETRIC>MAP@1</EVALMETRIC>/<EVALMETRIC>Potential@1</EVALMETRIC>/<EVALMETRIC>Precision@1</EVALMETRIC> -  <EVALMETRIC>MAP@3</EVALMETRIC> -  <EVALMETRIC>MAP@5</EVALMETRIC> -  <EVALMETRIC>MAP@10</EVALMETRIC> -  <EVALMETRIC>Potential@3</EVALMETRIC> -  <EVALMETRIC>Potential@5</EVALMETRIC> -  <EVALMETRIC>Potential@10</EVALMETRIC> -  <EVALMETRIC>Accuracy@1@top_gold_1</EVALMETRIC> -  <EVALMETRIC>Accuracy@2@top_gold_1</EVALMETRIC> -  <EVALMETRIC>Accuracy@3@top_gold_1</EVALMETRIC>    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following <EVALMETRIC>metrics</EVALMETRIC> are reported in the extended results: -  <EVALMETRIC>Potential@K</EVALMETRIC>  K={1..10}  -  <EVALMETRIC>MAP@K</EVALMETRIC>  K={1..10} -  <EVALMETRIC>Precision@K</EVALMETRIC>  K={1..10}  (macro-average) -  <EVALMETRIC>Recall@K</EVALMETRIC>  K={1..10}     (macro-average) -  <EVALMETRIC>Accuracy@K@top_gold_1</EVALMETRIC>   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following <EVALMETRIC>metric</EVALMETRIC>:      -  <EVALMETRIC>MAP@1</EVALMETRIC>/<EVALMETRIC>Potential@1</EVALMETRIC>/<EVALMETRIC>Precision@1</EVALMETRIC>     -  <EVALMETRIC>MAP@3</EVALMETRIC>     -  <EVALMETRIC>MAP@5</EVALMETRIC>     -  <EVALMETRIC>MAP@10</EVALMETRIC>     -  <EVALMETRIC>Potential@3</EVALMETRIC>     -  <EVALMETRIC>Potential@5</EVALMETRIC>     -  <EVALMETRIC>Potential@10</EVALMETRIC>     -  <EVALMETRIC>Accuracy@1@top_gold_1</EVALMETRIC>     -  <EVALMETRIC>Accuracy@2@top_gold_1</EVALMETRIC>     -  <EVALMETRIC>Accuracy@3@top_gold_1</EVALMETRIC>          Script options and help  ```console Evaluation Script for the <PROJECT>TSAR-2022 Lexical Simplification Shared Task</PROJECT>  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .
