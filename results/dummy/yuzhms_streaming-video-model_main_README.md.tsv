#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# Streaming Video Model  > \*\*Streaming Video Model\*\* <br> > Yucheng Zhao, Chong Luo, Chuanxin Tang, Dongdong Chen, Noel Codella, Zheng-Jun Zha <br> > \*CVPR 2023\* <br>  \[\[Paper\](https://openaccess.thecvf.com/content/CVPR2023/html/Zhao\_Streaming\_Video\_Model\_CVPR\_2023\_paper.html)\]  \[\[arXiv\](https://arxiv.org/abs/2303.17228)\]   ## Description     \*\*Streaming video model\*\* is a general video model, which is applicable to general video understanding tasks.
1-1	0-1	#	_	_
1-2	2-11	Streaming	_	_
1-3	12-17	Video	_	_
1-4	18-23	Model	_	_
1-5	25-26	>	_	_
1-6	27-28	\*	_	_
1-7	28-29	\*	_	_
1-8	29-38	Streaming	_	_
1-9	39-44	Video	_	_
1-10	45-50	Model	_	_
1-11	50-51	\*	_	_
1-12	51-52	\*	_	_
1-13	53-54	<	_	_
1-14	54-56	br	_	_
1-15	56-57	>	_	_
1-16	58-59	>	_	_
1-17	60-67	Yucheng	_	_
1-18	68-72	Zhao	_	_
1-19	72-73	,	_	_
1-20	74-79	Chong	_	_
1-21	80-83	Luo	_	_
1-22	83-84	,	_	_
1-23	85-93	Chuanxin	_	_
1-24	94-98	Tang	_	_
1-25	98-99	,	_	_
1-26	100-108	Dongdong	_	_
1-27	109-113	Chen	_	_
1-28	113-114	,	_	_
1-29	115-119	Noel	_	_
1-30	120-127	Codella	_	_
1-31	127-128	,	_	_
1-32	129-138	Zheng-Jun	_	_
1-33	139-142	Zha	_	_
1-34	143-144	<	_	_
1-35	144-146	br	_	_
1-36	146-147	>	_	_
1-37	148-149	>	_	_
1-38	150-151	\*	_	_
1-39	151-155	CVPR	_	_
1-40	156-160	2023	_	_
1-41	160-161	\*	_	_
1-42	162-163	<	_	_
1-43	163-165	br	_	_
1-44	165-166	>	_	_
1-45	168-169	\[	_	_
1-46	169-170	\[	_	_
1-47	170-175	Paper	_	_
1-48	175-176	\]	_	_
1-49	176-177	(	_	_
1-50	177-182	https	_	_
1-51	182-183	:	_	_
1-52	183-184	/	_	_
1-53	184-185	/	_	_
1-54	185-206	openaccess.thecvf.com	_	_
1-55	206-207	/	_	_
1-56	207-214	content	_	_
1-57	214-215	/	_	_
1-58	215-223	CVPR2023	_	_
1-59	223-224	/	_	_
1-60	224-228	html	_	_
1-61	228-229	/	_	_
1-62	229-260	Zhao\_Streaming\_Video\_Model\_CVPR	_	_
1-62	256-260	CVPR	_	_
1-63	260-261	\_	_	_
1-64	261-265	2023	_	_
1-65	265-266	\_	_	_
1-66	266-276	paper.html	_	_
1-67	276-277	)	_	_
1-68	277-278	\]	_	_
1-69	280-281	\[	_	_
1-70	281-282	\[	_	_
1-71	282-287	arXiv	_	_
1-72	287-288	\]	_	_
1-73	288-289	(	_	_
1-74	289-294	https	_	_
1-75	294-295	:	_	_
1-76	295-296	/	_	_
1-77	296-297	/	_	_
1-78	297-306	arxiv.org	_	_
1-79	306-307	/	_	_
1-80	307-310	abs	_	_
1-81	310-311	/	_	_
1-82	311-321	2303.17228	_	_
1-83	321-322	)	_	_
1-84	322-323	\]	_	_
1-85	326-327	#	_	_
1-86	327-328	#	_	_
1-87	329-340	Description	_	_
1-88	345-346	\*	_	_
1-89	346-347	\*	_	_
1-90	347-356	Streaming	_	_
1-91	357-362	video	_	_
1-92	363-368	model	_	_
1-93	368-369	\*	_	_
1-94	369-370	\*	_	_
1-95	371-373	is	_	_
1-96	374-375	a	_	_
1-97	376-383	general	_	_
1-98	384-389	video	_	_
1-99	390-395	model	_	_
1-100	395-396	,	_	_
1-101	397-402	which	_	_
1-102	403-405	is	_	_
1-103	406-416	applicable	_	_
1-104	417-419	to	_	_
1-105	420-427	general	_	_
1-106	428-433	video	_	_
1-107	434-447	understanding	*[547]	PUBLICATION[547]
1-108	448-453	tasks	_	_
1-109	453-454	.	_	_

#Text=Traditionally, video understanding tasks have been modeled by two separate architectures, specially tailored for two distinct tasks.
2-1	455-468	Traditionally	_	_
2-2	468-469	,	*[637]	SOFTWARE[637]
2-3	470-475	video	*[637]	SOFTWARE[637]
2-4	476-489	understanding	*[637]	SOFTWARE[637]
2-5	490-495	tasks	*[637]	SOFTWARE[637]
2-6	496-500	have	*[637]	SOFTWARE[637]
2-7	501-505	been	_	_
2-8	506-513	modeled	_	_
2-9	514-516	by	_	_
2-10	517-520	two	_	_
2-11	521-529	separate	_	_
2-12	530-543	architectures	_	_
2-13	543-544	,	_	_
2-14	545-554	specially	_	_
2-15	555-563	tailored	_	_
2-16	564-567	for	_	_
2-17	568-571	two	_	_
2-18	572-580	distinct	_	_
2-19	581-586	tasks	_	_
2-20	586-587	.	_	_

#Text=Streaming video model is the first deep learning architecture that unifies video understanding tasks.
3-1	588-597	Streaming	_	_
3-2	598-603	video	_	_
3-3	604-609	model	_	_
3-4	610-612	is	_	_
3-5	613-616	the	_	_
3-6	617-622	first	_	_
3-7	623-627	deep	_	_
3-8	628-636	learning	_	_
3-9	637-649	architecture	*[638]	SOFTWARE[638]
3-10	650-654	that	*[638]	SOFTWARE[638]
3-11	655-662	unifies	_	_
3-12	663-668	video	_	_
3-13	669-682	understanding	_	_
3-14	683-688	tasks	_	_
3-15	688-689	.	_	_

#Text=We build an instance of streaming video model, namely the streaming video Transformer (S-ViT).S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks.
4-1	690-692	We	_	_
4-2	693-698	build	_	_
4-3	699-701	an	_	_
4-4	702-710	instance	_	_
4-5	711-713	of	_	_
4-6	714-723	streaming	_	_
4-7	724-729	video	_	_
4-8	730-735	model	_	_
4-9	735-736	,	_	_
4-10	737-743	namely	_	_
4-11	744-747	the	_	_
4-12	748-757	streaming	_	_
4-13	758-763	video	_	_
4-14	764-775	Transformer	_	_
4-15	776-777	(	_	_
4-16	777-782	S-ViT	_	_
4-17	782-783	)	_	_
4-18	783-784	.	_	_
4-19	784-789	S-ViT	_	_
4-20	790-795	first	_	_
4-21	796-804	produces	_	_
4-22	805-816	frame-level	_	_
4-23	817-825	features	_	_
4-24	826-830	with	_	_
4-25	831-832	a	_	_
4-26	833-847	memory-enabled	_	_
4-27	848-864	temporally-aware	*[639]	SOFTWARE[639]
4-28	865-872	spatial	*[639]	SOFTWARE[639]
4-29	873-880	encoder	_	_
4-30	881-883	to	_	_
4-31	884-889	serve	_	_
4-32	890-893	the	_	_
4-33	894-905	frame-based	_	_
4-34	906-911	video	_	_
4-35	912-917	tasks	_	_
4-36	917-918	.	_	_

#Text=Then the frame features are input into a task-related temporal decoder to obtain spatiotemporal features for sequence-based tasks.
5-1	919-923	Then	_	_
5-2	924-927	the	_	_
5-3	928-933	frame	_	_
5-4	934-942	features	_	_
5-5	943-946	are	_	_
5-6	947-952	input	_	_
5-7	953-957	into	_	_
5-8	958-959	a	_	_
5-9	960-972	task-related	_	_
5-10	973-981	temporal	_	_
5-11	982-989	decoder	_	_
5-12	990-992	to	_	_
5-13	993-999	obtain	_	_
5-14	1000-1014	spatiotemporal	_	_
5-15	1015-1023	features	_	_
5-16	1024-1027	for	*[621]	LICENSE[621]
5-17	1028-1042	sequence-based	*[621]	LICENSE[621]
5-18	1043-1048	tasks	_	_
5-19	1048-1049	.	_	_

#Text=The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio\[ffmpeg\] path ```  ### Dataset preparation  Download \[MOT17\](https://motchallenge.net/data/MOT17/), \[crowdhuman\](https://www.crowdhuman.org/), and \[MOTSynth\](https://motchallenge.net/) datasets and put them under the data directory.
6-1	1050-1053	The	_	_
6-2	1054-1064	efficiency	_	_
6-3	1065-1068	and	_	_
6-4	1069-1077	efficacy	_	_
6-5	1078-1080	of	_	_
6-6	1081-1086	S-ViT	_	_
6-7	1087-1089	is	_	_
6-8	1090-1102	demonstrated	_	_
6-9	1103-1105	by	_	_
6-10	1106-1109	the	_	_
6-11	1110-1126	state-of-the-art	_	_
6-12	1127-1135	accuracy	_	_
6-13	1136-1138	in	_	_
6-14	1139-1142	the	_	_
6-15	1143-1157	sequence-based	_	_
6-16	1158-1164	action	_	_
6-17	1165-1176	recognition	_	_
6-18	1177-1181	task	_	_
6-19	1182-1185	and	_	_
6-20	1186-1189	the	_	_
6-21	1190-1201	competitive	_	_
6-22	1202-1211	advantage	_	_
6-23	1212-1216	over	_	_
6-24	1217-1229	conventional	_	_
6-25	1230-1242	architecture	_	_
6-26	1243-1245	in	_	_
6-27	1246-1249	the	_	_
6-28	1250-1261	frame-based	_	_
6-29	1262-1265	MOT	_	_
6-30	1266-1270	task	_	_
6-31	1270-1271	.	_	_
6-32	1274-1275	#	_	_
6-33	1275-1276	#	_	_
6-34	1277-1282	Usage	_	_
6-35	1283-1284	#	_	_
6-36	1284-1285	#	_	_
6-37	1285-1286	#	_	_
6-38	1287-1299	Installation	_	_
6-39	1301-1306	Clone	_	_
6-40	1307-1310	the	_	_
6-41	1311-1315	repo	_	_
6-42	1316-1319	and	_	_
6-43	1320-1327	install	_	_
6-44	1328-1340	requirements	_	_
6-45	1340-1341	:	_	_
6-46	1343-1344	`	_	_
6-47	1344-1345	`	_	_
6-48	1345-1346	`	_	_
6-49	1346-1351	shell	_	_
6-50	1352-1357	conda	_	_
6-51	1358-1364	create	_	_
6-52	1365-1366	-	_	_
6-53	1366-1367	n	_	_
6-54	1368-1371	svm	_	_
6-55	1372-1378	python	_	_
6-56	1378-1379	=	_	_
6-57	1379-1382	3.7	_	_
6-58	1383-1384	-	_	_
6-59	1384-1385	y	_	_
6-60	1386-1391	conda	_	_
6-61	1392-1400	activate	_	_
6-62	1401-1404	svm	_	_
6-63	1405-1410	conda	_	_
6-64	1411-1418	install	_	_
6-65	1419-1426	pytorch	_	_
6-66	1426-1427	=	_	_
6-67	1427-1428	=	_	_
6-68	1428-1434	1.12.0	_	_
6-69	1435-1446	torchvision	_	_
6-70	1446-1447	=	_	_
6-71	1447-1448	=	_	_
6-72	1448-1454	0.13.0	_	_
6-73	1455-1466	cudatoolkit	_	_
6-74	1466-1467	=	_	_
6-75	1467-1471	11.3	_	_
6-76	1472-1473	-	_	_
6-77	1473-1474	c	_	_
6-78	1475-1482	pytorch	_	_
6-79	1483-1486	pip	_	_
6-80	1487-1494	install	_	_
6-81	1495-1498	git	_	_
6-82	1498-1499	+	_	_
6-83	1499-1504	https	_	_
6-84	1504-1505	:	_	_
6-85	1505-1506	/	_	_
6-86	1506-1507	/	_	_
6-87	1507-1517	github.com	_	_
6-88	1517-1518	/	_	_
6-89	1518-1532	JonathonLuiten	_	_
6-90	1532-1533	/	_	_
6-91	1533-1546	TrackEval.git	_	_
6-92	1547-1550	pip	_	_
6-93	1551-1558	install	_	_
6-94	1559-1568	mmcv-full	_	_
6-95	1568-1569	=	_	_
6-96	1569-1570	=	_	_
6-97	1570-1575	1.7.0	_	_
6-98	1576-1577	-	_	_
6-99	1577-1578	f	_	_
6-100	1579-1584	https	_	_
6-101	1584-1585	:	_	_
6-102	1585-1586	/	_	_
6-103	1586-1587	/	_	_
6-104	1587-1609	download.openmmlab.com	_	_
6-105	1609-1610	/	_	_
6-106	1610-1614	mmcv	_	_
6-107	1614-1615	/	_	_
6-108	1615-1619	dist	_	_
6-109	1619-1620	/	_	_
6-110	1620-1625	cu113	_	_
6-111	1625-1626	/	_	_
6-112	1626-1635	torch1.12	_	_
6-113	1635-1636	/	_	_
6-114	1636-1646	index.html	_	_
6-115	1647-1650	pip	_	_
6-116	1651-1658	install	_	_
6-117	1659-1664	mmdet	_	_
6-118	1664-1665	=	_	_
6-119	1665-1666	=	_	_
6-120	1666-1672	2.26.0	_	_
6-121	1673-1676	pip	_	_
6-122	1677-1684	install	_	_
6-123	1685-1686	-	_	_
6-124	1686-1687	r	_	_
6-125	1688-1700	requirements	_	_
6-126	1700-1701	/	_	_
6-127	1701-1710	build.txt	_	_
6-128	1711-1714	pip	_	_
6-129	1715-1722	install	_	_
6-130	1723-1724	-	*[565]	WORKSHOP[565]
6-131	1724-1725	-	*[565]	WORKSHOP[565]
6-132	1725-1729	user	*[565]	WORKSHOP[565]
6-133	1730-1731	-	*[565]	WORKSHOP[565]
6-134	1731-1732	v	*[565]	WORKSHOP[565]
6-135	1733-1734	-	*[565]	WORKSHOP[565]
6-136	1734-1735	e	*[565]	WORKSHOP[565]
6-137	1736-1737	.	*[565]	WORKSHOP[565]
6-138	1738-1741	pip	*[565]	WORKSHOP[565]
6-139	1742-1749	install	*[565]	WORKSHOP[565]
6-140	1750-1756	einops	*[565]	WORKSHOP[565]
6-141	1757-1760	pip	*[565]	WORKSHOP[565]
6-142	1761-1768	install	*[565]	WORKSHOP[565]
6-143	1769-1775	future	*[565]	WORKSHOP[565]
6-144	1776-1787	tensorboard	*[565]	WORKSHOP[565]
6-145	1788-1791	pip	*[565]	WORKSHOP[565]
6-146	1792-1799	install	*[565]	WORKSHOP[565]
6-147	1800-1801	-	*[565]	WORKSHOP[565]
6-148	1801-1802	U	*[565]	WORKSHOP[565]
6-149	1803-1809	fvcore	*[565]	WORKSHOP[565]
6-150	1810-1813	pip	*[565]	WORKSHOP[565]
6-151	1814-1821	install	*[565]	WORKSHOP[565]
6-152	1822-1827	click	*[565]	WORKSHOP[565]
6-153	1828-1835	imageio	*[565]	WORKSHOP[565]
6-154	1835-1836	\[	*[565]	WORKSHOP[565]
6-155	1836-1842	ffmpeg	*[565]	WORKSHOP[565]
6-156	1842-1843	\]	*[565]	WORKSHOP[565]
6-157	1844-1848	path	*[565]	WORKSHOP[565]
6-158	1849-1850	`	*[565]	WORKSHOP[565]
6-159	1850-1851	`	*[565]	WORKSHOP[565]
6-160	1851-1852	`	*[565]	WORKSHOP[565]
6-161	1854-1855	#	*[565]	WORKSHOP[565]
6-162	1855-1856	#	*[565]	WORKSHOP[565]
6-163	1856-1857	#	*[565]	WORKSHOP[565]
6-164	1858-1865	Dataset	*[565]	WORKSHOP[565]
6-165	1866-1877	preparation	*[565]	WORKSHOP[565]
6-166	1879-1887	Download	*[565]	WORKSHOP[565]
6-167	1888-1889	\[	*[565]	WORKSHOP[565]
6-168	1889-1894	MOT17	*[565]	WORKSHOP[565]
6-169	1894-1895	\]	*[565]	WORKSHOP[565]
6-170	1895-1896	(	*[565]	WORKSHOP[565]
6-171	1896-1901	https	*[565]	WORKSHOP[565]
6-172	1901-1902	:	*[565]	WORKSHOP[565]
6-173	1902-1903	/	*[565]	WORKSHOP[565]
6-174	1903-1904	/	*[565]	WORKSHOP[565]
6-175	1904-1920	motchallenge.net	*[565]	WORKSHOP[565]
6-176	1920-1921	/	*[565]	WORKSHOP[565]
6-177	1921-1925	data	_	_
6-178	1925-1926	/	_	_
6-179	1926-1931	MOT17	_	_
6-180	1931-1932	/	_	_
6-181	1932-1933	)	_	_
6-182	1933-1934	,	_	_
6-183	1935-1936	\[	_	_
6-184	1936-1946	crowdhuman	_	_
6-185	1946-1947	\]	_	_
6-186	1947-1948	(	_	_
6-187	1948-1953	https	_	_
6-188	1953-1954	:	_	_
6-189	1954-1955	/	_	_
6-190	1955-1956	/	_	_
6-191	1956-1974	www.crowdhuman.org	_	_
6-192	1974-1975	/	_	_
6-193	1975-1976	)	_	_
6-194	1976-1977	,	_	_
6-195	1978-1981	and	_	_
6-196	1982-1983	\[	_	_
6-197	1983-1991	MOTSynth	_	_
6-198	1991-1992	\]	_	_
6-199	1992-1993	(	_	_
6-200	1993-1998	https	_	_
6-201	1998-1999	:	_	_
6-202	1999-2000	/	_	_
6-203	2000-2001	/	_	_
6-204	2001-2017	motchallenge.net	_	_
6-205	2017-2018	/	_	_
6-206	2018-2019	)	_	_
6-207	2020-2028	datasets	_	_
6-208	2029-2032	and	_	_
6-209	2033-2036	put	_	_
6-210	2037-2041	them	_	_
6-211	2042-2047	under	_	_
6-212	2048-2051	the	_	_
6-213	2052-2056	data	_	_
6-214	2057-2066	directory	_	_
6-215	2066-2067	.	_	_

#Text=The data directory is structured as follows:  ``` data \|-- crowdhuman │   ├── annotation\_train.odgt │   ├── annotation\_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman\_train01.zip │   │   ├── CrowdHuman\_train02.zip │   │   ├── CrowdHuman\_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman\_val.zip \|-- MOT17 │   ├── train │   ├── test \|-- MOTSynth \|   ├── videos │   ├── annotations ```  Then, we need to convert the all dataset to COCO format.
7-1	2068-2071	The	_	_
7-2	2072-2076	data	*[640]	SOFTWARE[640]
7-3	2077-2086	directory	*[640]	SOFTWARE[640]
7-4	2087-2089	is	*[640]	SOFTWARE[640]
7-5	2090-2100	structured	*[640]	SOFTWARE[640]
7-6	2101-2103	as	*[640]	SOFTWARE[640]
7-7	2104-2111	follows	*[640]	SOFTWARE[640]
7-8	2111-2112	:	*[640]	SOFTWARE[640]
7-9	2114-2115	`	*[640]	SOFTWARE[640]
7-10	2115-2116	`	*[640]	SOFTWARE[640]
7-11	2116-2117	`	*[640]	SOFTWARE[640]
7-12	2118-2122	data	*[640]	SOFTWARE[640]
7-13	2123-2124	\|	*[640]	SOFTWARE[640]
7-14	2124-2125	-	*[640]	SOFTWARE[640]
7-15	2125-2126	-	*[640]	SOFTWARE[640]
7-16	2127-2137	crowdhuman	*[640]	SOFTWARE[640]
7-17	2138-2139	│	*[640]	SOFTWARE[640]
7-18	2142-2143	├	*[640]	SOFTWARE[640]
7-19	2143-2144	─	*[640]	SOFTWARE[640]
7-20	2144-2145	─	*[640]	SOFTWARE[640]
7-21	2146-2167	annotation\_train.odgt	*[640]	SOFTWARE[640]
7-22	2168-2169	│	*[640]	SOFTWARE[640]
7-23	2172-2173	├	*[640]	SOFTWARE[640]
7-24	2173-2174	─	*[640]	SOFTWARE[640]
7-25	2174-2175	─	*[640]	SOFTWARE[640]
7-26	2176-2195	annotation\_val.odgt	*[640]	SOFTWARE[640]
7-27	2196-2197	│	*[640]	SOFTWARE[640]
7-28	2200-2201	├	*[640]	SOFTWARE[640]
7-29	2201-2202	─	*[640]	SOFTWARE[640]
7-30	2202-2203	─	*[640]	SOFTWARE[640]
7-31	2204-2209	train	*[640]	SOFTWARE[640]
7-32	2210-2211	│	*[640]	SOFTWARE[640]
7-33	2214-2215	│	*[640]	SOFTWARE[640]
7-34	2218-2219	├	*[640]	SOFTWARE[640]
7-35	2219-2220	─	*[640]	SOFTWARE[640]
7-36	2220-2221	─	*[640]	SOFTWARE[640]
7-37	2222-2228	Images	*[640]	SOFTWARE[640]
7-38	2229-2230	│	*[640]	SOFTWARE[640]
7-39	2233-2234	│	*[640]	SOFTWARE[640]
7-40	2237-2238	├	*[640]	SOFTWARE[640]
7-41	2238-2239	─	*[640]	SOFTWARE[640]
7-42	2239-2240	─	*[640]	SOFTWARE[640]
7-43	2241-2259	CrowdHuman\_train01	*[640]	SOFTWARE[640]
7-44	2259-2260	.	*[640]	SOFTWARE[640]
7-45	2260-2263	zip	*[640]	SOFTWARE[640]
7-46	2264-2265	│	*[640]	SOFTWARE[640]
7-47	2268-2269	│	*[640]	SOFTWARE[640]
7-48	2272-2273	├	*[640]	SOFTWARE[640]
7-49	2273-2274	─	*[640]	SOFTWARE[640]
7-50	2274-2275	─	*[640]	SOFTWARE[640]
7-51	2276-2294	CrowdHuman\_train02	*[640]	SOFTWARE[640]
7-52	2294-2295	.	*[640]	SOFTWARE[640]
7-53	2295-2298	zip	*[640]	SOFTWARE[640]
7-54	2299-2300	│	_	_
7-55	2303-2304	│	_	_
7-56	2307-2308	├	_	_
7-57	2308-2309	─	_	_
7-58	2309-2310	─	_	_
7-59	2311-2329	CrowdHuman\_train03	_	_
7-60	2329-2330	.	_	_
7-61	2330-2333	zip	_	_
7-62	2334-2335	│	_	_
7-63	2338-2339	├	_	_
7-64	2339-2340	─	_	_
7-65	2340-2341	─	_	_
7-66	2342-2345	val	_	_
7-67	2346-2347	│	_	_
7-68	2350-2351	│	_	_
7-69	2354-2355	├	_	_
7-70	2355-2356	─	_	_
7-71	2356-2357	─	_	_
7-72	2358-2364	Images	_	_
7-73	2365-2366	│	_	_
7-74	2369-2370	│	_	_
7-75	2373-2374	├	_	_
7-76	2374-2375	─	_	_
7-77	2375-2376	─	_	_
7-78	2377-2395	CrowdHuman\_val.zip	_	_
7-79	2396-2397	\|	_	_
7-80	2397-2398	-	_	_
7-81	2398-2399	-	_	_
7-82	2400-2405	MOT17	_	_
7-83	2406-2407	│	_	_
7-84	2410-2411	├	_	_
7-85	2411-2412	─	_	_
7-86	2412-2413	─	_	_
7-87	2414-2419	train	_	_
7-88	2420-2421	│	_	_
7-89	2424-2425	├	_	_
7-90	2425-2426	─	_	_
7-91	2426-2427	─	_	_
7-92	2428-2432	test	_	_
7-93	2433-2434	\|	_	_
7-94	2434-2435	-	_	_
7-95	2435-2436	-	_	_
7-96	2437-2445	MOTSynth	_	_
7-97	2446-2447	\|	_	_
7-98	2450-2451	├	_	_
7-99	2451-2452	─	_	_
7-100	2452-2453	─	_	_
7-101	2454-2460	videos	_	_
7-102	2461-2462	│	_	_
7-103	2465-2466	├	_	_
7-104	2466-2467	─	_	_
7-105	2467-2468	─	_	_
7-106	2469-2480	annotations	_	_
7-107	2481-2482	`	_	_
7-108	2482-2483	`	_	_
7-109	2483-2484	`	_	_
7-110	2486-2490	Then	_	_
7-111	2490-2491	,	_	_
7-112	2492-2494	we	_	_
7-113	2495-2499	need	_	_
7-114	2500-2502	to	_	_
7-115	2503-2510	convert	_	_
7-116	2511-2514	the	_	_
7-117	2515-2518	all	_	_
7-118	2519-2526	dataset	_	_
7-119	2527-2529	to	_	_
7-120	2530-2534	COCO	_	_
7-121	2535-2541	format	_	_
7-122	2541-2542	.	_	_

#Text=We provide scripts to do this:  ```shell # crowdhuman python .
8-1	2543-2545	We	_	_
8-2	2546-2553	provide	_	_
8-3	2554-2561	scripts	_	_
8-4	2562-2564	to	_	_
8-5	2565-2567	do	_	_
8-6	2568-2572	this	_	_
8-7	2572-2573	:	_	_
8-8	2575-2576	`	*[566]	WORKSHOP[566]
8-9	2576-2577	`	*[566]	WORKSHOP[566]
8-10	2577-2578	`	*[566]	WORKSHOP[566]
8-11	2578-2583	shell	_	_
8-12	2584-2585	#	_	_
8-13	2586-2596	crowdhuman	_	_
8-14	2597-2603	python	_	_
8-15	2604-2605	.	_	_

#Text=/tools/convert\_datasets/crowdhuman2coco.py -i .
9-1	2605-2606	/	_	_
9-2	2606-2611	tools	_	_
9-3	2611-2612	/	_	_
9-4	2612-2628	convert\_datasets	_	_
9-5	2628-2629	/	_	_
9-6	2629-2647	crowdhuman2coco.py	_	_
9-7	2648-2649	-	_	_
9-8	2649-2650	i	*[597]	PROJECT[597]
9-9	2651-2652	.	_	_

#Text=/data/crowdhuman -o .
10-1	2652-2653	/	_	_
10-2	2653-2657	data	_	_
10-3	2657-2658	/	_	_
10-4	2658-2668	crowdhuman	_	_
10-5	2669-2670	-	_	_
10-6	2670-2671	o	*[641]	SOFTWARE[641]
10-7	2672-2673	.	_	_

#Text=/data/crowdhuman/annotations  # MOT17 python .
11-1	2673-2674	/	_	_
11-2	2674-2678	data	_	_
11-3	2678-2679	/	_	_
11-4	2679-2689	crowdhuman	_	_
11-5	2689-2690	/	_	_
11-6	2690-2701	annotations	_	_
11-7	2703-2704	#	_	_
11-8	2705-2710	MOT17	_	_
11-9	2711-2717	python	_	_
11-10	2718-2719	.	_	_

#Text=/tools/convert\_datasets/mot2coco.py -i .
12-1	2719-2720	/	_	_
12-2	2720-2725	tools	*[548]	PUBLICATION[548]
12-3	2725-2726	/	*[548]	PUBLICATION[548]
12-4	2726-2742	convert\_datasets	*[548]	PUBLICATION[548]
12-5	2742-2743	/	*[548]	PUBLICATION[548]
12-6	2743-2754	mot2coco.py	_	_
12-7	2755-2756	-	_	_
12-8	2756-2757	i	_	_
12-9	2758-2759	.	_	_

#Text=/data/MOT17/ -o .
13-1	2759-2760	/	_	_
13-2	2760-2764	data	_	_
13-3	2764-2765	/	*[549]	PUBLICATION[549]
13-4	2765-2770	MOT17	_	_
13-5	2770-2771	/	_	_
13-6	2772-2773	-	_	_
13-7	2773-2774	o	_	_
13-8	2775-2776	.	_	_

#Text=/data/MOT17/annotations --split-train --convert-det  # MOTSynth python .
14-1	2776-2777	/	_	_
14-2	2777-2781	data	_	_
14-3	2781-2782	/	_	_
14-4	2782-2787	MOT17	_	_
14-5	2787-2788	/	_	_
14-6	2788-2799	annotations	_	_
14-7	2800-2801	-	_	_
14-8	2801-2802	-	_	_
14-9	2802-2813	split-train	_	_
14-10	2814-2815	-	_	_
14-11	2815-2816	-	_	_
14-12	2816-2827	convert-det	_	_
14-13	2829-2830	#	_	_
14-14	2831-2839	MOTSynth	_	_
14-15	2840-2846	python	_	_
14-16	2847-2848	.	_	_

#Text=/tools/convert\_datasets/extract\_motsynth.py --input\_dir\_path .
15-1	2848-2849	/	_	_
15-2	2849-2854	tools	_	_
15-3	2854-2855	/	_	_
15-4	2855-2871	convert\_datasets	_	_
15-5	2871-2872	/	_	_
15-6	2872-2891	extract\_motsynth.py	_	_
15-7	2892-2893	-	_	_
15-8	2893-2894	-	_	_
15-9	2894-2908	input\_dir\_path	_	_
15-10	2909-2910	.	_	_

#Text=/data/MOTSynth/video --out\_dir\_path .
16-1	2910-2911	/	_	_
16-2	2911-2915	data	_	_
16-3	2915-2916	/	_	_
16-4	2916-2924	MOTSynth	_	_
16-5	2924-2925	/	*[647]	EVALMETRIC[647]
16-6	2925-2930	video	_	_
16-7	2931-2932	-	_	_
16-8	2932-2933	-	_	_
16-9	2933-2945	out\_dir\_path	_	_
16-10	2946-2947	.	_	_

#Text=/data/MOTSynth/train/ python .
17-1	2947-2948	/	_	_
17-2	2948-2952	data	_	_
17-3	2952-2953	/	_	_
17-4	2953-2961	MOTSynth	_	_
17-5	2961-2962	/	_	_
17-6	2962-2967	train	_	_
17-7	2967-2968	/	*[588]	DATASET[588]
17-8	2969-2975	python	*[588]	DATASET[588]
17-9	2976-2977	.	_	_

#Text=/tools/convert\_datasets/motsynth2coco.py --anns .
18-1	2977-2978	/	_	_
18-2	2978-2983	tools	_	_
18-3	2983-2984	/	_	_
18-4	2984-3000	convert\_datasets	_	_
18-5	3000-3001	/	_	_
18-6	3001-3017	motsynth2coco.py	_	_
18-7	3018-3019	-	*[574]	ONTOLOGY[574]
18-8	3019-3020	-	*[574]	ONTOLOGY[574]
18-9	3020-3024	anns	_	_
18-10	3025-3026	.	_	_

#Text=/data/MOTSynth/annotations --out .
19-1	3026-3027	/	_	_
19-2	3027-3031	data	_	_
19-3	3031-3032	/	_	_
19-4	3032-3040	MOTSynth	_	_
19-5	3040-3041	/	*[598]	CONFERENCE[598]
19-6	3041-3052	annotations	_	_
19-7	3053-3054	-	_	_
19-8	3054-3055	-	_	_
19-9	3055-3058	out	_	_
19-10	3059-3060	.	_	_

#Text=/data/MOTSynth/all\_cocoformat.json ```  The processed dataset will be structured as follows:  ``` data \|-- crowdhuman │   ├── annotation\_train.odgt │   ├── annotation\_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman\_train01.zip │   │   ├── CrowdHuman\_train02.zip │   │   ├── CrowdHuman\_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman\_val.zip \|   ├── annotations │   │   ├── crowdhuman\_train.json │   │   ├── crowdhuman\_val.json \|-- MOT17 │   ├── train │   │   ├── MOT17-02-DPM │   │   ├── ... │   ├── test │   ├── annotations │   │   ├── half-train\_cocoformat.json │   │   ├── ... \|-- MOTSynth \|   ├── videos │   ├── annotations │   ├── train │   │   ├── 000 │   │   │   ├── img1 │   │   │   │   ├── 000001.jpg │   │   │   │   ├── ... │   │   ├── ... │   ├── all\_cocoformat.json ``` ### Pretrained models  We use CLIP pretrained ViT models.
20-1	3060-3061	/	_	_
20-2	3061-3065	data	_	_
20-3	3065-3066	/	_	_
20-4	3066-3074	MOTSynth	_	_
20-5	3074-3075	/	_	_
20-6	3075-3094	all\_cocoformat.json	_	_
20-7	3095-3096	`	_	_
20-8	3096-3097	`	_	_
20-9	3097-3098	`	_	_
20-10	3100-3103	The	_	_
20-11	3104-3113	processed	_	_
20-12	3114-3121	dataset	_	_
20-13	3122-3126	will	_	_
20-14	3127-3129	be	_	_
20-15	3130-3140	structured	_	_
20-16	3141-3143	as	_	_
20-17	3144-3151	follows	_	_
20-18	3151-3152	:	_	_
20-19	3154-3155	`	_	_
20-20	3155-3156	`	_	_
20-21	3156-3157	`	_	_
20-22	3158-3162	data	_	_
20-23	3163-3164	\|	_	_
20-24	3164-3165	-	_	_
20-25	3165-3166	-	_	_
20-26	3167-3177	crowdhuman	_	_
20-27	3178-3179	│	_	_
20-28	3182-3183	├	_	_
20-29	3183-3184	─	_	_
20-30	3184-3185	─	_	_
20-31	3186-3207	annotation\_train.odgt	_	_
20-32	3208-3209	│	_	_
20-33	3212-3213	├	_	_
20-34	3213-3214	─	_	_
20-35	3214-3215	─	_	_
20-36	3216-3235	annotation\_val.odgt	_	_
20-37	3236-3237	│	_	_
20-38	3240-3241	├	_	_
20-39	3241-3242	─	_	_
20-40	3242-3243	─	_	_
20-41	3244-3249	train	_	_
20-42	3250-3251	│	_	_
20-43	3254-3255	│	_	_
20-44	3258-3259	├	_	_
20-45	3259-3260	─	_	_
20-46	3260-3261	─	_	_
20-47	3262-3268	Images	_	_
20-48	3269-3270	│	_	_
20-49	3273-3274	│	_	_
20-50	3277-3278	├	_	_
20-51	3278-3279	─	_	_
20-52	3279-3280	─	_	_
20-53	3281-3299	CrowdHuman\_train01	_	_
20-54	3299-3300	.	_	_
20-55	3300-3303	zip	_	_
20-56	3304-3305	│	_	_
20-57	3308-3309	│	_	_
20-58	3312-3313	├	_	_
20-59	3313-3314	─	_	_
20-60	3314-3315	─	_	_
20-61	3316-3334	CrowdHuman\_train02	_	_
20-62	3334-3335	.	_	_
20-63	3335-3338	zip	_	_
20-64	3339-3340	│	_	_
20-65	3343-3344	│	_	_
20-66	3347-3348	├	_	_
20-67	3348-3349	─	_	_
20-68	3349-3350	─	_	_
20-69	3351-3369	CrowdHuman\_train03	_	_
20-70	3369-3370	.	_	_
20-71	3370-3373	zip	_	_
20-72	3374-3375	│	_	_
20-73	3378-3379	├	_	_
20-74	3379-3380	─	_	_
20-75	3380-3381	─	_	_
20-76	3382-3385	val	_	_
20-77	3386-3387	│	_	_
20-78	3390-3391	│	_	_
20-79	3394-3395	├	_	_
20-80	3395-3396	─	_	_
20-81	3396-3397	─	_	_
20-82	3398-3404	Images	_	_
20-83	3405-3406	│	_	_
20-84	3409-3410	│	_	_
20-85	3413-3414	├	_	_
20-86	3414-3415	─	_	_
20-87	3415-3416	─	_	_
20-88	3417-3435	CrowdHuman\_val.zip	_	_
20-89	3436-3437	\|	_	_
20-90	3440-3441	├	_	_
20-91	3441-3442	─	_	_
20-92	3442-3443	─	_	_
20-93	3444-3455	annotations	_	_
20-94	3456-3457	│	_	_
20-95	3460-3461	│	_	_
20-96	3464-3465	├	_	_
20-97	3465-3466	─	_	_
20-98	3466-3467	─	_	_
20-99	3468-3489	crowdhuman\_train.json	_	_
20-100	3490-3491	│	_	_
20-101	3494-3495	│	_	_
20-102	3498-3499	├	_	_
20-103	3499-3500	─	_	_
20-104	3500-3501	─	_	_
20-105	3502-3521	crowdhuman\_val.json	_	_
20-106	3522-3523	\|	_	_
20-107	3523-3524	-	_	_
20-108	3524-3525	-	_	_
20-109	3526-3531	MOT17	_	_
20-110	3532-3533	│	_	_
20-111	3536-3537	├	_	_
20-112	3537-3538	─	_	_
20-113	3538-3539	─	_	_
20-114	3540-3545	train	_	_
20-115	3546-3547	│	_	_
20-116	3550-3551	│	_	_
20-117	3554-3555	├	_	_
20-118	3555-3556	─	_	_
20-119	3556-3557	─	_	_
20-120	3558-3563	MOT17	_	_
20-121	3563-3564	-	_	_
20-122	3564-3566	02	_	_
20-123	3566-3567	-	_	_
20-124	3567-3570	DPM	_	_
20-125	3571-3572	│	_	_
20-126	3575-3576	│	_	_
20-127	3579-3580	├	_	_
20-128	3580-3581	─	_	_
20-129	3581-3582	─	_	_
20-130	3583-3584	.	_	_
20-131	3584-3585	.	_	_
20-132	3585-3586	.	_	_
20-133	3587-3588	│	_	_
20-134	3591-3592	├	_	_
20-135	3592-3593	─	_	_
20-136	3593-3594	─	_	_
20-137	3595-3599	test	_	_
20-138	3600-3601	│	_	_
20-139	3604-3605	├	_	_
20-140	3605-3606	─	_	_
20-141	3606-3607	─	_	_
20-142	3608-3619	annotations	_	_
20-143	3620-3621	│	_	_
20-144	3624-3625	│	_	_
20-145	3628-3629	├	_	_
20-146	3629-3630	─	_	_
20-147	3630-3631	─	_	_
20-148	3632-3658	half-train\_cocoformat.json	_	_
20-149	3659-3660	│	_	_
20-150	3663-3664	│	_	_
20-151	3667-3668	├	_	_
20-152	3668-3669	─	_	_
20-153	3669-3670	─	_	_
20-154	3671-3672	.	_	_
20-155	3672-3673	.	_	_
20-156	3673-3674	.	_	_
20-157	3675-3676	\|	_	_
20-158	3676-3677	-	_	_
20-159	3677-3678	-	_	_
20-160	3679-3687	MOTSynth	_	_
20-161	3688-3689	\|	_	_
20-162	3692-3693	├	_	_
20-163	3693-3694	─	_	_
20-164	3694-3695	─	_	_
20-165	3696-3702	videos	_	_
20-166	3703-3704	│	_	_
20-167	3707-3708	├	_	_
20-168	3708-3709	─	*[648]	EVALMETRIC[648]
20-169	3709-3710	─	*[648]	EVALMETRIC[648]
20-170	3711-3722	annotations	*[648]	EVALMETRIC[648]
20-171	3723-3724	│	*[648]	EVALMETRIC[648]
20-172	3727-3728	├	*[648]	EVALMETRIC[648]
20-173	3728-3729	─	*[648]	EVALMETRIC[648]
20-174	3729-3730	─	*[648]	EVALMETRIC[648]
20-175	3731-3736	train	*[648]	EVALMETRIC[648]
20-176	3737-3738	│	*[648]	EVALMETRIC[648]
20-177	3741-3742	│	*[648]	EVALMETRIC[648]
20-178	3745-3746	├	*[648]	EVALMETRIC[648]
20-179	3746-3747	─	*[648]	EVALMETRIC[648]
20-180	3747-3748	─	*[648]	EVALMETRIC[648]
20-181	3749-3752	000	*[648]	EVALMETRIC[648]
20-182	3753-3754	│	*[648]	EVALMETRIC[648]
20-183	3757-3758	│	*[648]	EVALMETRIC[648]
20-184	3761-3762	│	*[648]	EVALMETRIC[648]
20-185	3765-3766	├	*[648]	EVALMETRIC[648]
20-186	3766-3767	─	*[648]	EVALMETRIC[648]
20-187	3767-3768	─	_	_
20-188	3769-3773	img1	_	_
20-189	3774-3775	│	_	_
20-190	3778-3779	│	_	_
20-191	3782-3783	│	_	_
20-192	3786-3787	│	_	_
20-193	3790-3791	├	_	_
20-194	3791-3792	─	_	_
20-195	3792-3793	─	_	_
20-196	3794-3800	000001	_	_
20-197	3800-3801	.	_	_
20-198	3801-3804	jpg	_	_
20-199	3805-3806	│	_	_
20-200	3809-3810	│	_	_
20-201	3813-3814	│	_	_
20-202	3817-3818	│	_	_
20-203	3821-3822	├	_	_
20-204	3822-3823	─	_	_
20-205	3823-3824	─	_	_
20-206	3825-3826	.	_	_
20-207	3826-3827	.	_	_
20-208	3827-3828	.	_	_
20-209	3829-3830	│	_	_
20-210	3833-3834	│	_	_
20-211	3837-3838	├	_	_
20-212	3838-3839	─	_	_
20-213	3839-3840	─	_	_
20-214	3841-3842	.	_	_
20-215	3842-3843	.	_	_
20-216	3843-3844	.	_	_
20-217	3845-3846	│	_	_
20-218	3849-3850	├	_	_
20-219	3850-3851	─	_	_
20-220	3851-3852	─	_	_
20-221	3853-3872	all\_cocoformat.json	_	_
20-222	3873-3874	`	_	_
20-223	3874-3875	`	_	_
20-224	3875-3876	`	_	_
20-225	3877-3878	#	_	_
20-226	3878-3879	#	_	_
20-227	3879-3880	#	_	_
20-228	3881-3891	Pretrained	_	_
20-229	3892-3898	models	_	_
20-230	3900-3902	We	_	_
20-231	3903-3906	use	_	_
20-232	3907-3911	CLIP	_	_
20-233	3912-3922	pretrained	_	_
20-234	3923-3926	ViT	_	_
20-235	3927-3933	models	_	_
20-236	3933-3934	.	_	_

#Text=You can download them from \[here\](https://github.com/openai/CLIP/tree/main) and put them under the `pretrain` directory.  ### Training and Evaluation  Training on single node ```shell bash .
21-1	3935-3938	You	_	_
21-2	3939-3942	can	_	_
21-3	3943-3951	download	_	_
21-4	3952-3956	them	_	_
21-5	3957-3961	from	_	_
21-6	3962-3963	\[	*[567]	WORKSHOP[567]
21-7	3963-3967	here	*[567]	WORKSHOP[567]
21-8	3967-3968	\]	*[567]	WORKSHOP[567]
21-9	3968-3969	(	*[567]	WORKSHOP[567]
21-10	3969-3974	https	*[567]	WORKSHOP[567]
21-11	3974-3975	:	*[567]	WORKSHOP[567]
21-12	3975-3976	/	*[567]	WORKSHOP[567]
21-13	3976-3977	/	*[567]	WORKSHOP[567]
21-14	3977-3987	github.com	*[567]	WORKSHOP[567]
21-15	3987-3988	/	*[567]	WORKSHOP[567]
21-16	3988-3994	openai	*[567]	WORKSHOP[567]
21-17	3994-3995	/	*[567]	WORKSHOP[567]
21-18	3995-3999	CLIP	*[567]	WORKSHOP[567]
21-19	3999-4000	/	*[567]	WORKSHOP[567]
21-20	4000-4004	tree	*[567]	WORKSHOP[567]
21-21	4004-4005	/	*[567]	WORKSHOP[567]
21-22	4005-4009	main	*[567]	WORKSHOP[567]
21-23	4009-4010	)	*[567]	WORKSHOP[567]
21-24	4011-4014	and	*[567]	WORKSHOP[567]
21-25	4015-4018	put	*[567]	WORKSHOP[567]
21-26	4019-4023	them	*[567]	WORKSHOP[567]
21-27	4024-4029	under	*[567]	WORKSHOP[567]
21-28	4030-4033	the	*[567]	WORKSHOP[567]
21-29	4034-4035	`	*[567]	WORKSHOP[567]
21-30	4035-4043	pretrain	_	_
21-31	4043-4044	`	_	_
21-32	4045-4054	directory	_	_
21-33	4054-4055	.	_	_
21-34	4057-4058	#	_	_
21-35	4058-4059	#	_	_
21-36	4059-4060	#	_	_
21-37	4061-4069	Training	_	_
21-38	4070-4073	and	_	_
21-39	4074-4084	Evaluation	_	_
21-40	4086-4094	Training	_	_
21-41	4095-4097	on	_	_
21-42	4098-4104	single	_	_
21-43	4105-4109	node	_	_
21-44	4110-4111	`	_	_
21-45	4111-4112	`	_	_
21-46	4112-4113	`	_	_
21-47	4113-4118	shell	_	_
21-48	4119-4123	bash	_	_
21-49	4124-4125	.	_	_

#Text=/tools/dist\_train.sh configs/mot/svm/svm\_base.py 8 --cfg-options \\    model.detector.backbone.pretrain=.
22-1	4125-4126	/	_	_
22-2	4126-4131	tools	_	_
22-3	4131-4132	/	_	_
22-4	4132-4145	dist\_train.sh	_	_
22-5	4146-4153	configs	_	_
22-6	4153-4154	/	_	_
22-7	4154-4157	mot	_	_
22-8	4157-4158	/	_	_
22-9	4158-4161	svm	_	_
22-10	4161-4162	/	_	_
22-11	4162-4173	svm\_base.py	_	_
22-12	4174-4175	8	_	_
22-13	4176-4177	-	_	_
22-14	4177-4178	-	_	_
22-15	4178-4189	cfg-options	_	_
22-16	4190-4191	\\	_	_
22-17	4195-4227	model.detector.backbone.pretrain	_	_
22-18	4227-4228	=	_	_
22-19	4228-4229	.	_	_

#Text=/pretrain/ViT-B-16.pt ``` Evaluation on MOT17 half validation set ```shell bash .
23-1	4229-4230	/	_	_
23-2	4230-4238	pretrain	_	_
23-3	4238-4239	/	_	_
23-4	4239-4244	ViT-B	_	_
23-5	4244-4245	-	_	_
23-6	4245-4247	16	_	_
23-7	4247-4248	.	_	_
23-8	4248-4250	pt	_	_
23-9	4251-4252	`	_	_
23-10	4252-4253	`	_	_
23-11	4253-4254	`	_	_
23-12	4255-4265	Evaluation	_	_
23-13	4266-4268	on	_	_
23-14	4269-4274	MOT17	_	_
23-15	4275-4279	half	_	_
23-16	4280-4290	validation	_	_
23-17	4291-4294	set	_	_
23-18	4295-4296	`	_	_
23-19	4296-4297	`	_	_
23-20	4297-4298	`	_	_
23-21	4298-4303	shell	_	_
23-22	4304-4308	bash	_	_
23-23	4309-4310	.	_	_

#Text=/tools/dist\_test.sh configs/mot/svm/svm\_test.py 8 \\    --eval bbox track --checkpoint svm\_motsync\_ch\_mot17half.pth ```  ## Main Results ### MOT17 \| Method \| Dataset \|                Train Data                \| MOTA \| HOTA \| IDF1 \|    URL    \| \| :---: \| :---: \|:----------------------------------------:\|:----:\|:----:\|:----:\|:---------:\| \| SVM \| MOT17 \| MOT17 half-train + crowdhuman + MOTSynth \| 79.7 \| 68.1 \| 80.9 \| \[model\](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm\_mot17-half-val.pth) \|  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao\_2023\_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of \[MMTracking\](https://github.com/open-mmlab/mmtracking/) and \[CLIP\](https://github.com/openai/CLIP).
24-1	4310-4311	/	_	_
24-2	4311-4316	tools	_	_
24-3	4316-4317	/	_	_
24-4	4317-4329	dist\_test.sh	_	_
24-5	4330-4337	configs	_	_
24-6	4337-4338	/	_	_
24-7	4338-4341	mot	_	_
24-8	4341-4342	/	_	_
24-9	4342-4345	svm	_	_
24-10	4345-4346	/	_	_
24-11	4346-4357	svm\_test.py	_	_
24-12	4358-4359	8	_	_
24-13	4360-4361	\\	_	_
24-14	4365-4366	-	_	_
24-15	4366-4367	-	_	_
24-16	4367-4371	eval	_	_
24-17	4372-4376	bbox	_	_
24-18	4377-4382	track	_	_
24-19	4383-4384	-	_	_
24-20	4384-4385	-	_	_
24-21	4385-4395	checkpoint	_	_
24-22	4396-4424	svm\_motsync\_ch\_mot17half.pth	_	_
24-23	4425-4426	`	_	_
24-24	4426-4427	`	_	_
24-25	4427-4428	`	_	_
24-26	4430-4431	#	_	_
24-27	4431-4432	#	_	_
24-28	4433-4437	Main	_	_
24-29	4438-4445	Results	_	_
24-30	4446-4447	#	_	_
24-31	4447-4448	#	_	_
24-32	4448-4449	#	_	_
24-33	4450-4455	MOT17	_	_
24-34	4456-4457	\|	_	_
24-35	4458-4464	Method	_	_
24-36	4465-4466	\|	_	_
24-37	4467-4474	Dataset	_	_
24-38	4475-4476	\|	_	_
24-39	4492-4497	Train	_	_
24-40	4498-4502	Data	_	_
24-41	4518-4519	\|	_	_
24-42	4520-4524	MOTA	_	_
24-43	4525-4526	\|	_	_
24-44	4527-4531	HOTA	_	_
24-45	4532-4533	\|	_	_
24-46	4534-4538	IDF1	_	_
24-47	4539-4540	\|	_	_
24-48	4544-4547	URL	_	_
24-49	4551-4552	\|	_	_
24-50	4553-4554	\|	_	_
24-51	4555-4556	:	_	_
24-52	4556-4557	-	_	_
24-53	4557-4558	-	_	_
24-54	4558-4559	-	_	_
24-55	4559-4560	:	_	_
24-56	4561-4562	\|	_	_
24-57	4563-4564	:	_	_
24-58	4564-4565	-	_	_
24-59	4565-4566	-	_	_
24-60	4566-4567	-	_	_
24-61	4567-4568	:	_	_
24-62	4569-4570	\|	_	_
24-63	4570-4571	:	_	_
24-64	4571-4572	-	_	_
24-65	4572-4573	-	_	_
24-66	4573-4574	-	_	_
24-67	4574-4575	-	_	_
24-68	4575-4576	-	_	_
24-69	4576-4577	-	_	_
24-70	4577-4578	-	_	_
24-71	4578-4579	-	_	_
24-72	4579-4580	-	_	_
24-73	4580-4581	-	_	_
24-74	4581-4582	-	_	_
24-75	4582-4583	-	_	_
24-76	4583-4584	-	_	_
24-77	4584-4585	-	_	_
24-78	4585-4586	-	_	_
24-79	4586-4587	-	_	_
24-80	4587-4588	-	_	_
24-81	4588-4589	-	_	_
24-82	4589-4590	-	_	_
24-83	4590-4591	-	_	_
24-84	4591-4592	-	_	_
24-85	4592-4593	-	_	_
24-86	4593-4594	-	_	_
24-87	4594-4595	-	_	_
24-88	4595-4596	-	_	_
24-89	4596-4597	-	_	_
24-90	4597-4598	-	_	_
24-91	4598-4599	-	_	_
24-92	4599-4600	-	_	_
24-93	4600-4601	-	_	_
24-94	4601-4602	-	_	_
24-95	4602-4603	-	_	_
24-96	4603-4604	-	_	_
24-97	4604-4605	-	_	_
24-98	4605-4606	-	_	_
24-99	4606-4607	-	_	_
24-100	4607-4608	-	_	_
24-101	4608-4609	-	_	_
24-102	4609-4610	-	_	_
24-103	4610-4611	-	_	_
24-104	4611-4612	:	_	_
24-105	4612-4613	\|	_	_
24-106	4613-4614	:	_	_
24-107	4614-4615	-	_	_
24-108	4615-4616	-	_	_
24-109	4616-4617	-	_	_
24-110	4617-4618	-	_	_
24-111	4618-4619	:	_	_
24-112	4619-4620	\|	_	_
24-113	4620-4621	:	_	_
24-114	4621-4622	-	_	_
24-115	4622-4623	-	_	_
24-116	4623-4624	-	_	_
24-117	4624-4625	-	_	_
24-118	4625-4626	:	_	_
24-119	4626-4627	\|	_	_
24-120	4627-4628	:	_	_
24-121	4628-4629	-	_	_
24-122	4629-4630	-	_	_
24-123	4630-4631	-	_	_
24-124	4631-4632	-	_	_
24-125	4632-4633	:	_	_
24-126	4633-4634	\|	_	_
24-127	4634-4635	:	_	_
24-128	4635-4636	-	_	_
24-129	4636-4637	-	_	_
24-130	4637-4638	-	_	_
24-131	4638-4639	-	_	_
24-132	4639-4640	-	_	_
24-133	4640-4641	-	_	_
24-134	4641-4642	-	_	_
24-135	4642-4643	-	_	_
24-136	4643-4644	-	_	_
24-137	4644-4645	:	_	_
24-138	4645-4646	\|	_	_
24-139	4647-4648	\|	_	_
24-140	4649-4652	SVM	_	_
24-141	4653-4654	\|	_	_
24-142	4655-4660	MOT17	_	_
24-143	4661-4662	\|	_	_
24-144	4663-4668	MOT17	_	_
24-145	4669-4679	half-train	_	_
24-146	4680-4681	+	_	_
24-147	4682-4692	crowdhuman	_	_
24-148	4693-4694	+	_	_
24-149	4695-4703	MOTSynth	_	_
24-150	4704-4705	\|	_	_
24-151	4706-4710	79.7	_	_
24-152	4711-4712	\|	_	_
24-153	4713-4717	68.1	_	_
24-154	4718-4719	\|	_	_
24-155	4720-4724	80.9	_	_
24-156	4725-4726	\|	_	_
24-157	4727-4728	\[	_	_
24-158	4728-4733	model	_	_
24-159	4733-4734	\]	_	_
24-160	4734-4735	(	_	_
24-161	4735-4740	https	_	_
24-162	4740-4741	:	_	_
24-163	4741-4742	/	_	_
24-164	4742-4743	/	_	_
24-165	4743-4753	github.com	_	_
24-166	4753-4754	/	_	_
24-167	4754-4760	yuzhms	_	_
24-168	4760-4761	/	_	_
24-169	4761-4782	Streaming-Video-Model	_	_
24-170	4782-4783	/	_	_
24-171	4783-4791	releases	_	_
24-172	4791-4792	/	_	_
24-173	4792-4800	download	_	_
24-174	4800-4801	/	_	_
24-175	4801-4805	v1.0	_	_
24-176	4805-4806	/	_	_
24-177	4806-4815	svm\_mot17	_	_
24-178	4815-4816	-	_	_
24-179	4816-4828	half-val.pth	_	_
24-180	4828-4829	)	_	_
24-181	4830-4831	\|	_	_
24-182	4833-4834	#	_	_
24-183	4834-4835	#	_	_
24-184	4836-4844	Citation	_	_
24-185	4845-4847	If	_	_
24-186	4848-4851	you	_	_
24-187	4852-4856	find	_	_
24-188	4857-4861	this	_	_
24-189	4862-4866	work	_	_
24-190	4867-4873	useful	_	_
24-191	4874-4876	in	_	_
24-192	4877-4881	your	_	_
24-193	4882-4890	research	_	_
24-194	4890-4891	,	_	_
24-195	4892-4898	please	_	_
24-196	4899-4907	consider	_	_
24-197	4908-4914	citing	*[622]	LICENSE[622]
24-198	4914-4915	:	*[622]	LICENSE[622]
24-199	4916-4917	`	*[622]	LICENSE[622]
24-200	4917-4918	`	*[622]	LICENSE[622]
24-201	4918-4919	`	*[622]	LICENSE[622]
24-202	4920-4921	@	*[622]	LICENSE[622]
24-203	4921-4934	InProceedings	*[622]	LICENSE[622]
24-204	4934-4935	{	*[622]	LICENSE[622]
24-205	4935-4939	Zhao	*[622]	LICENSE[622]
24-206	4939-4940	\_	*[622]	LICENSE[622]
24-207	4940-4944	2023	*[622]	LICENSE[622]
24-208	4944-4945	\_	*[622]	LICENSE[622]
24-209	4945-4949	CVPR	*[622]	LICENSE[622]
24-210	4949-4950	,	*[622]	LICENSE[622]
24-211	4955-4961	author	*[622]	LICENSE[622]
24-212	4965-4966	=	*[622]	LICENSE[622]
24-213	4967-4968	{	*[622]	LICENSE[622]
24-214	4968-4972	Zhao	*[622]	LICENSE[622]
24-215	4972-4973	,	*[622]	LICENSE[622]
24-216	4974-4981	Yucheng	*[622]	LICENSE[622]
24-217	4982-4985	and	*[622]	LICENSE[622]
24-218	4986-4989	Luo	*[622]	LICENSE[622]
24-219	4989-4990	,	*[622]	LICENSE[622]
24-220	4991-4996	Chong	*[622]	LICENSE[622]
24-221	4997-5000	and	*[622]	LICENSE[622]
24-222	5001-5005	Tang	*[622]	LICENSE[622]
24-223	5005-5006	,	*[622]	LICENSE[622]
24-224	5007-5015	Chuanxin	*[622]	LICENSE[622]
24-225	5016-5019	and	*[622]	LICENSE[622]
24-226	5020-5024	Chen	*[622]	LICENSE[622]
24-227	5024-5025	,	*[622]	LICENSE[622]
24-228	5026-5034	Dongdong	*[622]	LICENSE[622]
24-229	5035-5038	and	*[622]	LICENSE[622]
24-230	5039-5046	Codella	*[622]	LICENSE[622]
24-231	5046-5047	,	*[622]	LICENSE[622]
24-232	5048-5052	Noel	*[622]	LICENSE[622]
24-233	5053-5056	and	*[622]	LICENSE[622]
24-234	5057-5060	Zha	*[622]	LICENSE[622]
24-235	5060-5061	,	*[622]	LICENSE[622]
24-236	5062-5071	Zheng-Jun	*[622]	LICENSE[622]
24-237	5071-5072	}	*[622]	LICENSE[622]
24-238	5072-5073	,	*[622]	LICENSE[622]
24-239	5078-5083	title	*[622]	LICENSE[622]
24-240	5088-5089	=	*[622]	LICENSE[622]
24-241	5090-5091	{	*[622]	LICENSE[622]
24-242	5091-5100	Streaming	*[622]	LICENSE[622]
24-243	5101-5106	Video	*[622]	LICENSE[622]
24-244	5107-5112	Model	*[622]	LICENSE[622]
24-245	5112-5113	}	*[622]	LICENSE[622]
24-246	5113-5114	,	*[622]	LICENSE[622]
24-247	5119-5128	booktitle	*[622]	LICENSE[622]
24-248	5129-5130	=	*[622]	LICENSE[622]
24-249	5131-5132	{	*[622]	LICENSE[622]
24-250	5132-5143	Proceedings	*[622]	LICENSE[622]
24-251	5144-5146	of	*[622]	LICENSE[622]
24-252	5147-5150	the	*[622]	LICENSE[622]
24-253	5151-5155	IEEE	*[622]	LICENSE[622]
24-254	5155-5156	/	*[622]	LICENSE[622]
24-255	5156-5159	CVF	*[622]	LICENSE[622]
24-256	5160-5170	Conference	*[622]	LICENSE[622]
24-257	5171-5173	on	*[622]	LICENSE[622]
24-258	5174-5182	Computer	*[622]	LICENSE[622]
24-259	5183-5189	Vision	*[622]	LICENSE[622]
24-260	5190-5193	and	*[622]	LICENSE[622]
24-261	5194-5201	Pattern	*[622]	LICENSE[622]
24-262	5202-5213	Recognition	*[622]	LICENSE[622]
24-263	5214-5215	(	*[622]	LICENSE[622]
24-264	5215-5219	CVPR	*[622]	LICENSE[622]
24-265	5219-5220	)	*[622]	LICENSE[622]
24-266	5220-5221	}	*[622]	LICENSE[622]
24-267	5221-5222	,	*[622]	LICENSE[622]
24-268	5227-5232	month	*[622]	LICENSE[622]
24-269	5237-5238	=	*[622]	LICENSE[622]
24-270	5239-5240	{	*[622]	LICENSE[622]
24-271	5240-5244	June	*[622]	LICENSE[622]
24-272	5244-5245	}	*[622]	LICENSE[622]
24-273	5245-5246	,	*[622]	LICENSE[622]
24-274	5251-5255	year	_	_
24-275	5261-5262	=	_	_
24-276	5263-5264	{	_	_
24-277	5264-5268	2023	_	_
24-278	5268-5269	}	_	_
24-279	5269-5270	,	_	_
24-280	5275-5280	pages	_	_
24-281	5285-5286	=	_	_
24-282	5287-5288	{	_	_
24-283	5288-5293	14602	_	_
24-284	5293-5294	-	_	_
24-285	5294-5299	14612	_	_
24-286	5299-5300	}	_	_
24-287	5301-5302	}	_	_
24-288	5303-5304	`	_	_
24-289	5304-5305	`	_	_
24-290	5305-5306	`	_	_
24-291	5307-5308	#	_	_
24-292	5308-5309	#	_	_
24-293	5310-5325	Acknowledgement	_	_
24-294	5326-5329	Our	_	_
24-295	5330-5334	code	_	_
24-296	5335-5338	are	_	_
24-297	5339-5344	built	_	_
24-298	5345-5347	on	_	_
24-299	5348-5351	top	_	_
24-300	5352-5354	of	_	_
24-301	5355-5356	\[	_	_
24-302	5356-5366	MMTracking	_	_
24-303	5366-5367	\]	_	_
24-304	5367-5368	(	_	_
24-305	5368-5373	https	_	_
24-306	5373-5374	:	_	_
24-307	5374-5375	/	_	_
24-308	5375-5376	/	_	_
24-309	5376-5386	github.com	_	_
24-310	5386-5387	/	_	_
24-311	5387-5397	open-mmlab	_	_
24-312	5397-5398	/	_	_
24-313	5398-5408	mmtracking	_	_
24-314	5408-5409	/	_	_
24-315	5409-5410	)	_	_
24-316	5411-5414	and	_	_
24-317	5415-5416	\[	_	_
24-318	5416-5420	CLIP	_	_
24-319	5420-5421	\]	_	_
24-320	5421-5422	(	_	_
24-321	5422-5427	https	_	_
24-322	5427-5428	:	_	_
24-323	5428-5429	/	_	_
24-324	5429-5430	/	_	_
24-325	5430-5440	github.com	_	_
24-326	5440-5441	/	_	_
24-327	5441-5447	openai	_	_
24-328	5447-5448	/	_	_
24-329	5448-5452	CLIP	_	_
24-330	5452-5453	)	_	_
24-331	5453-5454	.	_	_

#Text=Many thanks for their wonderful works.
25-1	5455-5459	Many	_	_
25-2	5460-5466	thanks	_	_
25-3	5467-5470	for	*[550]	PUBLICATION[550]
25-4	5471-5476	their	*[550]	PUBLICATION[550]
25-5	5477-5486	wonderful	_	_
25-6	5487-5492	works	_	_
25-7	5492-5493	.	_	_