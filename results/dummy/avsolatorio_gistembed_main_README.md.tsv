#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# GISTEmbed  The GISTEmbed framework (Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning) introduces an innovative approach to dynamically mine training negatives within a batch, serving as contrastive samples for fine-tuning embedding models.
1-1	0-1	#	_	_
1-2	2-11	GISTEmbed	_	_
1-3	13-16	The	_	_
1-4	17-26	GISTEmbed	_	_
1-5	27-36	framework	_	_
1-6	37-38	(	_	_
1-7	38-44	Guided	_	_
1-8	45-54	In-sample	_	_
1-9	55-64	Selection	_	_
1-10	65-67	of	_	_
1-11	68-76	Training	_	_
1-12	77-86	Negatives	_	_
1-13	87-90	for	_	_
1-14	91-95	Text	_	_
1-15	96-105	Embedding	_	_
1-16	106-117	Fine-tuning	_	_
1-17	117-118	)	_	_
1-18	119-129	introduces	_	_
1-19	130-132	an	_	_
1-20	133-143	innovative	_	_
1-21	144-152	approach	*[299]	ONTOLOGY[299]
1-22	153-155	to	*[299]	ONTOLOGY[299]
1-23	156-167	dynamically	*[299]	ONTOLOGY[299]
1-24	168-172	mine	_	_
1-25	173-181	training	_	_
1-26	182-191	negatives	_	_
1-27	192-198	within	_	_
1-28	199-200	a	_	_
1-29	201-206	batch	_	_
1-30	206-207	,	_	_
1-31	208-215	serving	_	_
1-32	216-218	as	_	_
1-33	219-230	contrastive	_	_
1-34	231-238	samples	_	_
1-35	239-242	for	_	_
1-36	243-254	fine-tuning	_	_
1-37	255-264	embedding	_	_
1-38	265-271	models	_	_
1-39	271-272	.	_	_

#Text=At the core of GISTEmbed is the utilization of a guide model, which assesses the relevance of samples in the batch against a query-positive pair.
2-1	273-275	At	_	_
2-2	276-279	the	_	_
2-3	280-284	core	_	_
2-4	285-287	of	_	_
2-5	288-297	GISTEmbed	_	_
2-6	298-300	is	_	_
2-7	301-304	the	_	_
2-8	305-316	utilization	_	_
2-9	317-319	of	_	_
2-10	320-321	a	_	_
2-11	322-327	guide	_	_
2-12	328-333	model	_	_
2-13	333-334	,	_	_
2-14	335-340	which	_	_
2-15	341-349	assesses	_	_
2-16	350-353	the	*[258]	EVALMETRIC[258]
2-17	354-363	relevance	*[258]	EVALMETRIC[258]
2-18	364-366	of	*[258]	EVALMETRIC[258]
2-19	367-374	samples	_	_
2-20	375-377	in	_	_
2-21	378-381	the	_	_
2-22	382-387	batch	_	_
2-23	388-395	against	_	_
2-24	396-397	a	_	_
2-25	398-412	query-positive	_	_
2-26	413-417	pair	_	_
2-27	417-418	.	_	_

#Text=This model ensures that only examples deemed irrelevant are selected as training negatives.
3-1	419-423	This	_	_
3-2	424-429	model	_	_
3-3	430-437	ensures	_	_
3-4	438-442	that	_	_
3-5	443-447	only	_	_
3-6	448-456	examples	_	_
3-7	457-463	deemed	_	_
3-8	464-474	irrelevant	_	_
3-9	475-478	are	_	_
3-10	479-487	selected	_	_
3-11	488-490	as	_	_
3-12	491-499	training	_	_
3-13	500-509	negatives	_	_
3-14	509-510	.	_	_

#Text=This methodology is particularly advantageous for fine-tuning smaller models, leading to notable improvements across a wide range of NLP tasks.
4-1	512-516	This	_	_
4-2	517-528	methodology	_	_
4-3	529-531	is	_	_
4-4	532-544	particularly	_	_
4-5	545-557	advantageous	_	_
4-6	558-561	for	_	_
4-7	562-573	fine-tuning	_	_
4-8	574-581	smaller	_	_
4-9	582-588	models	_	_
4-10	588-589	,	_	_
4-11	590-597	leading	_	_
4-12	598-600	to	_	_
4-13	601-608	notable	_	_
4-14	609-621	improvements	_	_
4-15	622-628	across	_	_
4-16	629-630	a	_	_
4-17	631-635	wide	_	_
4-18	636-641	range	*[273]	PROGLANG[273]
4-19	642-644	of	*[273]	PROGLANG[273]
4-20	645-648	NLP	*[273]	PROGLANG[273]
4-21	649-654	tasks	*[273]	PROGLANG[273]
4-22	654-655	.	_	_

#Text=By focusing on the in-sample selection of negatives, GISTEmbed addresses common challenges in contrastive learning, such as the efficient and effective identification of informative negative samples.
5-1	656-658	By	_	_
5-2	659-667	focusing	_	_
5-3	668-670	on	_	_
5-4	671-674	the	_	_
5-5	675-684	in-sample	_	_
5-6	685-694	selection	_	_
5-7	695-697	of	_	_
5-8	698-707	negatives	_	_
5-9	707-708	,	_	_
5-10	709-718	GISTEmbed	_	_
5-11	719-728	addresses	_	_
5-12	729-735	common	_	_
5-13	736-746	challenges	_	_
5-14	747-749	in	_	_
5-15	750-761	contrastive	_	_
5-16	762-770	learning	_	_
5-17	770-771	,	_	_
5-18	772-776	such	_	_
5-19	777-779	as	_	_
5-20	780-783	the	_	_
5-21	784-793	efficient	_	_
5-22	794-797	and	_	_
5-23	798-807	effective	_	_
5-24	808-822	identification	_	_
5-25	823-825	of	_	_
5-26	826-837	informative	_	_
5-27	838-846	negative	_	_
5-28	847-854	samples	_	_
5-29	854-855	.	_	_

#Text=Compared to traditional methods, which often rely on random or heuristic-based selection, GISTEmbed's guided approach ensures a higher quality of training negatives, contributing to more robust and generalizable embeddings.
6-1	857-865	Compared	_	_
6-2	866-868	to	_	_
6-3	869-880	traditional	_	_
6-4	881-888	methods	_	_
6-5	888-889	,	_	_
6-6	890-895	which	_	_
6-7	896-901	often	_	_
6-8	902-906	rely	_	_
6-9	907-909	on	_	_
6-10	910-916	random	_	_
6-11	917-919	or	_	_
6-12	920-935	heuristic-based	_	_
6-13	936-945	selection	_	_
6-14	945-946	,	_	_
6-15	947-958	GISTEmbed's	_	_
6-16	959-965	guided	_	_
6-17	966-974	approach	_	_
6-18	975-982	ensures	_	_
6-19	983-984	a	_	_
6-20	985-991	higher	_	_
6-21	992-999	quality	_	_
6-22	1000-1002	of	*[244]	LICENSE[244]
6-23	1003-1011	training	*[244]	LICENSE[244]
6-24	1012-1021	negatives	*[244]	LICENSE[244]
6-25	1021-1022	,	*[244]	LICENSE[244]
6-26	1023-1035	contributing	*[244]	LICENSE[244]
6-27	1036-1038	to	*[244]	LICENSE[244]
6-28	1039-1043	more	*[244]	LICENSE[244]
6-29	1044-1050	robust	*[244]	LICENSE[244]
6-30	1051-1054	and	*[244]	LICENSE[244]
6-31	1055-1068	generalizable	_	_
6-32	1069-1079	embeddings	_	_
6-33	1079-1080	.	_	_

#Text=<br> <br> <p align="center"> <img src="https://github.com/avsolatorio/GISTEmbed/raw/main/img/GISTEmbed%20Model.png" style="width:75%"/> </p> <p align="center"> <strong>GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning</strong> <br> <a href="https://arxiv.org/abs/2402.16829" target="\_blank">Paper on ArXiv</a> </p> <br>   The model does not require any instruction for generating embeddings.
7-1	1082-1083	<	_	_
7-2	1083-1085	br	_	_
7-3	1085-1086	>	_	_
7-4	1087-1088	<	_	_
7-5	1088-1090	br	_	_
7-6	1090-1091	>	_	_
7-7	1092-1093	<	_	_
7-8	1093-1094	p	_	_
7-9	1095-1100	align	_	_
7-10	1100-1101	=	_	_
7-11	1101-1102	"	_	_
7-12	1102-1108	center	_	_
7-13	1108-1109	"	_	_
7-14	1109-1110	>	_	_
7-15	1111-1112	<	_	_
7-16	1112-1115	img	_	_
7-17	1116-1119	src	_	_
7-18	1119-1120	=	_	_
7-19	1120-1121	"	_	_
7-20	1121-1126	https	_	_
7-21	1126-1127	:	_	_
7-22	1127-1128	/	_	_
7-23	1128-1129	/	_	_
7-24	1129-1139	github.com	_	_
7-25	1139-1140	/	_	_
7-26	1140-1151	avsolatorio	_	_
7-27	1151-1152	/	_	_
7-28	1152-1161	GISTEmbed	_	_
7-29	1161-1162	/	_	_
7-30	1162-1165	raw	_	_
7-31	1165-1166	/	_	_
7-32	1166-1170	main	_	_
7-33	1170-1171	/	_	_
7-34	1171-1174	img	_	_
7-35	1174-1175	/	_	_
7-36	1175-1184	GISTEmbed	_	_
7-37	1184-1185	%	_	_
7-38	1185-1196	20Model.png	_	_
7-39	1196-1197	"	_	_
7-40	1198-1203	style	_	_
7-41	1203-1204	=	_	_
7-42	1204-1205	"	_	_
7-43	1205-1210	width	_	_
7-44	1210-1211	:	_	_
7-45	1211-1214	75%	_	_
7-46	1214-1215	"	_	_
7-47	1215-1216	/	_	_
7-48	1216-1217	>	_	_
7-49	1218-1219	<	_	_
7-50	1219-1220	/	_	_
7-51	1220-1221	p	_	_
7-52	1221-1222	>	_	_
7-53	1223-1224	<	_	_
7-54	1224-1225	p	_	_
7-55	1226-1231	align	_	_
7-56	1231-1232	=	_	_
7-57	1232-1233	"	_	_
7-58	1233-1239	center	_	_
7-59	1239-1240	"	_	_
7-60	1240-1241	>	_	_
7-61	1242-1243	<	_	_
7-62	1243-1249	strong	_	_
7-63	1249-1250	>	_	_
7-64	1250-1259	GISTEmbed	_	_
7-65	1259-1260	:	_	_
7-66	1261-1267	Guided	_	_
7-67	1268-1277	In-sample	_	_
7-68	1278-1287	Selection	_	_
7-69	1288-1290	of	_	_
7-70	1291-1299	Training	_	_
7-71	1300-1309	Negatives	_	_
7-72	1310-1313	for	_	_
7-73	1314-1318	Text	_	_
7-74	1319-1328	Embedding	_	_
7-75	1329-1340	Fine-tuning	_	_
7-76	1340-1341	<	_	_
7-77	1341-1342	/	_	_
7-78	1342-1348	strong	_	_
7-79	1348-1349	>	_	_
7-80	1350-1351	<	_	_
7-81	1351-1353	br	_	_
7-82	1353-1354	>	_	_
7-83	1355-1356	<	_	_
7-84	1356-1357	a	_	_
7-85	1358-1362	href	_	_
7-86	1362-1363	=	_	_
7-87	1363-1364	"	_	_
7-88	1364-1369	https	_	_
7-89	1369-1370	:	_	_
7-90	1370-1371	/	_	_
7-91	1371-1372	/	_	_
7-92	1372-1381	arxiv.org	_	_
7-93	1381-1382	/	*[292]	SOFTWARE[292]
7-94	1382-1385	abs	*[292]	SOFTWARE[292]
7-95	1385-1386	/	*[292]	SOFTWARE[292]
7-96	1386-1396	2402.16829	*[292]	SOFTWARE[292]
7-97	1396-1397	"	*[292]	SOFTWARE[292]
7-98	1398-1404	target	*[292]	SOFTWARE[292]
7-99	1404-1405	=	*[292]	SOFTWARE[292]
7-100	1405-1406	"	*[292]	SOFTWARE[292]
7-101	1406-1407	\_	*[292]	SOFTWARE[292]
7-102	1407-1412	blank	*[292]	SOFTWARE[292]
7-103	1412-1413	"	*[292]	SOFTWARE[292]
7-104	1413-1414	>	*[292]	SOFTWARE[292]
7-105	1414-1419	Paper	*[292]	SOFTWARE[292]
7-106	1420-1422	on	*[292]	SOFTWARE[292]
7-107	1423-1428	ArXiv	*[292]	SOFTWARE[292]
7-108	1428-1429	<	*[292]	SOFTWARE[292]
7-109	1429-1430	/	*[292]	SOFTWARE[292]
7-110	1430-1431	a	*[292]	SOFTWARE[292]
7-111	1431-1432	>	*[292]	SOFTWARE[292]
7-112	1433-1434	<	*[292]	SOFTWARE[292]
7-113	1434-1435	/	*[292]	SOFTWARE[292]
7-114	1435-1436	p	*[292]	SOFTWARE[292]
7-115	1436-1437	>	*[292]	SOFTWARE[292]
7-116	1438-1439	<	*[292]	SOFTWARE[292]
7-117	1439-1441	br	*[292]	SOFTWARE[292]
7-118	1441-1442	>	*[292]	SOFTWARE[292]
7-119	1445-1448	The	*[292]	SOFTWARE[292]
7-120	1449-1454	model	*[292]	SOFTWARE[292]
7-121	1455-1459	does	*[292]	SOFTWARE[292]
7-122	1460-1463	not	*[292]	SOFTWARE[292]
7-123	1464-1471	require	*[292]	SOFTWARE[292]
7-124	1472-1475	any	*[292]	SOFTWARE[292]
7-125	1476-1487	instruction	*[292]	SOFTWARE[292]
7-126	1488-1491	for	*[292]	SOFTWARE[292]
7-127	1492-1502	generating	_	_
7-128	1503-1513	embeddings	_	_
7-129	1513-1514	.	_	_

#Text=This means that queries for retrieval tasks can be directly encoded without crafting instructions
8-1	1515-1519	This	_	_
8-2	1520-1525	means	_	_
8-3	1526-1530	that	_	_
8-4	1531-1538	queries	_	_
8-5	1539-1542	for	_	_
8-6	1543-1552	retrieval	_	_
8-7	1553-1558	tasks	_	_
8-8	1559-1562	can	_	_
8-9	1563-1565	be	_	_
8-10	1566-1574	directly	_	_
8-11	1575-1582	encoded	_	_
8-12	1583-1590	without	_	_
8-13	1591-1599	crafting	_	_
8-14	1600-1612	instructions	_	_

#Text=.
9-1	1612-1613	.	_	_

#Text=# Trained models  We have fine-tuned various models using the GISTEmbed framework.
10-1	1615-1616	#	_	_
10-2	1617-1624	Trained	_	_
10-3	1625-1631	models	_	_
10-4	1633-1635	We	_	_
10-5	1636-1640	have	_	_
10-6	1641-1651	fine-tuned	*[294]	SOFTWARE[294]
10-7	1652-1659	various	*[294]	SOFTWARE[294]
10-8	1660-1666	models	*[294]	SOFTWARE[294]
10-9	1667-1672	using	*[294]	SOFTWARE[294]
10-10	1673-1676	the	_	_
10-11	1677-1686	GISTEmbed	_	_
10-12	1687-1696	framework	_	_
10-13	1696-1697	.	_	_

#Text=The models are available on the Hugging Face model hub:  - \[avsolatorio/GIST-large-Embedding-v0\](https://huggingface.co/avsolatorio/GIST-large-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.
11-1	1698-1701	The	_	_
11-2	1702-1708	models	_	_
11-3	1709-1712	are	_	_
11-4	1713-1722	available	_	_
11-5	1723-1725	on	_	_
11-6	1726-1729	the	_	_
11-7	1730-1737	Hugging	_	_
11-8	1738-1742	Face	_	_
11-9	1743-1748	model	_	_
11-10	1749-1752	hub	_	_
11-11	1752-1753	:	_	_
11-12	1755-1756	-	_	_
11-13	1757-1758	\[	_	_
11-14	1758-1769	avsolatorio	_	_
11-15	1769-1770	/	_	_
11-16	1770-1793	GIST-large-Embedding-v0	_	_
11-17	1793-1794	\]	_	_
11-18	1794-1795	(	_	_
11-19	1795-1800	https	_	_
11-20	1800-1801	:	_	_
11-21	1801-1802	/	_	_
11-22	1802-1803	/	_	_
11-23	1803-1817	huggingface.co	_	_
11-24	1817-1818	/	_	_
11-25	1818-1829	avsolatorio	_	_
11-26	1829-1830	/	_	_
11-27	1830-1853	GIST-large-Embedding-v0	_	_
11-28	1853-1854	)	_	_
11-29	1854-1855	:	_	_
11-30	1856-1859	The	_	_
11-31	1860-1865	model	_	_
11-32	1866-1876	fine-tuned	_	_
11-33	1877-1882	using	_	_
11-34	1883-1886	the	_	_
11-35	1887-1896	GISTEmbed	_	_
11-36	1897-1906	framework	*[245]	LICENSE[245]
11-37	1907-1910	and	*[245]	LICENSE[245]
11-38	1911-1914	the	*[245]	LICENSE[245]
11-39	1915-1919	MEDI	*[245]	LICENSE[245]
11-40	1919-1920	+	*[245]	LICENSE[245]
11-41	1920-1927	MTEBcls	_	_
11-42	1928-1935	dataset	_	_
11-43	1935-1936	.	_	_

#Text=The base model used is the \[`BAAI/bge-large-en-v1.5`\](https://huggingface.co/BAAI/bge-large-en-v1.5). - \[avsolatorio/GIST-Embedding-v0\](https://huggingface.co/avsolatorio/GIST-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.
12-1	1937-1940	The	_	_
12-2	1941-1945	base	_	_
12-3	1946-1951	model	_	_
12-4	1952-1956	used	_	_
12-5	1957-1959	is	_	_
12-6	1960-1963	the	_	_
12-7	1964-1965	\[	_	_
12-8	1965-1966	`	_	_
12-9	1966-1970	BAAI	_	_
12-10	1970-1971	/	_	_
12-11	1971-1988	bge-large-en-v1.5	_	_
12-12	1988-1989	`	*[274]	PROGLANG[274]
12-13	1989-1990	\]	*[274]	PROGLANG[274]
12-14	1990-1991	(	*[274]	PROGLANG[274]
12-15	1991-1996	https	*[274]	PROGLANG[274]
12-16	1996-1997	:	*[274]	PROGLANG[274]
12-17	1997-1998	/	*[274]	PROGLANG[274]
12-18	1998-1999	/	*[274]	PROGLANG[274]
12-19	1999-2013	huggingface.co	*[274]	PROGLANG[274]
12-20	2013-2014	/	*[274]	PROGLANG[274]
12-21	2014-2018	BAAI	*[274]	PROGLANG[274]
12-22	2018-2019	/	*[274]	PROGLANG[274]
12-23	2019-2036	bge-large-en-v1.5	*[274]	PROGLANG[274]
12-24	2036-2037	)	*[274]	PROGLANG[274]
12-25	2037-2038	.	*[274]	PROGLANG[274]
12-26	2039-2040	-	*[274]	PROGLANG[274]
12-27	2041-2042	\[	*[274]	PROGLANG[274]
12-28	2042-2053	avsolatorio	*[274]	PROGLANG[274]
12-29	2053-2054	/	*[274]	PROGLANG[274]
12-30	2054-2071	GIST-Embedding-v0	*[274]	PROGLANG[274]
12-31	2071-2072	\]	*[274]	PROGLANG[274]
12-32	2072-2073	(	*[274]	PROGLANG[274]
12-33	2073-2078	https	*[274]	PROGLANG[274]
12-34	2078-2079	:	*[274]	PROGLANG[274]
12-35	2079-2080	/	*[274]	PROGLANG[274]
12-36	2080-2081	/	*[274]	PROGLANG[274]
12-37	2081-2095	huggingface.co	*[274]	PROGLANG[274]
12-38	2095-2096	/	*[274]	PROGLANG[274]
12-39	2096-2107	avsolatorio	*[274]	PROGLANG[274]
12-40	2107-2108	/	*[274]	PROGLANG[274]
12-41	2108-2125	GIST-Embedding-v0	*[274]	PROGLANG[274]
12-42	2125-2126	)	*[274]	PROGLANG[274]
12-43	2126-2127	:	*[274]	PROGLANG[274]
12-44	2128-2131	The	*[274]	PROGLANG[274]
12-45	2132-2137	model	*[274]	PROGLANG[274]
12-46	2138-2148	fine-tuned	*[274]	PROGLANG[274]
12-47	2149-2154	using	*[274]	PROGLANG[274]
12-48	2155-2158	the	*[274]	PROGLANG[274]
12-49	2159-2168	GISTEmbed	*[274]	PROGLANG[274]
12-50	2169-2178	framework	*[274]	PROGLANG[274]
12-51	2179-2182	and	_	_
12-52	2183-2186	the	_	_
12-53	2187-2191	MEDI	_	_
12-54	2191-2192	+	_	_
12-55	2192-2199	MTEBcls	_	_
12-56	2200-2207	dataset	_	_
12-57	2207-2208	.	_	_

#Text=The base model used is the \[`BAAI/bge-base-en-v1.5`\](https://huggingface.co/BAAI/bge-base-en-v1.5). - \[avsolatorio/GIST-small-Embedding-v0\](https://huggingface.co/avsolatorio/GIST-small-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.
13-1	2209-2212	The	_	_
13-2	2213-2217	base	_	_
13-3	2218-2223	model	_	_
13-4	2224-2228	used	_	_
13-5	2229-2231	is	_	_
13-6	2232-2235	the	_	_
13-7	2236-2237	\[	_	_
13-8	2237-2238	`	_	_
13-9	2238-2242	BAAI	_	_
13-10	2242-2243	/	_	_
13-11	2243-2259	bge-base-en-v1.5	_	_
13-12	2259-2260	`	_	_
13-13	2260-2261	\]	_	_
13-14	2261-2262	(	_	_
13-15	2262-2267	https	_	_
13-16	2267-2268	:	_	_
13-17	2268-2269	/	_	_
13-18	2269-2270	/	_	_
13-19	2270-2284	huggingface.co	_	_
13-20	2284-2285	/	_	_
13-21	2285-2289	BAAI	_	_
13-22	2289-2290	/	_	_
13-23	2290-2306	bge-base-en-v1.5	_	_
13-24	2306-2307	)	_	_
13-25	2307-2308	.	_	_
13-26	2309-2310	-	_	_
13-27	2311-2312	\[	_	_
13-28	2312-2323	avsolatorio	_	_
13-29	2323-2324	/	_	_
13-30	2324-2347	GIST-small-Embedding-v0	_	_
13-31	2347-2348	\]	_	_
13-32	2348-2349	(	_	_
13-33	2349-2354	https	_	_
13-34	2354-2355	:	_	_
13-35	2355-2356	/	_	_
13-36	2356-2357	/	_	_
13-37	2357-2371	huggingface.co	_	_
13-38	2371-2372	/	_	_
13-39	2372-2383	avsolatorio	_	_
13-40	2383-2384	/	_	_
13-41	2384-2407	GIST-small-Embedding-v0	_	_
13-42	2407-2408	)	_	_
13-43	2408-2409	:	_	_
13-44	2410-2413	The	_	_
13-45	2414-2419	model	_	_
13-46	2420-2430	fine-tuned	_	_
13-47	2431-2436	using	_	_
13-48	2437-2440	the	_	_
13-49	2441-2450	GISTEmbed	_	_
13-50	2451-2460	framework	_	_
13-51	2461-2464	and	*[275]	PROGLANG[275]
13-52	2465-2468	the	*[275]	PROGLANG[275]
13-53	2469-2473	MEDI	*[275]	PROGLANG[275]
13-54	2473-2474	+	*[275]	PROGLANG[275]
13-55	2474-2481	MTEBcls	_	_
13-56	2482-2489	dataset	_	_
13-57	2489-2490	.	_	_

#Text=The base model used is the \[`BAAI/bge-small-en-v1.5`\](https://huggingface.co/BAAI/bge-small-en-v1.5). - \[avsolatorio/GIST-all-MiniLM-L6-v2\](https://huggingface.co/avsolatorio/GIST-all-MiniLM-L6-v2): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.
14-1	2491-2494	The	_	_
14-2	2495-2499	base	_	_
14-3	2500-2505	model	_	_
14-4	2506-2510	used	_	_
14-5	2511-2513	is	_	_
14-6	2514-2517	the	_	_
14-7	2518-2519	\[	_	_
14-8	2519-2520	`	_	_
14-9	2520-2524	BAAI	_	_
14-10	2524-2525	/	_	_
14-11	2525-2542	bge-small-en-v1.5	_	_
14-12	2542-2543	`	_	_
14-13	2543-2544	\]	_	_
14-14	2544-2545	(	_	_
14-15	2545-2550	https	_	_
14-16	2550-2551	:	_	_
14-17	2551-2552	/	_	_
14-18	2552-2553	/	_	_
14-19	2553-2567	huggingface.co	_	_
14-20	2567-2568	/	_	_
14-21	2568-2572	BAAI	_	_
14-22	2572-2573	/	_	_
14-23	2573-2590	bge-small-en-v1.5	_	_
14-24	2590-2591	)	_	_
14-25	2591-2592	.	_	_
14-26	2593-2594	-	_	_
14-27	2595-2596	\[	*[246]	LICENSE[246]
14-28	2596-2607	avsolatorio	*[246]	LICENSE[246]
14-29	2607-2608	/	*[246]	LICENSE[246]
14-30	2608-2626	GIST-all-MiniLM-L6	*[246]	LICENSE[246]
14-31	2626-2627	-	*[246]	LICENSE[246]
14-32	2627-2629	v2	*[246]	LICENSE[246]
14-33	2629-2630	\]	*[246]	LICENSE[246]
14-34	2630-2631	(	*[246]	LICENSE[246]
14-35	2631-2636	https	*[246]	LICENSE[246]
14-36	2636-2637	:	*[246]	LICENSE[246]
14-37	2637-2638	/	*[246]	LICENSE[246]
14-38	2638-2639	/	*[246]	LICENSE[246]
14-39	2639-2653	huggingface.co	*[246]	LICENSE[246]
14-40	2653-2654	/	*[246]	LICENSE[246]
14-41	2654-2665	avsolatorio	*[246]	LICENSE[246]
14-42	2665-2666	/	*[246]	LICENSE[246]
14-43	2666-2684	GIST-all-MiniLM-L6	*[246]	LICENSE[246]
14-44	2684-2685	-	*[246]	LICENSE[246]
14-45	2685-2687	v2	*[246]	LICENSE[246]
14-46	2687-2688	)	*[246]	LICENSE[246]
14-47	2688-2689	:	*[246]	LICENSE[246]
14-48	2690-2693	The	*[246]	LICENSE[246]
14-49	2694-2699	model	*[246]	LICENSE[246]
14-50	2700-2710	fine-tuned	_	_
14-51	2711-2716	using	_	_
14-52	2717-2720	the	_	_
14-53	2721-2730	GISTEmbed	_	_
14-54	2731-2740	framework	_	_
14-55	2741-2744	and	_	_
14-56	2745-2748	the	_	_
14-57	2749-2753	MEDI	_	_
14-58	2753-2754	+	_	_
14-59	2754-2761	MTEBcls	_	_
14-60	2762-2769	dataset	_	_
14-61	2769-2770	.	_	_

#Text=The base model used is the \[`sentence-transformers/all-MiniLM-L6-v2`\](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
15-1	2771-2774	The	_	_
15-2	2775-2779	base	_	_
15-3	2780-2785	model	_	_
15-4	2786-2790	used	_	_
15-5	2791-2793	is	_	_
15-6	2794-2797	the	_	_
15-7	2798-2799	\[	_	_
15-8	2799-2800	`	_	_
15-9	2800-2821	sentence-transformers	_	_
15-10	2821-2822	/	*[260]	EVALMETRIC[260]
15-11	2822-2835	all-MiniLM-L6	_	_
15-12	2835-2836	-	_	_
15-13	2836-2838	v2	_	_
15-14	2838-2839	`	_	_
15-15	2839-2840	\]	_	_
15-16	2840-2841	(	_	_
15-17	2841-2846	https	_	_
15-18	2846-2847	:	_	_
15-19	2847-2848	/	_	_
15-20	2848-2849	/	_	_
15-21	2849-2863	huggingface.co	_	_
15-22	2863-2864	/	_	_
15-23	2864-2885	sentence-transformers	_	_
15-24	2885-2886	/	_	_
15-25	2886-2899	all-MiniLM-L6	_	_
15-26	2899-2900	-	_	_
15-27	2900-2902	v2	_	_
15-28	2902-2903	)	_	_

#Text=.
16-1	2903-2904	.	_	_

#Text=# Data  The dataset used is a compilation of the MEDI dataset and the MTEB Classification training dataset.
17-1	2907-2908	#	_	_
17-2	2909-2913	Data	_	_
17-3	2915-2918	The	_	_
17-4	2919-2926	dataset	_	_
17-5	2927-2931	used	_	_
17-6	2932-2934	is	_	_
17-7	2935-2936	a	_	_
17-8	2937-2948	compilation	_	_
17-9	2949-2951	of	_	_
17-10	2952-2955	the	_	_
17-11	2956-2960	MEDI	_	_
17-12	2961-2968	dataset	_	_
17-13	2969-2972	and	_	_
17-14	2973-2976	the	_	_
17-15	2977-2981	MTEB	_	_
17-16	2982-2996	Classification	_	_
17-17	2997-3005	training	_	_
17-18	3006-3013	dataset	*[235]	PUBLICATION[235]
17-19	3013-3014	.	_	_

#Text=Third-party datasets may be subject to additional terms and conditions under their associated licenses.
18-1	3015-3026	Third-party	_	_
18-2	3027-3035	datasets	_	_
18-3	3036-3039	may	_	_
18-4	3040-3042	be	*[261]	PROJECT[261]
18-5	3043-3050	subject	*[261]	PROJECT[261]
18-6	3051-3053	to	*[261]	PROJECT[261]
18-7	3054-3064	additional	*[261]	PROJECT[261]
18-8	3065-3070	terms	*[261]	PROJECT[261]
18-9	3071-3074	and	*[261]	PROJECT[261]
18-10	3075-3085	conditions	*[261]	PROJECT[261]
18-11	3086-3091	under	*[261]	PROJECT[261]
18-12	3092-3097	their	*[261]	PROJECT[261]
18-13	3098-3108	associated	_	_
18-14	3109-3117	licenses	_	_
18-15	3117-3118	.	_	_

#Text=A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:  - Dataset: \[avsolatorio/medi-data-mteb\_avs\_triplets\](https://huggingface.co/datasets/avsolatorio/medi-data-mteb\_avs\_triplets) - Revision: 238a0499b6e6b690cc64ea56fde8461daa8341bb  The dataset contains a `task\_type` key which can be used to select only the mteb classification tasks (prefixed with `mteb\_`).
19-1	3119-3120	A	_	_
19-2	3121-3132	HuggingFace	_	_
19-3	3133-3140	Dataset	_	_
19-4	3141-3148	version	_	_
19-5	3149-3151	of	_	_
19-6	3152-3155	the	_	_
19-7	3156-3164	compiled	_	_
19-8	3165-3172	dataset	_	_
19-9	3172-3173	,	_	_
19-10	3174-3177	and	_	_
19-11	3178-3181	the	_	_
19-12	3182-3190	specific	_	_
19-13	3191-3199	revision	_	_
19-14	3200-3204	used	_	_
19-15	3205-3207	to	_	_
19-16	3208-3213	train	_	_
19-17	3214-3217	the	_	_
19-18	3218-3223	model	_	_
19-19	3223-3224	,	_	_
19-20	3225-3227	is	_	_
19-21	3228-3237	available	_	_
19-22	3237-3238	:	_	_
19-23	3240-3241	-	_	_
19-24	3242-3249	Dataset	_	_
19-25	3249-3250	:	_	_
19-26	3251-3252	\[	_	_
19-27	3252-3263	avsolatorio	_	_
19-28	3263-3264	/	_	_
19-29	3264-3291	medi-data-mteb\_avs\_triplets	_	_
19-30	3291-3292	\]	_	_
19-31	3292-3293	(	_	_
19-32	3293-3298	https	_	_
19-33	3298-3299	:	_	_
19-34	3299-3300	/	_	_
19-35	3300-3301	/	_	_
19-36	3301-3315	huggingface.co	_	_
19-37	3315-3316	/	_	_
19-38	3316-3324	datasets	_	_
19-39	3324-3325	/	_	_
19-40	3325-3336	avsolatorio	_	_
19-41	3336-3337	/	_	_
19-42	3337-3364	medi-data-mteb\_avs\_triplets	_	_
19-43	3364-3365	)	*[261]	EVALMETRIC[261]
19-44	3366-3367	-	*[261]	EVALMETRIC[261]
19-45	3368-3376	Revision	*[261]	EVALMETRIC[261]
19-46	3376-3377	:	*[261]	EVALMETRIC[261]
19-47	3378-3418	238a0499b6e6b690cc64ea56fde8461daa8341bb	*[261]	EVALMETRIC[261]
19-48	3420-3423	The	*[261]	EVALMETRIC[261]
19-49	3424-3431	dataset	*[261]	EVALMETRIC[261]
19-50	3432-3440	contains	*[261]	EVALMETRIC[261]
19-51	3441-3442	a	*[261]	EVALMETRIC[261]
19-52	3443-3444	`	*[261]	EVALMETRIC[261]
19-53	3444-3453	task\_type	*[261]	EVALMETRIC[261]
19-54	3453-3454	`	*[261]	EVALMETRIC[261]
19-55	3455-3458	key	*[261]	EVALMETRIC[261]
19-56	3459-3464	which	_	_
19-57	3465-3468	can	_	_
19-58	3469-3471	be	_	_
19-59	3472-3476	used	_	_
19-60	3477-3479	to	_	_
19-61	3480-3486	select	_	_
19-62	3487-3491	only	_	_
19-63	3492-3495	the	_	_
19-64	3496-3500	mteb	_	_
19-65	3501-3515	classification	_	_
19-66	3516-3521	tasks	_	_
19-67	3522-3523	(	_	_
19-68	3523-3531	prefixed	_	_
19-69	3532-3536	with	_	_
19-70	3537-3538	`	_	_
19-71	3538-3542	mteb	_	_
19-72	3542-3543	\_	_	_
19-73	3543-3544	`	_	_
19-74	3544-3545	)	_	_
19-75	3545-3546	.	_	_

#Text=The \*\*MEDI Dataset\*\* is published in the following paper: \[One Embedder, Any Task: Instruction-Finetuned Text Embeddings\](https://arxiv.org/abs/2212.09741).
20-1	3548-3551	The	_	_
20-2	3552-3553	\*	_	_
20-3	3553-3554	\*	_	_
20-4	3554-3558	MEDI	_	_
20-5	3559-3566	Dataset	_	_
20-6	3566-3567	\*	_	_
20-7	3567-3568	\*	_	_
20-8	3569-3571	is	_	_
20-9	3572-3581	published	_	_
20-10	3582-3584	in	_	_
20-11	3585-3588	the	_	_
20-12	3589-3598	following	_	_
20-13	3599-3604	paper	_	_
20-14	3604-3605	:	_	_
20-15	3606-3607	\[	_	_
20-16	3607-3610	One	_	_
20-17	3611-3619	Embedder	_	_
20-18	3619-3620	,	_	_
20-19	3621-3624	Any	_	_
20-20	3625-3629	Task	_	_
20-21	3629-3630	:	_	_
20-22	3631-3652	Instruction-Finetuned	_	_
20-23	3653-3657	Text	_	_
20-24	3658-3668	Embeddings	_	_
20-25	3668-3669	\]	_	_
20-26	3669-3670	(	_	_
20-27	3670-3675	https	_	_
20-28	3675-3676	:	_	_
20-29	3676-3677	/	_	_
20-30	3677-3678	/	_	_
20-31	3678-3687	arxiv.org	_	_
20-32	3687-3688	/	_	_
20-33	3688-3691	abs	_	_
20-34	3691-3692	/	_	_
20-35	3692-3702	2212.09741	_	_
20-36	3702-3703	)	_	_
20-37	3703-3704	.	_	_

#Text=The MTEB Benchmark results of the GIST embedding model, compared with the base model, suggest that the fine-tuning dataset has perturbed the model considerably, which resulted in significant improvements in certain tasks while adversely degrading performance in some.
21-1	3706-3709	The	_	_
21-2	3710-3714	MTEB	_	_
21-3	3715-3724	Benchmark	_	_
21-4	3725-3732	results	_	_
21-5	3733-3735	of	_	_
21-6	3736-3739	the	_	_
21-7	3740-3744	GIST	_	_
21-8	3745-3754	embedding	_	_
21-9	3755-3760	model	_	_
21-10	3760-3761	,	_	_
21-11	3762-3770	compared	_	_
21-12	3771-3775	with	_	_
21-13	3776-3779	the	_	_
21-14	3780-3784	base	_	_
21-15	3785-3790	model	_	_
21-16	3790-3791	,	_	_
21-17	3792-3799	suggest	_	_
21-18	3800-3804	that	_	_
21-19	3805-3808	the	_	_
21-20	3809-3820	fine-tuning	_	_
21-21	3821-3828	dataset	_	_
21-22	3829-3832	has	_	_
21-23	3833-3842	perturbed	_	_
21-24	3843-3846	the	_	_
21-25	3847-3852	model	_	_
21-26	3853-3865	considerably	_	_
21-27	3865-3866	,	_	_
21-28	3867-3872	which	_	_
21-29	3873-3881	resulted	_	_
21-30	3882-3884	in	_	_
21-31	3885-3896	significant	_	_
21-32	3897-3909	improvements	_	_
21-33	3910-3912	in	_	_
21-34	3913-3920	certain	_	_
21-35	3921-3926	tasks	_	_
21-36	3927-3932	while	_	_
21-37	3933-3942	adversely	_	_
21-38	3943-3952	degrading	_	_
21-39	3953-3964	performance	_	_
21-40	3965-3967	in	_	_
21-41	3968-3972	some	_	_
21-42	3972-3973	.	_	_

#Text=The retrieval performance for the TRECCOVID task is of note.
22-1	3975-3978	The	_	_
22-2	3979-3988	retrieval	_	_
22-3	3989-4000	performance	_	_
22-4	4001-4004	for	_	_
22-5	4005-4008	the	_	_
22-6	4009-4018	TRECCOVID	_	_
22-7	4019-4023	task	_	_
22-8	4024-4026	is	_	_
22-9	4027-4029	of	_	_
22-10	4030-4034	note	_	_
22-11	4034-4035	.	_	_

#Text=The fine-tuning dataset does not contain significant knowledge about COVID, which could have caused the observed performance degradation.
23-1	4036-4039	The	_	_
23-2	4040-4051	fine-tuning	_	_
23-3	4052-4059	dataset	_	_
23-4	4060-4064	does	_	_
23-5	4065-4068	not	_	_
23-6	4069-4076	contain	_	_
23-7	4077-4088	significant	_	_
23-8	4089-4098	knowledge	_	_
23-9	4099-4104	about	_	_
23-10	4105-4110	COVID	_	_
23-11	4110-4111	,	_	_
23-12	4112-4117	which	_	_
23-13	4118-4123	could	_	_
23-14	4124-4128	have	_	_
23-15	4129-4135	caused	_	_
23-16	4136-4139	the	_	_
23-17	4140-4148	observed	_	_
23-18	4149-4160	performance	_	_
23-19	4161-4172	degradation	_	_
23-20	4172-4173	.	_	_

#Text=We found some evidence, detailed in the paper, that thematic coverage of the fine-tuning data can affect downstream performance
24-1	4174-4176	We	_	_
24-2	4177-4182	found	_	_
24-3	4183-4187	some	_	_
24-4	4188-4196	evidence	_	_
24-5	4196-4197	,	_	_
24-6	4198-4206	detailed	_	_
24-7	4207-4209	in	_	_
24-8	4210-4213	the	_	_
24-9	4214-4219	paper	_	_
24-10	4219-4220	,	_	_
24-11	4221-4225	that	_	_
24-12	4226-4234	thematic	_	_
24-13	4235-4243	coverage	_	_
24-14	4244-4246	of	_	_
24-15	4247-4250	the	_	_
24-16	4251-4262	fine-tuning	_	_
24-17	4263-4267	data	_	_
24-18	4268-4271	can	_	_
24-19	4272-4278	affect	_	_
24-20	4279-4289	downstream	_	_
24-21	4290-4301	performance	_	_

#Text=.
25-1	4301-4302	.	_	_

#Text=# Usage  The model can be easily loaded using the Sentence Transformers library.
26-1	4304-4305	#	_	_
26-2	4306-4311	Usage	_	_
26-3	4313-4316	The	_	_
26-4	4317-4322	model	_	_
26-5	4323-4326	can	_	_
26-6	4327-4329	be	_	_
26-7	4330-4336	easily	_	_
26-8	4337-4343	loaded	_	_
26-9	4344-4349	using	_	_
26-10	4350-4353	the	_	_
26-11	4354-4362	Sentence	_	_
26-12	4363-4375	Transformers	_	_
26-13	4376-4383	library	_	_
26-14	4383-4384	.	_	_

#Text=```Python import torch.nn.functional as F from sentence\_transformers import SentenceTransformer  revision = None  # Replace with the specific revision to ensure reproducibility in  case the model is updated.
27-1	4386-4387	`	_	_
27-2	4387-4388	`	_	_
27-3	4388-4389	`	_	_
27-4	4389-4395	Python	_	_
27-5	4396-4402	import	_	_
27-6	4403-4422	torch.nn.functional	_	_
27-6	4403-4408	torch	_	_
27-7	4423-4425	as	_	_
27-8	4426-4427	F	_	_
27-9	4428-4432	from	_	_
27-10	4433-4454	sentence\_transformers	_	_
27-11	4455-4461	import	*[247]	ONTOLOGY[247]
27-12	4462-4481	SentenceTransformer	*[247]	ONTOLOGY[247]
27-13	4483-4491	revision	*[247]	ONTOLOGY[247]
27-14	4492-4493	=	*[247]	ONTOLOGY[247]
27-15	4494-4498	None	*[247]	ONTOLOGY[247]
27-16	4500-4501	#	*[247]	ONTOLOGY[247]
27-17	4502-4509	Replace	*[247]	ONTOLOGY[247]
27-18	4510-4514	with	*[247]	ONTOLOGY[247]
27-19	4515-4518	the	*[247]	ONTOLOGY[247]
27-20	4519-4527	specific	*[247]	ONTOLOGY[247]
27-21	4528-4536	revision	*[247]	ONTOLOGY[247]
27-22	4537-4539	to	*[247]	ONTOLOGY[247]
27-23	4540-4546	ensure	*[247]	ONTOLOGY[247]
27-24	4547-4562	reproducibility	*[247]	ONTOLOGY[247]
27-25	4563-4565	in	_	_
27-26	4567-4571	case	_	_
27-27	4572-4575	the	_	_
27-28	4576-4581	model	_	_
27-29	4582-4584	is	_	_
27-30	4585-4592	updated	_	_
27-31	4592-4593	.	_	_

#Text=model = SentenceTransformer("avsolatorio/GIST-Embedding-v0", revision=revision)  texts = \[     "Illustration of the REaLTabFormer model.
28-1	4595-4600	model	_	_
28-2	4601-4602	=	_	_
28-3	4603-4622	SentenceTransformer	_	_
28-4	4622-4623	(	*[236]	DATASET[236]
28-5	4623-4624	"	*[236]	DATASET[236]
28-6	4624-4635	avsolatorio	*[236]	DATASET[236]
28-7	4635-4636	/	*[236]	DATASET[236]
28-8	4636-4653	GIST-Embedding-v0	*[236]	DATASET[236]
28-9	4653-4654	"	*[236]	DATASET[236]
28-10	4654-4655	,	*[236]	DATASET[236]
28-11	4656-4664	revision	*[236]	DATASET[236]
28-12	4664-4665	=	*[236]	DATASET[236]
28-13	4665-4673	revision	*[236]	DATASET[236]
28-14	4673-4674	)	*[236]	DATASET[236]
28-15	4676-4681	texts	*[236]	DATASET[236]
28-16	4682-4683	=	*[236]	DATASET[236]
28-17	4684-4685	\[	*[236]	DATASET[236]
28-18	4690-4691	"	*[236]	DATASET[236]
28-19	4691-4703	Illustration	*[236]	DATASET[236]
28-20	4704-4706	of	*[236]	DATASET[236]
28-21	4707-4710	the	_	_
28-22	4711-4724	REaLTabFormer	_	_
28-23	4725-4730	model	_	_
28-24	4730-4731	.	_	_

#Text=The left block shows the non-relational tabular data model using GPT-2 with a causal LM head.
29-1	4732-4735	The	_	_
29-2	4736-4740	left	_	_
29-3	4741-4746	block	_	_
29-4	4747-4752	shows	_	_
29-5	4753-4756	the	_	_
29-6	4757-4771	non-relational	_	_
29-7	4772-4779	tabular	_	_
29-8	4780-4784	data	*[297]	SOFTWARE[297]
29-9	4785-4790	model	*[297]	SOFTWARE[297]
29-10	4791-4796	using	*[297]	SOFTWARE[297]
29-11	4797-4800	GPT	*[297]	SOFTWARE[297]
29-12	4800-4801	-	*[297]	SOFTWARE[297]
29-13	4801-4802	2	*[297]	SOFTWARE[297]
29-14	4803-4807	with	*[297]	SOFTWARE[297]
29-15	4808-4809	a	*[297]	SOFTWARE[297]
29-16	4810-4816	causal	*[297]	SOFTWARE[297]
29-17	4817-4819	LM	_	_
29-18	4820-4824	head	_	_
29-19	4824-4825	.	_	_

#Text=In contrast, the right block shows how a relational dataset's child table is modeled using a sequence-to-sequence (Seq2Seq) model.
30-1	4826-4828	In	_	_
30-2	4829-4837	contrast	_	_
30-3	4837-4838	,	_	_
30-4	4839-4842	the	_	_
30-5	4843-4848	right	_	_
30-6	4849-4854	block	_	_
30-7	4855-4860	shows	_	_
30-8	4861-4864	how	_	_
30-9	4865-4866	a	_	_
30-10	4867-4877	relational	_	_
30-11	4878-4887	dataset's	_	_
30-12	4888-4893	child	_	_
30-13	4894-4899	table	*[262]	PROJECT[262]
30-14	4900-4902	is	*[262]	PROJECT[262]
30-15	4903-4910	modeled	*[262]	PROJECT[262]
30-16	4911-4916	using	*[262]	PROJECT[262]
30-17	4917-4918	a	*[262]	PROJECT[262]
30-18	4919-4939	sequence-to-sequence	_	_
30-19	4940-4941	(	_	_
30-20	4941-4948	Seq2Seq	_	_
30-21	4948-4949	)	_	_
30-22	4950-4955	model	_	_
30-23	4955-4956	.	_	_

#Text=The Seq2Seq model uses the observations in the parent table to condition the generation of the observations in the child table.
31-1	4957-4960	The	_	_
31-2	4961-4968	Seq2Seq	_	_
31-3	4969-4974	model	_	_
31-4	4975-4979	uses	_	_
31-5	4980-4983	the	*[257]	CONFERENCE[257]
31-6	4984-4996	observations	*[257]	CONFERENCE[257]
31-7	4997-4999	in	*[257]	CONFERENCE[257]
31-8	5000-5003	the	*[257]	CONFERENCE[257]
31-9	5004-5010	parent	*[257]	CONFERENCE[257]
31-10	5011-5016	table	*[257]	CONFERENCE[257]
31-11	5017-5019	to	*[257]	CONFERENCE[257]
31-12	5020-5029	condition	*[257]	CONFERENCE[257]
31-13	5030-5033	the	*[257]	CONFERENCE[257]
31-14	5034-5044	generation	_	_
31-15	5045-5047	of	_	_
31-16	5048-5051	the	_	_
31-17	5052-5064	observations	_	_
31-18	5065-5067	in	_	_
31-19	5068-5071	the	_	_
31-20	5072-5077	child	_	_
31-21	5078-5083	table	_	_
31-22	5083-5084	.	_	_

#Text=The trained GPT-2 model on the parent table, with weights frozen, is also used as the encoder in the Seq2Seq model.",     "Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread.
32-1	5085-5088	The	_	_
32-2	5089-5096	trained	_	_
32-3	5097-5100	GPT	_	_
32-4	5100-5101	-	_	_
32-5	5101-5102	2	_	_
32-6	5103-5108	model	_	_
32-7	5109-5111	on	_	_
32-8	5112-5115	the	_	_
32-9	5116-5122	parent	_	_
32-10	5123-5128	table	_	_
32-11	5128-5129	,	_	_
32-12	5130-5134	with	_	_
32-13	5135-5142	weights	_	_
32-14	5143-5149	frozen	_	_
32-15	5149-5150	,	*[247]	LICENSE[247]
32-16	5151-5153	is	*[247]	LICENSE[247]
32-17	5154-5158	also	*[247]	LICENSE[247]
32-18	5159-5163	used	*[247]	LICENSE[247]
32-19	5164-5166	as	*[247]	LICENSE[247]
32-20	5167-5170	the	*[247]	LICENSE[247]
32-21	5171-5178	encoder	*[247]	LICENSE[247]
32-22	5179-5181	in	*[247]	LICENSE[247]
32-23	5182-5185	the	*[247]	LICENSE[247]
32-24	5186-5193	Seq2Seq	*[247]	LICENSE[247]
32-25	5194-5199	model	*[247]	LICENSE[247]
32-26	5199-5200	.	*[247]	LICENSE[247]
32-27	5200-5201	"	*[247]	LICENSE[247]
32-28	5201-5202	,	*[247]	LICENSE[247]
32-29	5207-5208	"	*[247]	LICENSE[247]
32-30	5208-5218	Predicting	_	_
32-31	5219-5224	human	_	_
32-32	5225-5233	mobility	_	_
32-33	5234-5239	holds	_	_
32-34	5240-5251	significant	_	_
32-35	5252-5261	practical	_	_
32-36	5262-5267	value	_	_
32-37	5267-5268	,	_	_
32-38	5269-5273	with	_	_
32-39	5274-5286	applications	_	_
32-40	5287-5294	ranging	_	_
32-41	5295-5299	from	_	_
32-42	5300-5309	enhancing	_	_
32-43	5310-5318	disaster	_	_
32-44	5319-5323	risk	_	_
32-45	5324-5332	planning	_	_
32-46	5333-5335	to	_	_
32-47	5336-5346	simulating	_	_
32-48	5347-5355	epidemic	_	_
32-49	5356-5362	spread	_	_
32-50	5362-5363	.	_	_

#Text=In this paper, we present the GeoFormer, a decoder-only transformer model adapted from the GPT architecture to forecast human mobility.",     "As the economies of Southeast Asia continue adopting digital technologies, policy makers increasingly ask how to prepare the workforce for emerging labor demands.
33-1	5364-5366	In	_	_
33-2	5367-5371	this	_	_
33-3	5372-5377	paper	_	_
33-4	5377-5378	,	_	_
33-5	5379-5381	we	_	_
33-6	5382-5389	present	_	_
33-7	5390-5393	the	_	_
33-8	5394-5403	GeoFormer	_	_
33-9	5403-5404	,	_	_
33-10	5405-5406	a	_	_
33-11	5407-5419	decoder-only	_	_
33-12	5420-5431	transformer	_	_
33-13	5432-5437	model	_	_
33-14	5438-5445	adapted	_	_
33-15	5446-5450	from	_	_
33-16	5451-5454	the	_	_
33-17	5455-5458	GPT	_	_
33-18	5459-5471	architecture	_	_
33-19	5472-5474	to	_	_
33-20	5475-5483	forecast	_	_
33-21	5484-5489	human	_	_
33-22	5490-5498	mobility	_	_
33-23	5498-5499	.	_	_
33-24	5499-5500	"	_	_
33-25	5500-5501	,	_	_
33-26	5506-5507	"	_	_
33-27	5507-5509	As	_	_
33-28	5510-5513	the	_	_
33-29	5514-5523	economies	_	_
33-30	5524-5526	of	_	_
33-31	5527-5536	Southeast	_	_
33-32	5537-5541	Asia	_	_
33-33	5542-5550	continue	_	_
33-34	5551-5559	adopting	_	_
33-35	5560-5567	digital	_	_
33-36	5568-5580	technologies	*[248]	LICENSE[248]
33-37	5580-5581	,	*[248]	LICENSE[248]
33-38	5582-5588	policy	*[248]	LICENSE[248]
33-39	5589-5595	makers	*[248]	LICENSE[248]
33-40	5596-5608	increasingly	*[248]	LICENSE[248]
33-41	5609-5612	ask	*[248]	LICENSE[248]
33-42	5613-5616	how	*[248]	LICENSE[248]
33-43	5617-5619	to	*[248]	LICENSE[248]
33-44	5620-5627	prepare	*[248]	LICENSE[248]
33-45	5628-5631	the	*[248]	LICENSE[248]
33-46	5632-5641	workforce	*[248]	LICENSE[248]
33-47	5642-5645	for	*[248]	LICENSE[248]
33-48	5646-5654	emerging	_	_
33-49	5655-5660	labor	_	_
33-50	5661-5668	demands	_	_
33-51	5668-5669	.	_	_

#Text=However, little is known about the skills that workers need to adapt to these changes" \]  # Compute embeddings embeddings = model.encode(texts, convert\_to\_tensor=True)  # Compute cosine-similarity for each pair of sentences scores = F.cosine\_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=-1)  print(scores.cpu().numpy()) ```  # Guided in-batch constrastive loss  For anyone interested in the technical implementation of GISTEmbed as a training mechanism, please refer to the loss computation implemented in the loss function \[`guided\_in\_batch\_contrastive\_loss`\](https://github.com/avsolatorio/GISTEmbed/blob/538e3d749b1944e8362c5566385111763866fa4c/gist\_embed/trainer/loss.py#L599).
34-1	5670-5677	However	_	_
34-2	5677-5678	,	_	_
34-3	5679-5685	little	_	_
34-4	5686-5688	is	_	_
34-5	5689-5694	known	_	_
34-6	5695-5700	about	_	_
34-7	5701-5704	the	_	_
34-8	5705-5711	skills	_	_
34-9	5712-5716	that	_	_
34-10	5717-5724	workers	_	_
34-11	5725-5729	need	_	_
34-12	5730-5732	to	_	_
34-13	5733-5738	adapt	_	_
34-14	5739-5741	to	_	_
34-15	5742-5747	these	_	_
34-16	5748-5755	changes	_	_
34-17	5755-5756	"	_	_
34-18	5757-5758	\]	_	_
34-19	5760-5761	#	_	_
34-20	5762-5769	Compute	_	_
34-21	5770-5780	embeddings	_	_
34-22	5781-5791	embeddings	_	_
34-23	5792-5793	=	_	_
34-24	5794-5806	model.encode	_	_
34-25	5806-5807	(	_	_
34-26	5807-5812	texts	_	_
34-27	5812-5813	,	_	_
34-28	5814-5831	convert\_to\_tensor	_	_
34-29	5831-5832	=	_	_
34-30	5832-5836	True	_	_
34-31	5836-5837	)	_	_
34-32	5839-5840	#	_	_
34-33	5841-5848	Compute	_	_
34-34	5849-5866	cosine-similarity	_	_
34-35	5867-5870	for	_	_
34-36	5871-5875	each	_	_
34-37	5876-5880	pair	_	_
34-38	5881-5883	of	_	_
34-39	5884-5893	sentences	_	_
34-40	5894-5900	scores	_	_
34-41	5901-5902	=	_	_
34-42	5903-5922	F.cosine\_similarity	_	_
34-43	5922-5923	(	_	_
34-44	5923-5943	embeddings.unsqueeze	_	_
34-45	5943-5944	(	_	_
34-46	5944-5945	1	_	_
34-47	5945-5946	)	_	_
34-48	5946-5947	,	_	_
34-49	5948-5968	embeddings.unsqueeze	_	_
34-50	5968-5969	(	_	_
34-51	5969-5970	0	_	_
34-52	5970-5971	)	_	_
34-53	5971-5972	,	_	_
34-54	5973-5976	dim	_	_
34-55	5976-5977	=	_	_
34-56	5977-5978	-	_	_
34-57	5978-5979	1	_	_
34-58	5979-5980	)	_	_
34-59	5982-5987	print	_	_
34-60	5987-5988	(	_	_
34-61	5988-5998	scores.cpu	_	_
34-62	5998-5999	(	_	_
34-63	5999-6000	)	_	_
34-64	6000-6001	.	_	_
34-65	6001-6006	numpy	_	_
34-66	6006-6007	(	_	_
34-67	6007-6008	)	_	_
34-68	6008-6009	)	_	_
34-69	6010-6011	`	_	_
34-70	6011-6012	`	_	_
34-71	6012-6013	`	_	_
34-72	6015-6016	#	_	_
34-73	6017-6023	Guided	_	_
34-74	6024-6032	in-batch	_	_
34-75	6033-6045	constrastive	_	_
34-76	6046-6050	loss	_	_
34-77	6052-6055	For	_	_
34-78	6056-6062	anyone	_	_
34-79	6063-6073	interested	_	_
34-80	6074-6076	in	_	_
34-81	6077-6080	the	_	_
34-82	6081-6090	technical	_	_
34-83	6091-6105	implementation	*[248]	ONTOLOGY[248]
34-84	6106-6108	of	*[248]	ONTOLOGY[248]
34-85	6109-6118	GISTEmbed	*[248]	ONTOLOGY[248]
34-86	6119-6121	as	_	_
34-87	6122-6123	a	_	_
34-88	6124-6132	training	_	_
34-89	6133-6142	mechanism	_	_
34-90	6142-6143	,	_	_
34-91	6144-6150	please	_	_
34-92	6151-6156	refer	_	_
34-93	6157-6159	to	_	_
34-94	6160-6163	the	_	_
34-95	6164-6168	loss	_	_
34-96	6169-6180	computation	_	_
34-97	6181-6192	implemented	_	_
34-98	6193-6195	in	_	_
34-99	6196-6199	the	_	_
34-100	6200-6204	loss	_	_
34-101	6205-6213	function	_	_
34-102	6214-6215	\[	_	_
34-103	6215-6216	`	_	_
34-104	6216-6248	guided\_in\_batch\_contrastive\_loss	_	_
34-105	6248-6249	`	_	_
34-106	6249-6250	\]	_	_
34-107	6250-6251	(	_	_
34-108	6251-6256	https	_	_
34-109	6256-6257	:	_	_
34-110	6257-6258	/	_	_
34-111	6258-6259	/	_	_
34-112	6259-6269	github.com	_	_
34-113	6269-6270	/	_	_
34-114	6270-6281	avsolatorio	_	_
34-115	6281-6282	/	_	_
34-116	6282-6291	GISTEmbed	_	_
34-117	6291-6292	/	_	_
34-118	6292-6296	blob	_	_
34-119	6296-6297	/	_	_
34-120	6297-6337	538e3d749b1944e8362c5566385111763866fa4c	_	_
34-121	6337-6338	/	_	_
34-122	6338-6348	gist\_embed	_	_
34-123	6348-6349	/	_	_
34-124	6349-6356	trainer	_	_
34-125	6356-6357	/	_	_
34-126	6357-6364	loss.py	_	_
34-127	6364-6365	#	_	_
34-128	6365-6369	L599	_	_
34-129	6369-6370	)	_	_
34-130	6370-6371	.	_	_

#Text=This loss function is subsequently used in the \[`GISTTrainer`\](https://github.com/avsolatorio/GISTEmbed/blob/538e3d749b1944e8362c5566385111763866fa4c/gist\_embed/trainer/trainer.py#L127)
35-1	6373-6377	This	_	_
35-2	6378-6382	loss	_	_
35-3	6383-6391	function	_	_
35-4	6392-6394	is	_	_
35-5	6395-6407	subsequently	_	_
35-6	6408-6412	used	_	_
35-7	6413-6415	in	_	_
35-8	6416-6419	the	*[258]	CONFERENCE[258]
35-9	6420-6421	\[	*[258]	CONFERENCE[258]
35-10	6421-6422	`	*[258]	CONFERENCE[258]
35-11	6422-6433	GISTTrainer	*[258]	CONFERENCE[258]
35-12	6433-6434	`	*[258]	CONFERENCE[258]
35-13	6434-6435	\]	*[258]	CONFERENCE[258]
35-14	6435-6436	(	*[258]	CONFERENCE[258]
35-15	6436-6441	https	*[258]	CONFERENCE[258]
35-16	6441-6442	:	*[258]	CONFERENCE[258]
35-17	6442-6443	/	*[258]	CONFERENCE[258]
35-18	6443-6444	/	*[258]	CONFERENCE[258]
35-19	6444-6454	github.com	*[258]	CONFERENCE[258]
35-20	6454-6455	/	*[258]	CONFERENCE[258]
35-21	6455-6466	avsolatorio	*[258]	CONFERENCE[258]
35-22	6466-6467	/	*[258]	CONFERENCE[258]
35-23	6467-6476	GISTEmbed	_	_
35-24	6476-6477	/	_	_
35-25	6477-6481	blob	_	_
35-26	6481-6482	/	_	_
35-27	6482-6522	538e3d749b1944e8362c5566385111763866fa4c	_	_
35-28	6522-6523	/	_	_
35-29	6523-6533	gist\_embed	_	_
35-30	6533-6534	/	_	_
35-31	6534-6541	trainer	_	_
35-32	6541-6542	/	_	_
35-33	6542-6552	trainer.py	_	_
35-34	6552-6553	#	_	_
35-35	6553-6557	L127	_	_
35-36	6557-6558	)	_	_

#Text=.
36-1	6558-6559	.	_	_

#Text=# Reproducibility  This section outlines how to fine-tune models using the GISTEmbed framework.
37-1	6562-6563	#	_	_
37-2	6564-6579	Reproducibility	_	_
37-3	6581-6585	This	_	_
37-4	6586-6593	section	_	_
37-5	6594-6602	outlines	_	_
37-6	6603-6606	how	_	_
37-7	6607-6609	to	_	_
37-8	6610-6619	fine-tune	_	_
37-9	6620-6626	models	_	_
37-10	6627-6632	using	_	_
37-11	6633-6636	the	_	_
37-12	6637-6646	GISTEmbed	_	_
37-13	6647-6656	framework	_	_
37-14	6656-6657	.	_	_

#Text=The following steps are necessary to reproduce the results:   First, create a new conda environment and install poetry.  ``` conda create -n GISTEmbed python=3.10  conda activate GISTEmbed  pip install poetry ```  Next, clone the repository and install the dependencies.  ``` git clone https://github.com/avsolatorio/GISTEmbed.git  cd GISTEmbed  poetry install ```  To reduce the likelihood of encountering issues and unexpected training runs, we set up a convention that would validate the intended parameters and configurations.
38-1	6658-6661	The	_	_
38-2	6662-6671	following	_	_
38-3	6672-6677	steps	_	_
38-4	6678-6681	are	_	_
38-5	6682-6691	necessary	_	_
38-6	6692-6694	to	_	_
38-7	6695-6704	reproduce	_	_
38-8	6705-6708	the	_	_
38-9	6709-6716	results	_	_
38-10	6716-6717	:	_	_
38-11	6720-6725	First	_	_
38-12	6725-6726	,	_	_
38-13	6727-6733	create	_	_
38-14	6734-6735	a	_	_
38-15	6736-6739	new	_	_
38-16	6740-6745	conda	_	_
38-17	6746-6757	environment	_	_
38-18	6758-6761	and	_	_
38-19	6762-6769	install	*[263]	PROJECT[263]
38-20	6770-6776	poetry	*[263]	PROJECT[263]
38-21	6776-6777	.	*[263]	PROJECT[263]
38-22	6779-6780	`	*[263]	PROJECT[263]
38-23	6780-6781	`	*[263]	PROJECT[263]
38-24	6781-6782	`	*[263]	PROJECT[263]
38-25	6783-6788	conda	*[263]	PROJECT[263]
38-26	6789-6795	create	*[263]	PROJECT[263]
38-27	6796-6797	-	*[263]	PROJECT[263]
38-28	6797-6798	n	*[263]	PROJECT[263]
38-29	6799-6808	GISTEmbed	*[263]	PROJECT[263]
38-30	6809-6815	python	*[263]	PROJECT[263]
38-31	6815-6816	=	*[263]	PROJECT[263]
38-32	6816-6820	3.10	*[263]	PROJECT[263]
38-33	6822-6827	conda	*[263]	PROJECT[263]
38-34	6828-6836	activate	*[263]	PROJECT[263]
38-35	6837-6846	GISTEmbed	*[263]	PROJECT[263]
38-36	6848-6851	pip	*[263]	PROJECT[263]
38-37	6852-6859	install	*[263]	PROJECT[263]
38-38	6860-6866	poetry	*[263]	PROJECT[263]
38-39	6867-6868	`	*[263]	PROJECT[263]
38-40	6868-6869	`	*[263]	PROJECT[263]
38-41	6869-6870	`	*[263]	PROJECT[263]
38-42	6872-6876	Next	*[263]	PROJECT[263]
38-43	6876-6877	,	*[263]	PROJECT[263]
38-44	6878-6883	clone	_	_
38-45	6884-6887	the	_	_
38-46	6888-6898	repository	_	_
38-47	6899-6902	and	_	_
38-48	6903-6910	install	_	_
38-49	6911-6914	the	_	_
38-50	6915-6927	dependencies	_	_
38-51	6927-6928	.	_	_
38-52	6930-6931	`	_	_
38-53	6931-6932	`	_	_
38-54	6932-6933	`	_	_
38-55	6934-6937	git	_	_
38-56	6938-6943	clone	_	_
38-57	6944-6949	https	_	_
38-58	6949-6950	:	_	_
38-59	6950-6951	/	_	_
38-60	6951-6952	/	_	_
38-61	6952-6962	github.com	_	_
38-62	6962-6963	/	_	_
38-63	6963-6974	avsolatorio	_	_
38-64	6974-6975	/	_	_
38-65	6975-6988	GISTEmbed.git	_	_
38-66	6990-6992	cd	_	_
38-67	6993-7002	GISTEmbed	_	_
38-68	7004-7010	poetry	_	_
38-69	7011-7018	install	_	_
38-70	7019-7020	`	_	_
38-71	7020-7021	`	_	_
38-72	7021-7022	`	_	_
38-73	7024-7026	To	_	_
38-74	7027-7033	reduce	_	_
38-75	7034-7037	the	_	_
38-76	7038-7048	likelihood	_	_
38-77	7049-7051	of	_	_
38-78	7052-7064	encountering	_	_
38-79	7065-7071	issues	_	_
38-80	7072-7075	and	_	_
38-81	7076-7086	unexpected	_	_
38-82	7087-7095	training	_	_
38-83	7096-7100	runs	_	_
38-84	7100-7101	,	_	_
38-85	7102-7104	we	_	_
38-86	7105-7108	set	_	_
38-87	7109-7111	up	_	_
38-88	7112-7113	a	_	_
38-89	7114-7124	convention	_	_
38-90	7125-7129	that	_	_
38-91	7130-7135	would	_	_
38-92	7136-7144	validate	_	_
38-93	7145-7148	the	_	_
38-94	7149-7157	intended	_	_
38-95	7158-7168	parameters	_	_
38-96	7169-7172	and	_	_
38-97	7173-7187	configurations	_	_
38-98	7187-7188	.	_	_

#Text=One can refer to the \[gist\_embed/validator.py\](gist\_embed/validator.py) file to see the validation logic.
39-1	7190-7193	One	_	_
39-2	7194-7197	can	_	_
39-3	7198-7203	refer	_	_
39-4	7204-7206	to	_	_
39-5	7207-7210	the	_	_
39-6	7211-7212	\[	_	_
39-7	7212-7222	gist\_embed	_	_
39-8	7222-7223	/	_	_
39-9	7223-7235	validator.py	_	_
39-10	7235-7236	\]	_	_
39-11	7236-7237	(	_	_
39-12	7237-7247	gist\_embed	_	_
39-13	7247-7248	/	*[249]	LICENSE[249]
39-14	7248-7260	validator.py	*[249]	LICENSE[249]
39-15	7260-7261	)	*[249]	LICENSE[249]
39-16	7262-7266	file	*[249]	LICENSE[249]
39-17	7267-7269	to	*[249]	LICENSE[249]
39-18	7270-7273	see	*[249]	LICENSE[249]
39-19	7274-7277	the	*[249]	LICENSE[249]
39-20	7278-7288	validation	_	_
39-21	7289-7294	logic	_	_
39-22	7294-7295	.	_	_

#Text=Additional configurations must be registered in the validator to ensure that the intended parameters are correctly set.
40-1	7296-7306	Additional	_	_
40-2	7307-7321	configurations	_	_
40-3	7322-7326	must	*[263]	EVALMETRIC[263]
40-4	7327-7329	be	*[263]	EVALMETRIC[263]
40-5	7330-7340	registered	*[263]	EVALMETRIC[263]
40-6	7341-7343	in	*[263]	EVALMETRIC[263]
40-7	7344-7347	the	*[263]	EVALMETRIC[263]
40-8	7348-7357	validator	*[263]	EVALMETRIC[263]
40-9	7358-7360	to	*[263]	EVALMETRIC[263]
40-10	7361-7367	ensure	*[263]	EVALMETRIC[263]
40-11	7368-7372	that	*[263]	EVALMETRIC[263]
40-12	7373-7376	the	*[263]	EVALMETRIC[263]
40-13	7377-7385	intended	*[263]	EVALMETRIC[263]
40-14	7386-7396	parameters	*[263]	EVALMETRIC[263]
40-15	7397-7400	are	*[263]	EVALMETRIC[263]
40-16	7401-7410	correctly	_	_
40-17	7411-7414	set	_	_
40-18	7414-7415	.	_	_

#Text=After registering the intended configurations, an experiment script can be created to fine-tune the model.
41-1	7417-7422	After	_	_
41-2	7423-7434	registering	_	_
41-3	7435-7438	the	_	_
41-4	7439-7447	intended	_	_
41-5	7448-7462	configurations	_	_
41-6	7462-7463	,	_	_
41-7	7464-7466	an	_	_
41-8	7467-7477	experiment	_	_
41-9	7478-7484	script	_	_
41-10	7485-7488	can	*[259]	CONFERENCE[259]
41-11	7489-7491	be	*[259]	CONFERENCE[259]
41-12	7492-7499	created	*[259]	CONFERENCE[259]
41-13	7500-7502	to	*[259]	CONFERENCE[259]
41-14	7503-7512	fine-tune	_	_
41-15	7513-7516	the	_	_
41-16	7517-7522	model	_	_
41-17	7522-7523	.	_	_

#Text=See example: \[experiments/01-600-11-1-2-2-0-0-cls-normed-384-512\_run\_finetune\_experiment.sh\](experiments/01-600-11-1-2-2-0-0-cls-normed-384-512\_run\_finetune\_experiment.sh).
42-1	7524-7527	See	_	_
42-2	7528-7535	example	_	_
42-3	7535-7536	:	_	_
42-4	7537-7538	\[	_	_
42-5	7538-7549	experiments	_	_
42-6	7549-7550	/	_	_
42-7	7550-7552	01	_	_
42-8	7552-7553	-	_	_
42-9	7553-7556	600	_	_
42-10	7556-7557	-	_	_
42-11	7557-7559	11	_	_
42-12	7559-7560	-	_	_
42-13	7560-7561	1	*[298]	SOFTWARE[298]
42-14	7561-7562	-	*[298]	SOFTWARE[298]
42-15	7562-7563	2	*[298]	SOFTWARE[298]
42-16	7563-7564	-	*[298]	SOFTWARE[298]
42-17	7564-7565	2	*[298]	SOFTWARE[298]
42-18	7565-7566	-	*[298]	SOFTWARE[298]
42-19	7566-7567	0	*[298]	SOFTWARE[298]
42-20	7567-7568	-	*[298]	SOFTWARE[298]
42-21	7568-7569	0	*[298]	SOFTWARE[298]
42-22	7569-7570	-	*[298]	SOFTWARE[298]
42-23	7570-7580	cls-normed	*[298]	SOFTWARE[298]
42-24	7580-7581	-	*[298]	SOFTWARE[298]
42-25	7581-7584	384	*[298]	SOFTWARE[298]
42-26	7584-7585	-	*[298]	SOFTWARE[298]
42-27	7585-7588	512	*[298]	SOFTWARE[298]
42-28	7588-7589	\_	*[298]	SOFTWARE[298]
42-29	7589-7615	run\_finetune\_experiment.sh	*[298]	SOFTWARE[298]
42-30	7615-7616	\]	*[298]	SOFTWARE[298]
42-31	7616-7617	(	*[298]	SOFTWARE[298]
42-32	7617-7628	experiments	*[298]	SOFTWARE[298]
42-33	7628-7629	/	*[298]	SOFTWARE[298]
42-34	7629-7631	01	*[298]	SOFTWARE[298]
42-35	7631-7632	-	*[298]	SOFTWARE[298]
42-36	7632-7635	600	*[298]	SOFTWARE[298]
42-37	7635-7636	-	*[298]	SOFTWARE[298]
42-38	7636-7638	11	*[298]	SOFTWARE[298]
42-39	7638-7639	-	*[298]	SOFTWARE[298]
42-40	7639-7640	1	*[298]	SOFTWARE[298]
42-41	7640-7641	-	*[298]	SOFTWARE[298]
42-42	7641-7642	2	*[298]	SOFTWARE[298]
42-43	7642-7643	-	*[298]	SOFTWARE[298]
42-44	7643-7644	2	*[298]	SOFTWARE[298]
42-45	7644-7645	-	*[298]	SOFTWARE[298]
42-46	7645-7646	0	*[298]	SOFTWARE[298]
42-47	7646-7647	-	*[298]	SOFTWARE[298]
42-48	7647-7648	0	*[298]	SOFTWARE[298]
42-49	7648-7649	-	*[298]	SOFTWARE[298]
42-50	7649-7659	cls-normed	*[298]	SOFTWARE[298]
42-51	7659-7660	-	*[298]	SOFTWARE[298]
42-52	7660-7663	384	*[298]	SOFTWARE[298]
42-53	7663-7664	-	*[298]	SOFTWARE[298]
42-54	7664-7667	512	*[298]	SOFTWARE[298]
42-55	7667-7668	\_	*[298]	SOFTWARE[298]
42-56	7668-7694	run\_finetune\_experiment.sh	_	_
42-57	7694-7695	)	_	_
42-58	7695-7696	.	_	_

#Text=Details of the arguments used in the script can be found in the \[gist\_embed/trainer/arguments\](gist\_embed/trainer/arguments) file.
43-1	7698-7705	Details	_	_
43-2	7706-7708	of	_	_
43-3	7709-7712	the	_	_
43-4	7713-7722	arguments	_	_
43-5	7723-7727	used	_	_
43-6	7728-7730	in	_	_
43-7	7731-7734	the	_	_
43-8	7735-7741	script	_	_
43-9	7742-7745	can	_	_
43-10	7746-7748	be	_	_
43-11	7749-7754	found	_	_
43-12	7755-7757	in	_	_
43-13	7758-7761	the	_	_
43-14	7762-7763	\[	_	_
43-15	7763-7773	gist\_embed	_	_
43-16	7773-7774	/	_	_
43-17	7774-7781	trainer	_	_
43-18	7781-7782	/	*[250]	LICENSE[250]
43-19	7782-7791	arguments	*[250]	LICENSE[250]
43-20	7791-7792	\]	*[250]	LICENSE[250]
43-21	7792-7793	(	*[250]	LICENSE[250]
43-22	7793-7803	gist\_embed	*[250]	LICENSE[250]
43-23	7803-7804	/	*[250]	LICENSE[250]
43-24	7804-7811	trainer	_	_
43-25	7811-7812	/	_	_
43-26	7812-7821	arguments	_	_
43-27	7821-7822	)	_	_
43-28	7823-7827	file	_	_
43-29	7827-7828	.	_	_

#Text=To run the experiment, simply execute the following command:  ``` bash experiments/01-600-11-1-2-2-0-0-cls-normed-384-512\_run\_finetune\_experiment.sh ```  The script will execute the experiment and save the model to the specified output directory.
44-1	7830-7832	To	_	_
44-2	7833-7836	run	_	_
44-3	7837-7840	the	_	_
44-4	7841-7851	experiment	_	_
44-5	7851-7852	,	_	_
44-6	7853-7859	simply	_	_
44-7	7860-7867	execute	_	_
44-8	7868-7871	the	_	_
44-9	7872-7881	following	_	_
44-10	7882-7889	command	_	_
44-11	7889-7890	:	_	_
44-12	7892-7893	`	_	_
44-13	7893-7894	`	_	_
44-14	7894-7895	`	_	_
44-15	7896-7900	bash	_	_
44-16	7901-7912	experiments	_	_
44-17	7912-7913	/	_	_
44-18	7913-7915	01	_	_
44-19	7915-7916	-	_	_
44-20	7916-7919	600	_	_
44-21	7919-7920	-	_	_
44-22	7920-7922	11	_	_
44-23	7922-7923	-	_	_
44-24	7923-7924	1	_	_
44-25	7924-7925	-	_	_
44-26	7925-7926	2	_	_
44-27	7926-7927	-	_	_
44-28	7927-7928	2	_	_
44-29	7928-7929	-	_	_
44-30	7929-7930	0	_	_
44-31	7930-7931	-	_	_
44-32	7931-7932	0	_	_
44-33	7932-7933	-	_	_
44-34	7933-7943	cls-normed	_	_
44-35	7943-7944	-	_	_
44-36	7944-7947	384	_	_
44-37	7947-7948	-	_	_
44-38	7948-7951	512	_	_
44-39	7951-7952	\_	_	_
44-40	7952-7978	run\_finetune\_experiment.sh	_	_
44-41	7979-7980	`	_	_
44-42	7980-7981	`	_	_
44-43	7981-7982	`	_	_
44-44	7984-7987	The	_	_
44-45	7988-7994	script	_	_
44-46	7995-7999	will	_	_
44-47	8000-8007	execute	_	_
44-48	8008-8011	the	_	_
44-49	8012-8022	experiment	_	_
44-50	8023-8026	and	_	_
44-51	8027-8031	save	_	_
44-52	8032-8035	the	_	_
44-53	8036-8041	model	_	_
44-54	8042-8044	to	_	_
44-55	8045-8048	the	_	_
44-56	8049-8058	specified	_	_
44-57	8059-8065	output	*[249]	ONTOLOGY[249]
44-58	8066-8075	directory	_	_
44-59	8075-8076	.	_	_

#Text=There are configurations in the script that handles the model checkpointing to Hugging Face model hub.
45-1	8077-8082	There	_	_
45-2	8083-8086	are	_	_
45-3	8087-8101	configurations	_	_
45-4	8102-8104	in	_	_
45-5	8105-8108	the	_	_
45-6	8109-8115	script	_	_
45-7	8116-8120	that	_	_
45-8	8121-8128	handles	_	_
45-9	8129-8132	the	_	_
45-10	8133-8138	model	_	_
45-11	8139-8152	checkpointing	_	_
45-12	8153-8155	to	_	_
45-13	8156-8163	Hugging	_	_
45-14	8164-8168	Face	*[264]	PROJECT[264]
45-15	8169-8174	model	_	_
45-16	8175-8178	hub	_	_
45-17	8178-8179	.	_	_

#Text=Ensure to change the `--callback\_hub\_organization <organization>` to the appropriate organization.
46-1	8180-8186	Ensure	_	_
46-2	8187-8189	to	_	_
46-3	8190-8196	change	_	_
46-4	8197-8200	the	_	_
46-5	8201-8202	`	_	_
46-6	8202-8203	-	_	_
46-7	8203-8204	-	_	_
46-8	8204-8229	callback\_hub\_organization	_	_
46-9	8230-8231	<	_	_
46-10	8231-8243	organization	_	_
46-11	8243-8244	>	_	_
46-12	8244-8245	`	_	_
46-13	8246-8248	to	_	_
46-14	8249-8252	the	_	_
46-15	8253-8264	appropriate	_	_
46-16	8265-8277	organization	_	_
46-17	8277-8278	.	_	_

#Text=The script also uses WANDB for logging.
47-1	8280-8283	The	_	_
47-2	8284-8290	script	_	_
47-3	8291-8295	also	_	_
47-4	8296-8300	uses	_	_
47-5	8301-8306	WANDB	_	_
47-6	8307-8310	for	_	_
47-7	8311-8318	logging	_	_
47-8	8318-8319	.	_	_

#Text=Ensure to set the `WANDB\_API\_KEY` environment variable to enable logging to WANDB
48-1	8320-8326	Ensure	_	_
48-2	8327-8329	to	_	_
48-3	8330-8333	set	_	_
48-4	8334-8337	the	_	_
48-5	8338-8339	`	_	_
48-6	8339-8352	WANDB\_API\_KEY	_	_
48-6	8339-8344	WANDB	_	_
48-7	8352-8353	`	_	_
48-8	8354-8365	environment	_	_
48-9	8366-8374	variable	_	_
48-10	8375-8377	to	_	_
48-11	8378-8384	enable	_	_
48-12	8385-8392	logging	_	_
48-13	8393-8395	to	*[237]	DATASET[237]
48-14	8396-8401	WANDB	_	_

#Text=.
49-1	8401-8402	.	_	_

#Text=# Base model  We have implemented some tricks on top of the (excellent!)
50-1	8404-8405	#	_	_
50-2	8406-8410	Base	_	_
50-3	8411-8416	model	_	_
50-4	8418-8420	We	_	_
50-5	8421-8425	have	_	_
50-6	8426-8437	implemented	_	_
50-7	8438-8442	some	_	_
50-8	8443-8449	tricks	_	_
50-9	8450-8452	on	*[236]	PUBLICATION[236]
50-10	8453-8456	top	*[236]	PUBLICATION[236]
50-11	8457-8459	of	*[236]	PUBLICATION[236]
50-12	8460-8463	the	_	_
50-13	8464-8465	(	_	_
50-14	8465-8474	excellent	_	_
50-15	8474-8475	!	_	_
50-16	8475-8476	)	_	_

#Text=Sentence Transformers library to support the GISTEmbed framework.
51-1	8477-8485	Sentence	_	_
51-2	8486-8498	Transformers	_	_
51-3	8499-8506	library	_	_
51-4	8507-8509	to	_	_
51-5	8510-8517	support	_	_
51-6	8518-8521	the	_	_
51-7	8522-8531	GISTEmbed	_	_
51-8	8532-8541	framework	_	_
51-9	8541-8542	.	_	_

#Text=One notable trick is supporting gradient checkpointing for training the models.
52-1	8543-8546	One	_	_
52-2	8547-8554	notable	_	_
52-3	8555-8560	trick	_	_
52-4	8561-8563	is	_	_
52-5	8564-8574	supporting	_	_
52-6	8575-8583	gradient	_	_
52-7	8584-8597	checkpointing	_	_
52-8	8598-8601	for	*[237]	PUBLICATION[237]
52-9	8602-8610	training	*[237]	PUBLICATION[237]
52-10	8611-8614	the	*[237]	PUBLICATION[237]
52-11	8615-8621	models	*[237]	PUBLICATION[237]
52-12	8621-8622	.	_	_

#Text=This is particularly useful for training large models with limited GPU memory.
53-1	8623-8627	This	_	_
53-2	8628-8630	is	_	_
53-3	8631-8643	particularly	_	_
53-4	8644-8650	useful	_	_
53-5	8651-8654	for	_	_
53-6	8655-8663	training	_	_
53-7	8664-8669	large	_	_
53-8	8670-8676	models	_	_
53-9	8677-8681	with	_	_
53-10	8682-8689	limited	*[252]	LICENSE[252]
53-11	8690-8693	GPU	*[252]	LICENSE[252]
53-12	8694-8700	memory	_	_
53-13	8700-8701	.	_	_

#Text=See the \[gist\_embed/base.py\](gist\_embed/base.py) file for the implementation details
54-1	8703-8706	See	_	_
54-2	8707-8710	the	_	_
54-3	8711-8712	\[	_	_
54-4	8712-8722	gist\_embed	_	_
54-5	8722-8723	/	_	_
54-6	8723-8730	base.py	_	_
54-7	8730-8731	\]	_	_
54-8	8731-8732	(	_	_
54-9	8732-8742	gist\_embed	_	_
54-10	8742-8743	/	_	_
54-11	8743-8750	base.py	_	_
54-12	8750-8751	)	_	_
54-13	8752-8756	file	_	_
54-14	8757-8760	for	*[253]	WORKSHOP[253]
54-15	8761-8764	the	*[253]	WORKSHOP[253]
54-16	8765-8779	implementation	_	_
54-17	8780-8787	details	_	_

#Text=.
55-1	8787-8788	.	_	_

#Text=# Training Parameters  Below are the training parameters used to fine-tune the model:  ``` Epochs = 80 Warmup ratio = 0.1 Learning rate = 5e-6 Batch size = 32 Checkpoint step = 103500 Contrastive loss temperature = 0.01 ```  Specific training details and strategies will be published shortly
56-1	8790-8791	#	_	_
56-2	8792-8800	Training	_	_
56-3	8801-8811	Parameters	_	_
56-4	8813-8818	Below	_	_
56-5	8819-8822	are	_	_
56-6	8823-8826	the	_	_
56-7	8827-8835	training	_	_
56-8	8836-8846	parameters	_	_
56-9	8847-8851	used	*[250]	ONTOLOGY[250]
56-10	8852-8854	to	*[250]	ONTOLOGY[250]
56-11	8855-8864	fine-tune	*[250]	ONTOLOGY[250]
56-12	8865-8868	the	*[250]	ONTOLOGY[250]
56-13	8869-8874	model	*[250]	ONTOLOGY[250]
56-14	8874-8875	:	*[250]	ONTOLOGY[250]
56-15	8877-8878	`	*[250]	ONTOLOGY[250]
56-16	8878-8879	`	*[250]	ONTOLOGY[250]
56-17	8879-8880	`	*[250]	ONTOLOGY[250]
56-18	8881-8887	Epochs	*[250]	ONTOLOGY[250]
56-19	8888-8889	=	*[250]	ONTOLOGY[250]
56-20	8890-8892	80	*[250]	ONTOLOGY[250]
56-21	8893-8899	Warmup	*[250]	ONTOLOGY[250]
56-22	8900-8905	ratio	*[250]	ONTOLOGY[250]
56-23	8906-8907	=	*[250]	ONTOLOGY[250]
56-24	8908-8911	0.1	*[250]	ONTOLOGY[250]
56-25	8912-8920	Learning	*[250]	ONTOLOGY[250]
56-26	8921-8925	rate	*[250]	ONTOLOGY[250]
56-27	8926-8927	=	_	_
56-28	8928-8930	5e	_	_
56-29	8930-8931	-	_	_
56-30	8931-8932	6	_	_
56-31	8933-8938	Batch	_	_
56-32	8939-8943	size	_	_
56-33	8944-8945	=	_	_
56-34	8946-8948	32	_	_
56-35	8949-8959	Checkpoint	_	_
56-36	8960-8964	step	_	_
56-37	8965-8966	=	_	_
56-38	8967-8973	103500	_	_
56-39	8974-8985	Contrastive	_	_
56-40	8986-8990	loss	_	_
56-41	8991-9002	temperature	_	_
56-42	9003-9004	=	_	_
56-43	9005-9009	0.01	_	_
56-44	9010-9011	`	_	_
56-45	9011-9012	`	_	_
56-46	9012-9013	`	_	_
56-47	9015-9023	Specific	_	_
56-48	9024-9032	training	_	_
56-49	9033-9040	details	_	_
56-50	9041-9044	and	_	_
56-51	9045-9055	strategies	_	_
56-52	9056-9060	will	_	_
56-53	9061-9063	be	_	_
56-54	9064-9073	published	_	_
56-55	9074-9081	shortly	_	_

#Text=.
57-1	9081-9082	.	_	_

#Text=# Evaluation  The model was evaluated using the \[MTEB Evaluation\](https://huggingface.co/mteb) suite
58-1	9084-9085	#	_	_
58-2	9086-9096	Evaluation	_	_
58-3	9098-9101	The	_	_
58-4	9102-9107	model	_	_
58-5	9108-9111	was	_	_
58-6	9112-9121	evaluated	_	_
58-7	9122-9127	using	_	_
58-8	9128-9131	the	_	_
58-9	9132-9133	\[	_	_
58-10	9133-9137	MTEB	_	_
58-11	9138-9148	Evaluation	_	_
58-12	9148-9149	\]	_	_
58-13	9149-9150	(	_	_
58-14	9150-9155	https	_	_
58-15	9155-9156	:	_	_
58-16	9156-9157	/	_	_
58-17	9157-9158	/	_	_
58-18	9158-9172	huggingface.co	_	_
58-19	9172-9173	/	_	_
58-20	9173-9177	mteb	_	_
58-21	9177-9178	)	*[238]	DATASET[238]
58-22	9179-9184	suite	_	_

#Text=.
59-1	9184-9185	.	_	_

#Text=# Citation Please cite our work if you use GISTEmbed or the datasets we published in your projects or research  ``` @article{solatorio2024gistembed,     title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning},     author={Aivin V.
60-1	9188-9189	#	_	_
60-2	9190-9198	Citation	_	_
60-3	9199-9205	Please	_	_
60-4	9206-9210	cite	_	_
60-5	9211-9214	our	_	_
60-6	9215-9219	work	_	_
60-7	9220-9222	if	_	_
60-8	9223-9226	you	_	_
60-9	9227-9230	use	_	_
60-10	9231-9240	GISTEmbed	_	_
60-11	9241-9243	or	_	_
60-12	9244-9247	the	_	_
60-13	9248-9256	datasets	_	_
60-14	9257-9259	we	_	_
60-15	9260-9269	published	_	_
60-16	9270-9272	in	_	_
60-17	9273-9277	your	_	_
60-18	9278-9286	projects	_	_
60-19	9287-9289	or	_	_
60-20	9290-9298	research	_	_
60-21	9300-9301	`	_	_
60-22	9301-9302	`	_	_
60-23	9302-9303	`	_	_
60-24	9304-9305	@	_	_
60-25	9305-9312	article	_	_
60-26	9312-9313	{	_	_
60-27	9313-9335	solatorio2024gistembed	_	_
60-28	9335-9336	,	_	_
60-29	9341-9346	title	_	_
60-30	9346-9347	=	_	_
60-31	9347-9348	{	_	_
60-32	9348-9357	GISTEmbed	_	_
60-33	9357-9358	:	_	_
60-34	9359-9365	Guided	_	_
60-35	9366-9375	In-sample	_	_
60-36	9376-9385	Selection	_	_
60-37	9386-9388	of	_	_
60-38	9389-9397	Training	_	_
60-39	9398-9407	Negatives	_	_
60-40	9408-9411	for	*[276]	PROGLANG[276]
60-41	9412-9416	Text	_	_
60-42	9417-9426	Embedding	_	_
60-43	9427-9438	Fine-tuning	_	_
60-44	9438-9439	}	_	_
60-45	9439-9440	,	_	_
60-46	9445-9451	author	_	_
60-47	9451-9452	=	_	_
60-48	9452-9453	{	_	_
60-49	9453-9458	Aivin	_	_
60-50	9459-9460	V	_	_
60-51	9460-9461	.	_	_

#Text=Solatorio},     journal={arXiv preprint arXiv:2402.16829},     year={2024},     URL={https://arxiv.org/abs/2402.16829}     eprint={2402.16829},     archivePrefix={arXiv},     primaryClass={cs.LG} } ```  # Acknowledgements  This work is supported by the "KCP IV - Exploring Data Use in the Development Economics Literature using Large Language Models (AI and LLMs)" project funded by the \[Knowledge for Change Program (KCP)\](https://www.worldbank.org/en/programs/knowledge-for-change) of the World Bank - RA-P503405-RESE-TF0C3444.
61-1	9462-9471	Solatorio	_	_
61-2	9471-9472	}	_	_
61-3	9472-9473	,	_	_
61-4	9478-9485	journal	_	_
61-5	9485-9486	=	_	_
61-6	9486-9487	{	_	_
61-7	9487-9492	arXiv	_	_
61-8	9493-9501	preprint	_	_
61-9	9502-9507	arXiv	_	_
61-10	9507-9508	:	_	_
61-11	9508-9518	2402.16829	_	_
61-12	9518-9519	}	_	_
61-13	9519-9520	,	_	_
61-14	9525-9529	year	_	_
61-15	9529-9530	=	_	_
61-16	9530-9531	{	_	_
61-17	9531-9535	2024	_	_
61-18	9535-9536	}	_	_
61-19	9536-9537	,	_	_
61-20	9542-9545	URL	_	_
61-21	9545-9546	=	_	_
61-22	9546-9547	{	_	_
61-23	9547-9552	https	_	_
61-24	9552-9553	:	_	_
61-25	9553-9554	/	_	_
61-26	9554-9555	/	_	_
61-27	9555-9564	arxiv.org	_	_
61-28	9564-9565	/	_	_
61-29	9565-9568	abs	_	_
61-30	9568-9569	/	_	_
61-31	9569-9579	2402.16829	_	_
61-32	9579-9580	}	_	_
61-33	9585-9591	eprint	_	_
61-34	9591-9592	=	_	_
61-35	9592-9593	{	_	_
61-36	9593-9603	2402.16829	_	_
61-37	9603-9604	}	_	_
61-38	9604-9605	,	_	_
61-39	9610-9623	archivePrefix	_	_
61-40	9623-9624	=	_	_
61-41	9624-9625	{	_	_
61-42	9625-9630	arXiv	_	_
61-43	9630-9631	}	_	_
61-44	9631-9632	,	_	_
61-45	9637-9649	primaryClass	_	_
61-46	9649-9650	=	_	_
61-47	9650-9651	{	_	_
61-48	9651-9656	cs.LG	_	_
61-49	9656-9657	}	_	_
61-50	9658-9659	}	_	_
61-51	9660-9661	`	_	_
61-52	9661-9662	`	_	_
61-53	9662-9663	`	_	_
61-54	9665-9666	#	_	_
61-55	9667-9683	Acknowledgements	_	_
61-56	9685-9689	This	_	_
61-57	9690-9694	work	_	_
61-58	9695-9697	is	_	_
61-59	9698-9707	supported	_	_
61-60	9708-9710	by	_	_
61-61	9711-9714	the	_	_
61-62	9715-9716	"	_	_
61-63	9716-9719	KCP	_	_
61-64	9720-9722	IV	_	_
61-65	9723-9724	-	_	_
61-66	9725-9734	Exploring	_	_
61-67	9735-9739	Data	_	_
61-68	9740-9743	Use	_	_
61-69	9744-9746	in	_	_
61-70	9747-9750	the	_	_
61-71	9751-9762	Development	_	_
61-72	9763-9772	Economics	_	_
61-73	9773-9783	Literature	_	_
61-74	9784-9789	using	_	_
61-75	9790-9795	Large	_	_
61-76	9796-9804	Language	_	_
61-77	9805-9811	Models	_	_
61-78	9812-9813	(	_	_
61-79	9813-9815	AI	_	_
61-80	9816-9819	and	_	_
61-81	9820-9824	LLMs	_	_
61-82	9824-9825	)	_	_
61-83	9825-9826	"	_	_
61-84	9827-9834	project	_	_
61-85	9835-9841	funded	_	_
61-86	9842-9844	by	_	_
61-87	9845-9848	the	_	_
61-88	9849-9850	\[	_	_
61-89	9850-9859	Knowledge	_	_
61-90	9860-9863	for	_	_
61-91	9864-9870	Change	_	_
61-92	9871-9878	Program	_	_
61-93	9879-9880	(	_	_
61-94	9880-9883	KCP	_	_
61-95	9883-9884	)	_	_
61-96	9884-9885	\]	_	_
61-97	9885-9886	(	_	_
61-98	9886-9891	https	_	_
61-99	9891-9892	:	_	_
61-100	9892-9893	/	_	_
61-101	9893-9894	/	_	_
61-102	9894-9911	www.worldbank.org	_	_
61-103	9911-9912	/	*[277]	PROGLANG[277]
61-104	9912-9914	en	*[277]	PROGLANG[277]
61-105	9914-9915	/	*[277]	PROGLANG[277]
61-106	9915-9923	programs	*[277]	PROGLANG[277]
61-107	9923-9924	/	*[277]	PROGLANG[277]
61-108	9924-9944	knowledge-for-change	*[277]	PROGLANG[277]
61-109	9944-9945	)	*[277]	PROGLANG[277]
61-110	9946-9948	of	*[277]	PROGLANG[277]
61-111	9949-9952	the	*[277]	PROGLANG[277]
61-112	9953-9958	World	*[277]	PROGLANG[277]
61-113	9959-9963	Bank	*[277]	PROGLANG[277]
61-114	9964-9965	-	*[277]	PROGLANG[277]
61-115	9966-9976	RA-P503405	*[277]	PROGLANG[277]
61-116	9976-9977	-	_	_
61-117	9977-9990	RESE-TF0C3444	_	_
61-118	9990-9991	.	_	_

#Text=The findings, interpretations, and conclusions expressed in this material are entirely those of the authors.
62-1	9993-9996	The	_	_
62-2	9997-10005	findings	_	_
62-3	10005-10006	,	_	_
62-4	10007-10022	interpretations	_	_
62-5	10022-10023	,	_	_
62-6	10024-10027	and	_	_
62-7	10028-10039	conclusions	_	_
62-8	10040-10049	expressed	_	_
62-9	10050-10052	in	_	_
62-10	10053-10057	this	_	_
62-11	10058-10066	material	_	_
62-12	10067-10070	are	_	_
62-13	10071-10079	entirely	_	_
62-14	10080-10085	those	_	_
62-15	10086-10088	of	_	_
62-16	10089-10092	the	_	_
62-17	10093-10100	authors	_	_
62-18	10100-10101	.	_	_

#Text=They do not necessarily represent the views of the International Bank for Reconstruction and Development/World Bank and its affiliated organizations, or those of the Executive Directors of the World Bank or the governments they represent.
63-1	10102-10106	They	_	_
63-2	10107-10109	do	_	_
63-3	10110-10113	not	_	_
63-4	10114-10125	necessarily	_	_
63-5	10126-10135	represent	_	_
63-6	10136-10139	the	_	_
63-7	10140-10145	views	_	_
63-8	10146-10148	of	_	_
63-9	10149-10152	the	_	_
63-10	10153-10166	International	_	_
63-11	10167-10171	Bank	*[239]	DATASET[239]
63-12	10172-10175	for	*[239]	DATASET[239]
63-13	10176-10190	Reconstruction	*[239]	DATASET[239]
63-14	10191-10194	and	*[239]	DATASET[239]
63-15	10195-10206	Development	*[239]	DATASET[239]
63-16	10206-10207	/	*[239]	DATASET[239]
63-17	10207-10212	World	*[239]	DATASET[239]
63-18	10213-10217	Bank	*[239]	DATASET[239]
63-19	10218-10221	and	*[239]	DATASET[239]
63-20	10222-10225	its	*[239]	DATASET[239]
63-21	10226-10236	affiliated	*[239]	DATASET[239]
63-22	10237-10250	organizations	*[239]	DATASET[239]
63-23	10250-10251	,	*[239]	DATASET[239]
63-24	10252-10254	or	*[239]	DATASET[239]
63-25	10255-10260	those	*[239]	DATASET[239]
63-26	10261-10263	of	*[239]	DATASET[239]
63-27	10264-10267	the	*[239]	DATASET[239]
63-28	10268-10277	Executive	*[239]	DATASET[239]
63-29	10278-10287	Directors	*[239]	DATASET[239]
63-30	10288-10290	of	*[239]	DATASET[239]
63-31	10291-10294	the	*[239]	DATASET[239]
63-32	10295-10300	World	*[239]	DATASET[239]
63-33	10301-10305	Bank	*[239]	DATASET[239]
63-34	10306-10308	or	*[239]	DATASET[239]
63-35	10309-10312	the	*[239]	DATASET[239]
63-36	10313-10324	governments	_	_
63-37	10325-10329	they	_	_
63-38	10330-10339	represent	_	_
63-39	10339-10340	.	_	_

#Text=We also send 🤗  to the HuggingFace 🤗 , Sentence Transformers, PyTorch, and to all open-sourced projects for all the open-sourced software they release.
64-1	10342-10344	We	_	_
64-2	10345-10349	also	_	_
64-3	10350-10354	send	_	_
64-4	10355-10357	🤗	_	_
64-5	10358-10360	to	_	_
64-6	10361-10364	the	_	_
64-7	10365-10376	HuggingFace	_	_
64-8	10377-10379	🤗	_	_
64-9	10379-10380	,	_	_
64-10	10381-10389	Sentence	_	_
64-11	10390-10402	Transformers	_	_
64-12	10402-10403	,	_	_
64-13	10404-10411	PyTorch	_	_
64-14	10411-10412	,	_	_
64-15	10413-10416	and	_	_
64-16	10417-10419	to	_	_
64-17	10420-10423	all	_	_
64-18	10424-10436	open-sourced	_	_
64-19	10437-10445	projects	_	_
64-20	10446-10449	for	*[240]	DATASET[240]
64-21	10450-10453	all	*[240]	DATASET[240]
64-22	10454-10457	the	_	_
64-23	10458-10470	open-sourced	_	_
64-24	10471-10479	software	_	_
64-25	10480-10484	they	_	_
64-26	10485-10492	release	_	_
64-27	10492-10493	.	_	_