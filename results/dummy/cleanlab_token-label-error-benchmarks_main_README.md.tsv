#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# Benchmarking methods for label error detection in token classification data  Code to reproduce results from the paper:  \[\*\*Detecting Label Errors in Token Classification Data\*\*\](https://arxiv.org/abs/2210.03920)   \*NeurIPS 2022 Workshop on Interactive Learning for Natural Language Processing (InterNLP)\*  This repository is only for intended for scientific purposes.
1-1	0-1	#	_	_
1-2	2-14	Benchmarking	_	_
1-3	15-22	methods	_	_
1-4	23-26	for	_	_
1-5	27-32	label	_	_
1-6	33-38	error	_	_
1-7	39-48	detection	_	_
1-8	49-51	in	_	_
1-9	52-57	token	_	_
1-10	58-72	classification	_	_
1-11	73-77	data	_	_
1-12	79-83	Code	_	_
1-13	84-86	to	_	_
1-14	87-96	reproduce	_	_
1-15	97-104	results	_	_
1-16	105-109	from	_	_
1-17	110-113	the	_	_
1-18	114-119	paper	_	_
1-19	119-120	:	_	_
1-20	122-123	\[	_	_
1-21	123-124	\*	_	_
1-22	124-125	\*	_	_
1-23	125-134	Detecting	_	_
1-24	135-140	Label	_	_
1-25	141-147	Errors	_	_
1-26	148-150	in	_	_
1-27	151-156	Token	_	_
1-28	157-171	Classification	_	_
1-29	172-176	Data	_	_
1-30	176-177	\*	_	_
1-31	177-178	\*	_	_
1-32	178-179	\]	_	_
1-33	179-180	(	_	_
1-34	180-185	https	_	_
1-35	185-186	:	_	_
1-36	186-187	/	_	_
1-37	187-188	/	_	_
1-38	188-197	arxiv.org	_	_
1-39	197-198	/	_	_
1-40	198-201	abs	_	_
1-41	201-202	/	_	_
1-42	202-212	2210.03920	_	_
1-43	212-213	)	_	_
1-44	216-217	\*	_	_
1-45	217-224	NeurIPS	_	_
1-46	225-229	2022	_	_
1-47	230-238	Workshop	_	_
1-48	239-241	on	_	_
1-49	242-253	Interactive	_	_
1-50	254-262	Learning	_	_
1-51	263-266	for	_	_
1-52	267-274	Natural	_	_
1-53	275-283	Language	_	_
1-54	284-294	Processing	_	_
1-55	295-296	(	*[181]	PROGLANG[181]
1-56	296-304	InterNLP	*[181]	PROGLANG[181]
1-57	304-305	)	*[181]	PROGLANG[181]
1-58	305-306	\*	*[181]	PROGLANG[181]
1-59	308-312	This	*[181]	PROGLANG[181]
1-60	313-323	repository	*[181]	PROGLANG[181]
1-61	324-326	is	*[181]	PROGLANG[181]
1-62	327-331	only	*[181]	PROGLANG[181]
1-63	332-335	for	*[181]	PROGLANG[181]
1-64	336-344	intended	*[181]	PROGLANG[181]
1-65	345-348	for	*[181]	PROGLANG[181]
1-66	349-359	scientific	_	_
1-67	360-368	purposes	_	_
1-68	368-369	.	_	_

#Text=To find label errors in your own token classification data, you should instead use \[the implementation\](https://docs.cleanlab.ai/stable/tutorials/token\_classification.html) from the official \[cleanlab library\](https://github.com/cleanlab/cleanlab).   #### Install Cleanlab Package  ---  Install the Cleanlab version used for our experiments: `pip install .
2-1	370-372	To	_	_
2-2	373-377	find	_	_
2-3	378-383	label	_	_
2-4	384-390	errors	_	_
2-5	391-393	in	_	_
2-6	394-398	your	_	_
2-7	399-402	own	_	_
2-8	403-408	token	_	_
2-9	409-423	classification	_	_
2-10	424-428	data	_	_
2-11	428-429	,	_	_
2-12	430-433	you	_	_
2-13	434-440	should	_	_
2-14	441-448	instead	_	_
2-15	449-452	use	_	_
2-16	453-454	\[	_	_
2-17	454-457	the	_	_
2-18	458-472	implementation	_	_
2-19	472-473	\]	_	_
2-20	473-474	(	_	_
2-21	474-479	https	_	_
2-22	479-480	:	_	_
2-23	480-481	/	_	_
2-24	481-482	/	_	_
2-25	482-498	docs.cleanlab.ai	_	_
2-25	487-495	cleanlab	_	_
2-26	498-499	/	_	_
2-27	499-505	stable	_	_
2-28	505-506	/	_	_
2-29	506-515	tutorials	_	_
2-30	515-516	/	_	_
2-31	516-541	token\_classification.html	_	_
2-32	541-542	)	_	_
2-33	543-547	from	_	_
2-34	548-551	the	_	_
2-35	552-560	official	_	_
2-36	561-562	\[	*[163]	LICENSE[163]
2-37	562-570	cleanlab	*[163]	LICENSE[163]
2-38	571-578	library	*[163]	LICENSE[163]
2-39	578-579	\]	*[163]	LICENSE[163]
2-40	579-580	(	*[163]	LICENSE[163]
2-41	580-585	https	*[163]	LICENSE[163]
2-42	585-586	:	*[163]	LICENSE[163]
2-43	586-587	/	*[163]	LICENSE[163]
2-44	587-588	/	*[163]	LICENSE[163]
2-45	588-598	github.com	*[163]	LICENSE[163]
2-46	598-599	/	*[163]	LICENSE[163]
2-47	599-607	cleanlab	*[163]	LICENSE[163]
2-48	607-608	/	*[163]	LICENSE[163]
2-49	608-616	cleanlab	*[163]	LICENSE[163]
2-50	616-617	)	*[163]	LICENSE[163]
2-51	617-618	.	*[163]	LICENSE[163]
2-52	621-622	#	*[163]	LICENSE[163]
2-53	622-623	#	*[163]	LICENSE[163]
2-54	623-624	#	*[163]	LICENSE[163]
2-55	624-625	#	_	_
2-56	626-633	Install	_	_
2-57	634-642	Cleanlab	_	_
2-58	643-650	Package	_	_
2-59	652-653	-	_	_
2-60	653-654	-	_	_
2-61	654-655	-	_	_
2-62	657-664	Install	_	_
2-63	665-668	the	_	_
2-64	669-677	Cleanlab	_	_
2-65	678-685	version	_	_
2-66	686-690	used	_	_
2-67	691-694	for	_	_
2-68	695-698	our	_	_
2-69	699-710	experiments	_	_
2-70	710-711	:	_	_
2-71	712-713	`	_	_
2-72	713-716	pip	_	_
2-73	717-724	install	_	_
2-74	725-726	.	_	_

#Text=/cleanlab`  #### Download Datasets  --- CoNLL-2003:  - Original paper: \[Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition\](https://arxiv.org/pdf/cs/0306050v1.pdf)  - Original dataset: \[Papers with Code\](https://paperswithcode.com/dataset/conll-2003)
3-1	726-727	/	_	_
3-2	727-735	cleanlab	_	_
3-3	735-736	`	_	_
3-4	738-739	#	_	_
3-5	739-740	#	_	_
3-6	740-741	#	_	_
3-7	741-742	#	_	_
3-8	743-751	Download	_	_
3-9	752-760	Datasets	_	_
3-10	762-763	-	*[164]	LICENSE[164]
3-11	763-764	-	*[164]	LICENSE[164]
3-12	764-765	-	*[164]	LICENSE[164]
3-13	766-771	CoNLL	*[164]	LICENSE[164]
3-14	771-772	-	*[164]	LICENSE[164]
3-15	772-776	2003	*[164]	LICENSE[164]
3-16	776-777	:	*[164]	LICENSE[164]
3-17	779-780	-	*[164]	LICENSE[164]
3-18	781-789	Original	*[164]	LICENSE[164]
3-19	790-795	paper	*[164]	LICENSE[164]
3-20	795-796	:	*[164]	LICENSE[164]
3-21	797-798	\[	*[164]	LICENSE[164]
3-22	798-810	Introduction	*[164]	LICENSE[164]
3-23	811-813	to	*[164]	LICENSE[164]
3-24	814-817	the	*[164]	LICENSE[164]
3-25	818-823	CoNLL	*[164]	LICENSE[164]
3-26	823-824	-	*[164]	LICENSE[164]
3-27	824-828	2003	*[164]	LICENSE[164]
3-28	829-835	Shared	*[164]	LICENSE[164]
3-29	836-840	Task	*[164]	LICENSE[164]
3-30	840-841	:	*[164]	LICENSE[164]
3-31	842-862	Language-Independent	*[164]	LICENSE[164]
3-32	863-868	Named	_	_
3-33	869-875	Entity	_	_
3-34	876-887	Recognition	_	_
3-35	887-888	\]	_	_
3-36	888-889	(	_	_
3-37	889-894	https	_	_
3-38	894-895	:	_	_
3-39	895-896	/	_	_
3-40	896-897	/	_	_
3-41	897-906	arxiv.org	_	_
3-42	906-907	/	_	_
3-43	907-910	pdf	_	_
3-44	910-911	/	_	_
3-45	911-913	cs	_	_
3-46	913-914	/	_	_
3-47	914-923	0306050v1	_	_
3-48	923-924	.	_	_
3-49	924-927	pdf	_	_
3-50	927-928	)	_	_
3-51	930-931	-	_	_
3-52	932-940	Original	_	_
3-53	941-948	dataset	_	_
3-54	948-949	:	_	_
3-55	950-951	\[	_	_
3-56	951-957	Papers	_	_
3-57	958-962	with	_	_
3-58	963-967	Code	_	_
3-59	967-968	\]	_	_
3-60	968-969	(	_	_
3-61	969-974	https	_	_
3-62	974-975	:	_	_
3-63	975-976	/	_	_
3-64	976-977	/	_	_
3-65	977-995	paperswithcode.com	_	_
3-66	995-996	/	_	_
3-67	996-1003	dataset	_	_
3-68	1003-1004	/	_	_
3-69	1004-1009	conll	_	_
3-70	1009-1010	-	_	_
3-71	1010-1014	2003	_	_
3-72	1014-1015	)	_	_

#Text=.
4-1	1015-1016	.	_	_

#Text=- Verified Labels (CoNLL++): https://github.com/ZihanWangKi/CrossWeigh/tree/master/data   #### Experiments  ---   `token-classification-benchmark.ipynb`: We implement 11 different methods of aggregating the label quality scores for each token to obtain an overall score per sentence, and evaluate the precision-recall curve and related label-error detection metrics for each method.
5-1	1018-1019	-	_	_
5-2	1020-1028	Verified	_	_
5-3	1029-1035	Labels	_	_
5-4	1036-1037	(	_	_
5-5	1037-1042	CoNLL	_	_
5-6	1042-1043	+	_	_
5-7	1043-1044	+	_	_
5-8	1044-1045	)	_	_
5-9	1045-1046	:	_	_
5-10	1047-1052	https	_	_
5-11	1052-1053	:	_	_
5-12	1053-1054	/	_	_
5-13	1054-1055	/	_	_
5-14	1055-1065	github.com	_	_
5-15	1065-1066	/	_	_
5-16	1066-1077	ZihanWangKi	_	_
5-17	1077-1078	/	_	_
5-18	1078-1088	CrossWeigh	_	_
5-19	1088-1089	/	_	_
5-20	1089-1093	tree	_	_
5-21	1093-1094	/	_	_
5-22	1094-1100	master	_	_
5-23	1100-1101	/	_	_
5-24	1101-1105	data	_	_
5-25	1108-1109	#	_	_
5-26	1109-1110	#	_	_
5-27	1110-1111	#	_	_
5-28	1111-1112	#	_	_
5-29	1113-1124	Experiments	_	_
5-30	1126-1127	-	_	_
5-31	1127-1128	-	_	_
5-32	1128-1129	-	_	_
5-33	1132-1133	`	_	_
5-34	1133-1169	token-classification-benchmark.ipynb	_	_
5-35	1169-1170	`	_	_
5-36	1170-1171	:	_	_
5-37	1172-1174	We	_	_
5-38	1175-1184	implement	_	_
5-39	1185-1187	11	_	_
5-40	1188-1197	different	_	_
5-41	1198-1205	methods	_	_
5-42	1206-1208	of	_	_
5-43	1209-1220	aggregating	_	_
5-44	1221-1224	the	_	_
5-45	1225-1230	label	_	_
5-46	1231-1238	quality	_	_
5-47	1239-1245	scores	_	_
5-48	1246-1249	for	_	_
5-49	1250-1254	each	_	_
5-50	1255-1260	token	_	_
5-51	1261-1263	to	_	_
5-52	1264-1270	obtain	_	_
5-53	1271-1273	an	_	_
5-54	1274-1281	overall	_	_
5-55	1282-1287	score	_	_
5-56	1288-1291	per	_	_
5-57	1292-1300	sentence	_	_
5-58	1300-1301	,	_	_
5-59	1302-1305	and	_	_
5-60	1306-1314	evaluate	_	_
5-61	1315-1318	the	_	_
5-62	1319-1335	precision-recall	_	_
5-63	1336-1341	curve	_	_
5-64	1342-1345	and	_	_
5-65	1346-1353	related	_	_
5-66	1354-1365	label-error	_	_
5-67	1366-1375	detection	_	_
5-68	1376-1383	metrics	_	_
5-69	1384-1387	for	_	_
5-70	1388-1392	each	_	_
5-71	1393-1399	method	_	_
5-72	1399-1400	.	_	_

#Text=We consider the named entity recognition dataset CoNLL-2003, and use CoNLL++ as the ground truth.
6-1	1401-1403	We	_	_
6-2	1404-1412	consider	_	_
6-3	1413-1416	the	_	_
6-4	1417-1422	named	_	_
6-5	1423-1429	entity	*[149]	PUBLICATION[149]
6-6	1430-1441	recognition	_	_
6-7	1442-1449	dataset	_	_
6-8	1450-1455	CoNLL	_	_
6-9	1455-1456	-	_	_
6-10	1456-1460	2003	_	_
6-11	1460-1461	,	_	_
6-12	1462-1465	and	_	_
6-13	1466-1469	use	_	_
6-14	1470-1475	CoNLL	_	_
6-15	1475-1476	+	_	_
6-16	1476-1477	+	_	_
6-17	1478-1480	as	_	_
6-18	1481-1484	the	_	_
6-19	1485-1491	ground	_	_
6-20	1492-1497	truth	_	_
6-21	1497-1498	.	_	_

#Text=`token-level.ipynb`: We examine the token-level label errors for the same dataset (rather than sentence-level).
7-1	1500-1501	`	_	_
7-2	1501-1518	token-level.ipynb	_	_
7-3	1518-1519	`	_	_
7-4	1519-1520	:	_	_
7-5	1521-1523	We	_	_
7-6	1524-1531	examine	_	_
7-7	1532-1535	the	_	_
7-8	1536-1547	token-level	_	_
7-9	1548-1553	label	_	_
7-10	1554-1560	errors	_	_
7-11	1561-1564	for	_	_
7-12	1565-1568	the	_	_
7-13	1569-1573	same	_	_
7-14	1574-1581	dataset	_	_
7-15	1582-1583	(	_	_
7-16	1583-1589	rather	_	_
7-17	1590-1594	than	_	_
7-18	1595-1609	sentence-level	_	_
7-19	1609-1610	)	_	_
7-20	1610-1611	.	_	_

#Text=We examine the distribution of the label errors by class, and evaluate different label quality scoring methods on the token-level.
8-1	1612-1614	We	_	_
8-2	1615-1622	examine	_	_
8-3	1623-1626	the	_	_
8-4	1627-1639	distribution	_	_
8-5	1640-1642	of	_	_
8-6	1643-1646	the	_	_
8-7	1647-1652	label	_	_
8-8	1653-1659	errors	_	_
8-9	1660-1662	by	*[150]	PUBLICATION[150]
8-10	1663-1668	class	*[150]	PUBLICATION[150]
8-11	1668-1669	,	*[150]	PUBLICATION[150]
8-12	1670-1673	and	*[150]	PUBLICATION[150]
8-13	1674-1682	evaluate	*[150]	PUBLICATION[150]
8-14	1683-1692	different	_	_
8-15	1693-1698	label	_	_
8-16	1699-1706	quality	_	_
8-17	1707-1714	scoring	_	_
8-18	1715-1722	methods	_	_
8-19	1723-1725	on	_	_
8-20	1726-1729	the	_	_
8-21	1730-1741	token-level	_	_
8-22	1741-1742	.	_	_