#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=<h1 align="center">To Cool or not to Cool?
1-1	0-1	<	_	_
1-2	1-3	h1	_	_
1-3	4-9	align	_	_
1-4	9-10	=	_	_
1-5	10-11	"	_	_
1-6	11-17	center	_	_
1-7	17-18	"	_	_
1-8	18-19	>	_	_
1-9	19-21	To	_	_
1-10	22-26	Cool	_	_
1-11	27-29	or	*[150]	PROGLANG[150]
1-12	30-33	not	_	_
1-13	34-36	to	_	_
1-14	37-41	Cool	_	_
1-15	41-42	?	_	_

#Text=<br> Temperature Network Meets Large Foundation Models via DRO </h1>  The temperature parameter plays a profound role  during training and/or inference with large foundation models (LFMs) such as large language models (LLMs) and CLIP models.
2-1	43-44	<	_	_
2-2	44-46	br	_	_
2-3	46-47	>	_	_
2-4	48-59	Temperature	_	_
2-5	60-67	Network	_	_
2-6	68-73	Meets	_	_
2-7	74-79	Large	_	_
2-8	80-90	Foundation	_	_
2-9	91-97	Models	_	_
2-10	98-101	via	_	_
2-11	102-105	DRO	_	_
2-12	106-107	<	_	_
2-13	107-108	/	_	_
2-14	108-110	h1	_	_
2-15	110-111	>	_	_
2-16	113-116	The	_	_
2-17	117-128	temperature	_	_
2-18	129-138	parameter	_	_
2-19	139-144	plays	_	_
2-20	145-146	a	_	_
2-21	147-155	profound	_	_
2-22	156-160	role	_	_
2-23	162-168	during	_	_
2-24	169-177	training	_	_
2-25	178-181	and	_	_
2-26	181-182	/	_	_
2-27	182-184	or	_	_
2-28	185-194	inference	_	_
2-29	195-199	with	_	_
2-30	200-205	large	_	_
2-31	206-216	foundation	_	_
2-32	217-223	models	_	_
2-33	224-225	(	_	_
2-34	225-229	LFMs	_	_
2-35	229-230	)	_	_
2-36	231-235	such	_	_
2-37	236-238	as	_	_
2-38	239-244	large	_	_
2-39	245-253	language	_	_
2-40	254-260	models	_	_
2-41	261-262	(	_	_
2-42	262-266	LLMs	_	_
2-43	266-267	)	_	_
2-44	268-271	and	_	_
2-45	272-276	CLIP	_	_
2-46	277-283	models	_	_
2-47	283-284	.	_	_

#Text=Particularly, it adjusts the logits in the softmax function in LLMs, which is crucial for next token generation, and it scales the similarities in the contrastive loss for training CLIP models.
3-1	285-297	Particularly	_	_
3-2	297-298	,	_	_
3-3	299-301	it	_	_
3-4	302-309	adjusts	_	_
3-5	310-313	the	_	_
3-6	314-320	logits	_	_
3-7	321-323	in	_	_
3-8	324-327	the	_	_
3-9	328-335	softmax	_	_
3-10	336-344	function	_	_
3-11	345-347	in	_	_
3-12	348-352	LLMs	_	_
3-13	352-353	,	_	_
3-14	354-359	which	_	_
3-15	360-362	is	_	_
3-16	363-370	crucial	_	_
3-17	371-374	for	_	_
3-18	375-379	next	_	_
3-19	380-385	token	_	_
3-20	386-396	generation	_	_
3-21	396-397	,	_	_
3-22	398-401	and	_	_
3-23	402-404	it	_	_
3-24	405-411	scales	_	_
3-25	412-415	the	_	_
3-26	416-428	similarities	_	_
3-27	429-431	in	_	_
3-28	432-435	the	_	_
3-29	436-447	contrastive	_	_
3-30	448-452	loss	*[136]	EVALMETRIC[136]
3-31	453-456	for	*[136]	EVALMETRIC[136]
3-32	457-465	training	*[136]	EVALMETRIC[136]
3-33	466-470	CLIP	*[136]	EVALMETRIC[136]
3-34	471-477	models	*[136]	EVALMETRIC[136]
3-35	477-478	.	_	_

#Text=A significant question remains: "\*Is it viable to learn a neural network to predict a personalized temperature of any input data for enhancing LFMs?
4-1	479-480	A	_	_
4-2	481-492	significant	_	_
4-3	493-501	question	_	_
4-4	502-509	remains	_	_
4-5	509-510	:	*[141]	SOFTWARE[141]
4-6	511-512	"	*[141]	SOFTWARE[141]
4-7	512-513	\*	*[141]	SOFTWARE[141]
4-8	513-515	Is	*[141]	SOFTWARE[141]
4-9	516-518	it	*[141]	SOFTWARE[141]
4-10	519-525	viable	*[141]	SOFTWARE[141]
4-11	526-528	to	*[141]	SOFTWARE[141]
4-12	529-534	learn	*[141]	SOFTWARE[141]
4-13	535-536	a	_	_
4-14	537-543	neural	_	_
4-15	544-551	network	_	_
4-16	552-554	to	_	_
4-17	555-562	predict	_	_
4-18	563-564	a	_	_
4-19	565-577	personalized	_	_
4-20	578-589	temperature	_	_
4-21	590-592	of	_	_
4-22	593-596	any	_	_
4-23	597-602	input	_	_
4-24	603-607	data	_	_
4-25	608-611	for	_	_
4-26	612-621	enhancing	_	_
4-27	622-626	LFMs	_	_
4-28	626-627	?	_	_

#Text=\*"  In this paper, we present \*\*a principled framework\*\* for learning a small yet generalizable temperature prediction network (TempNet) to improve LFMs.
5-1	627-628	\*	_	_
5-2	628-629	"	_	_
5-3	631-633	In	_	_
5-4	634-638	this	_	_
5-5	639-644	paper	_	_
5-6	644-645	,	_	_
5-7	646-648	we	_	_
5-8	649-656	present	_	_
5-9	657-658	\*	_	_
5-10	658-659	\*	_	_
5-11	659-660	a	_	_
5-12	661-671	principled	_	_
5-13	672-681	framework	_	_
5-14	681-682	\*	_	_
5-15	682-683	\*	_	_
5-16	684-687	for	_	_
5-17	688-696	learning	_	_
5-18	697-698	a	_	_
5-19	699-704	small	_	_
5-20	705-708	yet	_	_
5-21	709-722	generalizable	_	_
5-22	723-734	temperature	_	_
5-23	735-745	prediction	_	_
5-24	746-753	network	_	_
5-25	754-755	(	_	_
5-26	755-762	TempNet	_	_
5-27	762-763	)	_	_
5-28	764-766	to	_	_
5-29	767-774	improve	_	_
5-30	775-779	LFMs	_	_
5-31	779-780	.	_	_

#Text=Our solution is composed of a novel learning framework with robust losses underpinned by constrained distributionally robust optimization (DRO), and a properly designed TempNet with theoretical inspiration.
6-1	781-784	Our	_	_
6-2	785-793	solution	_	_
6-3	794-796	is	_	_
6-4	797-805	composed	_	_
6-5	806-808	of	_	_
6-6	809-810	a	_	_
6-7	811-816	novel	_	_
6-8	817-825	learning	_	_
6-9	826-835	framework	_	_
6-10	836-840	with	_	_
6-11	841-847	robust	_	_
6-12	848-854	losses	_	_
6-13	855-866	underpinned	_	_
6-14	867-869	by	_	_
6-15	870-881	constrained	_	_
6-16	882-898	distributionally	_	_
6-17	899-905	robust	_	_
6-18	906-918	optimization	*[127]	DATASET[127]
6-19	919-920	(	*[127]	DATASET[127]
6-20	920-923	DRO	*[127]	DATASET[127]
6-21	923-924	)	*[127]	DATASET[127]
6-22	924-925	,	*[127]	DATASET[127]
6-23	926-929	and	*[127]	DATASET[127]
6-24	930-931	a	*[127]	DATASET[127]
6-25	932-940	properly	*[127]	DATASET[127]
6-26	941-949	designed	*[127]	DATASET[127]
6-27	950-957	TempNet	*[127]	DATASET[127]
6-28	958-962	with	*[127]	DATASET[127]
6-29	963-974	theoretical	*[127]	DATASET[127]
6-30	975-986	inspiration	_	_
6-31	986-987	.	_	_

#Text=TempNet can be trained together with a large foundation model from scratch or learned separately given a pretrained foundation model.
7-1	988-995	TempNet	_	_
7-2	996-999	can	_	_
7-3	1000-1002	be	_	_
7-4	1003-1010	trained	_	_
7-5	1011-1019	together	_	_
7-6	1020-1024	with	_	_
7-7	1025-1026	a	_	_
7-8	1027-1032	large	_	_
7-9	1033-1043	foundation	_	_
7-10	1044-1049	model	_	_
7-11	1050-1054	from	_	_
7-12	1055-1062	scratch	_	_
7-13	1063-1065	or	_	_
7-14	1066-1073	learned	_	_
7-15	1074-1084	separately	_	_
7-16	1085-1090	given	_	_
7-17	1091-1092	a	_	_
7-18	1093-1103	pretrained	_	_
7-19	1104-1114	foundation	_	_
7-20	1115-1120	model	_	_
7-21	1120-1121	.	_	_

#Text=It is not only useful for predicting personalized temperature to promote the training of LFMs but also generalizable and transferable to new tasks.
8-1	1122-1124	It	_	_
8-2	1125-1127	is	_	_
8-3	1128-1131	not	_	_
8-4	1132-1136	only	_	_
8-5	1137-1143	useful	_	_
8-6	1144-1147	for	_	_
8-7	1148-1158	predicting	_	_
8-8	1159-1171	personalized	*[137]	EVALMETRIC[137]
8-9	1172-1183	temperature	*[137]	EVALMETRIC[137]
8-10	1184-1186	to	*[137]	EVALMETRIC[137]
8-11	1187-1194	promote	*[137]	EVALMETRIC[137]
8-12	1195-1198	the	*[137]	EVALMETRIC[137]
8-13	1199-1207	training	*[137]	EVALMETRIC[137]
8-14	1208-1210	of	*[137]	EVALMETRIC[137]
8-15	1211-1215	LFMs	_	_
8-16	1216-1219	but	_	_
8-17	1220-1224	also	_	_
8-18	1225-1238	generalizable	_	_
8-19	1239-1242	and	_	_
8-20	1243-1255	transferable	_	_
8-21	1256-1258	to	_	_
8-22	1259-1262	new	_	_
8-23	1263-1268	tasks	_	_
8-24	1268-1269	.	_	_

#Text=Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models.  ### Table of Contents    - \[Introduction\](#introduction) - \[Training\](#training) - \[Inference\](#inference) - \[Acknowledgment\](#acknowledgment) - \[Citation\](#citation)  ## Introduction  ### Our Proposed Method  We introduce \*\*a principled framework\*\* for developing a small yet generalizable network for temperature prediction, TempNet, aimed at enhancing large foundation models (LFMs) such as large language models (LLMs) and CLIP models.
9-1	1270-1273	Our	_	_
9-2	1274-1285	experiments	_	_
9-3	1286-1288	on	_	_
9-4	1289-1293	LLMs	_	_
9-5	1294-1297	and	_	_
9-6	1298-1302	CLIP	_	_
9-7	1303-1309	models	_	_
9-8	1310-1321	demonstrate	_	_
9-9	1322-1326	that	_	_
9-10	1327-1334	TempNet	_	_
9-11	1335-1342	greatly	_	_
9-12	1343-1351	improves	_	_
9-13	1352-1355	the	_	_
9-14	1356-1367	performance	_	_
9-15	1368-1370	of	_	_
9-16	1371-1379	existing	_	_
9-17	1380-1389	solutions	_	_
9-18	1390-1392	or	_	_
9-19	1393-1399	models	_	_
9-20	1399-1400	.	_	_
9-21	1402-1403	#	_	_
9-22	1403-1404	#	_	_
9-23	1404-1405	#	_	_
9-24	1406-1411	Table	_	_
9-25	1412-1414	of	_	_
9-26	1415-1423	Contents	_	_
9-27	1427-1428	-	_	_
9-28	1429-1430	\[	_	_
9-29	1430-1442	Introduction	_	_
9-30	1442-1443	\]	_	_
9-31	1443-1444	(	_	_
9-32	1444-1445	#	_	_
9-33	1445-1457	introduction	_	_
9-34	1457-1458	)	_	_
9-35	1459-1460	-	_	_
9-36	1461-1462	\[	_	_
9-37	1462-1470	Training	_	_
9-38	1470-1471	\]	_	_
9-39	1471-1472	(	_	_
9-40	1472-1473	#	_	_
9-41	1473-1481	training	_	_
9-42	1481-1482	)	_	_
9-43	1483-1484	-	_	_
9-44	1485-1486	\[	_	_
9-45	1486-1495	Inference	_	_
9-46	1495-1496	\]	_	_
9-47	1496-1497	(	_	_
9-48	1497-1498	#	_	_
9-49	1498-1507	inference	_	_
9-50	1507-1508	)	_	_
9-51	1509-1510	-	_	_
9-52	1511-1512	\[	_	_
9-53	1512-1526	Acknowledgment	_	_
9-54	1526-1527	\]	_	_
9-55	1527-1528	(	_	_
9-56	1528-1529	#	*[143]	LICENSE[143]
9-57	1529-1543	acknowledgment	_	_
9-58	1543-1544	)	_	_
9-59	1545-1546	-	_	_
9-60	1547-1548	\[	_	_
9-61	1548-1556	Citation	_	_
9-62	1556-1557	\]	_	_
9-63	1557-1558	(	_	_
9-64	1558-1559	#	_	_
9-65	1559-1567	citation	_	_
9-66	1567-1568	)	_	_
9-67	1570-1571	#	_	_
9-68	1571-1572	#	_	_
9-69	1573-1585	Introduction	_	_
9-70	1587-1588	#	_	_
9-71	1588-1589	#	_	_
9-72	1589-1590	#	_	_
9-73	1591-1594	Our	_	_
9-74	1595-1603	Proposed	_	_
9-75	1604-1610	Method	_	_
9-76	1612-1614	We	_	_
9-77	1615-1624	introduce	_	_
9-78	1625-1626	\*	_	_
9-79	1626-1627	\*	_	_
9-80	1627-1628	a	_	_
9-81	1629-1639	principled	_	_
9-82	1640-1649	framework	_	_
9-83	1649-1650	\*	_	_
9-84	1650-1651	\*	_	_
9-85	1652-1655	for	_	_
9-86	1656-1666	developing	_	_
9-87	1667-1668	a	_	_
9-88	1669-1674	small	_	_
9-89	1675-1678	yet	_	_
9-90	1679-1692	generalizable	_	_
9-91	1693-1700	network	_	_
9-92	1701-1704	for	_	_
9-93	1705-1716	temperature	_	_
9-94	1717-1727	prediction	_	_
9-95	1727-1728	,	_	_
9-96	1729-1736	TempNet	_	_
9-97	1736-1737	,	_	_
9-98	1738-1743	aimed	_	_
9-99	1744-1746	at	_	_
9-100	1747-1756	enhancing	_	_
9-101	1757-1762	large	_	_
9-102	1763-1773	foundation	_	_
9-103	1774-1780	models	_	_
9-104	1781-1782	(	_	_
9-105	1782-1786	LFMs	_	_
9-106	1786-1787	)	_	_
9-107	1788-1792	such	_	_
9-108	1793-1795	as	_	_
9-109	1796-1801	large	_	_
9-110	1802-1810	language	_	_
9-111	1811-1817	models	_	_
9-112	1818-1819	(	_	_
9-113	1819-1823	LLMs	_	_
9-114	1823-1824	)	_	_
9-115	1825-1828	and	_	_
9-116	1829-1833	CLIP	_	_
9-117	1834-1840	models	_	_
9-118	1840-1841	.	_	_

#Text=The Temperature Network is a plug-and-play architecture that can be implemented atop LFMs.
10-1	1842-1845	The	_	_
10-2	1846-1857	Temperature	_	_
10-3	1858-1865	Network	_	_
10-4	1866-1868	is	_	_
10-5	1869-1870	a	_	_
10-6	1871-1884	plug-and-play	_	_
10-7	1885-1897	architecture	_	_
10-8	1898-1902	that	*[146]	PROJECT[146]
10-9	1903-1906	can	*[146]	PROJECT[146]
10-10	1907-1909	be	_	_
10-11	1910-1921	implemented	_	_
10-12	1922-1926	atop	_	_
10-13	1927-1931	LFMs	_	_
10-14	1931-1932	.	_	_

#Text=Our solution is composed of a novel learning framework with robust losses underpinned by constrained distributionally robust optimization (DRO), and a properly designed TempNet with theoretical inspiration.
11-1	1933-1936	Our	_	_
11-2	1937-1945	solution	_	_
11-3	1946-1948	is	_	_
11-4	1949-1957	composed	_	_
11-5	1958-1960	of	_	_
11-6	1961-1962	a	_	_
11-7	1963-1968	novel	_	_
11-8	1969-1977	learning	_	_
11-9	1978-1987	framework	_	_
11-10	1988-1992	with	_	_
11-11	1993-1999	robust	_	_
11-12	2000-2006	losses	_	_
11-13	2007-2018	underpinned	_	_
11-14	2019-2021	by	_	_
11-15	2022-2033	constrained	_	_
11-16	2034-2050	distributionally	_	_
11-17	2051-2057	robust	_	_
11-18	2058-2070	optimization	_	_
11-19	2071-2072	(	_	_
11-20	2072-2075	DRO	_	_
11-21	2075-2076	)	_	_
11-22	2076-2077	,	_	_
11-23	2078-2081	and	*[142]	SOFTWARE[142]
11-24	2082-2083	a	*[142]	SOFTWARE[142]
11-25	2084-2092	properly	*[142]	SOFTWARE[142]
11-26	2093-2101	designed	*[142]	SOFTWARE[142]
11-27	2102-2109	TempNet	*[142]	SOFTWARE[142]
11-28	2110-2114	with	*[142]	SOFTWARE[142]
11-29	2115-2126	theoretical	_	_
11-30	2127-2138	inspiration	_	_
11-31	2138-2139	.	_	_

#Text=TempNet can be trained together with a large foundation model from scratch or learned separately given a pretrained foundation model.
12-1	2140-2147	TempNet	_	_
12-2	2148-2151	can	_	_
12-3	2152-2154	be	*[144]	LICENSE[144]
12-4	2155-2162	trained	*[144]	LICENSE[144]
12-5	2163-2171	together	*[144]	LICENSE[144]
12-6	2172-2176	with	*[144]	LICENSE[144]
12-7	2177-2178	a	*[144]	LICENSE[144]
12-8	2179-2184	large	*[144]	LICENSE[144]
12-9	2185-2195	foundation	*[144]	LICENSE[144]
12-10	2196-2201	model	*[144]	LICENSE[144]
12-11	2202-2206	from	*[144]	LICENSE[144]
12-12	2207-2214	scratch	*[144]	LICENSE[144]
12-13	2215-2217	or	*[144]	LICENSE[144]
12-14	2218-2225	learned	*[144]	LICENSE[144]
12-15	2226-2236	separately	*[144]	LICENSE[144]
12-16	2237-2242	given	*[144]	LICENSE[144]
12-17	2243-2244	a	*[144]	LICENSE[144]
12-18	2245-2255	pretrained	*[144]	LICENSE[144]
12-19	2256-2266	foundation	*[144]	LICENSE[144]
12-20	2267-2272	model	_	_
12-21	2272-2273	.	_	_

#Text=It is not only useful for predicting personalized temperature to promote the training of LFMs but also generalizable and transferable to new tasks.
13-1	2274-2276	It	_	_
13-2	2277-2279	is	_	_
13-3	2280-2283	not	_	_
13-4	2284-2288	only	_	_
13-5	2289-2295	useful	_	_
13-6	2296-2299	for	_	_
13-7	2300-2310	predicting	_	_
13-8	2311-2323	personalized	_	_
13-9	2324-2335	temperature	_	_
13-10	2336-2338	to	_	_
13-11	2339-2346	promote	_	_
13-12	2347-2350	the	_	_
13-13	2351-2359	training	_	_
13-14	2360-2362	of	_	_
13-15	2363-2367	LFMs	_	_
13-16	2368-2371	but	_	_
13-17	2372-2376	also	_	_
13-18	2377-2390	generalizable	_	_
13-19	2391-2394	and	_	_
13-20	2395-2407	transferable	_	_
13-21	2408-2410	to	_	_
13-22	2411-2414	new	_	_
13-23	2415-2420	tasks	_	_
13-24	2420-2421	.	_	_

#Text=<div align="center" style="display: flex\; justify-content: center\; align-items: center\;">   <img src="images/tempnet\_overall.jpg" style="width: 70%\;"/> </div>  In the figure above, we present the framework of training LFMs with TempNet on the left and the structure of TempNet on the right.   ### Experimental Results  Results of training LLMs in various settings, including training from scratch, finetuning a pretrained LLM model, and learning TempNet only with a frozen LLM model.
14-1	2423-2424	<	_	_
14-2	2424-2427	div	_	_
14-3	2428-2433	align	_	_
14-4	2433-2434	=	_	_
14-5	2434-2435	"	_	_
14-6	2435-2441	center	_	_
14-7	2441-2442	"	_	_
14-8	2443-2448	style	_	_
14-9	2448-2449	=	_	_
14-10	2449-2450	"	_	_
14-11	2450-2457	display	_	_
14-12	2457-2458	:	_	_
14-13	2459-2463	flex	_	_
14-14	2463-2464	\;	*[147]	PROJECT[147]
14-15	2465-2480	justify-content	*[147]	PROJECT[147]
14-16	2480-2481	:	*[147]	PROJECT[147]
14-17	2482-2488	center	*[147]	PROJECT[147]
14-18	2488-2489	\;	*[147]	PROJECT[147]
14-19	2490-2501	align-items	*[147]	PROJECT[147]
14-20	2501-2502	:	*[147]	PROJECT[147]
14-21	2503-2509	center	*[147]	PROJECT[147]
14-22	2509-2510	\;	*[147]	PROJECT[147]
14-23	2510-2511	"	*[147]	PROJECT[147]
14-24	2511-2512	>	*[147]	PROJECT[147]
14-25	2515-2516	<	*[147]	PROJECT[147]
14-26	2516-2519	img	*[147]	PROJECT[147]
14-27	2520-2523	src	*[147]	PROJECT[147]
14-28	2523-2524	=	*[147]	PROJECT[147]
14-29	2524-2525	"	*[147]	PROJECT[147]
14-30	2525-2531	images	*[147]	PROJECT[147]
14-31	2531-2532	/	*[147]	PROJECT[147]
14-32	2532-2551	tempnet\_overall.jpg	*[147]	PROJECT[147]
14-33	2551-2552	"	*[147]	PROJECT[147]
14-34	2553-2558	style	*[147]	PROJECT[147]
14-35	2558-2559	=	*[147]	PROJECT[147]
14-36	2559-2560	"	*[147]	PROJECT[147]
14-37	2560-2565	width	*[147]	PROJECT[147]
14-38	2565-2566	:	*[147]	PROJECT[147]
14-39	2567-2570	70%	*[147]	PROJECT[147]
14-40	2570-2571	\;	*[147]	PROJECT[147]
14-41	2571-2572	"	*[147]	PROJECT[147]
14-42	2572-2573	/	*[147]	PROJECT[147]
14-43	2573-2574	>	*[147]	PROJECT[147]
14-44	2575-2576	<	*[147]	PROJECT[147]
14-45	2576-2577	/	*[147]	PROJECT[147]
14-46	2577-2580	div	*[147]	PROJECT[147]
14-47	2580-2581	>	*[147]	PROJECT[147]
14-48	2583-2585	In	*[147]	PROJECT[147]
14-49	2586-2589	the	*[147]	PROJECT[147]
14-50	2590-2596	figure	*[147]	PROJECT[147]
14-51	2597-2602	above	*[147]	PROJECT[147]
14-52	2602-2603	,	*[147]	PROJECT[147]
14-53	2604-2606	we	*[147]	PROJECT[147]
14-54	2607-2614	present	*[147]	PROJECT[147]
14-55	2615-2618	the	*[147]	PROJECT[147]
14-56	2619-2628	framework	*[147]	PROJECT[147]
14-57	2629-2631	of	*[147]	PROJECT[147]
14-58	2632-2640	training	*[147]	PROJECT[147]
14-59	2641-2645	LFMs	*[147]	PROJECT[147]
14-60	2646-2650	with	*[147]	PROJECT[147]
14-61	2651-2658	TempNet	*[147]	PROJECT[147]
14-62	2659-2661	on	*[147]	PROJECT[147]
14-63	2662-2665	the	*[147]	PROJECT[147]
14-64	2666-2670	left	*[147]	PROJECT[147]
14-65	2671-2674	and	*[147]	PROJECT[147]
14-66	2675-2678	the	*[147]	PROJECT[147]
14-67	2679-2688	structure	*[147]	PROJECT[147]
14-68	2689-2691	of	*[147]	PROJECT[147]
14-69	2692-2699	TempNet	_	_
14-70	2700-2702	on	_	_
14-71	2703-2706	the	_	_
14-72	2707-2712	right	_	_
14-73	2712-2713	.	_	_
14-74	2716-2717	#	_	_
14-75	2717-2718	#	_	_
14-76	2718-2719	#	_	_
14-77	2720-2732	Experimental	_	_
14-78	2733-2740	Results	_	_
14-79	2742-2749	Results	_	_
14-80	2750-2752	of	_	_
14-81	2753-2761	training	_	_
14-82	2762-2766	LLMs	_	_
14-83	2767-2769	in	_	_
14-84	2770-2777	various	_	_
14-85	2778-2786	settings	_	_
14-86	2786-2787	,	_	_
14-87	2788-2797	including	_	_
14-88	2798-2806	training	_	_
14-89	2807-2811	from	_	_
14-90	2812-2819	scratch	_	_
14-91	2819-2820	,	_	_
14-92	2821-2831	finetuning	_	_
14-93	2832-2833	a	_	_
14-94	2834-2844	pretrained	_	_
14-95	2845-2848	LLM	_	_
14-96	2849-2854	model	_	_
14-97	2854-2855	,	_	_
14-98	2856-2859	and	_	_
14-99	2860-2868	learning	_	_
14-100	2869-2876	TempNet	_	_
14-101	2877-2881	only	_	_
14-102	2882-2886	with	_	_
14-103	2887-2888	a	_	_
14-104	2889-2895	frozen	_	_
14-105	2896-2899	LLM	_	_
14-106	2900-2905	model	_	_
14-107	2905-2906	.	_	_

#Text=<div align="center">   <img src="images/exp1.jpg" width="80%"/> </div>  Results on contrastive learning.
15-1	2908-2909	<	_	_
15-2	2909-2912	div	_	_
15-3	2913-2918	align	_	_
15-4	2918-2919	=	_	_
15-5	2919-2920	"	_	_
15-6	2920-2926	center	_	_
15-7	2926-2927	"	_	_
15-8	2927-2928	>	_	_
15-9	2931-2932	<	_	_
15-10	2932-2935	img	_	_
15-11	2936-2939	src	_	_
15-12	2939-2940	=	_	_
15-13	2940-2941	"	_	_
15-14	2941-2947	images	_	_
15-15	2947-2948	/	_	_
15-16	2948-2952	exp1	_	_
15-17	2952-2953	.	_	_
15-18	2953-2956	jpg	_	_
15-19	2956-2957	"	_	_
15-20	2958-2963	width	_	_
15-21	2963-2964	=	_	_
15-22	2964-2965	"	_	_
15-23	2965-2968	80%	_	_
15-24	2968-2969	"	_	_
15-25	2969-2970	/	_	_
15-26	2970-2971	>	_	_
15-27	2972-2973	<	_	_
15-28	2973-2974	/	*[138]	EVALMETRIC[138]
15-29	2974-2977	div	*[138]	EVALMETRIC[138]
15-30	2977-2978	>	*[138]	EVALMETRIC[138]
15-31	2980-2987	Results	*[138]	EVALMETRIC[138]
15-32	2988-2990	on	*[138]	EVALMETRIC[138]
15-33	2991-3002	contrastive	_	_
15-34	3003-3011	learning	_	_
15-35	3011-3012	.	_	_

#Text=For image-text retrieval on Flickr30K and MSCOCO, we compute IR@1 and TR@1 for the Recall@1 on image-retrieval (IR) and text-retrieval (TR).
16-1	3013-3016	For	_	_
16-2	3017-3027	image-text	_	_
16-3	3028-3037	retrieval	_	_
16-4	3038-3040	on	_	_
16-5	3041-3050	Flickr30K	_	_
16-6	3051-3054	and	_	_
16-7	3055-3061	MSCOCO	_	_
16-8	3061-3062	,	_	_
16-9	3063-3065	we	_	_
16-10	3066-3073	compute	_	_
16-11	3074-3076	IR	_	_
16-12	3076-3077	@	_	_
16-13	3077-3078	1	_	_
16-14	3079-3082	and	_	_
16-15	3083-3085	TR	_	_
16-16	3085-3086	@	_	_
16-17	3086-3087	1	_	_
16-18	3088-3091	for	_	_
16-19	3092-3095	the	_	_
16-20	3096-3102	Recall	_	_
16-21	3102-3103	@	_	_
16-22	3103-3104	1	_	_
16-23	3105-3107	on	_	_
16-24	3108-3123	image-retrieval	_	_
16-25	3124-3125	(	*[151]	PROGLANG[151]
16-26	3125-3127	IR	*[151]	PROGLANG[151]
16-27	3127-3128	)	*[151]	PROGLANG[151]
16-28	3129-3132	and	_	_
16-29	3133-3147	text-retrieval	_	_
16-30	3148-3149	(	_	_
16-31	3149-3151	TR	_	_
16-32	3151-3152	)	_	_
16-33	3152-3153	.	_	_

#Text=For classification tasks, we compute top-1 accuracy (\\%).
17-1	3154-3157	For	_	_
17-2	3158-3172	classification	_	_
17-3	3173-3178	tasks	*[127]	PUBLICATION[127]
17-4	3178-3179	,	*[127]	PUBLICATION[127]
17-5	3180-3182	we	*[127]	PUBLICATION[127]
17-6	3183-3190	compute	*[127]	PUBLICATION[127]
17-7	3191-3194	top	_	_
17-8	3194-3195	-	_	_
17-9	3195-3196	1	_	_
17-10	3197-3205	accuracy	_	_
17-11	3206-3207	(	_	_
17-12	3207-3208	\\	_	_
17-13	3208-3209	%	_	_
17-14	3209-3210	)	_	_
17-15	3210-3211	.	_	_

#Text=We report the average of scores and standard deviation over 3 runs with different random seeds.
18-1	3212-3214	We	_	_
18-2	3215-3221	report	_	_
18-3	3222-3225	the	_	_
18-4	3226-3233	average	_	_
18-5	3234-3236	of	_	_
18-6	3237-3243	scores	_	_
18-7	3244-3247	and	_	_
18-8	3248-3256	standard	_	_
18-9	3257-3266	deviation	_	_
18-10	3267-3271	over	_	_
18-11	3272-3273	3	_	_
18-12	3274-3278	runs	_	_
18-13	3279-3283	with	_	_
18-14	3284-3293	different	_	_
18-15	3294-3300	random	_	_
18-16	3301-3306	seeds	_	_
18-17	3306-3307	.	_	_

#Text=<div align="center">   <img src="images/exp\_2.jpg" width="80%"/> </div>  In the following experiments, we investigate two components of our framework: the DRO-based robust loss and the role of TempNet.
19-1	3309-3310	<	_	_
19-2	3310-3313	div	_	_
19-3	3314-3319	align	_	_
19-4	3319-3320	=	_	_
19-5	3320-3321	"	_	_
19-6	3321-3327	center	_	_
19-7	3327-3328	"	_	_
19-8	3328-3329	>	_	_
19-9	3332-3333	<	_	_
19-10	3333-3336	img	_	_
19-11	3337-3340	src	_	_
19-12	3340-3341	=	_	_
19-13	3341-3342	"	_	_
19-14	3342-3348	images	_	_
19-15	3348-3349	/	_	_
19-16	3349-3352	exp	_	_
19-17	3352-3353	\_	_	_
19-18	3353-3354	2	_	_
19-19	3354-3355	.	_	_
19-20	3355-3358	jpg	_	_
19-21	3358-3359	"	_	_
19-22	3360-3365	width	_	_
19-23	3365-3366	=	_	_
19-24	3366-3367	"	_	_
19-25	3367-3370	80%	_	_
19-26	3370-3371	"	_	_
19-27	3371-3372	/	_	_
19-28	3372-3373	>	_	_
19-29	3374-3375	<	_	_
19-30	3375-3376	/	_	_
19-31	3376-3379	div	_	_
19-32	3379-3380	>	_	_
19-33	3382-3384	In	_	_
19-34	3385-3388	the	_	_
19-35	3389-3398	following	_	_
19-36	3399-3410	experiments	_	_
19-37	3410-3411	,	_	_
19-38	3412-3414	we	_	_
19-39	3415-3426	investigate	_	_
19-40	3427-3430	two	_	_
19-41	3431-3441	components	_	_
19-42	3442-3444	of	_	_
19-43	3445-3448	our	_	_
19-44	3449-3458	framework	_	_
19-45	3458-3459	:	_	_
19-46	3460-3463	the	_	_
19-47	3464-3473	DRO-based	_	_
19-48	3474-3480	robust	_	_
19-49	3481-3485	loss	*[152]	PROGLANG[152]
19-50	3486-3489	and	*[152]	PROGLANG[152]
19-51	3490-3493	the	*[152]	PROGLANG[152]
19-52	3494-3498	role	_	_
19-53	3499-3501	of	_	_
19-54	3502-3509	TempNet	_	_
19-55	3509-3510	.	_	_

#Text=One can observe that both components significantly impact performance.
20-1	3511-3514	One	_	_
20-2	3515-3518	can	_	_
20-3	3519-3526	observe	_	_
20-4	3527-3531	that	_	_
20-5	3532-3536	both	_	_
20-6	3537-3547	components	_	_
20-7	3548-3561	significantly	_	_
20-8	3562-3568	impact	_	_
20-9	3569-3580	performance	_	_
20-10	3580-3581	.	_	_

#Text=<div align="center">   <img src="images/exp3.jpg" width="80%"/> </div>  To test TempNet's performance in instruction-following tasks, we fix the LLaMA2 Chat models and trained TempNet, then test them on the \[AlpacaEval\](https://tatsu-lab.github.io/alpaca\_eval/) benchmark.
21-1	3583-3584	<	_	_
21-2	3584-3587	div	_	_
21-3	3588-3593	align	_	_
21-4	3593-3594	=	_	_
21-5	3594-3595	"	_	_
21-6	3595-3601	center	_	_
21-7	3601-3602	"	_	_
21-8	3602-3603	>	_	_
21-9	3606-3607	<	_	_
21-10	3607-3610	img	_	_
21-11	3611-3614	src	_	_
21-12	3614-3615	=	_	_
21-13	3615-3616	"	_	_
21-14	3616-3622	images	_	_
21-15	3622-3623	/	_	_
21-16	3623-3627	exp3	_	_
21-17	3627-3628	.	_	_
21-18	3628-3631	jpg	_	_
21-19	3631-3632	"	_	_
21-20	3633-3638	width	_	_
21-21	3638-3639	=	_	_
21-22	3639-3640	"	_	_
21-23	3640-3643	80%	_	_
21-24	3643-3644	"	_	_
21-25	3644-3645	/	_	_
21-26	3645-3646	>	_	_
21-27	3647-3648	<	_	_
21-28	3648-3649	/	_	_
21-29	3649-3652	div	_	_
21-30	3652-3653	>	_	_
21-31	3655-3657	To	_	_
21-32	3658-3662	test	_	_
21-33	3663-3672	TempNet's	_	_
21-34	3673-3684	performance	_	_
21-35	3685-3687	in	*[143]	SOFTWARE[143]
21-36	3688-3709	instruction-following	*[143]	SOFTWARE[143]
21-37	3710-3715	tasks	*[143]	SOFTWARE[143]
21-38	3715-3716	,	*[143]	SOFTWARE[143]
21-39	3717-3719	we	*[143]	SOFTWARE[143]
21-40	3720-3723	fix	*[143]	SOFTWARE[143]
21-41	3724-3727	the	*[143]	SOFTWARE[143]
21-42	3728-3734	LLaMA2	*[143]	SOFTWARE[143]
21-43	3735-3739	Chat	*[143]	SOFTWARE[143]
21-44	3740-3746	models	*[143]	SOFTWARE[143]
21-45	3747-3750	and	*[143]	SOFTWARE[143]
21-46	3751-3758	trained	*[143]	SOFTWARE[143]
21-47	3759-3766	TempNet	*[143]	SOFTWARE[143]
21-48	3766-3767	,	*[143]	SOFTWARE[143]
21-49	3768-3772	then	*[143]	SOFTWARE[143]
21-50	3773-3777	test	*[143]	SOFTWARE[143]
21-51	3778-3782	them	*[143]	SOFTWARE[143]
21-52	3783-3785	on	*[143]	SOFTWARE[143]
21-53	3786-3789	the	*[143]	SOFTWARE[143]
21-54	3790-3791	\[	*[143]	SOFTWARE[143]
21-55	3791-3801	AlpacaEval	_	_
21-56	3801-3802	\]	_	_
21-57	3802-3803	(	_	_
21-58	3803-3808	https	_	_
21-59	3808-3809	:	_	_
21-60	3809-3810	/	_	_
21-61	3810-3811	/	_	_
21-62	3811-3830	tatsu-lab.github.io	_	_
21-63	3830-3831	/	_	_
21-64	3831-3842	alpaca\_eval	_	_
21-65	3842-3843	/	_	_
21-66	3843-3844	)	_	_
21-67	3845-3854	benchmark	_	_
21-68	3854-3855	.	_	_

#Text=We present the results on three different model sizes in the table below, including the training times of TempNet on Nvidia A100-80G GPUs and their win rates on AlpacaEval data.
22-1	3856-3858	We	_	_
22-2	3859-3866	present	_	_
22-3	3867-3870	the	_	_
22-4	3871-3878	results	_	_
22-5	3879-3881	on	_	_
22-6	3882-3887	three	_	_
22-7	3888-3897	different	_	_
22-8	3898-3903	model	_	_
22-9	3904-3909	sizes	_	_
22-10	3910-3912	in	_	_
22-11	3913-3916	the	_	_
22-12	3917-3922	table	_	_
22-13	3923-3928	below	_	_
22-14	3928-3929	,	_	_
22-15	3930-3939	including	_	_
22-16	3940-3943	the	_	_
22-17	3944-3952	training	_	_
22-18	3953-3958	times	_	_
22-19	3959-3961	of	*[135]	ONTOLOGY[135]
22-20	3962-3969	TempNet	*[135]	ONTOLOGY[135]
22-21	3970-3972	on	*[135]	ONTOLOGY[135]
22-22	3973-3979	Nvidia	*[135]	ONTOLOGY[135]
22-23	3980-3984	A100	*[135]	ONTOLOGY[135]
22-24	3984-3985	-	*[135]	ONTOLOGY[135]
22-25	3985-3988	80G	*[135]	ONTOLOGY[135]
22-26	3989-3993	GPUs	*[135]	ONTOLOGY[135]
22-27	3994-3997	and	*[135]	ONTOLOGY[135]
22-28	3998-4003	their	*[135]	ONTOLOGY[135]
22-29	4004-4007	win	*[135]	ONTOLOGY[135]
22-30	4008-4013	rates	*[135]	ONTOLOGY[135]
22-31	4014-4016	on	*[135]	ONTOLOGY[135]
22-32	4017-4027	AlpacaEval	*[135]	ONTOLOGY[135]
22-33	4028-4032	data	*[135]	ONTOLOGY[135]
22-34	4032-4033	.	_	_

#Text=The results demonstrate that our TempNet can converge quickly and achieve consistent improvements.
23-1	4034-4037	The	_	_
23-2	4038-4045	results	_	_
23-3	4046-4057	demonstrate	_	_
23-4	4058-4062	that	_	_
23-5	4063-4066	our	_	_
23-6	4067-4074	TempNet	_	_
23-7	4075-4078	can	_	_
23-8	4079-4087	converge	_	_
23-9	4088-4095	quickly	_	_
23-10	4096-4099	and	_	_
23-11	4100-4107	achieve	_	_
23-12	4108-4118	consistent	_	_
23-13	4119-4131	improvements	_	_
23-14	4131-4132	.	_	_

#Text=<div align="center">   <img src="images/exp4.jpg" width="40%"/> </div>  Here, we reveal why TempNet enhances performance by comparing the performances of LLaMA2 7B Chat (with the default $\\tau=0.7$) and LLaMA2 7B Chat + TempNet on the AlpacaEval dataset.
24-1	4134-4135	<	_	_
24-2	4135-4138	div	_	_
24-3	4139-4144	align	_	_
24-4	4144-4145	=	_	_
24-5	4145-4146	"	_	_
24-6	4146-4152	center	_	_
24-7	4152-4153	"	_	_
24-8	4153-4154	>	_	_
24-9	4157-4158	<	_	_
24-10	4158-4161	img	_	_
24-11	4162-4165	src	_	_
24-12	4165-4166	=	_	_
24-13	4166-4167	"	_	_
24-14	4167-4173	images	_	_
24-15	4173-4174	/	_	_
24-16	4174-4178	exp4	_	_
24-17	4178-4179	.	_	_
24-18	4179-4182	jpg	_	_
24-19	4182-4183	"	_	_
24-20	4184-4189	width	_	_
24-21	4189-4190	=	_	_
24-22	4190-4191	"	_	_
24-23	4191-4194	40%	_	_
24-24	4194-4195	"	_	_
24-25	4195-4196	/	_	_
24-26	4196-4197	>	_	_
24-27	4198-4199	<	_	_
24-28	4199-4200	/	_	_
24-29	4200-4203	div	_	_
24-30	4203-4204	>	_	_
24-31	4206-4210	Here	_	_
24-32	4210-4211	,	_	_
24-33	4212-4214	we	_	_
24-34	4215-4221	reveal	_	_
24-35	4222-4225	why	_	_
24-36	4226-4233	TempNet	_	_
24-37	4234-4242	enhances	_	_
24-38	4243-4254	performance	_	_
24-39	4255-4257	by	_	_
24-40	4258-4267	comparing	_	_
24-41	4268-4271	the	_	_
24-42	4272-4284	performances	_	_
24-43	4285-4287	of	_	_
24-44	4288-4294	LLaMA2	_	_
24-45	4295-4297	7B	_	_
24-46	4298-4302	Chat	_	_
24-47	4303-4304	(	_	_
24-48	4304-4308	with	_	_
24-49	4309-4312	the	_	_
24-50	4313-4320	default	_	_
24-51	4321-4322	$	*[140]	EVALMETRIC[140]
24-52	4322-4323	\\	*[140]	EVALMETRIC[140]
24-53	4323-4326	tau	*[140]	EVALMETRIC[140]
24-54	4326-4327	=	*[140]	EVALMETRIC[140]
24-55	4327-4330	0.7	*[140]	EVALMETRIC[140]
24-56	4330-4331	$	*[140]	EVALMETRIC[140]
24-57	4331-4332	)	*[140]	EVALMETRIC[140]
24-58	4333-4336	and	*[140]	EVALMETRIC[140]
24-59	4337-4343	LLaMA2	*[140]	EVALMETRIC[140]
24-60	4344-4346	7B	*[140]	EVALMETRIC[140]
24-61	4347-4351	Chat	*[140]	EVALMETRIC[140]
24-62	4352-4353	+	*[140]	EVALMETRIC[140]
24-63	4354-4361	TempNet	*[140]	EVALMETRIC[140]
24-64	4362-4364	on	*[140]	EVALMETRIC[140]
24-65	4365-4368	the	*[140]	EVALMETRIC[140]
24-66	4369-4379	AlpacaEval	*[140]	EVALMETRIC[140]
24-67	4380-4387	dataset	_	_
24-68	4387-4388	.	_	_

#Text=We select a representative example for which the AlpacaEval annotator, GPT-4, deems the response from LLaMA2 + TempNet to be not only superior to that of LLaMA but also better than the baseline response generated by GPT-4.
25-1	4389-4391	We	_	_
25-2	4392-4398	select	*[129]	DATASET[129]
25-3	4399-4400	a	*[129]	DATASET[129]
25-4	4401-4415	representative	*[129]	DATASET[129]
25-5	4416-4423	example	*[129]	DATASET[129]
25-6	4424-4427	for	*[129]	DATASET[129]
25-7	4428-4433	which	*[129]	DATASET[129]
25-8	4434-4437	the	*[129]	DATASET[129]
25-9	4438-4448	AlpacaEval	*[129]	DATASET[129]
25-10	4449-4458	annotator	*[129]	DATASET[129]
25-11	4458-4459	,	*[129]	DATASET[129]
25-12	4460-4463	GPT	*[129]	DATASET[129]
25-13	4463-4464	-	*[129]	DATASET[129]
25-14	4464-4465	4	*[129]	DATASET[129]
25-15	4465-4466	,	*[129]	DATASET[129]
25-16	4467-4472	deems	*[129]	DATASET[129]
25-17	4473-4476	the	*[129]	DATASET[129]
25-18	4477-4485	response	*[129]	DATASET[129]
25-19	4486-4490	from	*[129]	DATASET[129]
25-20	4491-4497	LLaMA2	*[129]	DATASET[129]
25-21	4498-4499	+	*[129]	DATASET[129]
25-22	4500-4507	TempNet	*[129]	DATASET[129]
25-23	4508-4510	to	*[129]	DATASET[129]
25-24	4511-4513	be	*[129]	DATASET[129]
25-25	4514-4517	not	*[129]	DATASET[129]
25-26	4518-4522	only	*[129]	DATASET[129]
25-27	4523-4531	superior	*[129]	DATASET[129]
25-28	4532-4534	to	*[129]	DATASET[129]
25-29	4535-4539	that	*[129]	DATASET[129]
25-30	4540-4542	of	*[129]	DATASET[129]
25-31	4543-4548	LLaMA	*[129]	DATASET[129]
25-32	4549-4552	but	*[129]	DATASET[129]
25-33	4553-4557	also	*[129]	DATASET[129]
25-34	4558-4564	better	*[129]	DATASET[129]
25-35	4565-4569	than	*[129]	DATASET[129]
25-36	4570-4573	the	*[129]	DATASET[129]
25-37	4574-4582	baseline	*[129]	DATASET[129]
25-38	4583-4591	response	*[129]	DATASET[129]
25-39	4592-4601	generated	*[129]	DATASET[129]
25-40	4602-4604	by	*[129]	DATASET[129]
25-41	4605-4608	GPT	_	_
25-42	4608-4609	-	_	_
25-43	4609-4610	4	_	_
25-44	4610-4611	.	_	_

#Text=<div align="center">   <img src="images/exp5.jpg" width="60%"/> </div> <div align="center">   <img src="images/exp6.jpg" width="60%"/> </div>  It is a relatively subjective task to naming a dish.
26-1	4613-4614	<	_	_
26-2	4614-4617	div	_	_
26-3	4618-4623	align	_	_
26-4	4623-4624	=	*[153]	PROGLANG[153]
26-5	4624-4625	"	*[153]	PROGLANG[153]
26-6	4625-4631	center	*[153]	PROGLANG[153]
26-7	4631-4632	"	*[153]	PROGLANG[153]
26-8	4632-4633	>	*[153]	PROGLANG[153]
26-9	4636-4637	<	*[153]	PROGLANG[153]
26-10	4637-4640	img	*[153]	PROGLANG[153]
26-11	4641-4644	src	*[153]	PROGLANG[153]
26-12	4644-4645	=	*[153]	PROGLANG[153]
26-13	4645-4646	"	*[153]	PROGLANG[153]
26-14	4646-4652	images	*[153]	PROGLANG[153]
26-15	4652-4653	/	*[153]	PROGLANG[153]
26-16	4653-4657	exp5	*[153]	PROGLANG[153]
26-17	4657-4658	.	*[153]	PROGLANG[153]
26-18	4658-4661	jpg	*[153]	PROGLANG[153]
26-19	4661-4662	"	*[153]	PROGLANG[153]
26-20	4663-4668	width	*[153]	PROGLANG[153]
26-21	4668-4669	=	*[153]	PROGLANG[153]
26-22	4669-4670	"	*[153]	PROGLANG[153]
26-23	4670-4673	60%	*[153]	PROGLANG[153]
26-24	4673-4674	"	*[153]	PROGLANG[153]
26-25	4674-4675	/	*[153]	PROGLANG[153]
26-26	4675-4676	>	*[153]	PROGLANG[153]
26-27	4677-4678	<	*[153]	PROGLANG[153]
26-28	4678-4679	/	*[153]	PROGLANG[153]
26-29	4679-4682	div	*[153]	PROGLANG[153]
26-30	4682-4683	>	*[153]	PROGLANG[153]
26-31	4684-4685	<	*[153]	PROGLANG[153]
26-32	4685-4688	div	*[153]	PROGLANG[153]
26-33	4689-4694	align	*[153]	PROGLANG[153]
26-34	4694-4695	=	*[153]	PROGLANG[153]
26-35	4695-4696	"	*[153]	PROGLANG[153]
26-36	4696-4702	center	*[153]	PROGLANG[153]
26-37	4702-4703	"	*[153]	PROGLANG[153]
26-38	4703-4704	>	*[153]	PROGLANG[153]
26-39	4707-4708	<	*[153]	PROGLANG[153]
26-40	4708-4711	img	*[153]	PROGLANG[153]
26-41	4712-4715	src	*[153]	PROGLANG[153]
26-42	4715-4716	=	*[153]	PROGLANG[153]
26-43	4716-4717	"	*[153]	PROGLANG[153]
26-44	4717-4723	images	*[153]	PROGLANG[153]
26-45	4723-4724	/	*[153]	PROGLANG[153]
26-46	4724-4728	exp6	*[153]	PROGLANG[153]
26-47	4728-4729	.	*[153]	PROGLANG[153]
26-48	4729-4732	jpg	*[153]	PROGLANG[153]
26-49	4732-4733	"	*[153]	PROGLANG[153]
26-50	4734-4739	width	*[153]	PROGLANG[153]
26-51	4739-4740	=	*[153]	PROGLANG[153]
26-52	4740-4741	"	*[153]	PROGLANG[153]
26-53	4741-4744	60%	*[153]	PROGLANG[153]
26-54	4744-4745	"	*[153]	PROGLANG[153]
26-55	4745-4746	/	*[153]	PROGLANG[153]
26-56	4746-4747	>	*[153]	PROGLANG[153]
26-57	4748-4749	<	*[153]	PROGLANG[153]
26-58	4749-4750	/	*[153]	PROGLANG[153]
26-59	4750-4753	div	*[153]	PROGLANG[153]
26-60	4753-4754	>	*[153]	PROGLANG[153]
26-61	4756-4758	It	*[153]	PROGLANG[153]
26-62	4759-4761	is	*[153]	PROGLANG[153]
26-63	4762-4763	a	*[153]	PROGLANG[153]
26-64	4764-4774	relatively	_	_
26-65	4775-4785	subjective	_	_
26-66	4786-4790	task	_	_
26-67	4791-4793	to	_	_
26-68	4794-4800	naming	_	_
26-69	4801-4802	a	_	_
26-70	4803-4807	dish	_	_
26-71	4807-4808	.	_	_

#Text=When the temperature value is lower, it can be observed that the LLaMA2 7B Chat model's output is relatively fixed and lacks creativity.
27-1	4809-4813	When	_	_
27-2	4814-4817	the	_	_
27-3	4818-4829	temperature	_	_
27-4	4830-4835	value	_	_
27-5	4836-4838	is	_	_
27-6	4839-4844	lower	_	_
27-7	4844-4845	,	_	_
27-8	4846-4848	it	_	_
27-9	4849-4852	can	_	_
27-10	4853-4855	be	_	_
27-11	4856-4864	observed	_	_
27-12	4865-4869	that	_	_
27-13	4870-4873	the	_	_
27-14	4874-4880	LLaMA2	_	_
27-15	4881-4883	7B	*[145]	LICENSE[145]
27-16	4884-4888	Chat	*[145]	LICENSE[145]
27-17	4889-4896	model's	_	_
27-18	4897-4903	output	_	_
27-19	4904-4906	is	_	_
27-20	4907-4917	relatively	_	_
27-21	4918-4923	fixed	_	_
27-22	4924-4927	and	_	_
27-23	4928-4933	lacks	_	_
27-24	4934-4944	creativity	_	_
27-25	4944-4945	.	_	_

#Text=With a higher temperature, the model generates more creative names.
28-1	4946-4950	With	_	_
28-2	4951-4952	a	_	_
28-3	4953-4959	higher	_	_
28-4	4960-4971	temperature	_	_
28-5	4971-4972	,	_	_
28-6	4973-4976	the	_	_
28-7	4977-4982	model	_	_
28-8	4983-4992	generates	_	_
28-9	4993-4997	more	_	_
28-10	4998-5006	creative	_	_
28-11	5007-5012	names	*[154]	PROGLANG[154]
28-12	5012-5013	.	_	_

#Text=With TempNet, in the process of generating names for this task, LLaMA2 7B Chat produces a higher averaged temperature value of 0.82, ultimately creating a novel name \*\*Tunanadoes\*\*.
29-1	5014-5018	With	_	_
29-2	5019-5026	TempNet	_	_
29-3	5026-5027	,	_	_
29-4	5028-5030	in	_	_
29-5	5031-5034	the	_	_
29-6	5035-5042	process	_	_
29-7	5043-5045	of	_	_
29-8	5046-5056	generating	_	_
29-9	5057-5062	names	_	_
29-10	5063-5066	for	_	_
29-11	5067-5071	this	_	_
29-12	5072-5076	task	_	_
29-13	5076-5077	,	_	_
29-14	5078-5084	LLaMA2	_	_
29-15	5085-5087	7B	_	_
29-16	5088-5092	Chat	_	_
29-17	5093-5101	produces	_	_
29-18	5102-5103	a	_	_
29-19	5104-5110	higher	*[155]	PROGLANG[155]
29-20	5111-5119	averaged	*[155]	PROGLANG[155]
29-21	5120-5131	temperature	*[155]	PROGLANG[155]
29-22	5132-5137	value	*[155]	PROGLANG[155]
29-23	5138-5140	of	*[155]	PROGLANG[155]
29-24	5141-5145	0.82	*[155]	PROGLANG[155]
29-25	5145-5146	,	*[155]	PROGLANG[155]
29-26	5147-5157	ultimately	*[155]	PROGLANG[155]
29-27	5158-5166	creating	_	_
29-28	5167-5168	a	_	_
29-29	5169-5174	novel	_	_
29-30	5175-5179	name	_	_
29-31	5180-5181	\*	_	_
29-32	5181-5182	\*	_	_
29-33	5182-5192	Tunanadoes	_	_
29-34	5192-5193	\*	_	_
29-35	5193-5194	\*	_	_
29-36	5194-5195	.	_	_

#Text=We further demonstrate the predicted temperature parameter produced by the TempNet each time a token here.
30-1	5198-5200	We	_	_
30-2	5201-5208	further	_	_
30-3	5209-5220	demonstrate	_	_
30-4	5221-5224	the	*[141]	EVALMETRIC[141]
30-5	5225-5234	predicted	*[141]	EVALMETRIC[141]
30-6	5235-5246	temperature	*[141]	EVALMETRIC[141]
30-7	5247-5256	parameter	*[141]	EVALMETRIC[141]
30-8	5257-5265	produced	*[141]	EVALMETRIC[141]
30-9	5266-5268	by	*[141]	EVALMETRIC[141]
30-10	5269-5272	the	_	_
30-11	5273-5280	TempNet	_	_
30-12	5281-5285	each	_	_
30-13	5286-5290	time	_	_
30-14	5291-5292	a	_	_
30-15	5293-5298	token	_	_
30-16	5299-5303	here	_	_
30-17	5303-5304	.	_	_

#Text=One can clearly observe that when the potential possibilities for the token to be predicted are numerous, the temperature values are higher.
31-1	5305-5308	One	_	_
31-2	5309-5312	can	_	_
31-3	5313-5320	clearly	_	_
31-4	5321-5328	observe	_	_
31-5	5329-5333	that	_	_
31-6	5334-5338	when	_	_
31-7	5339-5342	the	_	_
31-8	5343-5352	potential	_	_
31-9	5353-5366	possibilities	_	_
31-10	5367-5370	for	_	_
31-11	5371-5374	the	*[128]	PUBLICATION[128]
31-12	5375-5380	token	*[128]	PUBLICATION[128]
31-13	5381-5383	to	*[128]	PUBLICATION[128]
31-14	5384-5386	be	*[128]	PUBLICATION[128]
31-15	5387-5396	predicted	*[128]	PUBLICATION[128]
31-16	5397-5400	are	*[128]	PUBLICATION[128]
31-17	5401-5409	numerous	_	_
31-18	5409-5410	,	_	_
31-19	5411-5414	the	_	_
31-20	5415-5426	temperature	_	_
31-21	5427-5433	values	_	_
31-22	5434-5437	are	_	_
31-23	5438-5444	higher	_	_
31-24	5444-5445	.	_	_

#Text=Conversely, when there are fewer potential possibilities for the token to be predicted, the temperature values are lower.
32-1	5446-5456	Conversely	_	_
32-2	5456-5457	,	_	_
32-3	5458-5462	when	_	_
32-4	5463-5468	there	_	_
32-5	5469-5472	are	_	_
32-6	5473-5478	fewer	_	_
32-7	5479-5488	potential	_	_
32-8	5489-5502	possibilities	_	_
32-9	5503-5506	for	_	_
32-10	5507-5510	the	_	_
32-11	5511-5516	token	_	_
32-12	5517-5519	to	_	_
32-13	5520-5522	be	_	_
32-14	5523-5532	predicted	_	_
32-15	5532-5533	,	*[130]	DATASET[130]
32-16	5534-5537	the	*[130]	DATASET[130]
32-17	5538-5549	temperature	_	_
32-18	5550-5556	values	_	_
32-19	5557-5560	are	_	_
32-20	5561-5566	lower	_	_
32-21	5566-5567	.	_	_

#Text=<div align="center">   <img src="images/pred\_tau.jpg" width="100%"/> </div>  ### More Details For more details, please refer to our \[paper\](http://arxiv.org/abs/2404.04575)    ## Training  We conduct experiments across various tasks and models to validate the effectiveness of TempNet.
33-1	5569-5570	<	_	_
33-2	5570-5573	div	_	_
33-3	5574-5579	align	_	_
33-4	5579-5580	=	_	_
33-5	5580-5581	"	_	_
33-6	5581-5587	center	_	_
33-7	5587-5588	"	_	_
33-8	5588-5589	>	_	_
33-9	5592-5593	<	_	_
33-10	5593-5596	img	_	_
33-11	5597-5600	src	_	_
33-12	5600-5601	=	_	_
33-13	5601-5602	"	_	_
33-14	5602-5608	images	_	_
33-15	5608-5609	/	_	_
33-16	5609-5621	pred\_tau.jpg	_	_
33-17	5621-5622	"	_	_
33-18	5623-5628	width	_	_
33-19	5628-5629	=	_	_
33-20	5629-5630	"	_	_
33-21	5630-5634	100%	_	_
33-22	5634-5635	"	_	_
33-23	5635-5636	/	_	_
33-24	5636-5637	>	_	_
33-25	5638-5639	<	_	_
33-26	5639-5640	/	_	_
33-27	5640-5643	div	_	_
33-28	5643-5644	>	_	_
33-29	5646-5647	#	_	_
33-30	5647-5648	#	_	_
33-31	5648-5649	#	_	_
33-32	5650-5654	More	_	_
33-33	5655-5662	Details	_	_
33-34	5663-5666	For	_	_
33-35	5667-5671	more	_	_
33-36	5672-5679	details	_	_
33-37	5679-5680	,	_	_
33-38	5681-5687	please	_	_
33-39	5688-5693	refer	_	_
33-40	5694-5696	to	_	_
33-41	5697-5700	our	_	_
33-42	5701-5702	\[	_	_
33-43	5702-5707	paper	_	_
33-44	5707-5708	\]	_	_
33-45	5708-5709	(	_	_
33-46	5709-5713	http	_	_
33-47	5713-5714	:	_	_
33-48	5714-5715	/	_	_
33-49	5715-5716	/	_	_
33-50	5716-5725	arxiv.org	_	_
33-51	5725-5726	/	_	_
33-52	5726-5729	abs	_	_
33-53	5729-5730	/	_	_
33-54	5730-5740	2404.04575	_	_
33-55	5740-5741	)	_	_
33-56	5745-5746	#	_	_
33-57	5746-5747	#	_	_
33-58	5748-5756	Training	_	_
33-59	5758-5760	We	_	_
33-60	5761-5768	conduct	_	_
33-61	5769-5780	experiments	_	_
33-62	5781-5787	across	*[136]	ONTOLOGY[136]
33-63	5788-5795	various	_	_
33-64	5796-5801	tasks	_	_
33-65	5802-5805	and	_	_
33-66	5806-5812	models	_	_
33-67	5813-5815	to	_	_
33-68	5816-5824	validate	_	_
33-69	5825-5828	the	_	_
33-70	5829-5842	effectiveness	_	_
33-71	5843-5845	of	_	_
33-72	5846-5853	TempNet	_	_
33-73	5853-5854	.	_	_

#Text=Given the different training frameworks required by each model, we distribute the training code for different models across four directories: `GPT\_2`, `LLaMA-1`, `LLaMA-2`, and `Bimodal-CL`.  ## Inference  We upload the base models for LLaMA 2 Chat 7B, 13B, 70B, and their respective TempNets to \[Hugging Face\](https://huggingface.co/LLM-Opt).
34-1	5855-5860	Given	_	_
34-2	5861-5864	the	_	_
34-3	5865-5874	different	_	_
34-4	5875-5883	training	_	_
34-5	5884-5894	frameworks	_	_
34-6	5895-5903	required	_	_
34-7	5904-5906	by	_	_
34-8	5907-5911	each	_	_
34-9	5912-5917	model	_	_
34-10	5917-5918	,	_	_
34-11	5919-5921	we	_	_
34-12	5922-5932	distribute	_	_
34-13	5933-5936	the	_	_
34-14	5937-5945	training	_	_
34-15	5946-5950	code	_	_
34-16	5951-5954	for	_	_
34-17	5955-5964	different	_	_
34-18	5965-5971	models	_	_
34-19	5972-5978	across	_	_
34-20	5979-5983	four	_	_
34-21	5984-5995	directories	_	_
34-22	5995-5996	:	_	_
34-23	5997-5998	`	_	_
34-24	5998-6001	GPT	_	_
34-25	6001-6002	\_	_	_
34-26	6002-6003	2	_	_
34-27	6003-6004	`	_	_
34-28	6004-6005	,	_	_
34-29	6006-6007	`	_	_
34-30	6007-6012	LLaMA	_	_
34-31	6012-6013	-	_	_
34-32	6013-6014	1	_	_
34-33	6014-6015	`	_	_
34-34	6015-6016	,	_	_
34-35	6017-6018	`	_	_
34-36	6018-6023	LLaMA	_	_
34-37	6023-6024	-	_	_
34-38	6024-6025	2	_	_
34-39	6025-6026	`	_	_
34-40	6026-6027	,	_	_
34-41	6028-6031	and	_	_
34-42	6032-6033	`	_	_
34-43	6033-6043	Bimodal-CL	_	_
34-44	6043-6044	`	_	_
34-45	6044-6045	.	_	_
34-46	6047-6048	#	_	_
34-47	6048-6049	#	_	_
34-48	6050-6059	Inference	_	_
34-49	6061-6063	We	_	_
34-50	6064-6070	upload	_	_
34-51	6071-6074	the	_	_
34-52	6075-6079	base	*[156]	PROGLANG[156]
34-53	6080-6086	models	*[156]	PROGLANG[156]
34-54	6087-6090	for	*[156]	PROGLANG[156]
34-55	6091-6096	LLaMA	*[156]	PROGLANG[156]
34-56	6097-6098	2	*[156]	PROGLANG[156]
34-57	6099-6103	Chat	*[156]	PROGLANG[156]
34-58	6104-6106	7B	*[156]	PROGLANG[156]
34-59	6106-6107	,	*[156]	PROGLANG[156]
34-60	6108-6111	13B	*[156]	PROGLANG[156]
34-61	6111-6112	,	*[156]	PROGLANG[156]
34-62	6113-6116	70B	*[156]	PROGLANG[156]
34-63	6116-6117	,	*[156]	PROGLANG[156]
34-64	6118-6121	and	*[156]	PROGLANG[156]
34-65	6122-6127	their	*[156]	PROGLANG[156]
34-66	6128-6138	respective	*[156]	PROGLANG[156]
34-67	6139-6147	TempNets	_	_
34-68	6148-6150	to	_	_
34-69	6151-6152	\[	_	_
34-70	6152-6159	Hugging	_	_
34-71	6160-6164	Face	_	_
34-72	6164-6165	\]	_	_
34-73	6165-6166	(	_	_
34-74	6166-6171	https	_	_
34-75	6171-6172	:	_	_
34-76	6172-6173	/	_	_
34-77	6173-6174	/	_	_
34-78	6174-6188	huggingface.co	_	_
34-79	6188-6189	/	_	_
34-80	6189-6196	LLM-Opt	_	_
34-81	6196-6197	)	_	_
34-82	6197-6198	.	_	_

#Text=The `tempnet.py` in the \[repository\](https://github.com/zhqiu/TempNet) contains the definition of the TempNet class and a class that inherits from Hugging Face's LLaMA, including TempNet.
35-1	6199-6202	The	_	_
35-2	6203-6204	`	_	_
35-3	6204-6214	tempnet.py	*[144]	SOFTWARE[144]
35-4	6214-6215	`	*[144]	SOFTWARE[144]
35-5	6216-6218	in	*[144]	SOFTWARE[144]
35-6	6219-6222	the	*[144]	SOFTWARE[144]
35-7	6223-6224	\[	*[144]	SOFTWARE[144]
35-8	6224-6234	repository	*[144]	SOFTWARE[144]
35-9	6234-6235	\]	*[144]	SOFTWARE[144]
35-10	6235-6236	(	*[144]	SOFTWARE[144]
35-11	6236-6241	https	*[144]	SOFTWARE[144]
35-12	6241-6242	:	*[144]	SOFTWARE[144]
35-13	6242-6243	/	*[144]	SOFTWARE[144]
35-14	6243-6244	/	*[144]	SOFTWARE[144]
35-15	6244-6254	github.com	*[144]	SOFTWARE[144]
35-16	6254-6255	/	*[144]	SOFTWARE[144]
35-17	6255-6260	zhqiu	*[144]	SOFTWARE[144]
35-18	6260-6261	/	*[144]	SOFTWARE[144]
35-19	6261-6268	TempNet	*[144]	SOFTWARE[144]
35-20	6268-6269	)	*[144]	SOFTWARE[144]
35-21	6270-6278	contains	*[144]	SOFTWARE[144]
35-22	6279-6282	the	*[144]	SOFTWARE[144]
35-23	6283-6293	definition	*[144]	SOFTWARE[144]
35-24	6294-6296	of	*[144]	SOFTWARE[144]
35-25	6297-6300	the	*[144]	SOFTWARE[144]
35-26	6301-6308	TempNet	*[144]	SOFTWARE[144]
35-27	6309-6314	class	*[144]	SOFTWARE[144]
35-28	6315-6318	and	*[144]	SOFTWARE[144]
35-29	6319-6320	a	*[144]	SOFTWARE[144]
35-30	6321-6326	class	*[144]	SOFTWARE[144]
35-31	6327-6331	that	*[144]	SOFTWARE[144]
35-32	6332-6340	inherits	*[144]	SOFTWARE[144]
35-33	6341-6345	from	*[144]	SOFTWARE[144]
35-34	6346-6353	Hugging	*[144]	SOFTWARE[144]
35-35	6354-6360	Face's	_	_
35-36	6361-6366	LLaMA	_	_
35-37	6366-6367	,	_	_
35-38	6368-6377	including	_	_
35-39	6378-6385	TempNet	_	_
35-40	6385-6386	.	_	_

#Text=People can download this file and use the following code to perform inference with LLaMA that incorporates TempNet.
36-1	6387-6393	People	_	_
36-2	6394-6397	can	_	_
36-3	6398-6406	download	_	_
36-4	6407-6411	this	_	_
36-5	6412-6416	file	_	_
36-6	6417-6420	and	_	_
36-7	6421-6424	use	_	_
36-8	6425-6428	the	_	_
36-9	6429-6438	following	_	_
36-10	6439-6443	code	_	_
36-11	6444-6446	to	_	_
36-12	6447-6454	perform	*[148]	PROJECT[148]
36-13	6455-6464	inference	*[148]	PROJECT[148]
36-14	6465-6469	with	*[148]	PROJECT[148]
36-15	6470-6475	LLaMA	_	_
36-16	6476-6480	that	_	_
36-17	6481-6493	incorporates	_	_
36-18	6494-6501	TempNet	_	_
36-19	6501-6502	.	_	_

#Text=```python import torch from tempnet import LLaMA\_TempNet from transformers import AutoTokenizer, GenerationConfig  model\_name = 'LLM-Opt/TempNet-LLaMA2-Chat-7B-v0.1'  tokenizer = AutoTokenizer.from\_pretrained(model\_name, legacy=False) generation\_config = GenerationConfig.from\_pretrained(model\_name) model = LLaMA\_TempNet.from\_pretrained(model\_name, device\_map="auto", torch\_dtype=torch.float16)  inputs = 'How do you get water in the desert?'
37-1	6504-6505	`	_	_
37-2	6505-6506	`	_	_
37-3	6506-6507	`	_	_
37-4	6507-6513	python	_	_
37-5	6514-6520	import	_	_
37-6	6521-6526	torch	_	_
37-7	6527-6531	from	_	_
37-8	6532-6539	tempnet	_	_
37-9	6540-6546	import	_	_
37-10	6547-6560	LLaMA\_TempNet	_	_
37-11	6561-6565	from	_	_
37-12	6566-6578	transformers	_	_
37-13	6579-6585	import	_	_
37-14	6586-6599	AutoTokenizer	_	_
37-15	6599-6600	,	_	_
37-16	6601-6617	GenerationConfig	_	_
37-17	6619-6629	model\_name	_	_
37-18	6630-6631	=	_	_
37-19	6632-6633	'	_	_
37-20	6633-6640	LLM-Opt	_	_
37-21	6640-6641	/	_	_
37-22	6641-6655	TempNet-LLaMA2	_	_
37-23	6655-6656	-	_	_
37-24	6656-6660	Chat	*[129]	PUBLICATION[129]
37-25	6660-6661	-	*[129]	PUBLICATION[129]
37-26	6661-6668	7B-v0.1	*[129]	PUBLICATION[129]
37-27	6668-6669	'	*[129]	PUBLICATION[129]
37-28	6671-6680	tokenizer	*[129]	PUBLICATION[129]
37-29	6681-6682	=	*[129]	PUBLICATION[129]
37-30	6683-6712	AutoTokenizer.from\_pretrained	*[129]	PUBLICATION[129]
37-31	6712-6713	(	*[129]	PUBLICATION[129]
37-32	6713-6723	model\_name	*[129]	PUBLICATION[129]
37-33	6723-6724	,	*[129]	PUBLICATION[129]
37-34	6725-6731	legacy	*[129]	PUBLICATION[129]
37-35	6731-6732	=	*[129]	PUBLICATION[129]
37-36	6732-6737	False	*[129]	PUBLICATION[129]
37-37	6737-6738	)	*[129]	PUBLICATION[129]
37-38	6739-6756	generation\_config	*[129]	PUBLICATION[129]
37-39	6757-6758	=	*[129]	PUBLICATION[129]
37-40	6759-6791	GenerationConfig.from\_pretrained	*[129]	PUBLICATION[129]
37-41	6791-6792	(	*[129]	PUBLICATION[129]
37-42	6792-6802	model\_name	*[129]	PUBLICATION[129]
37-43	6802-6803	)	*[129]	PUBLICATION[129]
37-44	6804-6809	model	*[129]	PUBLICATION[129]
37-45	6810-6811	=	*[129]	PUBLICATION[129]
37-46	6812-6841	LLaMA\_TempNet.from\_pretrained	_	_
37-47	6841-6842	(	_	_
37-48	6842-6852	model\_name	_	_
37-49	6852-6853	,	_	_
37-50	6854-6864	device\_map	_	_
37-51	6864-6865	=	_	_
37-52	6865-6866	"	_	_
37-53	6866-6870	auto	_	_
37-54	6870-6871	"	_	_
37-55	6871-6872	,	_	_
37-56	6873-6884	torch\_dtype	_	_
37-57	6884-6885	=	_	_
37-58	6885-6898	torch.float16	_	_
37-59	6898-6899	)	_	_
37-60	6901-6907	inputs	_	_
37-61	6908-6909	=	_	_
37-62	6910-6911	'	_	_
37-63	6911-6914	How	_	_
37-64	6915-6917	do	_	_
37-65	6918-6921	you	_	_
37-66	6922-6925	get	_	_
37-67	6926-6931	water	_	_
37-68	6932-6934	in	_	_
37-69	6935-6938	the	_	_
37-70	6939-6945	desert	_	_
37-71	6945-6946	?	_	_
37-72	6946-6947	'	_	_

#Text=input\_ids = tokenizer(inputs, return\_tensors="pt").input\_ids.cuda()  outputs = model.generate(input\_ids, generation\_config=generation\_config) response = tokenizer.decode(outputs\[0\], skip\_special\_tokens=True)\[len(inputs)-1:\].strip() ```  ## Acknowledgment  This repository benefits from \[ALBEF\](https://github.com/salesforce/ALBEF), \[GPT-NeoX\](https://github.com/EleutherAI/gpt-neox), \[LLaMA\](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), \[Megatron-LM\](https://github.com/NVIDIA/Megatron-LM), \[lm-evaluation-harness\](https://github.com/EleutherAI/lm-evaluation-harness), \[Stanford Alpaca\](https://github.com/tatsu-lab/stanford\_alpaca), and \[DeepSpeed\](https://github.com/microsoft/DeepSpeed).
38-1	6948-6957	input\_ids	_	_
38-2	6958-6959	=	_	_
38-3	6960-6969	tokenizer	_	_
38-4	6969-6970	(	_	_
38-5	6970-6976	inputs	_	_
38-6	6976-6977	,	_	_
38-7	6978-6992	return\_tensors	_	_
38-8	6992-6993	=	_	_
38-9	6993-6994	"	_	_
38-10	6994-6996	pt	_	_
38-11	6996-6997	"	_	_
38-12	6997-6998	)	_	_
38-13	6998-6999	.	_	_
38-14	6999-7013	input\_ids.cuda	_	_
38-15	7013-7014	(	_	_
38-16	7014-7015	)	_	_
38-17	7017-7024	outputs	_	_
38-18	7025-7026	=	_	_
38-19	7027-7041	model.generate	_	_
38-20	7041-7042	(	_	_
38-21	7042-7051	input\_ids	_	_
38-22	7051-7052	,	_	_
38-23	7053-7070	generation\_config	_	_
38-24	7070-7071	=	_	_
38-25	7071-7088	generation\_config	_	_
38-26	7088-7089	)	_	_
38-27	7090-7098	response	_	_
38-28	7099-7100	=	_	_
38-29	7101-7117	tokenizer.decode	_	_
38-30	7117-7118	(	_	_
38-31	7118-7125	outputs	_	_
38-32	7125-7126	\[	_	_
38-33	7126-7127	0	_	_
38-34	7127-7128	\]	_	_
38-35	7128-7129	,	_	_
38-36	7130-7149	skip\_special\_tokens	_	_
38-37	7149-7150	=	_	_
38-38	7150-7154	True	_	_
38-39	7154-7155	)	_	_
38-40	7155-7156	\[	_	_
38-41	7156-7159	len	_	_
38-42	7159-7160	(	_	_
38-43	7160-7166	inputs	_	_
38-44	7166-7167	)	_	_
38-45	7167-7168	-	_	_
38-46	7168-7169	1	_	_
38-47	7169-7170	:	_	_
38-48	7170-7171	\]	_	_
38-49	7171-7172	.	_	_
38-50	7172-7177	strip	_	_
38-51	7177-7178	(	_	_
38-52	7178-7179	)	_	_
38-53	7180-7181	`	_	_
38-54	7181-7182	`	_	_
38-55	7182-7183	`	_	_
38-56	7185-7186	#	_	_
38-57	7186-7187	#	_	_
38-58	7188-7202	Acknowledgment	_	_
38-59	7204-7208	This	_	_
38-60	7209-7219	repository	_	_
38-61	7220-7228	benefits	_	_
38-62	7229-7233	from	_	_
38-63	7234-7235	\[	_	_
38-64	7235-7240	ALBEF	_	_
38-65	7240-7241	\]	_	_
38-66	7241-7242	(	_	_
38-67	7242-7247	https	_	_
38-68	7247-7248	:	_	_
38-69	7248-7249	/	_	_
38-70	7249-7250	/	_	_
38-71	7250-7260	github.com	_	_
38-72	7260-7261	/	_	_
38-73	7261-7271	salesforce	_	_
38-74	7271-7272	/	_	_
38-75	7272-7277	ALBEF	_	_
38-76	7277-7278	)	_	_
38-77	7278-7279	,	_	_
38-78	7280-7281	\[	_	_
38-79	7281-7289	GPT-NeoX	_	_
38-80	7289-7290	\]	_	_
38-81	7290-7291	(	_	_
38-82	7291-7296	https	_	_
38-83	7296-7297	:	_	_
38-84	7297-7298	/	_	_
38-85	7298-7299	/	_	_
38-86	7299-7309	github.com	_	_
38-87	7309-7310	/	_	_
38-88	7310-7320	EleutherAI	_	_
38-89	7320-7321	/	_	_
38-90	7321-7329	gpt-neox	_	_
38-91	7329-7330	)	_	_
38-92	7330-7331	,	_	_
38-93	7332-7333	\[	_	_
38-94	7333-7338	LLaMA	_	_
38-95	7338-7339	\]	_	_
38-96	7339-7340	(	_	_
38-97	7340-7345	https	_	_
38-98	7345-7346	:	_	_
38-99	7346-7347	/	_	_
38-100	7347-7348	/	_	_
38-101	7348-7363	ai.facebook.com	_	_
38-102	7363-7364	/	_	_
38-103	7364-7368	blog	_	_
38-104	7368-7369	/	_	_
38-105	7369-7403	large-language-model-llama-meta-ai	_	_
38-106	7403-7404	)	_	_
38-107	7404-7405	,	_	_
38-108	7406-7407	\[	_	_
38-109	7407-7418	Megatron-LM	_	_
38-110	7418-7419	\]	_	_
38-111	7419-7420	(	_	_
38-112	7420-7425	https	_	_
38-113	7425-7426	:	_	_
38-114	7426-7427	/	_	_
38-115	7427-7428	/	_	_
38-116	7428-7438	github.com	_	_
38-117	7438-7439	/	_	_
38-118	7439-7445	NVIDIA	_	_
38-119	7445-7446	/	_	_
38-120	7446-7457	Megatron-LM	_	_
38-121	7457-7458	)	_	_
38-122	7458-7459	,	_	_
38-123	7460-7461	\[	_	_
38-124	7461-7482	lm-evaluation-harness	_	_
38-125	7482-7483	\]	_	_
38-126	7483-7484	(	_	_
38-127	7484-7489	https	_	_
38-128	7489-7490	:	_	_
38-129	7490-7491	/	_	_
38-130	7491-7492	/	_	_
38-131	7492-7502	github.com	_	_
38-132	7502-7503	/	_	_
38-133	7503-7513	EleutherAI	_	_
38-134	7513-7514	/	_	_
38-135	7514-7535	lm-evaluation-harness	_	_
38-136	7535-7536	)	_	_
38-137	7536-7537	,	_	_
38-138	7538-7539	\[	_	_
38-139	7539-7547	Stanford	_	_
38-140	7548-7554	Alpaca	_	_
38-141	7554-7555	\]	_	_
38-142	7555-7556	(	_	_
38-143	7556-7561	https	_	_
38-144	7561-7562	:	*[130]	PUBLICATION[130]
38-145	7562-7563	/	*[130]	PUBLICATION[130]
38-146	7563-7564	/	*[130]	PUBLICATION[130]
38-147	7564-7574	github.com	*[130]	PUBLICATION[130]
38-148	7574-7575	/	*[130]	PUBLICATION[130]
38-149	7575-7584	tatsu-lab	*[130]	PUBLICATION[130]
38-150	7584-7585	/	*[130]	PUBLICATION[130]
38-151	7585-7600	stanford\_alpaca	_	_
38-152	7600-7601	)	_	_
38-153	7601-7602	,	_	_
38-154	7603-7606	and	_	_
38-155	7607-7608	\[	_	_
38-156	7608-7617	DeepSpeed	_	_
38-157	7617-7618	\]	_	_
38-158	7618-7619	(	_	_
38-159	7619-7624	https	_	_
38-160	7624-7625	:	_	_
38-161	7625-7626	/	_	_
38-162	7626-7627	/	_	_
38-163	7627-7637	github.com	_	_
38-164	7637-7638	/	_	_
38-165	7638-7647	microsoft	_	_
38-166	7647-7648	/	_	_
38-167	7648-7657	DeepSpeed	_	_
38-168	7657-7658	)	_	_
38-169	7658-7659	.	_	_

#Text=Thanks for their wonderful works and their efforts to further research.  ## Citation If you find this tutorial helpful, please cite our paper: ``` @article{qiu2024to,   title={To Cool or not to Cool?
39-1	7661-7667	Thanks	_	_
39-2	7668-7671	for	_	_
39-3	7672-7677	their	_	_
39-4	7678-7687	wonderful	_	_
39-5	7688-7693	works	_	_
39-6	7694-7697	and	_	_
39-7	7698-7703	their	*[139]	WORKSHOP[139]
39-8	7704-7711	efforts	*[139]	WORKSHOP[139]
39-9	7712-7714	to	*[139]	WORKSHOP[139]
39-10	7715-7722	further	*[139]	WORKSHOP[139]
39-11	7723-7731	research	_	_
39-12	7731-7732	.	_	_
39-13	7734-7735	#	_	_
39-14	7735-7736	#	_	_
39-15	7737-7745	Citation	_	_
39-16	7746-7748	If	_	_
39-17	7749-7752	you	_	_
39-18	7753-7757	find	_	_
39-19	7758-7762	this	_	_
39-20	7763-7771	tutorial	_	_
39-21	7772-7779	helpful	_	_
39-22	7779-7780	,	_	_
39-23	7781-7787	please	_	_
39-24	7788-7792	cite	_	_
39-25	7793-7796	our	_	_
39-26	7797-7802	paper	_	_
39-27	7802-7803	:	_	_
39-28	7804-7805	`	_	_
39-29	7805-7806	`	_	_
39-30	7806-7807	`	_	_
39-31	7808-7809	@	_	_
39-32	7809-7816	article	_	_
39-33	7816-7817	{	_	_
39-34	7817-7826	qiu2024to	_	_
39-35	7826-7827	,	_	_
39-36	7830-7835	title	_	_
39-37	7835-7836	=	_	_
39-38	7836-7837	{	_	_
39-39	7837-7839	To	_	_
39-40	7840-7844	Cool	_	_
39-41	7845-7847	or	_	_
39-42	7848-7851	not	_	_
39-43	7852-7854	to	_	_
39-44	7855-7859	Cool	_	_
39-45	7859-7860	?	_	_

#Text=Temperature Network Meets Large Foundation Models via DRO},   author={Zi-Hao Qiu, Siqi Guo, Mao Xu, Tuo Zhao, Lijun Zhang, and Tianbao Yang},   journal={arXiv preprint arXiv:2404.04575},   year={2024} } ```
40-1	7861-7872	Temperature	_	_
40-2	7873-7880	Network	_	_
40-3	7881-7886	Meets	_	_
40-4	7887-7892	Large	_	_
40-5	7893-7903	Foundation	_	_
40-6	7904-7910	Models	_	_
40-7	7911-7914	via	_	_
40-8	7915-7918	DRO	_	_
40-9	7918-7919	}	_	_
40-10	7919-7920	,	_	_
40-11	7923-7929	author	_	_
40-12	7929-7930	=	_	_
40-13	7930-7931	{	_	_
40-14	7931-7937	Zi-Hao	_	_
40-15	7938-7941	Qiu	_	_
40-16	7941-7942	,	_	_
40-17	7943-7947	Siqi	_	_
40-18	7948-7951	Guo	_	_
40-19	7951-7952	,	_	_
40-20	7953-7956	Mao	_	_
40-21	7957-7959	Xu	_	_
40-22	7959-7960	,	_	_
40-23	7961-7964	Tuo	_	_
40-24	7965-7969	Zhao	_	_
40-25	7969-7970	,	_	_
40-26	7971-7976	Lijun	_	_
40-27	7977-7982	Zhang	_	_
40-28	7982-7983	,	_	_
40-29	7984-7987	and	_	_
40-30	7988-7995	Tianbao	_	_
40-31	7996-8000	Yang	_	_
40-32	8000-8001	}	_	_
40-33	8001-8002	,	_	_
40-34	8005-8012	journal	_	_
40-35	8012-8013	=	*[137]	ONTOLOGY[137]
40-36	8013-8014	{	_	_
40-37	8014-8019	arXiv	_	_
40-38	8020-8028	preprint	_	_
40-39	8029-8034	arXiv	_	_
40-40	8034-8035	:	_	_
40-41	8035-8045	2404.04575	_	_
40-42	8045-8046	}	_	_
40-43	8046-8047	,	_	_
40-44	8050-8054	year	_	_
40-45	8054-8055	=	_	_
40-46	8055-8056	{	_	_
40-47	8056-8060	2024	_	_
40-48	8060-8061	}	_	_
40-49	8062-8063	}	_	_
40-50	8064-8065	`	_	_
40-51	8065-8066	`	_	_
40-52	8066-8067	`	_	_