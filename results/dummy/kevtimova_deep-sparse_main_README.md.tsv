#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  \[Sparse Coding with Multi-Layer Decoders using Variance Regularization\](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - \[PyTorch\](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the \[MNIST\](http://yann.lecun.com/exdb/mnist/) dataset.
1-1	0-1	#	_	_
1-2	2-8	Sparse	_	_
1-3	9-15	Coding	_	_
1-4	16-20	with	_	_
1-5	21-32	Multi-Layer	_	_
1-6	33-41	Decoders	_	_
1-7	42-47	using	_	_
1-8	48-56	Variance	_	_
1-9	57-71	Regularization	_	_
1-10	72-76	This	_	_
1-11	77-79	is	_	_
1-12	80-81	a	_	_
1-13	82-89	PyTorch	_	_
1-14	90-104	implementation	_	_
1-15	105-108	for	_	_
1-16	109-112	the	_	_
1-17	113-118	setup	_	_
1-18	119-128	described	_	_
1-19	129-131	in	_	_
1-20	133-134	\[	_	_
1-21	134-140	Sparse	_	_
1-22	141-147	Coding	_	_
1-23	148-152	with	_	_
1-24	153-164	Multi-Layer	_	_
1-25	165-173	Decoders	_	_
1-26	174-179	using	_	_
1-27	180-188	Variance	_	_
1-28	189-203	Regularization	_	_
1-29	203-204	\]	_	_
1-30	204-205	(	_	_
1-31	205-210	https	_	_
1-32	210-211	:	_	_
1-33	211-212	/	_	_
1-34	212-213	/	_	_
1-35	213-222	arxiv.org	_	_
1-36	222-223	/	_	_
1-37	223-226	abs	_	_
1-38	226-227	/	_	_
1-39	227-237	2112.09214	_	_
1-40	237-238	)	_	_
1-41	238-239	.	_	_
1-42	242-243	#	_	_
1-43	243-244	#	_	_
1-44	244-245	#	_	_
1-45	246-258	Requirements	_	_
1-46	260-261	-	_	_
1-47	262-268	Python	_	_
1-48	269-272	3.7	_	_
1-49	273-274	-	_	_
1-50	275-276	\[	_	_
1-51	276-283	PyTorch	_	_
1-52	283-284	\]	_	_
1-53	284-285	(	_	_
1-54	285-290	https	_	_
1-55	290-291	:	_	_
1-56	291-292	/	_	_
1-57	292-293	/	_	_
1-58	293-304	pytorch.org	_	_
1-58	293-300	pytorch	_	_
1-59	304-305	/	_	_
1-60	305-316	get-started	_	_
1-61	316-317	/	_	_
1-62	317-334	previous-versions	_	_
1-63	334-335	/	_	_
1-64	335-336	)	_	_
1-65	337-342	1.6.0	_	_
1-66	343-347	with	_	_
1-67	348-359	torchvision	_	_
1-68	360-365	0.7.0	_	_
1-69	366-367	-	_	_
1-70	368-373	Other	_	_
1-71	374-386	dependencies	_	_
1-72	386-387	:	_	_
1-73	388-393	numpy	_	_
1-74	393-394	,	_	_
1-75	395-407	tensorboardX	_	_
1-76	409-410	#	_	_
1-77	410-411	#	_	_
1-78	411-412	#	_	_
1-79	413-421	Datasets	_	_
1-80	423-425	In	_	_
1-81	426-429	our	_	_
1-82	430-441	experiments	_	_
1-83	441-442	,	_	_
1-84	443-445	we	_	_
1-85	446-449	use	_	_
1-86	449-450	:	_	_
1-87	451-452	-	_	_
1-88	453-456	the	_	_
1-89	457-458	\[	_	_
1-90	458-463	MNIST	_	_
1-91	463-464	\]	_	_
1-92	464-465	(	_	_
1-93	465-469	http	_	_
1-94	469-470	:	_	_
1-95	470-471	/	_	_
1-96	471-472	/	_	_
1-97	472-486	yann.lecun.com	_	_
1-98	486-487	/	_	_
1-99	487-491	exdb	_	_
1-100	491-492	/	_	_
1-101	492-497	mnist	_	_
1-102	497-498	/	_	_
1-103	498-499	)	_	_
1-104	500-507	dataset	_	_
1-105	507-508	.	_	_

#Text=We provide the train and validation splits in  ```data/MNIST\_train.npy``` and ```data/MNIST\_val.npy```. - a custom dataset with 200,000 gray-scale natural image patches of size 28x28 extracted from  \[ImageNet\](https://www.image-net.org/index.php).
2-1	509-511	We	_	_
2-2	512-519	provide	_	_
2-3	520-523	the	_	_
2-4	524-529	train	_	_
2-5	530-533	and	_	_
2-6	534-544	validation	_	_
2-7	545-551	splits	_	_
2-8	552-554	in	_	_
2-9	556-557	`	_	_
2-10	557-558	`	_	_
2-11	558-559	`	_	_
2-12	559-563	data	_	_
2-13	563-564	/	_	_
2-14	564-579	MNIST\_train.npy	_	_
2-14	564-569	MNIST	_	_
2-15	579-580	`	_	_
2-16	580-581	`	_	_
2-17	581-582	`	_	_
2-18	583-586	and	_	_
2-19	587-588	`	_	_
2-20	588-589	`	_	_
2-21	589-590	`	_	_
2-22	590-594	data	_	_
2-23	594-595	/	_	_
2-24	595-608	MNIST\_val.npy	_	_
2-24	595-600	MNIST	_	_
2-25	608-609	`	_	_
2-26	609-610	`	_	_
2-27	610-611	`	_	_
2-28	611-612	.	_	_
2-29	613-614	-	_	_
2-30	615-616	a	_	_
2-31	617-623	custom	_	_
2-32	624-631	dataset	_	_
2-33	632-636	with	_	_
2-34	637-644	200,000	_	_
2-35	645-655	gray-scale	_	_
2-36	656-663	natural	_	_
2-37	664-669	image	_	_
2-38	670-677	patches	_	_
2-39	678-680	of	_	_
2-40	681-685	size	_	_
2-41	686-691	28x28	*[179]	WORKSHOP[179]
2-42	692-701	extracted	*[179]	WORKSHOP[179]
2-43	702-706	from	*[179]	WORKSHOP[179]
2-44	708-709	\[	*[179]	WORKSHOP[179]
2-45	709-717	ImageNet	*[179]	WORKSHOP[179]
2-46	717-718	\]	*[179]	WORKSHOP[179]
2-47	718-719	(	*[179]	WORKSHOP[179]
2-48	719-724	https	*[179]	WORKSHOP[179]
2-49	724-725	:	*[179]	WORKSHOP[179]
2-50	725-726	/	*[179]	WORKSHOP[179]
2-51	726-727	/	*[179]	WORKSHOP[179]
2-52	727-744	www.image-net.org	_	_
2-52	731-740	image-net	_	_
2-53	744-745	/	_	_
2-54	745-754	index.php	_	_
2-55	754-755	)	_	_
2-56	755-756	.	_	_

#Text=The script to generate it is  \[build\_imagenet\_LCN.sh\](https://github.com/kevtimova/deep-sparse/blob/main/scripts/build\_ImageNet\_LCN.sh).  ### Training  The scripts below can be used to train sparse autoencoders with our different setups
3-1	757-760	The	_	_
3-2	761-767	script	_	_
3-3	768-770	to	_	_
3-4	771-779	generate	_	_
3-5	780-782	it	_	_
3-6	783-785	is	_	_
3-7	787-788	\[	_	_
3-8	788-809	build\_imagenet\_LCN.sh	_	_
3-8	794-802	imagenet	*[173]	PUBLICATION[173]
3-9	809-810	\]	*[173]	PUBLICATION[173]
3-10	810-811	(	*[173]	PUBLICATION[173]
3-11	811-816	https	*[173]	PUBLICATION[173]
3-12	816-817	:	*[173]	PUBLICATION[173]
3-13	817-818	/	*[173]	PUBLICATION[173]
3-14	818-819	/	*[173]	PUBLICATION[173]
3-15	819-829	github.com	*[173]	PUBLICATION[173]
3-16	829-830	/	*[173]	PUBLICATION[173]
3-17	830-839	kevtimova	*[173]	PUBLICATION[173]
3-18	839-840	/	*[173]	PUBLICATION[173]
3-19	840-851	deep-sparse	*[173]	PUBLICATION[173]
3-20	851-852	/	*[173]	PUBLICATION[173]
3-21	852-856	blob	*[173]	PUBLICATION[173]
3-22	856-857	/	*[173]	PUBLICATION[173]
3-23	857-861	main	*[173]	PUBLICATION[173]
3-24	861-862	/	*[173]	PUBLICATION[173]
3-25	862-869	scripts	*[173]	PUBLICATION[173]
3-26	869-870	/	*[173]	PUBLICATION[173]
3-27	870-891	build\_ImageNet\_LCN.sh	*[173]	PUBLICATION[173]
3-27	876-884	ImageNet	*[173]	PUBLICATION[173]
3-28	891-892	)	*[173]	PUBLICATION[173]
3-29	892-893	.	*[173]	PUBLICATION[173]
3-30	895-896	#	*[173]	PUBLICATION[173]
3-31	896-897	#	*[173]	PUBLICATION[173]
3-32	897-898	#	*[173]	PUBLICATION[173]
3-33	899-907	Training	*[173]	PUBLICATION[173]
3-34	909-912	The	*[173]	PUBLICATION[173]
3-35	913-920	scripts	*[173]	PUBLICATION[173]
3-36	921-926	below	*[173]	PUBLICATION[173]
3-37	927-930	can	*[173]	PUBLICATION[173]
3-38	931-933	be	_	_
3-39	934-938	used	_	_
3-40	939-941	to	_	_
3-41	942-947	train	_	_
3-42	948-954	sparse	_	_
3-43	955-967	autoencoders	_	_
3-44	968-972	with	_	_
3-45	973-976	our	_	_
3-46	977-986	different	_	_
3-47	987-993	setups	_	_

#Text=.
4-1	993-994	.	_	_

#Text=\| dataset          \| model    \| script \| \|------------------\|----------\|--------\| \| MNIST            \| SDL      \| \[link\](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST\_SDL.sh)       \| \| MNIST            \| SDL-NL   \| \[link\](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST\_SDL-NL.sh)       \| \| MNIST            \| VDL      \| \[link\](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST\_VDL.sh)       \| \| MNIST            \| VDL-NL   \| \[link\](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST\_VDL-NL.sh)       \| \| ImageNet\_patches \| SDL      \| \[link\](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet\_SDL.sh)       \| \| ImageNet\_patches \| SDL-NL   \| \[link\](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet\_SDL-NL.sh)       \| \| ImageNet\_patches \| VDL      \| \[link\](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet\_VDL.sh)       \| \| Imagenet\_patches \| VDL-NL   \| \[link\](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet\_VDL-NL.sh)       \|  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise\; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
5-1	996-997	\|	_	_
5-2	998-1005	dataset	_	_
5-3	1015-1016	\|	_	_
5-4	1017-1022	model	_	_
5-5	1026-1027	\|	_	_
5-6	1028-1034	script	_	_
5-7	1035-1036	\|	_	_
5-8	1037-1038	\|	_	_
5-9	1038-1039	-	_	_
5-10	1039-1040	-	_	_
5-11	1040-1041	-	_	_
5-12	1041-1042	-	_	_
5-13	1042-1043	-	_	_
5-14	1043-1044	-	_	_
5-15	1044-1045	-	_	_
5-16	1045-1046	-	_	_
5-17	1046-1047	-	_	_
5-18	1047-1048	-	_	_
5-19	1048-1049	-	_	_
5-20	1049-1050	-	_	_
5-21	1050-1051	-	_	_
5-22	1051-1052	-	_	_
5-23	1052-1053	-	_	_
5-24	1053-1054	-	_	_
5-25	1054-1055	-	_	_
5-26	1055-1056	-	_	_
5-27	1056-1057	\|	_	_
5-28	1057-1058	-	_	_
5-29	1058-1059	-	_	_
5-30	1059-1060	-	_	_
5-31	1060-1061	-	_	_
5-32	1061-1062	-	_	_
5-33	1062-1063	-	_	_
5-34	1063-1064	-	_	_
5-35	1064-1065	-	_	_
5-36	1065-1066	-	_	_
5-37	1066-1067	-	_	_
5-38	1067-1068	\|	_	_
5-39	1068-1069	-	_	_
5-40	1069-1070	-	_	_
5-41	1070-1071	-	_	_
5-42	1071-1072	-	_	_
5-43	1072-1073	-	_	_
5-44	1073-1074	-	_	_
5-45	1074-1075	-	_	_
5-46	1075-1076	-	_	_
5-47	1076-1077	\|	_	_
5-48	1078-1079	\|	_	_
5-49	1080-1085	MNIST	_	_
5-50	1097-1098	\|	_	_
5-51	1099-1102	SDL	_	_
5-52	1108-1109	\|	_	_
5-53	1110-1111	\[	_	_
5-54	1111-1115	link	_	_
5-55	1115-1116	\]	_	_
5-56	1116-1117	(	_	_
5-57	1117-1122	https	_	_
5-58	1122-1123	:	_	_
5-59	1123-1124	/	_	_
5-60	1124-1125	/	_	_
5-61	1125-1135	github.com	_	_
5-62	1135-1136	/	_	_
5-63	1136-1145	kevtimova	_	_
5-64	1145-1146	/	_	_
5-65	1146-1157	deep-sparse	_	_
5-66	1157-1158	/	_	_
5-67	1158-1162	blob	_	_
5-68	1162-1163	/	_	_
5-69	1163-1167	main	_	_
5-70	1167-1168	/	_	_
5-71	1168-1175	scripts	_	_
5-72	1175-1176	/	_	_
5-73	1176-1188	MNIST\_SDL.sh	_	_
5-73	1176-1181	MNIST	_	_
5-74	1188-1189	)	_	_
5-75	1196-1197	\|	_	_
5-76	1198-1199	\|	_	_
5-77	1200-1205	MNIST	_	_
5-78	1217-1218	\|	_	_
5-79	1219-1225	SDL-NL	_	_
5-80	1228-1229	\|	_	_
5-81	1230-1231	\[	_	_
5-82	1231-1235	link	_	_
5-83	1235-1236	\]	_	_
5-84	1236-1237	(	_	_
5-85	1237-1242	https	_	_
5-86	1242-1243	:	_	_
5-87	1243-1244	/	_	_
5-88	1244-1245	/	_	_
5-89	1245-1255	github.com	_	_
5-90	1255-1256	/	_	_
5-91	1256-1265	kevtimova	_	_
5-92	1265-1266	/	_	_
5-93	1266-1277	deep-sparse	_	_
5-94	1277-1278	/	_	_
5-95	1278-1282	blob	_	_
5-96	1282-1283	/	_	_
5-97	1283-1287	main	_	_
5-98	1287-1288	/	_	_
5-99	1288-1295	scripts	_	_
5-100	1295-1296	/	_	_
5-101	1296-1311	MNIST\_SDL-NL.sh	_	_
5-102	1311-1312	)	_	_
5-103	1319-1320	\|	_	_
5-104	1321-1322	\|	_	_
5-105	1323-1328	MNIST	_	_
5-106	1340-1341	\|	_	_
5-107	1342-1345	VDL	_	_
5-108	1351-1352	\|	_	_
5-109	1353-1354	\[	_	_
5-110	1354-1358	link	_	_
5-111	1358-1359	\]	_	_
5-112	1359-1360	(	_	_
5-113	1360-1365	https	_	_
5-114	1365-1366	:	_	_
5-115	1366-1367	/	_	_
5-116	1367-1368	/	_	_
5-117	1368-1378	github.com	_	_
5-118	1378-1379	/	_	_
5-119	1379-1388	kevtimova	_	_
5-120	1388-1389	/	_	_
5-121	1389-1400	deep-sparse	_	_
5-122	1400-1401	/	_	_
5-123	1401-1405	blob	_	_
5-124	1405-1406	/	_	_
5-125	1406-1410	main	_	_
5-126	1410-1411	/	_	_
5-127	1411-1418	scripts	_	_
5-128	1418-1419	/	_	_
5-129	1419-1431	MNIST\_VDL.sh	_	_
5-129	1419-1424	MNIST	_	_
5-130	1431-1432	)	_	_
5-131	1439-1440	\|	_	_
5-132	1441-1442	\|	_	_
5-133	1443-1448	MNIST	_	_
5-134	1460-1461	\|	_	_
5-135	1462-1468	VDL-NL	_	_
5-136	1471-1472	\|	_	_
5-137	1473-1474	\[	_	_
5-138	1474-1478	link	_	_
5-139	1478-1479	\]	_	_
5-140	1479-1480	(	_	_
5-141	1480-1485	https	_	_
5-142	1485-1486	:	_	_
5-143	1486-1487	/	_	_
5-144	1487-1488	/	_	_
5-145	1488-1498	github.com	_	_
5-146	1498-1499	/	_	_
5-147	1499-1508	kevtimova	_	_
5-148	1508-1509	/	_	_
5-149	1509-1520	deep-sparse	_	_
5-150	1520-1521	/	_	_
5-151	1521-1525	blob	_	_
5-152	1525-1526	/	_	_
5-153	1526-1530	main	_	_
5-154	1530-1531	/	_	_
5-155	1531-1538	scripts	_	_
5-156	1538-1539	/	_	_
5-157	1539-1554	MNIST\_VDL-NL.sh	_	_
5-157	1539-1544	MNIST	_	_
5-158	1554-1555	)	_	_
5-159	1562-1563	\|	_	_
5-160	1564-1565	\|	_	_
5-161	1566-1582	ImageNet\_patches	_	_
5-162	1583-1584	\|	_	_
5-163	1585-1588	SDL	_	_
5-164	1594-1595	\|	_	_
5-165	1596-1597	\[	_	_
5-166	1597-1601	link	_	_
5-167	1601-1602	\]	_	_
5-168	1602-1603	(	_	_
5-169	1603-1608	https	_	_
5-170	1608-1609	:	_	_
5-171	1609-1610	/	_	_
5-172	1610-1611	/	_	_
5-173	1611-1621	github.com	_	_
5-174	1621-1622	/	_	_
5-175	1622-1631	kevtimova	_	_
5-176	1631-1632	/	_	_
5-177	1632-1643	deep-sparse	_	_
5-178	1643-1644	/	_	_
5-179	1644-1648	blob	_	_
5-180	1648-1649	/	_	_
5-181	1649-1653	main	_	_
5-182	1653-1654	/	_	_
5-183	1654-1661	scripts	_	_
5-184	1661-1662	/	_	_
5-185	1662-1677	ImageNet\_SDL.sh	_	_
5-185	1662-1670	ImageNet	_	_
5-186	1677-1678	)	_	_
5-187	1685-1686	\|	_	_
5-188	1687-1688	\|	_	_
5-189	1689-1705	ImageNet\_patches	_	_
5-190	1706-1707	\|	_	_
5-191	1708-1714	SDL-NL	_	_
5-192	1717-1718	\|	_	_
5-193	1719-1720	\[	_	_
5-194	1720-1724	link	_	_
5-195	1724-1725	\]	_	_
5-196	1725-1726	(	_	_
5-197	1726-1731	https	_	_
5-198	1731-1732	:	_	_
5-199	1732-1733	/	_	_
5-200	1733-1734	/	_	_
5-201	1734-1744	github.com	_	_
5-202	1744-1745	/	_	_
5-203	1745-1754	kevtimova	_	_
5-204	1754-1755	/	_	_
5-205	1755-1766	deep-sparse	_	_
5-206	1766-1767	/	_	_
5-207	1767-1771	blob	_	_
5-208	1771-1772	/	_	_
5-209	1772-1776	main	_	_
5-210	1776-1777	/	_	_
5-211	1777-1784	scripts	_	_
5-212	1784-1785	/	_	_
5-213	1785-1803	ImageNet\_SDL-NL.sh	_	_
5-213	1785-1793	ImageNet	_	_
5-214	1803-1804	)	_	_
5-215	1811-1812	\|	_	_
5-216	1813-1814	\|	_	_
5-217	1815-1831	ImageNet\_patches	_	_
5-218	1832-1833	\|	_	_
5-219	1834-1837	VDL	_	_
5-220	1843-1844	\|	_	_
5-221	1845-1846	\[	_	_
5-222	1846-1850	link	_	_
5-223	1850-1851	\]	_	_
5-224	1851-1852	(	_	_
5-225	1852-1857	https	_	_
5-226	1857-1858	:	_	_
5-227	1858-1859	/	_	_
5-228	1859-1860	/	_	_
5-229	1860-1870	github.com	_	_
5-230	1870-1871	/	_	_
5-231	1871-1880	kevtimova	_	_
5-232	1880-1881	/	_	_
5-233	1881-1892	deep-sparse	_	_
5-234	1892-1893	/	_	_
5-235	1893-1897	blob	_	_
5-236	1897-1898	/	_	_
5-237	1898-1902	main	_	_
5-238	1902-1903	/	_	_
5-239	1903-1910	scripts	_	_
5-240	1910-1911	/	_	_
5-241	1911-1926	ImageNet\_VDL.sh	_	_
5-241	1911-1919	ImageNet	_	_
5-242	1926-1927	)	_	_
5-243	1934-1935	\|	_	_
5-244	1936-1937	\|	_	_
5-245	1938-1954	Imagenet\_patches	_	_
5-246	1955-1956	\|	_	_
5-247	1957-1963	VDL-NL	_	_
5-248	1966-1967	\|	_	_
5-249	1968-1969	\[	_	_
5-250	1969-1973	link	_	_
5-251	1973-1974	\]	_	_
5-252	1974-1975	(	_	_
5-253	1975-1980	https	_	_
5-254	1980-1981	:	_	_
5-255	1981-1982	/	_	_
5-256	1982-1983	/	_	_
5-257	1983-1993	github.com	_	_
5-258	1993-1994	/	_	_
5-259	1994-2003	kevtimova	_	_
5-260	2003-2004	/	_	_
5-261	2004-2015	deep-sparse	_	_
5-262	2015-2016	/	_	_
5-263	2016-2020	blob	_	_
5-264	2020-2021	/	_	_
5-265	2021-2025	main	_	_
5-266	2025-2026	/	_	_
5-267	2026-2033	scripts	_	_
5-268	2033-2034	/	_	_
5-269	2034-2052	ImageNet\_VDL-NL.sh	_	_
5-269	2034-2042	ImageNet	_	_
5-270	2052-2053	)	_	_
5-271	2060-2061	\|	_	_
5-272	2063-2064	#	_	_
5-273	2064-2065	#	_	_
5-274	2065-2066	#	_	_
5-275	2067-2077	Evaluation	_	_
5-276	2079-2081	We	_	_
5-277	2082-2090	evaluate	_	_
5-278	2091-2094	our	_	_
5-279	2095-2106	pre-trained	_	_
5-280	2107-2113	sparse	_	_
5-281	2114-2126	autoencoders	_	_
5-282	2127-2129	on	_	_
5-283	2130-2133	the	_	_
5-284	2134-2144	downstream	_	_
5-285	2145-2150	tasks	_	_
5-286	2151-2153	of	_	_
5-287	2154-2163	denoising	_	_
5-288	2164-2165	(	_	_
5-289	2165-2168	for	_	_
5-290	2169-2174	MNIST	_	_
5-291	2175-2178	and	_	_
5-292	2179-2182	our	_	_
5-293	2183-2189	custom	_	_
5-294	2191-2199	ImageNet	_	_
5-295	2200-2207	patches	_	_
5-296	2208-2215	dataset	_	_
5-297	2215-2216	)	_	_
5-298	2217-2220	and	_	_
5-299	2221-2235	classification	_	_
5-300	2236-2238	in	_	_
5-301	2239-2242	the	_	_
5-302	2243-2251	low-data	_	_
5-303	2252-2258	regime	_	_
5-304	2259-2260	(	_	_
5-305	2260-2263	for	_	_
5-306	2264-2269	MNIST	_	_
5-307	2270-2274	only	_	_
5-308	2274-2275	)	_	_
5-309	2275-2276	.	_	_
5-310	2278-2279	#	_	_
5-311	2279-2280	#	_	_
5-312	2280-2281	#	*[170]	DATASET[170]
5-313	2281-2282	#	*[170]	DATASET[170]
5-314	2283-2292	Denoising	*[170]	DATASET[170]
5-315	2294-2297	The	*[170]	DATASET[170]
5-316	2298-2307	denoising	*[170]	DATASET[170]
5-317	2308-2318	perfomance	*[170]	DATASET[170]
5-318	2319-2321	on	*[170]	DATASET[170]
5-319	2322-2325	the	*[170]	DATASET[170]
5-320	2326-2330	test	*[170]	DATASET[170]
5-321	2331-2334	set	*[170]	DATASET[170]
5-322	2335-2338	can	*[170]	DATASET[170]
5-323	2339-2341	be	*[170]	DATASET[170]
5-324	2342-2350	measured	*[170]	DATASET[170]
5-325	2351-2353	at	*[170]	DATASET[170]
5-326	2354-2357	the	*[170]	DATASET[170]
5-327	2358-2361	end	*[170]	DATASET[170]
5-328	2362-2364	of	*[170]	DATASET[170]
5-329	2365-2373	training	*[170]	DATASET[170]
5-330	2374-2376	by	*[170]	DATASET[170]
5-331	2377-2386	providing	_	_
5-332	2387-2388	a	_	_
5-333	2389-2393	list	_	_
5-334	2394-2398	with	_	_
5-335	2399-2405	levels	_	_
5-336	2406-2408	of	_	_
5-337	2410-2416	random	_	_
5-338	2417-2422	noise	_	_
5-339	2423-2424	(	_	_
5-340	2424-2432	measured	_	_
5-341	2433-2435	by	_	_
5-342	2436-2439	std	_	_
5-343	2440-2442	of	_	_
5-344	2443-2451	Gaussian	_	_
5-345	2452-2457	noise	_	_
5-346	2457-2458	\;	_	_
5-347	2459-2462	the	_	_
5-348	2463-2468	noise	_	_
5-349	2469-2471	is	_	_
5-350	2472-2477	added	_	_
5-351	2478-2480	to	_	_
5-352	2481-2484	the	_	_
5-353	2485-2490	input	_	_
5-354	2491-2497	images	_	_
5-355	2497-2498	)	_	_
5-356	2499-2501	in	_	_
5-357	2502-2505	the	_	_
5-358	2506-2507	`	_	_
5-359	2507-2508	`	_	_
5-360	2508-2509	`	_	_
5-361	2509-2514	noise	_	_
5-362	2514-2515	`	_	_
5-363	2515-2516	`	_	_
5-364	2516-2517	`	_	_
5-365	2518-2526	argument	_	_
5-366	2528-2530	in	_	_
5-367	2531-2532	`	_	_
5-368	2532-2533	`	_	_
5-369	2533-2534	`	_	_
5-370	2534-2541	main.py	_	_
5-371	2541-2542	`	_	_
5-372	2542-2543	`	_	_
5-373	2543-2544	`	_	_
5-374	2544-2545	.	_	_

#Text=Alternatively, ```eval\_denoising.py``` can be used given a pre-trained autoencoder.  #### Classification  To evaluate the linear separability of codes obtained from the sparse autoencoders, we provide the steps below.
6-1	2547-2560	Alternatively	_	_
6-2	2560-2561	,	_	_
6-3	2562-2563	`	_	_
6-4	2563-2564	`	_	_
6-5	2564-2565	`	_	_
6-6	2565-2582	eval\_denoising.py	_	_
6-7	2582-2583	`	_	_
6-8	2583-2584	`	_	_
6-9	2584-2585	`	_	_
6-10	2586-2589	can	_	_
6-11	2590-2592	be	_	_
6-12	2593-2597	used	_	_
6-13	2598-2603	given	_	_
6-14	2604-2605	a	_	_
6-15	2606-2617	pre-trained	_	_
6-16	2618-2629	autoencoder	_	_
6-17	2629-2630	.	_	_
6-18	2632-2633	#	_	_
6-19	2633-2634	#	_	_
6-20	2634-2635	#	_	_
6-21	2635-2636	#	_	_
6-22	2637-2651	Classification	_	_
6-23	2653-2655	To	_	_
6-24	2656-2664	evaluate	*[208]	SOFTWARE[208]
6-25	2665-2668	the	*[208]	SOFTWARE[208]
6-26	2669-2675	linear	*[208]	SOFTWARE[208]
6-27	2676-2688	separability	*[208]	SOFTWARE[208]
6-28	2689-2691	of	*[208]	SOFTWARE[208]
6-29	2692-2697	codes	*[208]	SOFTWARE[208]
6-30	2698-2706	obtained	*[208]	SOFTWARE[208]
6-31	2707-2711	from	*[208]	SOFTWARE[208]
6-32	2712-2715	the	*[208]	SOFTWARE[208]
6-33	2716-2722	sparse	*[208]	SOFTWARE[208]
6-34	2723-2735	autoencoders	*[208]	SOFTWARE[208]
6-35	2735-2736	,	*[208]	SOFTWARE[208]
6-36	2737-2739	we	*[208]	SOFTWARE[208]
6-37	2740-2747	provide	*[208]	SOFTWARE[208]
6-38	2748-2751	the	*[208]	SOFTWARE[208]
6-39	2752-2757	steps	_	_
6-40	2758-2763	below	_	_
6-41	2763-2764	.	_	_

#Text=Step 1: Given a pre-trained encoder, ```compute\_codes.py``` can be used to create a dataset containing the codes  for each MNIST image.
7-1	2766-2770	Step	_	_
7-2	2771-2772	1	_	_
7-3	2772-2773	:	*[180]	WORKSHOP[180]
7-4	2774-2779	Given	*[180]	WORKSHOP[180]
7-5	2780-2781	a	*[180]	WORKSHOP[180]
7-6	2782-2793	pre-trained	*[180]	WORKSHOP[180]
7-7	2794-2801	encoder	*[180]	WORKSHOP[180]
7-8	2801-2802	,	*[180]	WORKSHOP[180]
7-9	2803-2804	`	*[180]	WORKSHOP[180]
7-10	2804-2805	`	*[180]	WORKSHOP[180]
7-11	2805-2806	`	*[180]	WORKSHOP[180]
7-12	2806-2822	compute\_codes.py	*[180]	WORKSHOP[180]
7-13	2822-2823	`	*[180]	WORKSHOP[180]
7-14	2823-2824	`	*[180]	WORKSHOP[180]
7-15	2824-2825	`	*[180]	WORKSHOP[180]
7-16	2826-2829	can	*[180]	WORKSHOP[180]
7-17	2830-2832	be	*[180]	WORKSHOP[180]
7-18	2833-2837	used	*[180]	WORKSHOP[180]
7-19	2838-2840	to	*[180]	WORKSHOP[180]
7-20	2841-2847	create	*[180]	WORKSHOP[180]
7-21	2848-2849	a	*[180]	WORKSHOP[180]
7-22	2850-2857	dataset	*[180]	WORKSHOP[180]
7-23	2858-2868	containing	*[180]	WORKSHOP[180]
7-24	2869-2872	the	*[180]	WORKSHOP[180]
7-25	2873-2878	codes	*[180]	WORKSHOP[180]
7-26	2880-2883	for	*[180]	WORKSHOP[180]
7-27	2884-2888	each	*[180]	WORKSHOP[180]
7-28	2889-2894	MNIST	*[180]	WORKSHOP[180]
7-29	2895-2900	image	_	_
7-30	2900-2901	.	_	_

#Text=Step 2: Using the dataset from the previous step, ```eval\_classification.py``` can be used to measure classification  performance with a set number of training samples per class.
8-1	2903-2907	Step	_	_
8-2	2908-2909	2	_	_
8-3	2909-2910	:	_	_
8-4	2911-2916	Using	_	_
8-5	2917-2920	the	_	_
8-6	2921-2928	dataset	_	_
8-7	2929-2933	from	_	_
8-8	2934-2937	the	_	_
8-9	2938-2946	previous	_	_
8-10	2947-2951	step	_	_
8-11	2951-2952	,	_	_
8-12	2953-2954	`	_	_
8-13	2954-2955	`	_	_
8-14	2955-2956	`	_	_
8-15	2956-2978	eval\_classification.py	_	_
8-16	2978-2979	`	*[199]	PROGLANG[199]
8-17	2979-2980	`	*[199]	PROGLANG[199]
8-18	2980-2981	`	*[199]	PROGLANG[199]
8-19	2982-2985	can	*[199]	PROGLANG[199]
8-20	2986-2988	be	*[199]	PROGLANG[199]
8-21	2989-2993	used	*[199]	PROGLANG[199]
8-22	2994-2996	to	*[199]	PROGLANG[199]
8-23	2997-3004	measure	*[199]	PROGLANG[199]
8-24	3005-3019	classification	_	_
8-25	3021-3032	performance	_	_
8-26	3033-3037	with	_	_
8-27	3038-3039	a	_	_
8-28	3040-3043	set	_	_
8-29	3044-3050	number	_	_
8-30	3051-3053	of	_	_
8-31	3054-3062	training	_	_
8-32	3063-3070	samples	_	_
8-33	3071-3074	per	_	_
8-34	3075-3080	class	_	_
8-35	3080-3081	.	_	_

#Text=There are two options for the classifier - a linear classifier (located in ```modles/linear\_classifier.py```) and a classifier which uses a randomly initialized LISTA encoder module  followed by a linear classification layer (located in ```modles/lista\_classifier.py```).
9-1	3083-3088	There	_	_
9-2	3089-3092	are	_	_
9-3	3093-3096	two	_	_
9-4	3097-3104	options	_	_
9-5	3105-3108	for	_	_
9-6	3109-3112	the	_	_
9-7	3113-3123	classifier	_	_
9-8	3124-3125	-	*[175]	CONFERENCE[175]
9-9	3126-3127	a	*[175]	CONFERENCE[175]
9-10	3128-3134	linear	*[175]	CONFERENCE[175]
9-11	3135-3145	classifier	*[175]	CONFERENCE[175]
9-12	3146-3147	(	*[175]	CONFERENCE[175]
9-13	3147-3154	located	*[175]	CONFERENCE[175]
9-14	3155-3157	in	*[175]	CONFERENCE[175]
9-15	3158-3159	`	*[175]	CONFERENCE[175]
9-16	3159-3160	`	*[175]	CONFERENCE[175]
9-17	3160-3161	`	*[175]	CONFERENCE[175]
9-18	3161-3167	modles	*[175]	CONFERENCE[175]
9-19	3167-3168	/	*[175]	CONFERENCE[175]
9-20	3168-3188	linear\_classifier.py	*[175]	CONFERENCE[175]
9-21	3188-3189	`	*[175]	CONFERENCE[175]
9-22	3189-3190	`	*[175]	CONFERENCE[175]
9-23	3190-3191	`	*[175]	CONFERENCE[175]
9-24	3191-3192	)	*[175]	CONFERENCE[175]
9-25	3193-3196	and	*[175]	CONFERENCE[175]
9-26	3197-3198	a	*[175]	CONFERENCE[175]
9-27	3199-3209	classifier	*[175]	CONFERENCE[175]
9-28	3210-3215	which	*[175]	CONFERENCE[175]
9-29	3216-3220	uses	*[175]	CONFERENCE[175]
9-30	3221-3222	a	*[175]	CONFERENCE[175]
9-31	3223-3231	randomly	*[175]	CONFERENCE[175]
9-32	3232-3243	initialized	*[175]	CONFERENCE[175]
9-33	3244-3249	LISTA	*[175]	CONFERENCE[175]
9-34	3250-3257	encoder	*[175]	CONFERENCE[175]
9-35	3258-3264	module	*[175]	CONFERENCE[175]
9-36	3266-3274	followed	*[175]	CONFERENCE[175]
9-37	3275-3277	by	*[175]	CONFERENCE[175]
9-38	3278-3279	a	*[175]	CONFERENCE[175]
9-39	3280-3286	linear	*[175]	CONFERENCE[175]
9-40	3287-3301	classification	_	_
9-41	3302-3307	layer	_	_
9-42	3308-3309	(	_	_
9-43	3309-3316	located	_	_
9-44	3317-3319	in	_	_
9-45	3320-3321	`	_	_
9-46	3321-3322	`	_	_
9-47	3322-3323	`	_	_
9-48	3323-3329	modles	_	_
9-49	3329-3330	/	_	_
9-50	3330-3349	lista\_classifier.py	_	_
9-51	3349-3350	`	_	_
9-52	3350-3351	`	_	_
9-53	3351-3352	`	_	_
9-54	3352-3353	)	_	_
9-55	3353-3354	.	_	_