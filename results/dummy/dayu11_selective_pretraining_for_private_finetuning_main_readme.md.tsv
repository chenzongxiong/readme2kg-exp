#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# Setup  \[This repo\](https://github.com/dayu11/selective\_pretraining\_for\_private\_finetuning) implements the framework in \[this paper\](https://arxiv.org/abs/2305.13865) on SST-2.
1-1	0-1	#	_	_
1-2	2-7	Setup	_	_
1-3	9-10	\[	_	_
1-4	10-14	This	_	_
1-5	15-19	repo	_	_
1-6	19-20	\]	_	_
1-7	20-21	(	_	_
1-8	21-26	https	_	_
1-9	26-27	:	_	_
1-10	27-28	/	_	_
1-11	28-29	/	_	_
1-12	29-39	github.com	_	_
1-13	39-40	/	_	_
1-14	40-46	dayu11	_	_
1-15	46-47	/	_	_
1-16	47-91	selective\_pretraining\_for\_private\_finetuning	_	_
1-17	91-92	)	_	_
1-18	93-103	implements	_	_
1-19	104-107	the	_	_
1-20	108-117	framework	_	_
1-21	118-120	in	_	_
1-22	121-122	\[	_	_
1-23	122-126	this	_	_
1-24	127-132	paper	_	_
1-25	132-133	\]	_	_
1-26	133-134	(	_	_
1-27	134-139	https	_	_
1-28	139-140	:	_	_
1-29	140-141	/	_	_
1-30	141-142	/	_	_
1-31	142-151	arxiv.org	_	_
1-32	151-152	/	*[160]	LICENSE[160]
1-33	152-155	abs	*[160]	LICENSE[160]
1-34	155-156	/	*[160]	LICENSE[160]
1-35	156-166	2305.13865	*[160]	LICENSE[160]
1-36	166-167	)	*[160]	LICENSE[160]
1-37	168-170	on	*[160]	LICENSE[160]
1-38	171-174	SST	_	_
1-39	174-175	-	_	_
1-40	175-176	2	_	_
1-41	176-177	.	_	_

#Text=Please see the example commands below for details.
2-1	178-184	Please	_	_
2-2	185-188	see	_	_
2-3	189-192	the	_	_
2-4	193-200	example	_	_
2-5	201-209	commands	_	_
2-6	210-215	below	*[138]	ONTOLOGY[138]
2-7	216-219	for	_	_
2-8	220-227	details	_	_
2-9	227-228	.	_	_

#Text=The code requires GPUs that support half-precision training.
3-1	229-232	The	_	_
3-2	233-237	code	_	_
3-3	238-246	requires	*[140]	WORKSHOP[140]
3-4	247-251	GPUs	*[140]	WORKSHOP[140]
3-5	252-256	that	*[140]	WORKSHOP[140]
3-6	257-264	support	*[140]	WORKSHOP[140]
3-7	265-279	half-precision	_	_
3-8	280-288	training	_	_
3-9	288-289	.	_	_

#Text=Please first install the following python packages. ``` torch==1.12.1 datasets evaluate accelerate tqdm ```  ## Implementation of DP fine-tuning  The transformers package in `dp\_finetuning` enables DP training.
4-1	290-296	Please	_	_
4-2	297-302	first	_	_
4-3	303-310	install	_	_
4-4	311-314	the	_	_
4-5	315-324	following	_	_
4-6	325-331	python	_	_
4-7	332-340	packages	_	_
4-8	340-341	.	_	_
4-9	342-343	`	_	_
4-10	343-344	`	_	_
4-11	344-345	`	_	_
4-12	346-351	torch	_	_
4-13	351-352	=	_	_
4-14	352-353	=	_	_
4-15	353-359	1.12.1	_	_
4-16	360-368	datasets	_	_
4-17	369-377	evaluate	_	_
4-18	378-388	accelerate	_	_
4-19	389-393	tqdm	_	_
4-20	394-395	`	_	_
4-21	395-396	`	_	_
4-22	396-397	`	_	_
4-23	399-400	#	_	_
4-24	400-401	#	_	_
4-25	402-416	Implementation	_	_
4-26	417-419	of	*[131]	PUBLICATION[131]
4-27	420-422	DP	*[131]	PUBLICATION[131]
4-28	423-434	fine-tuning	*[131]	PUBLICATION[131]
4-29	436-439	The	_	_
4-30	440-452	transformers	_	_
4-31	453-460	package	_	_
4-32	461-463	in	_	_
4-33	464-465	`	_	_
4-34	465-478	dp\_finetuning	_	_
4-35	478-479	`	_	_
4-36	480-487	enables	_	_
4-37	488-490	DP	_	_
4-38	491-499	training	_	_
4-39	499-500	.	_	_

#Text=We register Pytorch backward hooks to linear layers to enable per-example gradient computation.
5-1	502-504	We	_	_
5-2	505-513	register	_	_
5-3	514-521	Pytorch	_	_
5-4	522-530	backward	_	_
5-5	531-536	hooks	_	_
5-6	537-539	to	_	_
5-7	540-546	linear	_	_
5-8	547-553	layers	_	_
5-9	554-556	to	_	_
5-10	557-563	enable	_	_
5-11	564-575	per-example	_	_
5-12	576-584	gradient	_	_
5-13	585-596	computation	_	_
5-14	596-597	.	_	_

#Text=The implementation of hooks is in `src/transformers/models/grad\_sample\_utils.py`.
6-1	599-602	The	_	_
6-2	603-617	implementation	_	_
6-3	618-620	of	_	_
6-4	621-626	hooks	_	_
6-5	627-629	is	*[131]	DATASET[131]
6-6	630-632	in	*[131]	DATASET[131]
6-7	633-634	`	*[131]	DATASET[131]
6-8	634-637	src	*[131]	DATASET[131]
6-9	637-638	/	*[131]	DATASET[131]
6-10	638-650	transformers	*[131]	DATASET[131]
6-11	650-651	/	*[131]	DATASET[131]
6-12	651-657	models	*[131]	DATASET[131]
6-13	657-658	/	*[131]	DATASET[131]
6-14	658-678	grad\_sample\_utils.py	*[131]	DATASET[131]
6-15	678-679	`	_	_
6-16	679-680	.	_	_

#Text=The hooks are attached to the model in `src/transformers/models/gpt2.py`  ## The first stage: selective pre-training.  1.
7-1	681-684	The	_	_
7-2	685-690	hooks	_	_
7-3	691-694	are	_	_
7-4	695-703	attached	_	_
7-5	704-706	to	_	_
7-6	707-710	the	_	_
7-7	711-716	model	_	_
7-8	717-719	in	_	_
7-9	720-721	`	*[133]	PUBLICATION[133]
7-10	721-724	src	*[133]	PUBLICATION[133]
7-11	724-725	/	*[133]	PUBLICATION[133]
7-12	725-737	transformers	*[133]	PUBLICATION[133]
7-13	737-738	/	*[133]	PUBLICATION[133]
7-14	738-744	models	*[133]	PUBLICATION[133]
7-15	744-745	/	*[133]	PUBLICATION[133]
7-16	745-749	gpt2	*[133]	PUBLICATION[133]
7-17	749-750	.	*[133]	PUBLICATION[133]
7-18	750-752	py	*[133]	PUBLICATION[133]
7-19	752-753	`	*[133]	PUBLICATION[133]
7-20	755-756	#	*[133]	PUBLICATION[133]
7-21	756-757	#	*[133]	PUBLICATION[133]
7-22	758-761	The	*[133]	PUBLICATION[133]
7-23	762-767	first	*[133]	PUBLICATION[133]
7-24	768-773	stage	_	_
7-25	773-774	:	_	_
7-26	775-784	selective	_	_
7-27	785-797	pre-training	_	_
7-28	797-798	.	_	_
7-29	800-801	1	_	_
7-30	801-802	.	_	_

#Text=Run the following command to download Wikipedia + BookCorpus, and split the documents into natural sentences.
8-1	803-806	Run	_	_
8-2	807-810	the	_	_
8-3	811-820	following	_	_
8-4	821-828	command	_	_
8-5	829-831	to	_	_
8-6	832-840	download	_	_
8-7	841-850	Wikipedia	_	_
8-8	851-852	+	_	_
8-9	853-863	BookCorpus	_	_
8-10	863-864	,	_	_
8-11	865-868	and	_	_
8-12	869-874	split	_	_
8-13	875-878	the	_	_
8-14	879-888	documents	_	_
8-15	889-893	into	_	_
8-16	894-901	natural	_	_
8-17	902-911	sentences	_	_
8-18	911-912	.	_	_

#Text=The resulting dataset is saved at `data/flatten\_wiki\_book\_sentences.ds`.  ``` cd pretraining\_data\_selection python splitting\_into\_sentences.py ```  2.
9-1	913-916	The	_	_
9-2	917-926	resulting	_	_
9-3	927-934	dataset	_	_
9-4	935-937	is	_	_
9-5	938-943	saved	_	_
9-6	944-946	at	_	_
9-7	947-948	`	_	_
9-8	948-952	data	_	_
9-9	952-953	/	_	_
9-10	953-983	flatten\_wiki\_book\_sentences.ds	_	_
9-11	983-984	`	*[141]	WORKSHOP[141]
9-12	984-985	.	*[141]	WORKSHOP[141]
9-13	987-988	`	*[141]	WORKSHOP[141]
9-14	988-989	`	*[141]	WORKSHOP[141]
9-15	989-990	`	*[141]	WORKSHOP[141]
9-16	991-993	cd	_	_
9-17	994-1020	pretraining\_data\_selection	_	_
9-18	1021-1027	python	_	_
9-19	1028-1055	splitting\_into\_sentences.py	_	_
9-20	1056-1057	`	_	_
9-21	1057-1058	`	_	_
9-22	1058-1059	`	_	_
9-23	1061-1062	2	_	_
9-24	1062-1063	.	_	_

#Text=Prepare the data for training the domain classifier.
10-1	1064-1071	Prepare	_	_
10-2	1072-1075	the	_	_
10-3	1076-1080	data	_	_
10-4	1081-1084	for	_	_
10-5	1085-1093	training	_	_
10-6	1094-1097	the	_	_
10-7	1098-1104	domain	_	_
10-8	1105-1115	classifier	_	_
10-9	1115-1116	.	_	_

#Text=The results are saved at `data/sst2/filter\_train\_nonewline.json` and `data/sst2/filter\_val\_nonewline.json`.  ``` python build\_classifier\_data\_for\_sst2.py ```  3.
11-1	1117-1120	The	_	_
11-2	1121-1128	results	_	_
11-3	1129-1132	are	_	_
11-4	1133-1138	saved	_	_
11-5	1139-1141	at	_	_
11-6	1142-1143	`	_	_
11-7	1143-1147	data	_	_
11-8	1147-1148	/	_	_
11-9	1148-1152	sst2	_	_
11-10	1152-1153	/	_	_
11-11	1153-1180	filter\_train\_nonewline.json	_	_
11-12	1180-1181	`	_	_
11-13	1182-1185	and	_	_
11-14	1186-1187	`	*[157]	PROGLANG[157]
11-15	1187-1191	data	*[157]	PROGLANG[157]
11-16	1191-1192	/	*[157]	PROGLANG[157]
11-17	1192-1196	sst2	_	_
11-18	1196-1197	/	_	_
11-19	1197-1222	filter\_val\_nonewline.json	_	_
11-20	1222-1223	`	_	_
11-21	1223-1224	.	_	_
11-22	1226-1227	`	_	_
11-23	1227-1228	`	_	_
11-24	1228-1229	`	_	_
11-25	1230-1236	python	_	_
11-26	1237-1267	build\_classifier\_data\_for\_sst2	_	_
11-26	1263-1267	sst2	_	_
11-27	1267-1268	.	_	_
11-28	1268-1270	py	_	_
11-29	1271-1272	`	_	_
11-30	1272-1273	`	_	_
11-31	1273-1274	`	_	_
11-32	1276-1277	3	_	_
11-33	1277-1278	.	_	_

#Text=Train the domain classifier with DP-Adam and compute the logits of the trained classifier on all pre-training sentences.
12-1	1279-1284	Train	_	_
12-2	1285-1288	the	_	_
12-3	1289-1295	domain	_	_
12-4	1296-1306	classifier	_	_
12-5	1307-1311	with	_	_
12-6	1312-1319	DP-Adam	_	_
12-7	1320-1323	and	_	_
12-8	1324-1331	compute	_	_
12-9	1332-1335	the	_	_
12-10	1336-1342	logits	_	_
12-11	1343-1345	of	_	_
12-12	1346-1349	the	_	_
12-13	1350-1357	trained	_	_
12-14	1358-1368	classifier	_	_
12-15	1369-1371	on	_	_
12-16	1372-1375	all	_	_
12-17	1376-1388	pre-training	_	_
12-18	1389-1398	sentences	_	_
12-19	1398-1399	.	_	_

#Text=The logits are saved at `dp\_finetuning/sst2/domain\_classifier\_output`.
13-1	1400-1403	The	_	_
13-2	1404-1410	logits	_	_
13-3	1411-1414	are	_	_
13-4	1415-1420	saved	_	_
13-5	1421-1423	at	_	_
13-6	1424-1425	`	_	_
13-7	1425-1438	dp\_finetuning	_	_
13-8	1438-1439	/	_	_
13-9	1439-1443	sst2	_	_
13-10	1443-1444	/	_	_
13-11	1444-1468	domain\_classifier\_output	_	_
13-12	1468-1469	`	*[149]	PROJECT[149]
13-13	1469-1470	.	_	_

#Text=Please install the local transformers directory, in which we implement per-example gradient computation (`transformers/src/transformers/models/grad\_sample\_utils.py`), and clipping + noising (`line 709-744 in dp\_finetuning/run\_glue\_no\_trainer.py`).
14-1	1471-1477	Please	_	_
14-2	1478-1485	install	_	_
14-3	1486-1489	the	_	_
14-4	1490-1495	local	_	_
14-5	1496-1508	transformers	_	_
14-6	1509-1518	directory	_	_
14-7	1518-1519	,	_	_
14-8	1520-1522	in	_	_
14-9	1523-1528	which	_	_
14-10	1529-1531	we	_	_
14-11	1532-1541	implement	_	_
14-12	1542-1553	per-example	_	_
14-13	1554-1562	gradient	_	_
14-14	1563-1574	computation	_	_
14-15	1575-1576	(	_	_
14-16	1576-1577	`	_	_
14-17	1577-1589	transformers	_	_
14-18	1589-1590	/	_	_
14-19	1590-1593	src	*[139]	ONTOLOGY[139]
14-20	1593-1594	/	*[139]	ONTOLOGY[139]
14-21	1594-1606	transformers	*[139]	ONTOLOGY[139]
14-22	1606-1607	/	*[139]	ONTOLOGY[139]
14-23	1607-1613	models	*[139]	ONTOLOGY[139]
14-24	1613-1614	/	_	_
14-25	1614-1634	grad\_sample\_utils.py	_	_
14-26	1634-1635	`	_	_
14-27	1635-1636	)	_	_
14-28	1636-1637	,	_	_
14-29	1638-1641	and	_	_
14-30	1642-1650	clipping	_	_
14-31	1651-1652	+	_	_
14-32	1653-1660	noising	_	_
14-33	1661-1662	(	_	_
14-34	1662-1663	`	_	_
14-35	1663-1667	line	_	_
14-36	1668-1671	709	_	_
14-37	1671-1672	-	_	_
14-38	1672-1675	744	_	_
14-39	1676-1678	in	_	_
14-40	1679-1692	dp\_finetuning	_	_
14-41	1692-1693	/	_	_
14-42	1693-1715	run\_glue\_no\_trainer.py	_	_
14-43	1715-1716	`	_	_
14-44	1716-1717	)	_	_
14-45	1717-1718	.	_	_

#Text=This implementation only supports single GPU training for dp fine-tuning.  ``` cd ..
15-1	1719-1723	This	_	_
15-2	1724-1738	implementation	_	_
15-3	1739-1743	only	_	_
15-4	1744-1752	supports	_	_
15-5	1753-1759	single	_	_
15-6	1760-1763	GPU	_	_
15-7	1764-1772	training	_	_
15-8	1773-1776	for	_	_
15-9	1777-1779	dp	_	_
15-10	1780-1791	fine-tuning	_	_
15-11	1791-1792	.	*[145]	SOFTWARE[145]
15-12	1794-1795	`	*[145]	SOFTWARE[145]
15-13	1795-1796	`	*[145]	SOFTWARE[145]
15-14	1796-1797	`	_	_
15-15	1798-1800	cd	_	_
15-16	1801-1802	.	_	_
15-17	1802-1803	.	_	_

#Text=/dp\_finetuning cd transformers pip install --editable . cd .. ```   ``` bash scripts/train\_domain\_classifier.sh 1.4 1 64 32 1e-3 ```  > The arguments are: noise\_multiplier, clip, pergpu\_bs, gradient accumulation steps, and learning rate.
16-1	1803-1804	/	_	_
16-2	1804-1817	dp\_finetuning	_	_
16-3	1818-1820	cd	_	_
16-4	1821-1833	transformers	_	_
16-5	1834-1837	pip	_	_
16-6	1838-1845	install	_	_
16-7	1846-1847	-	_	_
16-8	1847-1848	-	_	_
16-9	1848-1856	editable	_	_
16-10	1857-1858	.	_	_
16-11	1859-1861	cd	_	_
16-12	1862-1863	.	_	_
16-13	1863-1864	.	_	_
16-14	1865-1866	`	_	_
16-15	1866-1867	`	_	_
16-16	1867-1868	`	_	_
16-17	1871-1872	`	_	_
16-18	1872-1873	`	_	_
16-19	1873-1874	`	_	_
16-20	1875-1879	bash	_	_
16-21	1880-1887	scripts	_	_
16-22	1887-1888	/	_	_
16-23	1888-1914	train\_domain\_classifier.sh	_	_
16-24	1915-1918	1.4	_	_
16-25	1919-1920	1	_	_
16-26	1921-1923	64	_	_
16-27	1924-1926	32	_	_
16-28	1927-1929	1e	_	_
16-29	1929-1930	-	_	_
16-30	1930-1931	3	_	_
16-31	1932-1933	`	_	_
16-32	1933-1934	`	_	_
16-33	1934-1935	`	_	_
16-34	1937-1938	>	_	_
16-35	1939-1942	The	_	_
16-36	1943-1952	arguments	_	_
16-37	1953-1956	are	_	_
16-38	1956-1957	:	_	_
16-39	1958-1974	noise\_multiplier	_	_
16-40	1974-1975	,	_	_
16-41	1976-1980	clip	_	_
16-42	1980-1981	,	_	_
16-43	1982-1991	pergpu\_bs	_	_
16-44	1991-1992	,	_	_
16-45	1993-2001	gradient	_	_
16-46	2002-2014	accumulation	_	_
16-47	2015-2020	steps	*[150]	PROJECT[150]
16-48	2020-2021	,	_	_
16-49	2022-2025	and	_	_
16-50	2026-2034	learning	_	_
16-51	2035-2039	rate	_	_
16-52	2039-2040	.	_	_

#Text=We compute the logits in the function 'filter\_pretraining\_data' in 'run\_glue\_no\_trainer.py'.
17-1	2041-2043	We	_	_
17-2	2044-2051	compute	_	_
17-3	2052-2055	the	_	_
17-4	2056-2062	logits	_	_
17-5	2063-2065	in	_	_
17-6	2066-2069	the	_	_
17-7	2070-2078	function	_	_
17-8	2079-2080	'	_	_
17-9	2080-2103	filter\_pretraining\_data	_	_
17-10	2103-2104	'	_	_
17-11	2105-2107	in	_	_
17-12	2108-2109	'	_	_
17-13	2109-2131	run\_glue\_no\_trainer.py	_	_
17-14	2131-2132	'	_	_
17-15	2132-2133	.	_	_

#Text=In this implementation, only 1 GPU is used for training the domain classifier and computing the logits.   4.
18-1	2134-2136	In	_	_
18-2	2137-2141	this	_	_
18-3	2142-2156	implementation	_	_
18-4	2156-2157	,	_	_
18-5	2158-2162	only	_	_
18-6	2163-2164	1	_	_
18-7	2165-2168	GPU	_	_
18-8	2169-2171	is	_	_
18-9	2172-2176	used	_	_
18-10	2177-2180	for	_	_
18-11	2181-2189	training	*[142]	EVALMETRIC[142]
18-12	2190-2193	the	_	_
18-13	2194-2200	domain	_	_
18-14	2201-2211	classifier	_	_
18-15	2212-2215	and	_	_
18-16	2216-2225	computing	_	_
18-17	2226-2229	the	_	_
18-18	2230-2236	logits	_	_
18-19	2236-2237	.	_	_
18-20	2240-2241	4	_	_
18-21	2241-2242	.	_	_

#Text=Use the logits to select pre-training sentences.
19-1	2243-2246	Use	_	_
19-2	2247-2250	the	_	_
19-3	2251-2257	logits	_	_
19-4	2258-2260	to	_	_
19-5	2261-2267	select	_	_
19-6	2268-2280	pre-training	_	_
19-7	2281-2290	sentences	_	_
19-8	2290-2291	.	_	_

#Text=The argument is target number of pre-training tokens (in M).
20-1	2292-2295	The	_	_
20-2	2296-2304	argument	_	_
20-3	2305-2307	is	_	_
20-4	2308-2314	target	_	_
20-5	2315-2321	number	_	_
20-6	2322-2324	of	_	_
20-7	2325-2337	pre-training	_	_
20-8	2338-2344	tokens	_	_
20-9	2345-2346	(	_	_
20-10	2346-2348	in	_	_
20-11	2349-2350	M	_	_
20-12	2350-2351	)	_	_
20-13	2351-2352	.	_	_

#Text=\*\*In this simple implementation, we directly select natural sentences instead of fixed-length sequences.\*\* We found that this simple implementation is enough to achieve good performance on SST-2.   ``` cd ..
21-1	2353-2354	\*	_	_
21-2	2354-2355	\*	_	_
21-3	2355-2357	In	_	_
21-4	2358-2362	this	_	_
21-5	2363-2369	simple	_	_
21-6	2370-2384	implementation	_	_
21-7	2384-2385	,	_	_
21-8	2386-2388	we	_	_
21-9	2389-2397	directly	_	_
21-10	2398-2404	select	_	_
21-11	2405-2412	natural	*[158]	PROGLANG[158]
21-12	2413-2422	sentences	*[158]	PROGLANG[158]
21-13	2423-2430	instead	*[158]	PROGLANG[158]
21-14	2431-2433	of	*[158]	PROGLANG[158]
21-15	2434-2446	fixed-length	*[158]	PROGLANG[158]
21-16	2447-2456	sequences	*[158]	PROGLANG[158]
21-17	2456-2457	.	*[158]	PROGLANG[158]
21-18	2457-2458	\*	*[158]	PROGLANG[158]
21-19	2458-2459	\*	*[158]	PROGLANG[158]
21-20	2460-2462	We	*[158]	PROGLANG[158]
21-21	2463-2468	found	*[158]	PROGLANG[158]
21-22	2469-2473	that	*[158]	PROGLANG[158]
21-23	2474-2478	this	*[158]	PROGLANG[158]
21-24	2479-2485	simple	*[158]	PROGLANG[158]
21-25	2486-2500	implementation	*[158]	PROGLANG[158]
21-26	2501-2503	is	*[158]	PROGLANG[158]
21-27	2504-2510	enough	*[158]	PROGLANG[158]
21-28	2511-2513	to	*[158]	PROGLANG[158]
21-29	2514-2521	achieve	*[158]	PROGLANG[158]
21-30	2522-2526	good	*[158]	PROGLANG[158]
21-31	2527-2538	performance	*[158]	PROGLANG[158]
21-32	2539-2541	on	*[158]	PROGLANG[158]
21-33	2542-2545	SST	*[158]	PROGLANG[158]
21-34	2545-2546	-	*[158]	PROGLANG[158]
21-35	2546-2547	2	_	_
21-36	2547-2548	.	_	_
21-37	2551-2552	`	_	_
21-38	2552-2553	`	_	_
21-39	2553-2554	`	_	_
21-40	2555-2557	cd	_	_
21-41	2558-2559	.	_	_
21-42	2559-2560	.	_	_

#Text=/pretraining\_data\_selection python sampling\_with\_logits.py --num\_tokens 40 ```  > You can also select random sentences.  ``` python sampling\_with\_logits.py --num\_tokens 40 --random ```  5.
22-1	2560-2561	/	_	_
22-2	2561-2587	pretraining\_data\_selection	_	_
22-3	2588-2594	python	_	_
22-4	2595-2618	sampling\_with\_logits.py	_	_
22-5	2619-2620	-	*[146]	SOFTWARE[146]
22-6	2620-2621	-	*[146]	SOFTWARE[146]
22-7	2621-2631	num\_tokens	*[146]	SOFTWARE[146]
22-8	2632-2634	40	*[146]	SOFTWARE[146]
22-9	2635-2636	`	*[146]	SOFTWARE[146]
22-10	2636-2637	`	*[146]	SOFTWARE[146]
22-11	2637-2638	`	*[146]	SOFTWARE[146]
22-12	2640-2641	>	*[146]	SOFTWARE[146]
22-13	2642-2645	You	*[146]	SOFTWARE[146]
22-14	2646-2649	can	*[146]	SOFTWARE[146]
22-15	2650-2654	also	_	_
22-16	2655-2661	select	_	_
22-17	2662-2668	random	_	_
22-18	2669-2678	sentences	_	_
22-19	2678-2679	.	_	_
22-20	2681-2682	`	_	_
22-21	2682-2683	`	_	_
22-22	2683-2684	`	_	_
22-23	2685-2691	python	_	_
22-24	2692-2715	sampling\_with\_logits.py	_	_
22-25	2716-2717	-	_	_
22-26	2717-2718	-	_	_
22-27	2718-2728	num\_tokens	_	_
22-28	2729-2731	40	_	_
22-29	2732-2733	-	_	_
22-30	2733-2734	-	_	_
22-31	2734-2740	random	_	_
22-32	2741-2742	`	_	_
22-33	2742-2743	`	_	_
22-34	2743-2744	`	_	_
22-35	2746-2747	5	_	_
22-36	2747-2748	.	_	_

#Text=Pre-training on selected data.
23-1	2749-2761	Pre-training	_	_
23-2	2762-2764	on	*[142]	WORKSHOP[142]
23-3	2765-2773	selected	_	_
23-4	2774-2778	data	_	_
23-5	2778-2779	.	_	_

#Text=Install standard transformers package, i.e., without pre-example gradients computation.
24-1	2780-2787	Install	_	_
24-2	2788-2796	standard	_	_
24-3	2797-2809	transformers	_	_
24-4	2810-2817	package	_	_
24-5	2817-2818	,	_	_
24-6	2819-2822	i.e	_	_
24-7	2822-2823	.	_	_
24-8	2823-2824	,	_	_
24-9	2825-2832	without	_	_
24-10	2833-2844	pre-example	_	_
24-11	2845-2854	gradients	_	_
24-12	2855-2866	computation	_	_
24-13	2866-2867	.	_	_

#Text=This also enables pre-training with multi gpus.  ``` cd ..
25-1	2868-2872	This	_	_
25-2	2873-2877	also	_	_
25-3	2878-2885	enables	_	_
25-4	2886-2898	pre-training	_	_
25-5	2899-2903	with	_	_
25-6	2904-2909	multi	_	_
25-7	2910-2914	gpus	_	_
25-8	2914-2915	.	*[148]	SOFTWARE[148]
25-9	2917-2918	`	*[148]	SOFTWARE[148]
25-10	2918-2919	`	*[148]	SOFTWARE[148]
25-11	2919-2920	`	_	_
25-12	2921-2923	cd	_	_
25-13	2924-2925	.	_	_
25-14	2925-2926	.	_	_

#Text=/pretraining cd transformers pip install --editable . cd .. ```    ``` bash scripts/pretraining.sh pretraining\_data\_40m.ds tiny 3e-4 1000000 32 8 1 ```  > The arguments are: pre-training data path, model size (tiny=5M), lr, pre-training steps, per-gpu-bs, num\_gpus, gradient accumulation
26-1	2926-2927	/	_	_
26-2	2927-2938	pretraining	_	_
26-3	2939-2941	cd	_	_
26-4	2942-2954	transformers	_	_
26-5	2955-2958	pip	*[131]	CONFERENCE[131]
26-6	2959-2966	install	*[131]	CONFERENCE[131]
26-7	2967-2968	-	*[131]	CONFERENCE[131]
26-8	2968-2969	-	*[131]	CONFERENCE[131]
26-9	2969-2977	editable	*[131]	CONFERENCE[131]
26-10	2978-2979	.	*[131]	CONFERENCE[131]
26-11	2980-2982	cd	*[131]	CONFERENCE[131]
26-12	2983-2984	.	*[131]	CONFERENCE[131]
26-13	2984-2985	.	*[131]	CONFERENCE[131]
26-14	2986-2987	`	*[131]	CONFERENCE[131]
26-15	2987-2988	`	*[131]	CONFERENCE[131]
26-16	2988-2989	`	*[131]	CONFERENCE[131]
26-17	2993-2994	`	*[131]	CONFERENCE[131]
26-18	2994-2995	`	*[131]	CONFERENCE[131]
26-19	2995-2996	`	*[131]	CONFERENCE[131]
26-20	2997-3001	bash	*[131]	CONFERENCE[131]
26-21	3002-3009	scripts	*[131]	CONFERENCE[131]
26-22	3009-3010	/	*[131]	CONFERENCE[131]
26-23	3010-3024	pretraining.sh	*[131]	CONFERENCE[131]
26-24	3025-3041	pretraining\_data	*[131]	CONFERENCE[131]
26-25	3041-3042	\_	*[131]	CONFERENCE[131]
26-26	3042-3048	40m.ds	*[131]	CONFERENCE[131]
26-27	3049-3053	tiny	*[131]	CONFERENCE[131]
26-28	3054-3056	3e	*[131]	CONFERENCE[131]
26-29	3056-3057	-	*[131]	CONFERENCE[131]
26-30	3057-3058	4	*[131]	CONFERENCE[131]
26-31	3059-3066	1000000	*[131]	CONFERENCE[131]
26-32	3067-3069	32	*[131]	CONFERENCE[131]
26-33	3070-3071	8	*[131]	CONFERENCE[131]
26-34	3072-3073	1	*[131]	CONFERENCE[131]
26-35	3074-3075	`	*[131]	CONFERENCE[131]
26-36	3075-3076	`	*[131]	CONFERENCE[131]
26-37	3076-3077	`	*[131]	CONFERENCE[131]
26-38	3079-3080	>	*[131]	CONFERENCE[131]
26-39	3081-3084	The	*[131]	CONFERENCE[131]
26-40	3085-3094	arguments	*[131]	CONFERENCE[131]
26-41	3095-3098	are	*[131]	CONFERENCE[131]
26-42	3098-3099	:	*[131]	CONFERENCE[131]
26-43	3100-3112	pre-training	_	_
26-44	3113-3117	data	_	_
26-45	3118-3122	path	_	_
26-46	3122-3123	,	_	_
26-47	3124-3129	model	_	_
26-48	3130-3134	size	_	_
26-49	3135-3136	(	_	_
26-50	3136-3140	tiny	_	_
26-51	3140-3141	=	_	_
26-52	3141-3143	5M	_	_
26-53	3143-3144	)	_	_
26-54	3144-3145	,	_	_
26-55	3146-3148	lr	_	_
26-56	3148-3149	,	_	_
26-57	3150-3162	pre-training	_	_
26-58	3163-3168	steps	_	_
26-59	3168-3169	,	_	_
26-60	3170-3180	per-gpu-bs	_	_
26-61	3180-3181	,	_	_
26-62	3182-3190	num\_gpus	_	_
26-63	3190-3191	,	_	_
26-64	3192-3200	gradient	_	_
26-65	3201-3213	accumulation	_	_

#Text=.
27-1	3213-3214	.	_	_

#Text=> You can also pre-train on random data.  ``` bash scripts/pretraining.sh pretraining\_data\_random\_40m.ds tiny 3e-4 1000000 32 8 1 ```  ## Finally, the second stage: private fine-tuning.  6.
28-1	3216-3217	>	_	_
28-2	3218-3221	You	_	_
28-3	3222-3225	can	_	_
28-4	3226-3230	also	_	_
28-5	3231-3240	pre-train	_	_
28-6	3241-3243	on	_	_
28-7	3244-3250	random	_	_
28-8	3251-3255	data	_	_
28-9	3255-3256	.	_	_
28-10	3258-3259	`	_	_
28-11	3259-3260	`	_	_
28-12	3260-3261	`	_	_
28-13	3262-3266	bash	_	_
28-14	3267-3274	scripts	_	_
28-15	3274-3275	/	_	_
28-16	3275-3289	pretraining.sh	_	_
28-17	3290-3313	pretraining\_data\_random	_	_
28-18	3313-3314	\_	_	_
28-19	3314-3320	40m.ds	_	_
28-20	3321-3325	tiny	_	_
28-21	3326-3328	3e	_	_
28-22	3328-3329	-	_	_
28-23	3329-3330	4	_	_
28-24	3331-3338	1000000	_	_
28-25	3339-3341	32	*[140]	ONTOLOGY[140]
28-26	3342-3343	8	*[140]	ONTOLOGY[140]
28-27	3344-3345	1	*[140]	ONTOLOGY[140]
28-28	3346-3347	`	*[140]	ONTOLOGY[140]
28-29	3347-3348	`	*[140]	ONTOLOGY[140]
28-30	3348-3349	`	*[140]	ONTOLOGY[140]
28-31	3351-3352	#	*[140]	ONTOLOGY[140]
28-32	3352-3353	#	*[140]	ONTOLOGY[140]
28-33	3354-3361	Finally	*[140]	ONTOLOGY[140]
28-34	3361-3362	,	*[140]	ONTOLOGY[140]
28-35	3363-3366	the	*[140]	ONTOLOGY[140]
28-36	3367-3373	second	*[140]	ONTOLOGY[140]
28-37	3374-3379	stage	_	_
28-38	3379-3380	:	_	_
28-39	3381-3388	private	_	_
28-40	3389-3400	fine-tuning	_	_
28-41	3400-3401	.	_	_
28-42	3403-3404	6	_	_
28-43	3404-3405	.	_	_

#Text=Private fine-tuning on sst-2.
29-1	3406-3413	Private	_	_
29-2	3414-3425	fine-tuning	_	_
29-3	3426-3428	on	_	_
29-4	3429-3432	sst	_	_
29-5	3432-3433	-	_	_
29-6	3433-3434	2	_	_
29-7	3434-3435	.	_	_

#Text=Don't forget installing dp enabled transformers package.  ``` cd ..
30-1	3436-3441	Don't	_	_
30-2	3442-3448	forget	_	_
30-3	3449-3459	installing	_	_
30-4	3460-3462	dp	_	_
30-5	3463-3470	enabled	_	_
30-6	3471-3483	transformers	_	_
30-7	3484-3491	package	*[132]	CONFERENCE[132]
30-8	3491-3492	.	*[132]	CONFERENCE[132]
30-9	3494-3495	`	*[132]	CONFERENCE[132]
30-10	3495-3496	`	*[132]	CONFERENCE[132]
30-11	3496-3497	`	*[132]	CONFERENCE[132]
30-12	3498-3500	cd	_	_
30-13	3501-3502	.	_	_
30-14	3502-3503	.	_	_

#Text=/dp\_finetuning cd transformers pip install --editable . cd .. ```  > The argumentments are: pre-trained model path, noise\_multiplier, clip, pergpu\_bs, gradient accumulation, lr, epochs, seed.
31-1	3503-3504	/	_	_
31-2	3504-3517	dp\_finetuning	_	_
31-3	3518-3520	cd	_	_
31-4	3521-3533	transformers	_	_
31-5	3534-3537	pip	_	_
31-6	3538-3545	install	_	_
31-7	3546-3547	-	_	_
31-8	3547-3548	-	_	_
31-9	3548-3556	editable	_	_
31-10	3557-3558	.	_	_
31-11	3559-3561	cd	_	_
31-12	3562-3563	.	_	_
31-13	3563-3564	.	_	_
31-14	3565-3566	`	_	_
31-15	3566-3567	`	_	_
31-16	3567-3568	`	_	_
31-17	3570-3571	>	_	_
31-18	3572-3575	The	_	_
31-19	3576-3589	argumentments	_	_
31-20	3590-3593	are	_	_
31-21	3593-3594	:	_	_
31-22	3595-3606	pre-trained	_	_
31-23	3607-3612	model	*[144]	EVALMETRIC[144]
31-24	3613-3617	path	*[144]	EVALMETRIC[144]
31-25	3617-3618	,	*[144]	EVALMETRIC[144]
31-26	3619-3635	noise\_multiplier	_	_
31-27	3635-3636	,	_	_
31-28	3637-3641	clip	_	_
31-29	3641-3642	,	_	_
31-30	3643-3652	pergpu\_bs	_	_
31-31	3652-3653	,	_	_
31-32	3654-3662	gradient	_	_
31-33	3663-3675	accumulation	_	_
31-34	3675-3676	,	_	_
31-35	3677-3679	lr	_	_
31-36	3679-3680	,	_	_
31-37	3681-3687	epochs	_	_
31-38	3687-3688	,	_	_
31-39	3689-3693	seed	_	_
31-40	3693-3694	.	_	_

#Text=Replace checkpoint-XXXX with your checkpoint.  ``` bash scripts/train\_sst2.sh ..
32-1	3695-3702	Replace	_	_
32-2	3703-3718	checkpoint-XXXX	_	_
32-3	3719-3723	with	_	_
32-4	3724-3728	your	_	_
32-5	3729-3739	checkpoint	_	_
32-6	3739-3740	.	_	_
32-7	3742-3743	`	_	_
32-8	3743-3744	`	_	_
32-9	3744-3745	`	_	_
32-10	3746-3750	bash	_	_
32-11	3751-3758	scripts	_	_
32-12	3758-3759	/	_	_
32-13	3759-3769	train\_sst2	_	_
32-14	3769-3770	.	_	_
32-15	3770-3772	sh	_	_
32-16	3773-3774	.	_	_
32-17	3774-3775	.	_	_

#Text=/pretraining/results/pretraining\_data\_random\_40m.ds\_lr3e-4\_maxsteps1000000\_tiny/checkpoint-XXXX 1.4 1 32 64 1e-3 30 0 ```
33-1	3775-3776	/	_	_
33-2	3776-3787	pretraining	_	_
33-3	3787-3788	/	_	_
33-4	3788-3795	results	_	_
33-5	3795-3796	/	_	_
33-6	3796-3819	pretraining\_data\_random	_	_
33-7	3819-3820	\_	_	_
33-8	3820-3831	40m.ds\_lr3e	_	_
33-9	3831-3832	-	_	_
33-10	3832-3833	4	_	_
33-11	3833-3834	\_	_	_
33-12	3834-3849	maxsteps1000000	_	_
33-13	3849-3850	\_	_	_
33-14	3850-3854	tiny	_	_
33-15	3854-3855	/	_	_
33-16	3855-3870	checkpoint-XXXX	_	_
33-17	3871-3874	1.4	*[159]	PROGLANG[159]
33-18	3875-3876	1	*[159]	PROGLANG[159]
33-19	3877-3879	32	*[159]	PROGLANG[159]
33-20	3880-3882	64	*[159]	PROGLANG[159]
33-21	3883-3885	1e	*[159]	PROGLANG[159]
33-22	3885-3886	-	*[159]	PROGLANG[159]
33-23	3886-3887	3	*[159]	PROGLANG[159]
33-24	3888-3890	30	*[159]	PROGLANG[159]
33-25	3891-3892	0	*[159]	PROGLANG[159]
33-26	3893-3894	`	*[159]	PROGLANG[159]
33-27	3894-3895	`	*[159]	PROGLANG[159]
33-28	3895-3896	`	_	_