#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# \[PAV-SOD: A New Task Towards Panoramic Audiovisual Saliency Detection (TOMM 2022)\](https://drive.google.com/file/d/1-1RcARcbz4pACFzkjXcp6MP8R9CGScqI/view?
1-1	0-1	#	_	_
1-2	2-3	\[	_	_
1-3	3-10	PAV-SOD	*[4]	PUBLICATION[4]
1-4	10-11	:	*[4]	PUBLICATION[4]
1-5	12-13	A	*[4]	PUBLICATION[4]
1-6	14-17	New	*[4]	PUBLICATION[4]
1-7	18-22	Task	*[4]	PUBLICATION[4]
1-8	23-30	Towards	*[4]	PUBLICATION[4]
1-9	31-40	Panoramic	*[4]	PUBLICATION[4]
1-10	41-52	Audiovisual	*[4]	PUBLICATION[4]
1-11	53-61	Saliency	*[4]	PUBLICATION[4]
1-12	62-71	Detection	_	_
1-13	72-73	(	_	_
1-14	73-77	TOMM	*[5]	PUBLICATION[5]
1-15	78-82	2022	*[5]	PUBLICATION[5]
1-16	82-83	)	_	_
1-17	83-84	\]	_	_
1-18	84-85	(	_	_
1-19	85-90	https	_	_
1-20	90-91	:	_	_
1-21	91-92	/	_	_
1-22	92-93	/	_	_
1-23	93-109	drive.google.com	_	_
1-24	109-110	/	_	_
1-25	110-114	file	_	_
1-26	114-115	/	_	_
1-27	115-116	d	_	_
1-28	116-117	/	_	_
1-29	117-118	1	_	_
1-30	118-119	-	_	_
1-31	119-150	1RcARcbz4pACFzkjXcp6MP8R9CGScqI	_	_
1-32	150-151	/	_	_
1-33	151-155	view	_	_
1-34	155-156	?	_	_

#Text=usp=sharing)  Object-level audiovisual saliency detection in 360° panoramic real-life dynamic scenes is important for exploring and modeling human perception in immersive environments, also for aiding the development of virtual, augmented and mixed reality applications in the fields of such as education, social network, entertainment and training.
2-1	156-159	usp	_	_
2-2	159-160	=	_	_
2-3	160-167	sharing	_	_
2-4	167-168	)	_	_
2-5	170-182	Object-level	_	_
2-6	183-194	audiovisual	_	_
2-7	195-203	saliency	_	_
2-8	204-213	detection	_	_
2-9	214-216	in	_	_
2-10	217-220	360	_	_
2-11	220-221	°	_	_
2-12	222-231	panoramic	_	_
2-13	232-241	real-life	_	_
2-14	242-249	dynamic	_	_
2-15	250-256	scenes	_	_
2-16	257-259	is	_	_
2-17	260-269	important	_	_
2-18	270-273	for	_	_
2-19	274-283	exploring	_	_
2-20	284-287	and	_	_
2-21	288-296	modeling	_	_
2-22	297-302	human	_	_
2-23	303-313	perception	_	_
2-24	314-316	in	_	_
2-25	317-326	immersive	_	_
2-26	327-339	environments	_	_
2-27	339-340	,	_	_
2-28	341-345	also	_	_
2-29	346-349	for	_	_
2-30	350-356	aiding	_	_
2-31	357-360	the	_	_
2-32	361-372	development	_	_
2-33	373-375	of	_	_
2-34	376-383	virtual	_	_
2-35	383-384	,	_	_
2-36	385-394	augmented	_	_
2-37	395-398	and	_	_
2-38	399-404	mixed	_	_
2-39	405-412	reality	_	_
2-40	413-425	applications	_	_
2-41	426-428	in	_	_
2-42	429-432	the	_	_
2-43	433-439	fields	_	_
2-44	440-442	of	_	_
2-45	443-447	such	_	_
2-46	448-450	as	_	_
2-47	451-460	education	_	_
2-48	460-461	,	_	_
2-49	462-468	social	_	_
2-50	469-476	network	_	_
2-51	476-477	,	_	_
2-52	478-491	entertainment	_	_
2-53	492-495	and	_	_
2-54	496-504	training	_	_
2-55	504-505	.	_	_

#Text=To this end, we propose a new task, panoramic audiovisual salient object detection (PAV-SOD), which aims to segment the objects grasping most of the human attention in 360° panoramic videos reflecting real-life daily scenes.
3-1	506-508	To	_	_
3-2	509-513	this	_	_
3-3	514-517	end	_	_
3-4	517-518	,	_	_
3-5	519-521	we	_	_
3-6	522-529	propose	_	_
3-7	530-531	a	_	_
3-8	532-535	new	_	_
3-9	536-540	task	_	_
3-10	540-541	,	_	_
3-11	542-551	panoramic	_	_
3-12	552-563	audiovisual	_	_
3-13	564-571	salient	_	_
3-14	572-578	object	_	_
3-15	579-588	detection	_	_
3-16	589-590	(	_	_
3-17	590-597	PAV-SOD	_	_
3-18	597-598	)	_	_
3-19	598-599	,	_	_
3-20	600-605	which	_	_
3-21	606-610	aims	_	_
3-22	611-613	to	_	_
3-23	614-621	segment	_	_
3-24	622-625	the	_	_
3-25	626-633	objects	_	_
3-26	634-642	grasping	_	_
3-27	643-647	most	_	_
3-28	648-650	of	_	_
3-29	651-654	the	_	_
3-30	655-660	human	_	_
3-31	661-670	attention	_	_
3-32	671-673	in	_	_
3-33	674-677	360	_	_
3-34	677-678	°	_	_
3-35	679-688	panoramic	_	_
3-36	689-695	videos	_	_
3-37	696-706	reflecting	_	_
3-38	707-716	real-life	_	_
3-39	717-722	daily	_	_
3-40	723-729	scenes	_	_
3-41	729-730	.	_	_

#Text=To support the task, we collect PAVS10K, the first panoramic video dataset for audiovisual salient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting PAV-SOD, and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks.
4-1	731-733	To	_	_
4-2	734-741	support	_	_
4-3	742-745	the	_	_
4-4	746-750	task	_	_
4-5	750-751	,	_	_
4-6	752-754	we	_	_
4-7	755-762	collect	_	_
4-8	763-770	PAVS10K	_	_
4-9	770-771	,	_	_
4-10	772-775	the	_	_
4-11	776-781	first	_	_
4-12	782-791	panoramic	_	_
4-13	792-797	video	_	_
4-14	798-805	dataset	_	_
4-15	806-809	for	_	_
4-16	810-821	audiovisual	_	_
4-17	822-829	salient	_	_
4-18	830-836	object	_	_
4-19	837-846	detection	_	_
4-20	846-847	,	_	_
4-21	848-853	which	_	_
4-22	854-862	consists	_	_
4-23	863-865	of	_	_
4-24	866-868	67	_	_
4-25	869-882	4K-resolution	_	_
4-26	883-898	equirectangular	_	_
4-27	899-905	videos	_	_
4-28	906-910	with	_	_
4-29	911-920	per-video	_	_
4-30	921-927	labels	_	_
4-31	928-937	including	_	_
4-32	938-950	hierarchical	_	_
4-33	951-956	scene	_	_
4-34	957-967	categories	_	_
4-35	968-971	and	_	_
4-36	972-982	associated	_	_
4-37	983-993	attributes	_	_
4-38	994-1003	depicting	_	_
4-39	1004-1012	specific	_	_
4-40	1013-1023	challenges	_	_
4-41	1024-1027	for	_	_
4-42	1028-1038	conducting	_	_
4-43	1039-1046	PAV-SOD	_	_
4-44	1046-1047	,	_	_
4-45	1048-1051	and	_	_
4-46	1052-1058	10,465	_	_
4-47	1059-1068	uniformly	_	_
4-48	1069-1076	sampled	_	_
4-49	1077-1082	video	_	_
4-50	1083-1089	frames	_	_
4-51	1090-1094	with	_	_
4-52	1095-1103	manually	_	_
4-53	1104-1113	annotated	_	_
4-54	1114-1126	object-level	_	_
4-55	1127-1130	and	_	_
4-56	1131-1145	instance-level	_	_
4-57	1146-1156	pixel-wise	_	_
4-58	1157-1162	masks	_	_
4-59	1162-1163	.	_	_

#Text=The coarse-to-fine annotations enable multi-perspective analysis regarding PAV-SOD modeling.
5-1	1164-1167	The	_	_
5-2	1168-1182	coarse-to-fine	_	_
5-3	1183-1194	annotations	_	_
5-4	1195-1201	enable	_	_
5-5	1202-1219	multi-perspective	_	_
5-6	1220-1228	analysis	_	_
5-7	1229-1238	regarding	_	_
5-8	1239-1246	PAV-SOD	_	_
5-9	1247-1255	modeling	_	_
5-10	1255-1256	.	_	_

#Text=We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our PAVS10K.
6-1	1257-1259	We	_	_
6-2	1260-1267	further	_	_
6-3	1268-1282	systematically	_	_
6-4	1283-1292	benchmark	_	_
6-5	1293-1295	13	_	_
6-6	1296-1312	state-of-the-art	_	_
6-7	1313-1320	salient	_	_
6-8	1321-1327	object	_	_
6-9	1328-1337	detection	_	_
6-10	1338-1339	(	_	_
6-11	1339-1342	SOD	_	_
6-12	1342-1343	)	_	_
6-13	1343-1344	/	_	_
6-14	1344-1349	video	_	_
6-15	1350-1356	object	_	_
6-16	1357-1369	segmentation	_	_
6-17	1370-1371	(	_	_
6-18	1371-1374	VOS	_	_
6-19	1374-1375	)	_	_
6-20	1376-1383	methods	_	_
6-21	1384-1389	based	_	_
6-22	1390-1392	on	_	_
6-23	1393-1396	our	_	_
6-24	1397-1404	PAVS10K	_	_
6-25	1404-1405	.	_	_

#Text=Besides, we propose a new baseline network, which takes advantage of both visual and audio cues of 360° video frames by using a new conditional variational auto-encoder (CVAE).
7-1	1406-1413	Besides	_	_
7-2	1413-1414	,	_	_
7-3	1415-1417	we	_	_
7-4	1418-1425	propose	_	_
7-5	1426-1427	a	_	_
7-6	1428-1431	new	_	_
7-7	1432-1440	baseline	_	_
7-8	1441-1448	network	_	_
7-9	1448-1449	,	_	_
7-10	1450-1455	which	_	_
7-11	1456-1461	takes	_	_
7-12	1462-1471	advantage	_	_
7-13	1472-1474	of	_	_
7-14	1475-1479	both	_	_
7-15	1480-1486	visual	_	_
7-16	1487-1490	and	_	_
7-17	1491-1496	audio	_	_
7-18	1497-1501	cues	_	_
7-19	1502-1504	of	_	_
7-20	1505-1508	360	_	_
7-21	1508-1509	°	_	_
7-22	1510-1515	video	_	_
7-23	1516-1522	frames	_	_
7-24	1523-1525	by	_	_
7-25	1526-1531	using	_	_
7-26	1532-1533	a	_	_
7-27	1534-1537	new	_	_
7-28	1538-1549	conditional	_	_
7-29	1550-1561	variational	_	_
7-30	1562-1574	auto-encoder	_	_
7-31	1575-1576	(	_	_
7-32	1576-1580	CVAE	_	_
7-33	1580-1581	)	_	_
7-34	1581-1582	.	_	_

#Text=Our CVAE-based audiovisual network, namely CAV-Net, consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network and audiovisual distribution estimation modules.
8-1	1583-1586	Our	_	_
8-2	1587-1597	CVAE-based	_	_
8-3	1598-1609	audiovisual	_	_
8-4	1610-1617	network	_	_
8-5	1617-1618	,	_	_
8-6	1619-1625	namely	_	_
8-7	1626-1633	CAV-Net	_	_
8-8	1633-1634	,	_	_
8-9	1635-1643	consists	_	_
8-10	1644-1646	of	_	_
8-11	1647-1648	a	_	_
8-12	1649-1665	spatial-temporal	_	_
8-13	1666-1672	visual	_	_
8-14	1673-1685	segmentation	_	_
8-15	1686-1693	network	_	_
8-16	1693-1694	,	_	_
8-17	1695-1696	a	_	_
8-18	1697-1710	convolutional	_	_
8-19	1711-1725	audio-encoding	_	_
8-20	1726-1733	network	_	_
8-21	1734-1737	and	_	_
8-22	1738-1749	audiovisual	_	_
8-23	1750-1762	distribution	_	_
8-24	1763-1773	estimation	_	_
8-25	1774-1781	modules	_	_
8-26	1781-1782	.	_	_

#Text=As a result, our CAV-Net outperforms all competing models and is able to estimate the aleatoric uncertainties within PAVS10K.
9-1	1783-1785	As	_	_
9-2	1786-1787	a	_	_
9-3	1788-1794	result	_	_
9-4	1794-1795	,	_	_
9-5	1796-1799	our	_	_
9-6	1800-1807	CAV-Net	_	_
9-7	1808-1819	outperforms	_	_
9-8	1820-1823	all	_	_
9-9	1824-1833	competing	_	_
9-10	1834-1840	models	_	_
9-11	1841-1844	and	_	_
9-12	1845-1847	is	_	_
9-13	1848-1852	able	_	_
9-14	1853-1855	to	_	_
9-15	1856-1864	estimate	_	_
9-16	1865-1868	the	_	_
9-17	1869-1878	aleatoric	_	_
9-18	1879-1892	uncertainties	_	_
9-19	1893-1899	within	_	_
9-20	1900-1907	PAVS10K	_	_
9-21	1907-1908	.	_	_

#Text=With extensive experimental results, we gain several findings about PAV-SOD challenges and insights towards PAV-SOD model interpretability.
10-1	1909-1913	With	_	_
10-2	1914-1923	extensive	_	_
10-3	1924-1936	experimental	_	_
10-4	1937-1944	results	_	_
10-5	1944-1945	,	_	_
10-6	1946-1948	we	_	_
10-7	1949-1953	gain	_	_
10-8	1954-1961	several	_	_
10-9	1962-1970	findings	_	_
10-10	1971-1976	about	_	_
10-11	1977-1984	PAV-SOD	_	_
10-12	1985-1995	challenges	_	_
10-13	1996-1999	and	_	_
10-14	2000-2008	insights	_	_
10-15	2009-2016	towards	_	_
10-16	2017-2024	PAV-SOD	_	_
10-17	2025-2030	model	_	_
10-17	2025-2025		_	_
10-18	2031-2047	interpretability	_	_
10-19	2047-2048	.	_	_

#Text=We hope that our work could serve as a starting point for advancing SOD towards immersive media.  ------  # PAVS10K  <p align="center">     <img src=".
11-1	2049-2051	We	_	_
11-2	2052-2056	hope	_	_
11-3	2057-2061	that	_	_
11-4	2062-2065	our	_	_
11-5	2066-2070	work	_	_
11-6	2071-2076	could	_	_
11-7	2077-2082	serve	_	_
11-8	2083-2085	as	_	_
11-9	2086-2087	a	_	_
11-10	2088-2096	starting	_	_
11-11	2097-2102	point	_	_
11-12	2103-2106	for	_	_
11-13	2107-2116	advancing	_	_
11-14	2117-2120	SOD	_	_
11-15	2121-2128	towards	_	_
11-16	2129-2138	immersive	_	_
11-17	2139-2144	media	_	_
11-18	2144-2145	.	_	_
11-19	2147-2148	-	_	_
11-20	2148-2149	-	_	_
11-21	2149-2150	-	_	_
11-22	2150-2151	-	_	_
11-23	2151-2152	-	_	_
11-24	2152-2153	-	_	_
11-25	2155-2156	#	_	_
11-26	2157-2164	PAVS10K	_	_
11-27	2166-2167	<	_	_
11-28	2167-2168	p	_	_
11-29	2169-2174	align	_	_
11-30	2174-2175	=	_	_
11-31	2175-2176	"	_	_
11-32	2176-2182	center	_	_
11-33	2182-2183	"	_	_
11-34	2183-2184	>	_	_
11-35	2189-2190	<	_	_
11-36	2190-2193	img	_	_
11-37	2194-2197	src	_	_
11-38	2197-2198	=	_	_
11-39	2198-2199	"	_	_
11-40	2199-2200	.	_	_

#Text=/figures/fig\_teaser.jpg"/> <br />     <em>      Figure 1: An example of our PAVS10K where coarse-to-fine annotations are provided, based on a guidance of fixations acquired from subjective experiments conducted by multiple (N) subjects wearing Head-Mounted Displays (HMDs) and headphones.
12-1	2200-2201	/	_	_
12-2	2201-2208	figures	_	_
12-3	2208-2209	/	_	_
12-4	2209-2223	fig\_teaser.jpg	_	_
12-5	2223-2224	"	_	_
12-6	2224-2225	/	_	_
12-7	2225-2226	>	_	_
12-8	2227-2228	<	_	_
12-9	2228-2230	br	_	_
12-10	2231-2232	/	_	_
12-11	2232-2233	>	_	_
12-12	2238-2239	<	_	_
12-13	2239-2241	em	_	_
12-14	2241-2242	>	_	_
12-15	2248-2254	Figure	_	_
12-16	2255-2256	1	_	_
12-17	2256-2257	:	_	_
12-18	2258-2260	An	_	_
12-19	2261-2268	example	_	_
12-20	2269-2271	of	_	_
12-21	2272-2275	our	_	_
12-22	2276-2283	PAVS10K	_	_
12-23	2284-2289	where	_	_
12-24	2290-2304	coarse-to-fine	_	_
12-25	2305-2316	annotations	_	_
12-26	2317-2320	are	_	_
12-27	2321-2329	provided	_	_
12-28	2329-2330	,	_	_
12-29	2331-2336	based	_	_
12-30	2337-2339	on	_	_
12-31	2340-2341	a	_	_
12-32	2342-2350	guidance	_	_
12-33	2351-2353	of	_	_
12-34	2354-2363	fixations	_	_
12-35	2364-2372	acquired	_	_
12-36	2373-2377	from	_	_
12-37	2378-2388	subjective	_	_
12-38	2389-2400	experiments	_	_
12-39	2401-2410	conducted	_	_
12-40	2411-2413	by	_	_
12-41	2414-2422	multiple	_	_
12-42	2423-2424	(	_	_
12-43	2424-2425	N	_	_
12-44	2425-2426	)	_	_
12-45	2427-2435	subjects	_	_
12-46	2436-2443	wearing	_	_
12-47	2444-2456	Head-Mounted	_	_
12-48	2457-2465	Displays	_	_
12-49	2466-2467	(	_	_
12-50	2467-2471	HMDs	_	_
12-51	2471-2472	)	_	_
12-52	2473-2476	and	_	_
12-53	2477-2487	headphones	_	_
12-54	2487-2488	.	_	_

#Text=Each (e.g., fk, fl and fn, where random integral values {k, l, n} ∈ \[1, T \]) of the total equirectangular (ER) video frames T of the sequence “Speaking”(Super-class)-“Brothers”(sub-class) are manually labeled with both object-level and instance-level pixel-wise masks.
13-1	2489-2493	Each	_	_
13-2	2494-2495	(	_	_
13-3	2495-2498	e.g	_	_
13-4	2498-2499	.	_	_
13-5	2499-2500	,	_	_
13-6	2501-2503	fk	_	_
13-7	2503-2504	,	_	_
13-8	2505-2507	fl	_	_
13-9	2508-2511	and	_	_
13-10	2512-2514	fn	_	_
13-11	2514-2515	,	_	_
13-12	2516-2521	where	_	_
13-13	2522-2528	random	_	_
13-14	2529-2537	integral	_	_
13-15	2538-2544	values	_	_
13-16	2545-2546	{	_	_
13-17	2546-2547	k	_	_
13-18	2547-2548	,	_	_
13-19	2549-2550	l	_	_
13-20	2550-2551	,	_	_
13-21	2552-2553	n	_	_
13-22	2553-2554	}	_	_
13-23	2555-2556	∈	_	_
13-24	2557-2558	\[	_	_
13-25	2558-2559	1	_	_
13-26	2559-2560	,	_	_
13-27	2561-2562	T	_	_
13-28	2563-2564	\]	_	_
13-29	2564-2565	)	_	_
13-30	2566-2568	of	_	_
13-31	2569-2572	the	_	_
13-32	2573-2578	total	_	_
13-33	2579-2594	equirectangular	_	_
13-34	2595-2596	(	_	_
13-35	2596-2598	ER	_	_
13-36	2598-2599	)	_	_
13-37	2600-2605	video	_	_
13-38	2606-2612	frames	_	_
13-39	2613-2614	T	_	_
13-40	2615-2617	of	_	_
13-41	2618-2621	the	_	_
13-42	2622-2630	sequence	_	_
13-43	2631-2632	“	_	_
13-44	2632-2640	Speaking	_	_
13-45	2640-2641	”	_	_
13-46	2641-2642	(	_	_
13-47	2642-2653	Super-class	_	_
13-48	2653-2654	)	_	_
13-49	2654-2655	-	_	_
13-50	2655-2656	“	_	_
13-51	2656-2664	Brothers	_	_
13-52	2664-2665	”	_	_
13-53	2665-2666	(	_	_
13-54	2666-2675	sub-class	_	_
13-55	2675-2676	)	_	_
13-56	2677-2680	are	_	_
13-57	2681-2689	manually	_	_
13-58	2690-2697	labeled	_	_
13-59	2698-2702	with	_	_
13-60	2703-2707	both	_	_
13-61	2708-2720	object-level	_	_
13-62	2721-2724	and	_	_
13-63	2725-2739	instance-level	_	_
13-64	2740-2750	pixel-wise	_	_
13-65	2751-2756	masks	_	_
13-66	2756-2757	.	_	_

#Text=According to the features of defined salient objects within each of the sequences, multiple attributes, e.g., “multiple objects” (MO), “competing sounds” (CS), “geometrical distortion” (GD), “motion blur” (MB), “occlusions” (OC) and “low resolution” (LR) are further annotated to enable detailed analysis for PAV-SOD modeling.
14-1	2758-2767	According	_	_
14-2	2768-2770	to	_	_
14-3	2771-2774	the	_	_
14-4	2775-2783	features	_	_
14-5	2784-2786	of	_	_
14-6	2787-2794	defined	_	_
14-7	2795-2802	salient	_	_
14-8	2803-2810	objects	_	_
14-9	2811-2817	within	_	_
14-10	2818-2822	each	_	_
14-11	2823-2825	of	_	_
14-12	2826-2829	the	_	_
14-13	2830-2839	sequences	_	_
14-14	2839-2840	,	_	_
14-15	2841-2849	multiple	_	_
14-16	2850-2860	attributes	_	_
14-17	2860-2861	,	_	_
14-18	2862-2865	e.g	_	_
14-19	2865-2866	.	_	_
14-20	2866-2867	,	_	_
14-21	2868-2869	“	_	_
14-22	2869-2877	multiple	_	_
14-23	2878-2885	objects	_	_
14-24	2885-2886	”	_	_
14-25	2887-2888	(	_	_
14-26	2888-2890	MO	_	_
14-27	2890-2891	)	_	_
14-28	2891-2892	,	_	_
14-29	2893-2894	“	_	_
14-30	2894-2903	competing	_	_
14-31	2904-2910	sounds	_	_
14-32	2910-2911	”	_	_
14-33	2912-2913	(	_	_
14-34	2913-2915	CS	_	_
14-35	2915-2916	)	_	_
14-36	2916-2917	,	_	_
14-37	2918-2919	“	_	_
14-38	2919-2930	geometrical	_	_
14-39	2931-2941	distortion	_	_
14-40	2941-2942	”	_	_
14-41	2943-2944	(	_	_
14-42	2944-2946	GD	_	_
14-43	2946-2947	)	_	_
14-44	2947-2948	,	_	_
14-45	2949-2950	“	_	_
14-46	2950-2956	motion	_	_
14-47	2957-2961	blur	_	_
14-48	2961-2962	”	_	_
14-49	2963-2964	(	_	_
14-50	2964-2966	MB	_	_
14-51	2966-2967	)	_	_
14-52	2967-2968	,	_	_
14-53	2969-2970	“	_	_
14-54	2970-2980	occlusions	_	_
14-55	2980-2981	”	_	_
14-56	2982-2983	(	_	_
14-57	2983-2985	OC	_	_
14-58	2985-2986	)	_	_
14-59	2987-2990	and	_	_
14-60	2991-2992	“	_	_
14-61	2992-2995	low	_	_
14-62	2996-3006	resolution	_	_
14-63	3006-3007	”	_	_
14-64	3008-3009	(	_	_
14-65	3009-3011	LR	_	_
14-66	3011-3012	)	_	_
14-67	3013-3016	are	_	_
14-68	3017-3024	further	_	_
14-69	3025-3034	annotated	_	_
14-70	3035-3037	to	_	_
14-71	3038-3044	enable	_	_
14-72	3045-3053	detailed	_	_
14-73	3054-3062	analysis	_	_
14-74	3063-3066	for	_	_
14-75	3067-3074	PAV-SOD	_	_
14-76	3075-3083	modeling	_	_
14-77	3083-3084	.	_	_

#Text=</em> </p>  <p align="center">     <img src=".
15-1	3089-3090	<	_	_
15-2	3090-3091	/	_	_
15-3	3091-3093	em	_	_
15-4	3093-3094	>	_	_
15-5	3095-3096	<	_	_
15-6	3096-3097	/	_	_
15-7	3097-3098	p	_	_
15-8	3098-3099	>	_	_
15-9	3101-3102	<	_	_
15-10	3102-3103	p	_	_
15-11	3104-3109	align	_	_
15-12	3109-3110	=	_	_
15-13	3110-3111	"	_	_
15-14	3111-3117	center	_	_
15-15	3117-3118	"	_	_
15-16	3118-3119	>	_	_
15-17	3124-3125	<	_	_
15-18	3125-3128	img	_	_
15-19	3129-3132	src	_	_
15-20	3132-3133	=	_	_
15-21	3133-3134	"	_	_
15-22	3134-3135	.	_	_

#Text=/figures/fig\_related\_datasets.jpg"/> <br />     <em>      Figure 2: Summary of widely used salient object detection (SOD)/video object segmentation (VOS) datasets and PAVS10K.
16-1	3135-3136	/	_	_
16-2	3136-3143	figures	_	_
16-3	3143-3144	/	_	_
16-4	3144-3168	fig\_related\_datasets.jpg	_	_
16-5	3168-3169	"	_	_
16-6	3169-3170	/	_	_
16-7	3170-3171	>	_	_
16-8	3172-3173	<	_	_
16-9	3173-3175	br	_	_
16-10	3176-3177	/	_	_
16-11	3177-3178	>	_	_
16-12	3183-3184	<	_	_
16-13	3184-3186	em	_	_
16-14	3186-3187	>	_	_
16-15	3193-3199	Figure	_	_
16-16	3200-3201	2	_	_
16-17	3201-3202	:	_	_
16-18	3203-3210	Summary	_	_
16-19	3211-3213	of	_	_
16-20	3214-3220	widely	_	_
16-21	3221-3225	used	_	_
16-22	3226-3233	salient	_	_
16-23	3234-3240	object	_	_
16-24	3241-3250	detection	_	_
16-25	3251-3252	(	_	_
16-26	3252-3255	SOD	_	_
16-27	3255-3256	)	_	_
16-28	3256-3257	/	_	_
16-29	3257-3262	video	_	_
16-30	3263-3269	object	_	_
16-31	3270-3282	segmentation	_	_
16-32	3283-3284	(	_	_
16-33	3284-3287	VOS	_	_
16-34	3287-3288	)	_	_
16-35	3289-3297	datasets	_	_
16-36	3298-3301	and	_	_
16-37	3302-3309	PAVS10K	_	_
16-38	3309-3310	.	_	_

#Text=#Img: The number of images/video frames.
17-1	3311-3312	#	_	_
17-2	3312-3315	Img	_	_
17-3	3315-3316	:	_	_
17-4	3317-3320	The	_	_
17-5	3321-3327	number	_	_
17-6	3328-3330	of	_	_
17-7	3331-3337	images	_	_
17-8	3337-3338	/	_	_
17-9	3338-3343	video	_	_
17-10	3344-3350	frames	_	_
17-11	3350-3351	.	_	_

#Text=#GT: The number of object-level pixel-wise masks (ground truth for SOD).
18-1	3352-3353	#	_	_
18-2	3353-3355	GT	_	_
18-3	3355-3356	:	_	_
18-4	3357-3360	The	_	_
18-5	3361-3367	number	_	_
18-6	3368-3370	of	_	_
18-7	3371-3383	object-level	_	_
18-8	3384-3394	pixel-wise	_	_
18-9	3395-3400	masks	_	_
18-10	3401-3402	(	_	_
18-11	3402-3408	ground	_	_
18-12	3409-3414	truth	_	_
18-13	3415-3418	for	_	_
18-14	3419-3422	SOD	_	_
18-15	3422-3423	)	_	_
18-16	3423-3424	.	_	_

#Text=Pub. = Publication.
19-1	3425-3428	Pub	_	_
19-2	3428-3429	.	_	_
19-3	3430-3431	=	_	_
19-4	3432-3443	Publication	_	_
19-5	3443-3444	.	_	_

#Text=Obj.
20-1	3445-3448	Obj	_	_
20-2	3448-3449	.	_	_

#Text=-Level = Object-Level Labels.
21-1	3449-3450	-	_	_
21-2	3450-3455	Level	_	_
21-3	3456-3457	=	_	_
21-4	3458-3470	Object-Level	_	_
21-5	3471-3477	Labels	_	_
21-6	3477-3478	.	_	_

#Text=Ins.
22-1	3479-3482	Ins	_	_
22-2	3482-3483	.	_	_

#Text=-Level = Instance-Level Labels.
23-1	3483-3484	-	_	_
23-2	3484-3489	Level	_	_
23-3	3490-3491	=	_	_
23-4	3492-3506	Instance-Level	_	_
23-5	3507-3513	Labels	_	_
23-6	3513-3514	.	_	_

#Text=Fix.
24-1	3515-3518	Fix	_	_
24-2	3518-3519	.	_	_

#Text=GT = Fixation Maps. † denotes equirectangular images.
25-1	3520-3522	GT	_	_
25-2	3523-3524	=	_	_
25-3	3525-3533	Fixation	_	_
25-4	3534-3538	Maps	_	_
25-5	3538-3539	.	_	_
25-6	3540-3541	†	_	_
25-7	3542-3549	denotes	_	_
25-8	3550-3565	equirectangular	_	_
25-9	3566-3572	images	_	_
25-10	3572-3573	.	_	_

#Text=</em> </p>  <p align="center">     <img src=".
26-1	3578-3579	<	_	_
26-2	3579-3580	/	_	_
26-3	3580-3582	em	_	_
26-4	3582-3583	>	_	_
26-5	3584-3585	<	_	_
26-6	3585-3586	/	_	_
26-7	3586-3587	p	_	_
26-8	3587-3588	>	_	_
26-9	3590-3591	<	_	_
26-10	3591-3592	p	_	_
26-11	3593-3598	align	_	_
26-12	3598-3599	=	_	_
26-13	3599-3600	"	_	_
26-14	3600-3606	center	_	_
26-15	3606-3607	"	_	_
26-16	3607-3608	>	_	_
26-17	3613-3614	<	_	_
26-18	3614-3617	img	_	_
26-19	3618-3621	src	_	_
26-20	3621-3622	=	_	_
26-21	3622-3623	"	_	_
26-22	3623-3624	.	_	_

#Text=/figures/fig\_dataset\_examples.jpg"/> <br />     <em>      Figure 3: Examples of challenging attributes on equirectangular images from our PAVS10K, with instance-level ground truth and fixations as annotation guidance.
27-1	3624-3625	/	_	_
27-2	3625-3632	figures	_	_
27-3	3632-3633	/	_	_
27-4	3633-3657	fig\_dataset\_examples.jpg	_	_
27-5	3657-3658	"	_	_
27-6	3658-3659	/	_	_
27-7	3659-3660	>	_	_
27-8	3661-3662	<	_	_
27-9	3662-3664	br	_	_
27-10	3665-3666	/	_	_
27-11	3666-3667	>	_	_
27-12	3672-3673	<	_	_
27-13	3673-3675	em	_	_
27-14	3675-3676	>	_	_
27-15	3682-3688	Figure	_	_
27-16	3689-3690	3	_	_
27-17	3690-3691	:	_	_
27-18	3692-3700	Examples	_	_
27-19	3701-3703	of	_	_
27-20	3704-3715	challenging	_	_
27-21	3716-3726	attributes	_	_
27-22	3727-3729	on	_	_
27-23	3730-3745	equirectangular	_	_
27-24	3746-3752	images	_	_
27-25	3753-3757	from	_	_
27-26	3758-3761	our	_	_
27-27	3762-3769	PAVS10K	_	_
27-28	3769-3770	,	_	_
27-29	3771-3775	with	_	_
27-30	3776-3790	instance-level	_	_
27-31	3791-3797	ground	_	_
27-32	3798-3803	truth	_	_
27-33	3804-3807	and	_	_
27-34	3808-3817	fixations	_	_
27-35	3818-3820	as	_	_
27-36	3821-3831	annotation	_	_
27-37	3832-3840	guidance	_	_
27-38	3840-3841	.	_	_

#Text={𝑓𝑘  , 𝑓𝑙  , 𝑓𝑛  } denote random frames of a given video.
28-1	3842-3843	{	_	_
28-2	3843-3847	𝑓𝑘	_	_
28-3	3847-3848	,	_	_
28-4	3849-3853	𝑓𝑙	_	_
28-5	3853-3854	,	_	_
28-6	3855-3859	𝑓𝑛	_	_
28-7	3859-3860	}	_	_
28-8	3861-3867	denote	_	_
28-9	3868-3874	random	_	_
28-10	3875-3881	frames	_	_
28-11	3882-3884	of	_	_
28-12	3885-3886	a	_	_
28-13	3887-3892	given	_	_
28-14	3893-3898	video	_	_
28-15	3898-3899	.	_	_

#Text=</em> </p>  <p align="center">     <img src=".
29-1	3904-3905	<	_	_
29-2	3905-3906	/	_	_
29-3	3906-3908	em	_	_
29-4	3908-3909	>	_	_
29-5	3910-3911	<	_	_
29-6	3911-3912	/	_	_
29-7	3912-3913	p	_	_
29-8	3913-3914	>	_	_
29-9	3916-3917	<	_	_
29-10	3917-3918	p	_	_
29-11	3919-3924	align	_	_
29-12	3924-3925	=	_	_
29-13	3925-3926	"	_	_
29-14	3926-3932	center	_	_
29-15	3932-3933	"	_	_
29-16	3933-3934	>	_	_
29-17	3939-3940	<	_	_
29-18	3940-3943	img	_	_
29-19	3944-3947	src	_	_
29-20	3947-3948	=	_	_
29-21	3948-3949	"	_	_
29-22	3949-3950	.	_	_

#Text=/figures/fig\_dataset\_statistics.jpg"/> <br />     <em>      Figure 4: Statistics of the proposed PAVS10K.
30-1	3950-3951	/	_	_
30-2	3951-3958	figures	_	_
30-3	3958-3959	/	_	_
30-4	3959-3985	fig\_dataset\_statistics.jpg	_	_
30-5	3985-3986	"	_	_
30-6	3986-3987	/	_	_
30-7	3987-3988	>	_	_
30-8	3989-3990	<	_	_
30-9	3990-3992	br	_	_
30-10	3993-3994	/	_	_
30-11	3994-3995	>	_	_
30-12	4000-4001	<	_	_
30-13	4001-4003	em	_	_
30-14	4003-4004	>	_	_
30-15	4010-4016	Figure	_	_
30-16	4017-4018	4	_	_
30-17	4018-4019	:	_	_
30-18	4020-4030	Statistics	_	_
30-19	4031-4033	of	_	_
30-20	4034-4037	the	_	_
30-21	4038-4046	proposed	_	_
30-22	4047-4054	PAVS10K	_	_
30-23	4054-4055	.	_	_

#Text=(a) Super-/sub-category information.
31-1	4056-4057	(	_	_
31-2	4057-4058	a	_	_
31-3	4058-4059	)	_	_
31-4	4060-4065	Super	_	_
31-5	4065-4066	-	_	_
31-6	4066-4067	/	_	_
31-7	4067-4079	sub-category	_	_
31-8	4080-4091	information	_	_
31-9	4091-4092	.	_	_

#Text=(b) Instance density (labeled frames per sequence) of each sub-class.
32-1	4093-4094	(	_	_
32-2	4094-4095	b	_	_
32-3	4095-4096	)	_	_
32-4	4097-4105	Instance	_	_
32-5	4106-4113	density	_	_
32-6	4114-4115	(	_	_
32-7	4115-4122	labeled	_	_
32-8	4123-4129	frames	_	_
32-9	4130-4133	per	_	_
32-10	4134-4142	sequence	_	_
32-11	4142-4143	)	_	_
32-12	4144-4146	of	_	_
32-13	4147-4151	each	_	_
32-14	4152-4161	sub-class	_	_
32-15	4161-4162	.	_	_

#Text=(c) Sound sources of PAVS10K scenes, such as musical instruments, human instances and animals.
33-1	4163-4164	(	_	_
33-2	4164-4165	c	_	_
33-3	4165-4166	)	_	_
33-4	4167-4172	Sound	_	_
33-5	4173-4180	sources	_	_
33-6	4181-4183	of	_	_
33-7	4184-4191	PAVS10K	_	_
33-8	4192-4198	scenes	_	_
33-9	4198-4199	,	_	_
33-10	4200-4204	such	_	_
33-11	4205-4207	as	_	_
33-12	4208-4215	musical	_	_
33-13	4216-4227	instruments	_	_
33-14	4227-4228	,	_	_
33-15	4229-4234	human	_	_
33-16	4235-4244	instances	_	_
33-17	4245-4248	and	_	_
33-18	4249-4256	animals	_	_
33-19	4256-4257	.	_	_

#Text=</em> </p>  ------  # Benchmark Models  \*\*No.\*\* \| \*\*Year\*\* \| \*\*Pub.\*\* \| \*\*Title\*\* \| \*\*Links\*\*  :-: \| :-:\| :-: \| :-  \| :-:  01 \| \*\*2019\*\*\| \*\*CVPR\*\* \| Cascaded Partial Decoder for Fast and Accurate Salient Object Detection \| \[Paper\](https://openaccess.thecvf.com/content\_CVPR\_2019/papers/Wu\_Cascaded\_Partial\_Decoder\_for\_Fast\_and\_Accurate\_Salient\_Object\_Detection\_CVPR\_2019\_paper.pdf)/\[Code\](https://github.com/wuzhe71/CPD)  02 \| \*\*2019\*\*\| \*\*CVPR\*\* \| See More, Know More: Unsupervised Video Object Segmentation with Co-Attention Siamese Networks \| \[Paper\](https://openaccess.thecvf.com/content\_CVPR\_2019/papers/Lu\_See\_More\_Know\_More\_Unsupervised\_Video\_Object\_Segmentation\_With\_Co-Attention\_CVPR\_2019\_paper.pdf)/\[Code\](https://github.com/carrierlxk/COSNet)  03 \| \*\*2019\*\*\| \*\*ICCV\*\* \| Stacked Cross Refinement Network for Edge-Aware Salient Object Detection \| \[Paper\](https://openaccess.thecvf.com/content\_ICCV\_2019/papers/Wu\_Stacked\_Cross\_Refinement\_Network\_for\_Edge-Aware\_Salient\_Object\_Detection\_ICCV\_2019\_paper.pdf)/\[Code\](https://github.com/wuzhe71/SCRN) 04 \| \*\*2019\*\*\| \*\*ICCV\*\* \| Semi-Supervised Video Salient Object Detection Using Pseudo-Labels \| \[Paper\](https://openaccess.thecvf.com/content\_ICCV\_2019/papers/Yan\_Semi-Supervised\_Video\_Salient\_Object\_Detection\_Using\_Pseudo-Labels\_ICCV\_2019\_paper.pdf)/\[Code\](https://github.com/Kinpzz/RCRNet-Pytorch) 05 \| \*\*2020\*\*\| \*\*AAAI\*\* \| F³Net: Fusion, Feedback and Focus for Salient Object Detection \| \[Paper\](https://ojs.aaai.org/index.php/AAAI/article/download/6916/6770)/\[Code\](https://github.com/weijun88/F3Net) 06 \| \*\*2020\*\*\| \*\*AAAI\*\* \| Pyramid Constrained Self-Attention Network for Fast Video Salient Object Detection \| \[Paper\](https://ojs.aaai.org/index.php/AAAI/article/view/6718/6572)/\[Code\](https://github.com/guyuchao/PyramidCSA) 07 \| \*\*2020\*\*\| \*\*CVPR\*\* \| Multi-scale Interactive Network for Salient Object Detection \| \[Paper\](https://openaccess.thecvf.com/content\_CVPR\_2020/papers/Pang\_Multi-Scale\_Interactive\_Network\_for\_Salient\_Object\_Detection\_CVPR\_2020\_paper.pdf)/\[Code\](https://github.com/lartpang/MINet) 08 \| \*\*2020\*\*\| \*\*CVPR\*\* \| Label Decoupling Framework for Salient Object Detection \| \[Paper\](https://openaccess.thecvf.com/content\_CVPR\_2020/papers/Wei\_Label\_Decoupling\_Framework\_for\_Salient\_Object\_Detection\_CVPR\_2020\_paper.pdf)/\[Code\](https://github.com/weijun88/LDF) 09 \| \*\*2020\*\*\| \*\*ECCV\*\* \| Highly Efficient Salient Object Detection with 100K Parameters \| \[Paper\](http://mftp.mmcheng.net/Papers/20EccvSal100k.pdf)/\[Code\](https://github.com/ShangHua-Gao/SOD100K) 10 \| \*\*2020\*\*\| \*\*ECCV\*\* \| Suppress and Balance: A Simple Gated Network for Salient Object Detection \| \[Paper\](https://www.ecva.net/papers/eccv\_2020/papers\_ECCV/papers/123470035.pdf)/\[Code\](https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency) 11 \| \*\*2020\*\*\| \*\*BMVC\*\* \| Making a Case for 3D Convolutions for Object Segmentation in Videos \| \[Paper\](https://www.bmvc2020-conference.com/assets/papers/0233.pdf)/\[Code\](https://github.com/sabarim/3DC-Seg) 12 \| \*\*2020\*\*\| \*\*SPL\*\* \| FANet: Features Adaptation Network for 360° Omnidirectional Salient Object Detection \| \[Paper\](https://ieeexplore.ieee.org/document/9211754)/\[Code\](https://github.com/DreaMKHuang/FANet) 13 \| \*\*2021\*\*\| \*\*CVPR\*\* \| Reciprocal Transformations for Unsupervised Video Object Segmentation \| \[Paper\](https://openaccess.thecvf.com/content/CVPR2021/papers/Ren\_Reciprocal\_Transformations\_for\_Unsupervised\_Video\_Object\_Segmentation\_CVPR\_2021\_paper.pdf)/\[Code\](https://github.com/OliverRensu/RTNet)  ------  # CAV-Net  The codes are available at \[src\](https://github.com/PanoAsh/PAV-SOD/tree/main/src).
34-1	4262-4263	<	_	_
34-2	4263-4264	/	_	_
34-3	4264-4266	em	_	_
34-4	4266-4267	>	_	_
34-5	4268-4269	<	_	_
34-6	4269-4270	/	_	_
34-7	4270-4271	p	_	_
34-8	4271-4272	>	_	_
34-9	4274-4275	-	_	_
34-10	4275-4276	-	_	_
34-11	4276-4277	-	_	_
34-12	4277-4278	-	_	_
34-13	4278-4279	-	_	_
34-14	4279-4280	-	_	_
34-15	4282-4283	#	_	_
34-16	4284-4293	Benchmark	_	_
34-17	4294-4300	Models	_	_
34-18	4302-4303	\*	_	_
34-19	4303-4304	\*	_	_
34-20	4304-4306	No	_	_
34-21	4306-4307	.	_	_
34-22	4307-4308	\*	_	_
34-23	4308-4309	\*	_	_
34-24	4310-4311	\|	_	_
34-25	4312-4313	\*	_	_
34-26	4313-4314	\*	_	_
34-27	4314-4318	Year	_	_
34-28	4318-4319	\*	_	_
34-29	4319-4320	\*	_	_
34-30	4321-4322	\|	_	_
34-31	4323-4324	\*	_	_
34-32	4324-4325	\*	_	_
34-33	4325-4328	Pub	_	_
34-34	4328-4329	.	_	_
34-35	4329-4330	\*	_	_
34-36	4330-4331	\*	_	_
34-37	4332-4333	\|	_	_
34-38	4334-4335	\*	_	_
34-39	4335-4336	\*	_	_
34-40	4336-4341	Title	_	_
34-41	4341-4342	\*	_	_
34-42	4342-4343	\*	_	_
34-43	4344-4345	\|	_	_
34-44	4346-4347	\*	_	_
34-45	4347-4348	\*	_	_
34-46	4348-4353	Links	_	_
34-47	4353-4354	\*	_	_
34-48	4354-4355	\*	_	_
34-49	4357-4358	:	_	_
34-50	4358-4359	-	_	_
34-51	4359-4360	:	_	_
34-52	4361-4362	\|	_	_
34-53	4363-4364	:	_	_
34-54	4364-4365	-	_	_
34-55	4365-4366	:	_	_
34-56	4366-4367	\|	_	_
34-57	4368-4369	:	_	_
34-58	4369-4370	-	_	_
34-59	4370-4371	:	_	_
34-60	4372-4373	\|	_	_
34-61	4374-4375	:	_	_
34-62	4375-4376	-	_	_
34-63	4378-4379	\|	_	_
34-64	4380-4381	:	_	_
34-65	4381-4382	-	_	_
34-66	4382-4383	:	_	_
34-67	4385-4387	01	_	_
34-68	4388-4389	\|	_	_
34-69	4390-4391	\*	_	_
34-70	4391-4392	\*	_	_
34-71	4392-4396	2019	_	_
34-72	4396-4397	\*	_	_
34-73	4397-4398	\*	_	_
34-74	4398-4399	\|	_	_
34-75	4400-4401	\*	_	_
34-76	4401-4402	\*	_	_
34-77	4402-4406	CVPR	_	_
34-78	4406-4407	\*	_	_
34-79	4407-4408	\*	_	_
34-80	4409-4410	\|	_	_
34-81	4411-4419	Cascaded	_	_
34-82	4420-4427	Partial	_	_
34-83	4428-4435	Decoder	_	_
34-84	4436-4439	for	_	_
34-85	4440-4444	Fast	_	_
34-86	4445-4448	and	_	_
34-87	4449-4457	Accurate	_	_
34-88	4458-4465	Salient	_	_
34-89	4466-4472	Object	_	_
34-90	4473-4482	Detection	_	_
34-91	4483-4484	\|	_	_
34-92	4485-4486	\[	_	_
34-93	4486-4491	Paper	_	_
34-94	4491-4492	\]	_	_
34-95	4492-4493	(	_	_
34-96	4493-4498	https	_	_
34-97	4498-4499	:	_	_
34-98	4499-4500	/	_	_
34-99	4500-4501	/	_	_
34-100	4501-4522	openaccess.thecvf.com	_	_
34-101	4522-4523	/	_	_
34-102	4523-4535	content\_CVPR	_	_
34-102	4531-4535	CVPR	_	_
34-103	4535-4536	\_	_	_
34-104	4536-4540	2019	_	_
34-105	4540-4541	/	_	_
34-106	4541-4547	papers	_	_
34-107	4547-4548	/	_	_
34-108	4548-4627	Wu\_Cascaded\_Partial\_Decoder\_for\_Fast\_and\_Accurate\_Salient\_Object\_Detection\_CVPR	_	_
34-108	4623-4627	CVPR	_	_
34-109	4627-4628	\_	_	_
34-110	4628-4632	2019	_	_
34-111	4632-4633	\_	_	_
34-112	4633-4642	paper.pdf	_	_
34-113	4642-4643	)	_	_
34-114	4643-4644	/	_	_
34-115	4644-4645	\[	_	_
34-116	4645-4649	Code	_	_
34-117	4649-4650	\]	_	_
34-118	4650-4651	(	_	_
34-119	4651-4656	https	_	_
34-120	4656-4657	:	_	_
34-121	4657-4658	/	_	_
34-122	4658-4659	/	_	_
34-123	4659-4669	github.com	_	_
34-124	4669-4670	/	_	_
34-125	4670-4677	wuzhe71	_	_
34-126	4677-4678	/	_	_
34-127	4678-4681	CPD	_	_
34-128	4681-4682	)	_	_
34-129	4684-4686	02	_	_
34-130	4687-4688	\|	_	_
34-131	4689-4690	\*	_	_
34-132	4690-4691	\*	_	_
34-133	4691-4695	2019	_	_
34-134	4695-4696	\*	_	_
34-135	4696-4697	\*	_	_
34-136	4697-4698	\|	_	_
34-137	4699-4700	\*	_	_
34-138	4700-4701	\*	_	_
34-139	4701-4705	CVPR	_	_
34-140	4705-4706	\*	_	_
34-141	4706-4707	\*	_	_
34-142	4708-4709	\|	_	_
34-143	4710-4713	See	_	_
34-144	4714-4718	More	_	_
34-145	4718-4719	,	_	_
34-146	4720-4724	Know	_	_
34-147	4725-4729	More	_	_
34-148	4729-4730	:	_	_
34-149	4731-4743	Unsupervised	_	_
34-150	4744-4749	Video	_	_
34-151	4750-4756	Object	_	_
34-152	4757-4769	Segmentation	_	_
34-153	4770-4774	with	_	_
34-154	4775-4787	Co-Attention	_	_
34-155	4788-4795	Siamese	_	_
34-156	4796-4804	Networks	_	_
34-157	4805-4806	\|	_	_
34-158	4807-4808	\[	_	_
34-159	4808-4813	Paper	_	_
34-160	4813-4814	\]	_	_
34-161	4814-4815	(	_	_
34-162	4815-4820	https	_	_
34-163	4820-4821	:	_	_
34-164	4821-4822	/	_	_
34-165	4822-4823	/	_	_
34-166	4823-4844	openaccess.thecvf.com	_	_
34-167	4844-4845	/	_	_
34-168	4845-4857	content\_CVPR	_	_
34-168	4853-4857	CVPR	_	_
34-169	4857-4858	\_	_	_
34-170	4858-4862	2019	_	_
34-171	4862-4863	/	_	_
34-172	4863-4869	papers	_	_
34-173	4869-4870	/	_	_
34-174	4870-4953	Lu\_See\_More\_Know\_More\_Unsupervised\_Video\_Object\_Segmentation\_With\_Co-Attention\_CVPR	_	_
34-174	4949-4953	CVPR	_	_
34-175	4953-4954	\_	_	_
34-176	4954-4958	2019	_	_
34-177	4958-4959	\_	_	_
34-178	4959-4968	paper.pdf	_	_
34-179	4968-4969	)	_	_
34-180	4969-4970	/	_	_
34-181	4970-4971	\[	_	_
34-182	4971-4975	Code	_	_
34-183	4975-4976	\]	_	_
34-184	4976-4977	(	_	_
34-185	4977-4982	https	_	_
34-186	4982-4983	:	_	_
34-187	4983-4984	/	_	_
34-188	4984-4985	/	_	_
34-189	4985-4995	github.com	_	_
34-190	4995-4996	/	_	_
34-191	4996-5006	carrierlxk	_	_
34-192	5006-5007	/	_	_
34-193	5007-5013	COSNet	_	_
34-194	5013-5014	)	_	_
34-195	5016-5018	03	_	_
34-196	5019-5020	\|	_	_
34-197	5021-5022	\*	_	_
34-198	5022-5023	\*	_	_
34-199	5023-5027	2019	_	_
34-200	5027-5028	\*	_	_
34-201	5028-5029	\*	_	_
34-202	5029-5030	\|	_	_
34-203	5031-5032	\*	_	_
34-204	5032-5033	\*	_	_
34-205	5033-5037	ICCV	_	_
34-206	5037-5038	\*	_	_
34-207	5038-5039	\*	_	_
34-208	5040-5041	\|	_	_
34-209	5042-5049	Stacked	_	_
34-210	5050-5055	Cross	_	_
34-211	5056-5066	Refinement	_	_
34-212	5067-5074	Network	_	_
34-213	5075-5078	for	_	_
34-214	5079-5089	Edge-Aware	_	_
34-215	5090-5097	Salient	_	_
34-216	5098-5104	Object	_	_
34-217	5105-5114	Detection	_	_
34-218	5115-5116	\|	_	_
34-219	5117-5118	\[	_	_
34-220	5118-5123	Paper	_	_
34-221	5123-5124	\]	_	_
34-222	5124-5125	(	_	_
34-223	5125-5130	https	_	_
34-224	5130-5131	:	_	_
34-225	5131-5132	/	_	_
34-226	5132-5133	/	_	_
34-227	5133-5154	openaccess.thecvf.com	_	_
34-228	5154-5155	/	_	_
34-229	5155-5167	content\_ICCV	_	_
34-229	5163-5167	ICCV	_	_
34-230	5167-5168	\_	_	_
34-231	5168-5172	2019	_	_
34-232	5172-5173	/	_	_
34-233	5173-5179	papers	_	_
34-234	5179-5180	/	_	_
34-235	5180-5260	Wu\_Stacked\_Cross\_Refinement\_Network\_for\_Edge-Aware\_Salient\_Object\_Detection\_ICCV	_	_
34-235	5256-5260	ICCV	_	_
34-236	5260-5261	\_	_	_
34-237	5261-5265	2019	_	_
34-238	5265-5266	\_	_	_
34-239	5266-5275	paper.pdf	_	_
34-240	5275-5276	)	_	_
34-241	5276-5277	/	_	_
34-242	5277-5278	\[	_	_
34-243	5278-5282	Code	_	_
34-244	5282-5283	\]	_	_
34-245	5283-5284	(	_	_
34-246	5284-5289	https	_	_
34-247	5289-5290	:	_	_
34-248	5290-5291	/	_	_
34-249	5291-5292	/	_	_
34-250	5292-5302	github.com	_	_
34-251	5302-5303	/	_	_
34-252	5303-5310	wuzhe71	_	_
34-253	5310-5311	/	_	_
34-254	5311-5315	SCRN	_	_
34-255	5315-5316	)	_	_
34-256	5317-5319	04	_	_
34-257	5320-5321	\|	_	_
34-258	5322-5323	\*	_	_
34-259	5323-5324	\*	_	_
34-260	5324-5328	2019	_	_
34-261	5328-5329	\*	_	_
34-262	5329-5330	\*	_	_
34-263	5330-5331	\|	_	_
34-264	5332-5333	\*	_	_
34-265	5333-5334	\*	_	_
34-266	5334-5338	ICCV	_	_
34-267	5338-5339	\*	_	_
34-268	5339-5340	\*	_	_
34-269	5341-5342	\|	_	_
34-270	5343-5358	Semi-Supervised	_	_
34-271	5359-5364	Video	_	_
34-272	5365-5372	Salient	_	_
34-273	5373-5379	Object	_	_
34-274	5380-5389	Detection	_	_
34-275	5390-5395	Using	_	_
34-276	5396-5409	Pseudo-Labels	_	_
34-277	5410-5411	\|	_	_
34-278	5412-5413	\[	_	_
34-279	5413-5418	Paper	_	_
34-280	5418-5419	\]	_	_
34-281	5419-5420	(	_	_
34-282	5420-5425	https	_	_
34-283	5425-5426	:	_	_
34-284	5426-5427	/	_	_
34-285	5427-5428	/	_	_
34-286	5428-5449	openaccess.thecvf.com	_	_
34-287	5449-5450	/	_	_
34-288	5450-5462	content\_ICCV	_	_
34-288	5458-5462	ICCV	_	_
34-289	5462-5463	\_	_	_
34-290	5463-5467	2019	_	_
34-291	5467-5468	/	_	_
34-292	5468-5474	papers	_	_
34-293	5474-5475	/	_	_
34-294	5475-5550	Yan\_Semi-Supervised\_Video\_Salient\_Object\_Detection\_Using\_Pseudo-Labels\_ICCV	_	_
34-294	5546-5550	ICCV	_	_
34-295	5550-5551	\_	_	_
34-296	5551-5555	2019	_	_
34-297	5555-5556	\_	_	_
34-298	5556-5565	paper.pdf	_	_
34-299	5565-5566	)	_	_
34-300	5566-5567	/	_	_
34-301	5567-5568	\[	_	_
34-302	5568-5572	Code	_	_
34-303	5572-5573	\]	_	_
34-304	5573-5574	(	_	_
34-305	5574-5579	https	_	_
34-306	5579-5580	:	_	_
34-307	5580-5581	/	_	_
34-308	5581-5582	/	_	_
34-309	5582-5592	github.com	_	_
34-310	5592-5593	/	_	_
34-311	5593-5599	Kinpzz	_	_
34-312	5599-5600	/	_	_
34-313	5600-5614	RCRNet-Pytorch	_	_
34-314	5614-5615	)	_	_
34-315	5616-5618	05	_	_
34-316	5619-5620	\|	_	_
34-317	5621-5622	\*	_	_
34-318	5622-5623	\*	_	_
34-319	5623-5627	2020	_	_
34-320	5627-5628	\*	_	_
34-321	5628-5629	\*	_	_
34-322	5629-5630	\|	_	_
34-323	5631-5632	\*	_	_
34-324	5632-5633	\*	_	_
34-325	5633-5637	AAAI	_	_
34-326	5637-5638	\*	_	_
34-327	5638-5639	\*	_	_
34-328	5640-5641	\|	_	_
34-329	5642-5647	F³Net	_	_
34-330	5647-5648	:	_	_
34-331	5649-5655	Fusion	_	_
34-332	5655-5656	,	_	_
34-333	5657-5665	Feedback	_	_
34-334	5666-5669	and	_	_
34-335	5670-5675	Focus	_	_
34-336	5676-5679	for	_	_
34-337	5680-5687	Salient	_	_
34-338	5688-5694	Object	_	_
34-339	5695-5704	Detection	_	_
34-340	5705-5706	\|	_	_
34-341	5707-5708	\[	_	_
34-342	5708-5713	Paper	_	_
34-343	5713-5714	\]	_	_
34-344	5714-5715	(	_	_
34-345	5715-5720	https	_	_
34-346	5720-5721	:	_	_
34-347	5721-5722	/	_	_
34-348	5722-5723	/	_	_
34-349	5723-5735	ojs.aaai.org	_	_
34-350	5735-5736	/	_	_
34-351	5736-5745	index.php	_	_
34-352	5745-5746	/	_	_
34-353	5746-5750	AAAI	_	_
34-354	5750-5751	/	_	_
34-355	5751-5758	article	_	_
34-356	5758-5759	/	_	_
34-357	5759-5767	download	_	_
34-358	5767-5768	/	_	_
34-359	5768-5772	6916	_	_
34-360	5772-5773	/	_	_
34-361	5773-5777	6770	_	_
34-362	5777-5778	)	_	_
34-363	5778-5779	/	_	_
34-364	5779-5780	\[	_	_
34-365	5780-5784	Code	_	_
34-366	5784-5785	\]	_	_
34-367	5785-5786	(	_	_
34-368	5786-5791	https	_	_
34-369	5791-5792	:	_	_
34-370	5792-5793	/	_	_
34-371	5793-5794	/	_	_
34-372	5794-5804	github.com	_	_
34-373	5804-5805	/	_	_
34-374	5805-5813	weijun88	_	_
34-375	5813-5814	/	_	_
34-376	5814-5819	F3Net	_	_
34-377	5819-5820	)	_	_
34-378	5821-5823	06	_	_
34-379	5824-5825	\|	_	_
34-380	5826-5827	\*	_	_
34-381	5827-5828	\*	_	_
34-382	5828-5832	2020	_	_
34-383	5832-5833	\*	_	_
34-384	5833-5834	\*	_	_
34-385	5834-5835	\|	_	_
34-386	5836-5837	\*	_	_
34-387	5837-5838	\*	_	_
34-388	5838-5842	AAAI	_	_
34-389	5842-5843	\*	_	_
34-390	5843-5844	\*	_	_
34-391	5845-5846	\|	_	_
34-392	5847-5854	Pyramid	_	_
34-393	5855-5866	Constrained	_	_
34-394	5867-5881	Self-Attention	_	_
34-395	5882-5889	Network	_	_
34-396	5890-5893	for	_	_
34-397	5894-5898	Fast	_	_
34-398	5899-5904	Video	_	_
34-399	5905-5912	Salient	_	_
34-400	5913-5919	Object	_	_
34-401	5920-5929	Detection	_	_
34-402	5930-5931	\|	_	_
34-403	5932-5933	\[	_	_
34-404	5933-5938	Paper	_	_
34-405	5938-5939	\]	_	_
34-406	5939-5940	(	_	_
34-407	5940-5945	https	_	_
34-408	5945-5946	:	_	_
34-409	5946-5947	/	_	_
34-410	5947-5948	/	_	_
34-411	5948-5960	ojs.aaai.org	_	_
34-412	5960-5961	/	_	_
34-413	5961-5970	index.php	_	_
34-414	5970-5971	/	_	_
34-415	5971-5975	AAAI	_	_
34-416	5975-5976	/	_	_
34-417	5976-5983	article	_	_
34-418	5983-5984	/	_	_
34-419	5984-5988	view	_	_
34-420	5988-5989	/	_	_
34-421	5989-5993	6718	_	_
34-422	5993-5994	/	_	_
34-423	5994-5998	6572	_	_
34-424	5998-5999	)	_	_
34-425	5999-6000	/	_	_
34-426	6000-6001	\[	_	_
34-427	6001-6005	Code	_	_
34-428	6005-6006	\]	_	_
34-429	6006-6007	(	_	_
34-430	6007-6012	https	_	_
34-431	6012-6013	:	_	_
34-432	6013-6014	/	_	_
34-433	6014-6015	/	_	_
34-434	6015-6025	github.com	_	_
34-435	6025-6026	/	_	_
34-436	6026-6034	guyuchao	_	_
34-437	6034-6035	/	_	_
34-438	6035-6045	PyramidCSA	_	_
34-439	6045-6046	)	_	_
34-440	6047-6049	07	_	_
34-441	6050-6051	\|	_	_
34-442	6052-6053	\*	_	_
34-443	6053-6054	\*	_	_
34-444	6054-6058	2020	_	_
34-445	6058-6059	\*	_	_
34-446	6059-6060	\*	_	_
34-447	6060-6061	\|	_	_
34-448	6062-6063	\*	_	_
34-449	6063-6064	\*	_	_
34-450	6064-6068	CVPR	_	_
34-451	6068-6069	\*	_	_
34-452	6069-6070	\*	_	_
34-453	6071-6072	\|	_	_
34-454	6073-6084	Multi-scale	_	_
34-455	6085-6096	Interactive	_	_
34-456	6097-6104	Network	_	_
34-457	6105-6108	for	_	_
34-458	6109-6116	Salient	_	_
34-459	6117-6123	Object	_	_
34-460	6124-6133	Detection	_	_
34-461	6134-6135	\|	_	_
34-462	6136-6137	\[	_	_
34-463	6137-6142	Paper	_	_
34-464	6142-6143	\]	_	_
34-465	6143-6144	(	_	_
34-466	6144-6149	https	_	_
34-467	6149-6150	:	_	_
34-468	6150-6151	/	_	_
34-469	6151-6152	/	_	_
34-470	6152-6173	openaccess.thecvf.com	_	_
34-471	6173-6174	/	_	_
34-472	6174-6186	content\_CVPR	_	_
34-472	6182-6186	CVPR	_	_
34-473	6186-6187	\_	_	_
34-474	6187-6191	2020	_	_
34-475	6191-6192	/	_	_
34-476	6192-6198	papers	_	_
34-477	6198-6199	/	_	_
34-478	6199-6269	Pang\_Multi-Scale\_Interactive\_Network\_for\_Salient\_Object\_Detection\_CVPR	_	_
34-478	6265-6269	CVPR	_	_
34-479	6269-6270	\_	_	_
34-480	6270-6274	2020	_	_
34-481	6274-6275	\_	_	_
34-482	6275-6284	paper.pdf	_	_
34-483	6284-6285	)	_	_
34-484	6285-6286	/	_	_
34-485	6286-6287	\[	_	_
34-486	6287-6291	Code	_	_
34-487	6291-6292	\]	_	_
34-488	6292-6293	(	_	_
34-489	6293-6298	https	_	_
34-490	6298-6299	:	_	_
34-491	6299-6300	/	_	_
34-492	6300-6301	/	_	_
34-493	6301-6311	github.com	_	_
34-494	6311-6312	/	_	_
34-495	6312-6320	lartpang	_	_
34-496	6320-6321	/	_	_
34-497	6321-6326	MINet	_	_
34-498	6326-6327	)	_	_
34-499	6328-6330	08	_	_
34-500	6331-6332	\|	_	_
34-501	6333-6334	\*	_	_
34-502	6334-6335	\*	_	_
34-503	6335-6339	2020	_	_
34-504	6339-6340	\*	_	_
34-505	6340-6341	\*	_	_
34-506	6341-6342	\|	_	_
34-507	6343-6344	\*	_	_
34-508	6344-6345	\*	_	_
34-509	6345-6349	CVPR	_	_
34-510	6349-6350	\*	_	_
34-511	6350-6351	\*	_	_
34-512	6352-6353	\|	_	_
34-513	6354-6359	Label	_	_
34-514	6360-6370	Decoupling	_	_
34-515	6371-6380	Framework	_	_
34-516	6381-6384	for	_	_
34-517	6385-6392	Salient	_	_
34-518	6393-6399	Object	_	_
34-519	6400-6409	Detection	_	_
34-520	6410-6411	\|	_	_
34-521	6412-6413	\[	_	_
34-522	6413-6418	Paper	_	_
34-523	6418-6419	\]	_	_
34-524	6419-6420	(	_	_
34-525	6420-6425	https	_	_
34-526	6425-6426	:	_	_
34-527	6426-6427	/	_	_
34-528	6427-6428	/	_	_
34-529	6428-6449	openaccess.thecvf.com	_	_
34-530	6449-6450	/	_	_
34-531	6450-6462	content\_CVPR	_	_
34-531	6458-6462	CVPR	_	_
34-532	6462-6463	\_	_	_
34-533	6463-6467	2020	_	_
34-534	6467-6468	/	_	_
34-535	6468-6474	papers	_	_
34-536	6474-6475	/	_	_
34-537	6475-6539	Wei\_Label\_Decoupling\_Framework\_for\_Salient\_Object\_Detection\_CVPR	_	_
34-537	6535-6539	CVPR	_	_
34-538	6539-6540	\_	_	_
34-539	6540-6544	2020	_	_
34-540	6544-6545	\_	_	_
34-541	6545-6554	paper.pdf	_	_
34-542	6554-6555	)	_	_
34-543	6555-6556	/	_	_
34-544	6556-6557	\[	_	_
34-545	6557-6561	Code	_	_
34-546	6561-6562	\]	_	_
34-547	6562-6563	(	_	_
34-548	6563-6568	https	_	_
34-549	6568-6569	:	_	_
34-550	6569-6570	/	_	_
34-551	6570-6571	/	_	_
34-552	6571-6581	github.com	_	_
34-553	6581-6582	/	_	_
34-554	6582-6590	weijun88	_	_
34-555	6590-6591	/	_	_
34-556	6591-6594	LDF	_	_
34-557	6594-6595	)	_	_
34-558	6596-6598	09	_	_
34-559	6599-6600	\|	_	_
34-560	6601-6602	\*	_	_
34-561	6602-6603	\*	_	_
34-562	6603-6607	2020	_	_
34-563	6607-6608	\*	_	_
34-564	6608-6609	\*	_	_
34-565	6609-6610	\|	_	_
34-566	6611-6612	\*	_	_
34-567	6612-6613	\*	_	_
34-568	6613-6617	ECCV	_	_
34-569	6617-6618	\*	_	_
34-570	6618-6619	\*	_	_
34-571	6620-6621	\|	_	_
34-572	6622-6628	Highly	_	_
34-573	6629-6638	Efficient	_	_
34-574	6639-6646	Salient	_	_
34-575	6647-6653	Object	_	_
34-576	6654-6663	Detection	_	_
34-577	6664-6668	with	_	_
34-578	6669-6673	100K	_	_
34-579	6674-6684	Parameters	_	_
34-580	6685-6686	\|	_	_
34-581	6687-6688	\[	_	_
34-582	6688-6693	Paper	_	_
34-583	6693-6694	\]	_	_
34-584	6694-6695	(	_	_
34-585	6695-6699	http	_	_
34-586	6699-6700	:	_	_
34-587	6700-6701	/	_	_
34-588	6701-6702	/	_	_
34-589	6702-6718	mftp.mmcheng.net	_	_
34-590	6718-6719	/	_	_
34-591	6719-6725	Papers	_	_
34-592	6725-6726	/	_	_
34-593	6726-6743	20EccvSal100k.pdf	_	_
34-594	6743-6744	)	_	_
34-595	6744-6745	/	_	_
34-596	6745-6746	\[	_	_
34-597	6746-6750	Code	_	_
34-598	6750-6751	\]	_	_
34-599	6751-6752	(	_	_
34-600	6752-6757	https	_	_
34-601	6757-6758	:	_	_
34-602	6758-6759	/	_	_
34-603	6759-6760	/	_	_
34-604	6760-6770	github.com	_	_
34-605	6770-6771	/	_	_
34-606	6771-6783	ShangHua-Gao	_	_
34-607	6783-6784	/	_	_
34-608	6784-6791	SOD100K	_	_
34-609	6791-6792	)	_	_
34-610	6793-6795	10	_	_
34-611	6796-6797	\|	_	_
34-612	6798-6799	\*	_	_
34-613	6799-6800	\*	_	_
34-614	6800-6804	2020	_	_
34-615	6804-6805	\*	_	_
34-616	6805-6806	\*	_	_
34-617	6806-6807	\|	_	_
34-618	6808-6809	\*	_	_
34-619	6809-6810	\*	_	_
34-620	6810-6814	ECCV	_	_
34-621	6814-6815	\*	_	_
34-622	6815-6816	\*	_	_
34-623	6817-6818	\|	_	_
34-624	6819-6827	Suppress	_	_
34-625	6828-6831	and	_	_
34-626	6832-6839	Balance	_	_
34-627	6839-6840	:	_	_
34-628	6841-6842	A	_	_
34-629	6843-6849	Simple	_	_
34-630	6850-6855	Gated	_	_
34-631	6856-6863	Network	_	_
34-632	6864-6867	for	_	_
34-633	6868-6875	Salient	_	_
34-634	6876-6882	Object	_	_
34-635	6883-6892	Detection	_	_
34-636	6893-6894	\|	_	_
34-637	6895-6896	\[	_	_
34-638	6896-6901	Paper	_	_
34-639	6901-6902	\]	_	_
34-640	6902-6903	(	_	_
34-641	6903-6908	https	_	_
34-642	6908-6909	:	_	_
34-643	6909-6910	/	_	_
34-644	6910-6911	/	_	_
34-645	6911-6923	www.ecva.net	_	_
34-646	6923-6924	/	_	_
34-647	6924-6930	papers	_	_
34-648	6930-6931	/	_	_
34-649	6931-6935	eccv	_	_
34-650	6935-6936	\_	_	_
34-651	6936-6940	2020	_	_
34-652	6940-6941	/	_	_
34-653	6941-6952	papers\_ECCV	_	_
34-653	6948-6952	ECCV	_	_
34-654	6952-6953	/	_	_
34-655	6953-6959	papers	_	_
34-656	6959-6960	/	_	_
34-657	6960-6969	123470035	_	_
34-658	6969-6970	.	_	_
34-659	6970-6973	pdf	_	_
34-660	6973-6974	)	_	_
34-661	6974-6975	/	_	_
34-662	6975-6976	\[	_	_
34-663	6976-6980	Code	_	_
34-664	6980-6981	\]	_	_
34-665	6981-6982	(	_	_
34-666	6982-6987	https	_	_
34-667	6987-6988	:	_	_
34-668	6988-6989	/	_	_
34-669	6989-6990	/	_	_
34-670	6990-7000	github.com	_	_
34-671	7000-7001	/	_	_
34-672	7001-7017	Xiaoqi-Zhao-DLUT	_	_
34-673	7017-7018	/	_	_
34-674	7018-7038	GateNet-RGB-Saliency	_	_
34-675	7038-7039	)	_	_
34-676	7040-7042	11	_	_
34-677	7043-7044	\|	_	_
34-678	7045-7046	\*	_	_
34-679	7046-7047	\*	_	_
34-680	7047-7051	2020	_	_
34-681	7051-7052	\*	_	_
34-682	7052-7053	\*	_	_
34-683	7053-7054	\|	_	_
34-684	7055-7056	\*	_	_
34-685	7056-7057	\*	_	_
34-686	7057-7061	BMVC	_	_
34-687	7061-7062	\*	_	_
34-688	7062-7063	\*	_	_
34-689	7064-7065	\|	_	_
34-690	7066-7072	Making	_	_
34-691	7073-7074	a	_	_
34-692	7075-7079	Case	_	_
34-693	7080-7083	for	_	_
34-694	7084-7086	3D	_	_
34-695	7087-7099	Convolutions	_	_
34-696	7100-7103	for	_	_
34-697	7104-7110	Object	_	_
34-698	7111-7123	Segmentation	_	_
34-699	7124-7126	in	_	_
34-700	7127-7133	Videos	_	_
34-701	7134-7135	\|	_	_
34-702	7136-7137	\[	_	_
34-703	7137-7142	Paper	_	_
34-704	7142-7143	\]	_	_
34-705	7143-7144	(	_	_
34-706	7144-7149	https	_	_
34-707	7149-7150	:	_	_
34-708	7150-7151	/	_	_
34-709	7151-7152	/	_	_
34-710	7152-7164	www.bmvc2020	_	_
34-711	7164-7165	-	_	_
34-712	7165-7179	conference.com	_	_
34-713	7179-7180	/	_	_
34-714	7180-7186	assets	_	_
34-715	7186-7187	/	_	_
34-716	7187-7193	papers	_	_
34-717	7193-7194	/	_	_
34-718	7194-7198	0233	_	_
34-719	7198-7199	.	_	_
34-720	7199-7202	pdf	_	_
34-721	7202-7203	)	_	_
34-722	7203-7204	/	_	_
34-723	7204-7205	\[	_	_
34-724	7205-7209	Code	_	_
34-725	7209-7210	\]	_	_
34-726	7210-7211	(	_	_
34-727	7211-7216	https	_	_
34-728	7216-7217	:	_	_
34-729	7217-7218	/	_	_
34-730	7218-7219	/	_	_
34-731	7219-7229	github.com	_	_
34-732	7229-7230	/	_	_
34-733	7230-7237	sabarim	_	_
34-734	7237-7238	/	_	_
34-735	7238-7245	3DC-Seg	_	_
34-736	7245-7246	)	_	_
34-737	7247-7249	12	_	_
34-738	7250-7251	\|	_	_
34-739	7252-7253	\*	_	_
34-740	7253-7254	\*	_	_
34-741	7254-7258	2020	_	_
34-742	7258-7259	\*	_	_
34-743	7259-7260	\*	_	_
34-744	7260-7261	\|	_	_
34-745	7262-7263	\*	_	_
34-746	7263-7264	\*	_	_
34-747	7264-7267	SPL	_	_
34-748	7267-7268	\*	_	_
34-749	7268-7269	\*	_	_
34-750	7270-7271	\|	_	_
34-751	7272-7277	FANet	_	_
34-752	7277-7278	:	_	_
34-753	7279-7287	Features	_	_
34-754	7288-7298	Adaptation	_	_
34-755	7299-7306	Network	_	_
34-756	7307-7310	for	_	_
34-757	7311-7314	360	_	_
34-758	7314-7315	°	_	_
34-759	7316-7331	Omnidirectional	_	_
34-760	7332-7339	Salient	_	_
34-761	7340-7346	Object	_	_
34-762	7347-7356	Detection	_	_
34-763	7357-7358	\|	_	_
34-764	7359-7360	\[	_	_
34-765	7360-7365	Paper	_	_
34-766	7365-7366	\]	_	_
34-767	7366-7367	(	_	_
34-768	7367-7372	https	_	_
34-769	7372-7373	:	_	_
34-770	7373-7374	/	_	_
34-771	7374-7375	/	_	_
34-772	7375-7394	ieeexplore.ieee.org	_	_
34-773	7394-7395	/	_	_
34-774	7395-7403	document	_	_
34-775	7403-7404	/	_	_
34-776	7404-7411	9211754	_	_
34-777	7411-7412	)	_	_
34-778	7412-7413	/	_	_
34-779	7413-7414	\[	_	_
34-780	7414-7418	Code	_	_
34-781	7418-7419	\]	_	_
34-782	7419-7420	(	_	_
34-783	7420-7425	https	_	_
34-784	7425-7426	:	_	_
34-785	7426-7427	/	_	_
34-786	7427-7428	/	_	_
34-787	7428-7438	github.com	_	_
34-788	7438-7439	/	_	_
34-789	7439-7450	DreaMKHuang	_	_
34-790	7450-7451	/	_	_
34-791	7451-7456	FANet	_	_
34-792	7456-7457	)	_	_
34-793	7458-7460	13	_	_
34-794	7461-7462	\|	_	_
34-795	7463-7464	\*	_	_
34-796	7464-7465	\*	_	_
34-797	7465-7469	2021	_	_
34-798	7469-7470	\*	_	_
34-799	7470-7471	\*	_	_
34-800	7471-7472	\|	_	_
34-801	7473-7474	\*	_	_
34-802	7474-7475	\*	_	_
34-803	7475-7479	CVPR	_	_
34-804	7479-7480	\*	_	_
34-805	7480-7481	\*	_	_
34-806	7482-7483	\|	_	_
34-807	7484-7494	Reciprocal	_	_
34-808	7495-7510	Transformations	_	_
34-809	7511-7514	for	_	_
34-810	7515-7527	Unsupervised	_	_
34-811	7528-7533	Video	_	_
34-812	7534-7540	Object	_	_
34-813	7541-7553	Segmentation	_	_
34-814	7554-7555	\|	_	_
34-815	7556-7557	\[	_	_
34-816	7557-7562	Paper	_	_
34-817	7562-7563	\]	_	_
34-818	7563-7564	(	_	_
34-819	7564-7569	https	_	_
34-820	7569-7570	:	_	_
34-821	7570-7571	/	_	_
34-822	7571-7572	/	_	_
34-823	7572-7593	openaccess.thecvf.com	_	_
34-824	7593-7594	/	_	_
34-825	7594-7601	content	_	_
34-826	7601-7602	/	_	_
34-827	7602-7610	CVPR2021	_	_
34-828	7610-7611	/	_	_
34-829	7611-7617	papers	_	_
34-830	7617-7618	/	_	_
34-831	7618-7696	Ren\_Reciprocal\_Transformations\_for\_Unsupervised\_Video\_Object\_Segmentation\_CVPR	_	_
34-831	7692-7696	CVPR	_	_
34-832	7696-7697	\_	_	_
34-833	7697-7701	2021	_	_
34-834	7701-7702	\_	_	_
34-835	7702-7711	paper.pdf	_	_
34-836	7711-7712	)	_	_
34-837	7712-7713	/	_	_
34-838	7713-7714	\[	_	_
34-839	7714-7718	Code	_	_
34-840	7718-7719	\]	_	_
34-841	7719-7720	(	_	_
34-842	7720-7725	https	_	_
34-843	7725-7726	:	_	_
34-844	7726-7727	/	_	_
34-845	7727-7728	/	_	_
34-846	7728-7738	github.com	_	_
34-847	7738-7739	/	_	_
34-848	7739-7750	OliverRensu	_	_
34-849	7750-7751	/	_	_
34-850	7751-7756	RTNet	_	_
34-851	7756-7757	)	_	_
34-852	7759-7760	-	_	_
34-853	7760-7761	-	_	_
34-854	7761-7762	-	_	_
34-855	7762-7763	-	_	_
34-856	7763-7764	-	_	_
34-857	7764-7765	-	_	_
34-858	7767-7768	#	_	_
34-859	7769-7776	CAV-Net	_	_
34-860	7778-7781	The	_	_
34-861	7782-7787	codes	_	_
34-862	7788-7791	are	_	_
34-863	7792-7801	available	_	_
34-864	7802-7804	at	_	_
34-865	7805-7806	\[	_	_
34-866	7806-7809	src	_	_
34-867	7809-7810	\]	_	_
34-868	7810-7811	(	_	_
34-869	7811-7816	https	_	_
34-870	7816-7817	:	_	_
34-871	7817-7818	/	_	_
34-872	7818-7819	/	_	_
34-873	7819-7829	github.com	_	_
34-874	7829-7830	/	_	_
34-875	7830-7837	PanoAsh	_	_
34-876	7837-7838	/	_	_
34-877	7838-7845	PAV-SOD	_	_
34-878	7845-7846	/	_	_
34-879	7846-7850	tree	_	_
34-880	7850-7851	/	_	_
34-881	7851-7855	main	_	_
34-882	7855-7856	/	_	_
34-883	7856-7859	src	_	_
34-884	7859-7860	)	_	_
34-885	7860-7861	.	_	_

#Text=The pre-trained models can be downloaded at \[Google Drive\](https://drive.google.com/file/d/1gNWmgmlBfJqCYE5phDuHTMFIou1TAmXs/view?
35-1	7863-7866	The	_	_
35-2	7867-7878	pre-trained	_	_
35-3	7879-7885	models	_	_
35-4	7886-7889	can	_	_
35-5	7890-7892	be	_	_
35-6	7893-7903	downloaded	_	_
35-7	7904-7906	at	_	_
35-8	7907-7908	\[	_	_
35-9	7908-7914	Google	_	_
35-10	7915-7920	Drive	_	_
35-11	7920-7921	\]	_	_
35-12	7921-7922	(	_	_
35-13	7922-7927	https	_	_
35-14	7927-7928	:	_	_
35-15	7928-7929	/	_	_
35-16	7929-7930	/	_	_
35-17	7930-7946	drive.google.com	_	_
35-18	7946-7947	/	_	_
35-19	7947-7951	file	_	_
35-20	7951-7952	/	_	_
35-21	7952-7953	d	_	_
35-22	7953-7954	/	_	_
35-23	7954-7987	1gNWmgmlBfJqCYE5phDuHTMFIou1TAmXs	_	_
35-24	7987-7988	/	_	_
35-25	7988-7992	view	_	_
35-26	7992-7993	?	_	_

#Text=usp=sharing).  ------  # Dataset Downloads  The whole object-/instance-level ground truth with default split can be downloaded from \[Google Drive\](https://drive.google.com/file/d/1Whp\_ftuXza8-vkjNtICdxdRebcmzcrFi/view?
36-1	7993-7996	usp	_	_
36-2	7996-7997	=	_	_
36-3	7997-8004	sharing	_	_
36-4	8004-8005	)	_	_
36-5	8005-8006	.	_	_
36-6	8008-8009	-	_	_
36-7	8009-8010	-	_	_
36-8	8010-8011	-	_	_
36-9	8011-8012	-	_	_
36-10	8012-8013	-	_	_
36-11	8013-8014	-	_	_
36-12	8016-8017	#	_	_
36-13	8018-8025	Dataset	_	_
36-14	8026-8035	Downloads	_	_
36-15	8037-8040	The	_	_
36-16	8041-8046	whole	_	_
36-17	8047-8053	object	_	_
36-18	8053-8054	-	_	_
36-19	8054-8055	/	_	_
36-20	8055-8069	instance-level	_	_
36-21	8070-8076	ground	_	_
36-22	8077-8082	truth	_	_
36-23	8083-8087	with	_	_
36-24	8088-8095	default	_	_
36-25	8096-8101	split	_	_
36-26	8102-8105	can	_	_
36-27	8106-8108	be	_	_
36-28	8109-8119	downloaded	_	_
36-29	8120-8124	from	_	_
36-30	8125-8126	\[	_	_
36-31	8126-8132	Google	_	_
36-32	8133-8138	Drive	_	_
36-33	8138-8139	\]	_	_
36-34	8139-8140	(	_	_
36-35	8140-8145	https	_	_
36-36	8145-8146	:	_	_
36-37	8146-8147	/	_	_
36-38	8147-8148	/	_	_
36-39	8148-8164	drive.google.com	_	_
36-40	8164-8165	/	_	_
36-41	8165-8169	file	_	_
36-42	8169-8170	/	_	_
36-43	8170-8171	d	_	_
36-44	8171-8172	/	_	_
36-45	8172-8184	1Whp\_ftuXza8	_	_
36-46	8184-8185	-	_	_
36-47	8185-8205	vkjNtICdxdRebcmzcrFi	_	_
36-48	8205-8206	/	_	_
36-49	8206-8210	view	_	_
36-50	8210-8211	?	_	_

#Text=usp=sharing).
37-1	8211-8214	usp	_	_
37-2	8214-8215	=	_	_
37-3	8215-8222	sharing	_	_
37-4	8222-8223	)	_	_
37-5	8223-8224	.	_	_

#Text=The videos (with ambisonics) with default split can be downloaded from \[Google Drive\](https://drive.google.com/file/d/13FEv1yAyMmK4GkiZ2Mce6gJxQuME7vG3/view?
38-1	8226-8229	The	_	_
38-2	8230-8236	videos	_	_
38-3	8237-8238	(	_	_
38-4	8238-8242	with	_	_
38-5	8243-8253	ambisonics	_	_
38-6	8253-8254	)	_	_
38-7	8255-8259	with	_	_
38-8	8260-8267	default	_	_
38-9	8268-8273	split	_	_
38-10	8274-8277	can	_	_
38-11	8278-8280	be	_	_
38-12	8281-8291	downloaded	_	_
38-13	8292-8296	from	_	_
38-14	8297-8298	\[	_	_
38-15	8298-8304	Google	_	_
38-16	8305-8310	Drive	_	_
38-17	8310-8311	\]	_	_
38-18	8311-8312	(	_	_
38-19	8312-8317	https	_	_
38-20	8317-8318	:	_	_
38-21	8318-8319	/	_	_
38-22	8319-8320	/	_	_
38-23	8320-8336	drive.google.com	_	_
38-24	8336-8337	/	_	_
38-25	8337-8341	file	_	_
38-26	8341-8342	/	_	_
38-27	8342-8343	d	_	_
38-28	8343-8344	/	_	_
38-29	8344-8377	13FEv1yAyMmK4GkiZ2Mce6gJxQuME7vG3	_	_
38-30	8377-8378	/	_	_
38-31	8378-8382	view	_	_
38-32	8382-8383	?	_	_

#Text=usp=sharing).
39-1	8383-8386	usp	_	_
39-2	8386-8387	=	_	_
39-3	8387-8394	sharing	_	_
39-4	8394-8395	)	_	_
39-5	8395-8396	.	_	_

#Text=The videos (with mono sound) can be downloaded from \[Google Drive\](https://drive.google.com/file/d/1klJnHSiUM7Ow2LkdaLe-O6CEQ9qmdo2F/view?
40-1	8399-8402	The	_	_
40-2	8403-8409	videos	_	_
40-3	8410-8411	(	_	_
40-4	8411-8415	with	_	_
40-5	8416-8420	mono	_	_
40-6	8421-8426	sound	_	_
40-7	8426-8427	)	_	_
40-8	8428-8431	can	_	_
40-9	8432-8434	be	_	_
40-10	8435-8445	downloaded	_	_
40-11	8446-8450	from	_	_
40-12	8451-8452	\[	_	_
40-13	8452-8458	Google	_	_
40-14	8459-8464	Drive	_	_
40-15	8464-8465	\]	_	_
40-16	8465-8466	(	_	_
40-17	8466-8471	https	_	_
40-18	8471-8472	:	_	_
40-19	8472-8473	/	_	_
40-20	8473-8474	/	_	_
40-21	8474-8490	drive.google.com	_	_
40-22	8490-8491	/	_	_
40-23	8491-8495	file	_	_
40-24	8495-8496	/	_	_
40-25	8496-8497	d	_	_
40-26	8497-8498	/	_	_
40-27	8498-8531	1klJnHSiUM7Ow2LkdaLe-O6CEQ9qmdo2F	_	_
40-28	8531-8532	/	_	_
40-29	8532-8536	view	_	_
40-30	8536-8537	?	_	_

#Text=usp=sharing)  The audio files (.wav) can be downloaded from \[Google Drive\](https://drive.google.com/file/d/1-jqDArcm8vBhyku3Xb8HLopG1XmpAS13/view?
41-1	8537-8540	usp	_	_
41-2	8540-8541	=	_	_
41-3	8541-8548	sharing	_	_
41-4	8548-8549	)	_	_
41-5	8551-8554	The	_	_
41-6	8555-8560	audio	_	_
41-7	8561-8566	files	_	_
41-8	8567-8568	(	_	_
41-9	8568-8569	.	_	_
41-10	8569-8572	wav	_	_
41-11	8572-8573	)	_	_
41-12	8574-8577	can	_	_
41-13	8578-8580	be	_	_
41-14	8581-8591	downloaded	_	_
41-15	8592-8596	from	_	_
41-16	8597-8598	\[	_	_
41-17	8598-8604	Google	_	_
41-18	8605-8610	Drive	_	_
41-19	8610-8611	\]	_	_
41-20	8611-8612	(	_	_
41-21	8612-8617	https	_	_
41-22	8617-8618	:	_	_
41-23	8618-8619	/	_	_
41-24	8619-8620	/	_	_
41-25	8620-8636	drive.google.com	_	_
41-26	8636-8637	/	_	_
41-27	8637-8641	file	_	_
41-28	8641-8642	/	_	_
41-29	8642-8643	d	_	_
41-30	8643-8644	/	_	_
41-31	8644-8645	1	_	_
41-32	8645-8646	-	_	_
41-33	8646-8677	jqDArcm8vBhyku3Xb8HLopG1XmpAS13	_	_
41-34	8677-8678	/	_	_
41-35	8678-8682	view	_	_
41-36	8682-8683	?	_	_

#Text=usp=sharing).
42-1	8683-8686	usp	_	_
42-2	8686-8687	=	_	_
42-3	8687-8694	sharing	_	_
42-4	8694-8695	)	_	_
42-5	8695-8696	.	_	_

#Text=The head movement and eye fixation data can be downloaded from \[Google Drive\](https://drive.google.com/drive/folders/1EpWc7GVcGFAn5VigV3c2-ZtIZElfXPX1?
43-1	8698-8701	The	_	_
43-2	8702-8706	head	_	_
43-3	8707-8715	movement	_	_
43-4	8716-8719	and	_	_
43-5	8720-8723	eye	_	_
43-6	8724-8732	fixation	_	_
43-7	8733-8737	data	_	_
43-8	8738-8741	can	_	_
43-9	8742-8744	be	_	_
43-10	8745-8755	downloaded	_	_
43-11	8756-8760	from	_	_
43-12	8761-8762	\[	_	_
43-13	8762-8768	Google	_	_
43-14	8769-8774	Drive	_	_
43-15	8774-8775	\]	_	_
43-16	8775-8776	(	_	_
43-17	8776-8781	https	_	_
43-18	8781-8782	:	_	_
43-19	8782-8783	/	_	_
43-20	8783-8784	/	_	_
43-21	8784-8800	drive.google.com	_	_
43-22	8800-8801	/	_	_
43-23	8801-8806	drive	_	_
43-24	8806-8807	/	_	_
43-25	8807-8814	folders	_	_
43-26	8814-8815	/	_	_
43-27	8815-8836	1EpWc7GVcGFAn5VigV3c2	_	_
43-28	8836-8837	-	_	_
43-29	8837-8848	ZtIZElfXPX1	_	_
43-30	8848-8849	?	_	_

#Text=usp=sharing).
44-1	8849-8852	usp	_	_
44-2	8852-8853	=	_	_
44-3	8853-8860	sharing	_	_
44-4	8860-8861	)	_	_
44-5	8861-8862	.	_	_

#Text=To generate video frames, please refer to \[video\_to\_frames.py\](https://github.com/PanoAsh/ASOD60K/blob/main/video\_to\_frames.py).
45-1	8864-8866	To	_	_
45-2	8867-8875	generate	_	_
45-3	8876-8881	video	_	_
45-4	8882-8888	frames	_	_
45-5	8888-8889	,	_	_
45-6	8890-8896	please	_	_
45-7	8897-8902	refer	_	_
45-8	8903-8905	to	_	_
45-9	8906-8907	\[	_	_
45-10	8907-8925	video\_to\_frames.py	_	_
45-11	8925-8926	\]	_	_
45-12	8926-8927	(	_	_
45-13	8927-8932	https	_	_
45-14	8932-8933	:	_	_
45-15	8933-8934	/	_	_
45-16	8934-8935	/	_	_
45-17	8935-8945	github.com	_	_
45-18	8945-8946	/	_	_
45-19	8946-8953	PanoAsh	_	_
45-20	8953-8954	/	_	_
45-21	8954-8961	ASOD60K	_	_
45-22	8961-8962	/	_	_
45-23	8962-8966	blob	_	_
45-24	8966-8967	/	_	_
45-25	8967-8971	main	_	_
45-26	8971-8972	/	_	_
45-27	8972-8990	video\_to\_frames.py	_	_
45-28	8990-8991	)	_	_
45-29	8991-8992	.	_	_

#Text=To get access to raw videos on YouTube, please refer to \[video\_seq\_link\](https://github.com/PanoAsh/ASOD60K/blob/main/video\_seq\_link)
46-1	8994-8996	To	_	_
46-2	8997-9000	get	_	_
46-3	9001-9007	access	_	_
46-4	9008-9010	to	_	_
46-5	9011-9014	raw	_	_
46-6	9015-9021	videos	_	_
46-7	9022-9024	on	_	_
46-8	9025-9032	YouTube	_	_
46-9	9032-9033	,	_	_
46-10	9034-9040	please	_	_
46-11	9041-9046	refer	_	_
46-12	9047-9049	to	_	_
46-13	9050-9051	\[	_	_
46-14	9051-9065	video\_seq\_link	_	_
46-15	9065-9066	\]	_	_
46-16	9066-9067	(	_	_
46-17	9067-9072	https	_	_
46-18	9072-9073	:	_	_
46-19	9073-9074	/	_	_
46-20	9074-9075	/	_	_
46-21	9075-9085	github.com	_	_
46-22	9085-9086	/	_	_
46-23	9086-9093	PanoAsh	_	_
46-24	9093-9094	/	_	_
46-25	9094-9101	ASOD60K	_	_
46-26	9101-9102	/	_	_
46-27	9102-9106	blob	_	_
46-28	9106-9107	/	_	_
46-29	9107-9111	main	_	_
46-30	9111-9112	/	_	_
46-31	9112-9126	video\_seq\_link	_	_
46-32	9126-9127	)	_	_

#Text=.
47-1	9127-9128	.	_	_

#Text=> Note: The PAVS10K dataset does not own the copyright of videos.
48-1	9131-9132	>	_	_
48-2	9133-9137	Note	_	_
48-3	9137-9138	:	_	_
48-4	9139-9142	The	_	_
48-5	9143-9150	PAVS10K	_	_
48-6	9151-9158	dataset	_	_
48-7	9159-9163	does	_	_
48-8	9164-9167	not	_	_
48-9	9168-9171	own	_	_
48-10	9172-9175	the	_	_
48-11	9176-9185	copyright	_	_
48-12	9186-9188	of	_	_
48-13	9189-9195	videos	_	_
48-14	9195-9196	.	_	_

#Text=Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to PAVS10K.  ------  # Citation          @article{zhang2023pav,       title={PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={ACM Transactions on Multimedia Computing, Communications and Applications},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).
49-1	9197-9201	Only	_	_
49-2	9202-9213	researchers	_	_
49-3	9214-9217	and	_	_
49-4	9218-9227	educators	_	_
49-5	9228-9231	who	_	_
49-6	9232-9236	wish	_	_
49-7	9237-9239	to	_	_
49-8	9240-9243	use	_	_
49-9	9244-9247	the	_	_
49-10	9248-9254	videos	_	_
49-11	9255-9258	for	_	_
49-12	9259-9273	non-commercial	_	_
49-13	9274-9284	researches	_	_
49-14	9285-9288	and	_	_
49-15	9288-9289	/	_	_
49-16	9289-9291	or	_	_
49-17	9292-9303	educational	_	_
49-18	9304-9312	purposes	_	_
49-19	9312-9313	,	_	_
49-20	9314-9318	have	_	_
49-21	9319-9325	access	_	_
49-22	9326-9328	to	_	_
49-23	9329-9336	PAVS10K	_	_
49-24	9336-9337	.	_	_
49-25	9339-9340	-	_	_
49-26	9340-9341	-	_	_
49-27	9341-9342	-	_	_
49-28	9342-9343	-	_	_
49-29	9343-9344	-	_	_
49-30	9344-9345	-	_	_
49-31	9347-9348	#	_	_
49-32	9349-9357	Citation	_	_
49-33	9367-9368	@	_	_
49-34	9368-9375	article	_	_
49-35	9375-9376	{	_	_
49-36	9376-9388	zhang2023pav	_	_
49-37	9388-9389	,	_	_
49-38	9396-9401	title	_	_
49-39	9401-9402	=	_	_
49-40	9402-9403	{	_	_
49-41	9403-9410	PAV-SOD	_	_
49-42	9410-9411	:	_	_
49-43	9412-9413	A	_	_
49-44	9414-9417	New	_	_
49-45	9418-9422	Task	_	_
49-46	9423-9430	towards	_	_
49-47	9431-9440	Panoramic	_	_
49-48	9441-9452	Audiovisual	_	_
49-49	9453-9461	Saliency	_	_
49-50	9462-9471	Detection	_	_
49-51	9471-9472	}	_	_
49-52	9472-9473	,	_	_
49-53	9480-9486	author	_	_
49-54	9486-9487	=	_	_
49-55	9487-9488	{	_	_
49-56	9488-9493	Zhang	_	_
49-57	9493-9494	,	_	_
49-58	9495-9497	Yi	_	_
49-59	9498-9501	and	_	_
49-60	9502-9506	Chao	_	_
49-61	9506-9507	,	_	_
49-62	9508-9515	Fang-Yi	_	_
49-63	9516-9519	and	_	_
49-64	9520-9530	Hamidouche	_	_
49-65	9530-9531	,	_	_
49-66	9532-9538	Wassim	_	_
49-67	9539-9542	and	_	_
49-68	9543-9551	Deforges	_	_
49-69	9551-9552	,	_	_
49-70	9553-9560	Olivier	_	_
49-71	9560-9561	}	_	_
49-72	9561-9562	,	_	_
49-73	9569-9576	journal	_	_
49-74	9576-9577	=	_	_
49-75	9577-9578	{	_	_
49-76	9578-9581	ACM	_	_
49-77	9582-9594	Transactions	_	_
49-78	9595-9597	on	_	_
49-79	9598-9608	Multimedia	_	_
49-80	9609-9618	Computing	_	_
49-81	9618-9619	,	_	_
49-82	9620-9634	Communications	_	_
49-83	9635-9638	and	_	_
49-84	9639-9651	Applications	_	_
49-85	9651-9652	}	_	_
49-86	9652-9653	,	_	_
49-87	9660-9666	volume	_	_
49-88	9666-9667	=	_	_
49-89	9667-9668	{	_	_
49-90	9668-9670	19	_	_
49-91	9670-9671	}	_	_
49-92	9671-9672	,	_	_
49-93	9679-9685	number	_	_
49-94	9685-9686	=	_	_
49-95	9686-9687	{	_	_
49-96	9687-9688	3	_	_
49-97	9688-9689	}	_	_
49-98	9689-9690	,	_	_
49-99	9697-9702	pages	_	_
49-100	9702-9703	=	_	_
49-101	9703-9704	{	_	_
49-102	9704-9705	1	_	_
49-103	9705-9706	-	_	_
49-104	9706-9707	-	_	_
49-105	9707-9709	26	_	_
49-106	9709-9710	}	_	_
49-107	9710-9711	,	_	_
49-108	9718-9722	year	_	_
49-109	9722-9723	=	_	_
49-110	9723-9724	{	_	_
49-111	9724-9728	2023	_	_
49-112	9728-9729	}	_	_
49-113	9729-9730	,	_	_
49-114	9737-9746	publisher	_	_
49-115	9746-9747	=	_	_
49-116	9747-9748	{	_	_
49-117	9748-9751	ACM	_	_
49-118	9752-9755	New	_	_
49-119	9756-9760	York	_	_
49-120	9760-9761	,	_	_
49-121	9762-9764	NY	_	_
49-122	9764-9765	}	_	_
49-123	9770-9771	}	_	_
49-124	9773-9774	-	_	_
49-125	9774-9775	-	_	_
49-126	9775-9776	-	_	_
49-127	9776-9777	-	_	_
49-128	9777-9778	-	_	_
49-129	9778-9779	-	_	_
49-130	9781-9782	#	_	_
49-131	9783-9790	Contact	_	_
49-132	9792-9801	yi23zhang	_	_
49-133	9801-9806	.2022	_	_
49-134	9806-9807	@	_	_
49-135	9807-9816	gmail.com	_	_
49-136	9817-9819	or	_	_
49-137	9821-9834	fangyichao428	_	_
49-138	9834-9835	@	_	_
49-139	9835-9844	gmail.com	_	_
49-140	9845-9846	(	_	_
49-141	9846-9849	for	_	_
49-142	9850-9857	details	_	_
49-143	9858-9860	of	_	_
49-144	9861-9865	head	_	_
49-145	9866-9874	movement	_	_
49-146	9875-9878	and	_	_
49-147	9879-9882	eye	_	_
49-148	9883-9891	fixation	_	_
49-149	9892-9896	data	_	_
49-150	9896-9897	)	_	_
49-151	9897-9898	.	_	_