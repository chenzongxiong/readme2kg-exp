```markdown
Please find the example pipeline shown below.  ### Models + LSTM - `<SOFTWARE>lstm_luong_wmt_en_de</SOFTWARE>` + Transformer - `<SOFTWARE>transformer_iwslt_de_en</SOFTWARE>` + Dynamic Conv. - `<SOFTWARE>lightconv_iwslt_de_en</SOFTWARE>`  ### BPE ``` <SOFTWARE>examples/translation/subword-nmt/apply_bpe.py</SOFTWARE> -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=<DATASET>examples/translation/iwslt14.tokenized.de-en</DATASET> <SOFTWARE>fairseq-preprocess</SOFTWARE> --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` <SOFTWARE>fairseq-train</SOFTWARE> \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch <SOFTWARE>lstm_luong_wmt_en_de</SOFTWARE> --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` <SOFTWARE>fairseq-train</SOFTWARE> \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch <SOFTWARE>transformer_iwslt_de_en</SOFTWARE> --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` <SOFTWARE>fairseq-train</SOFTWARE> \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch <SOFTWARE>lightconv_iwslt_de_en</SOFTWARE> \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` <SOFTWARE>fairseq-generate</SOFTWARE> data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring <EVALMETRIC>bleu</EVALMETRIC> --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` <SOFTWARE>fairseq-generate</SOFTWARE> data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring <EVALMETRIC>sacrebleu</EVALMETRIC> --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = "Revisit Systematic Generalization via Meaningful Learning",     author = "Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan",     booktitle = "<PUBLICATION>Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</PUBLICATION>",     month = dec,     year = "2022",     address = "Abu Dhabi, United Arab Emirates (Hybrid)",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2022.blackboxnlp-1.6",     pages = "62--79",     abstract = "Humans can systematically generalize to novel compositions of existing concepts.
```