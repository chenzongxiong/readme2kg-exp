```markdown
- ***<PROJECT>Hierarchical Mixtures of Experts (HME)</PROJECT>***  
  - **"<PUBLICATION>Hierarchical Mixtures of Experts and the EM Algorithm</PUBLICATION>"**, <PUBLICATION>Neural computation</PUBLICATION>, 1994  
    - Michael I Jordan, Robert A Jacobs *(the original HME, a tree-structured model for regression and classification.)*  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6796382)  
  - **"<PUBLICATION>Classification using hierarchical mixtures of experts</PUBLICATION>"**, <CONFERENCE>NNSP</CONFERENCE>, 1994  
    - Steve R Waterhouse, Anthony J Robinson *(each leaf expert is non-linear and performs multi-way classification)*  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/366050)  
  - **"<PUBLICATION>Bayesian Hierarchical Mixtures of Experts</PUBLICATION>"**, <PUBLICATION>arXiv</PUBLICATION>, 2012  
    - Christopher M Bishop, Naonori Ueda, Steve Waterhouse *(bayesian treatments of the HME model to prevent the severe overfitting caused by maximum likelihood)*  
    - [[Paper]](https://arxiv.org/abs/1212.2447)  

- ***Generalized HMEs in advanced frameworks***  
  - **"<PUBLICATION>Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization</PUBLICATION>"**, <CONFERENCE>CVPR</CONFERENCE>, 2020  
    - Ruyi Ji *et al.* *(incorporate convolutional operations along edges and use attention transformer modules to capture discriminative features)*  
    - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ji_Attention_Convolutional_Binary_Neural_Tree_for_Fine-Grained_Visual_Categorization_CVPR_2020_paper.html) [[Code]](https://isrc.iscas.ac.cn/gitlab/research/acnet)  
  - **"<PUBLICATION>NDT: Neual Decision Tree Towards Fully Functioned Neural Graph</PUBLICATION>"**, <PUBLICATION>arXiv</PUBLICATION>, 2017  
    - Han Xiao *(reformulate the non-differentiable information gain in the form of Dirac symbol and approximate it as a continuous function)*  
    - [[Paper]](https://arxiv.org/abs/1712.05934)  
  - **"<PUBLICATION>Decision Forests, Convolutional Networks and the Models in-Between</PUBLICATION>"**, <PUBLICATION>arXiv</PUBLICATION>, 2016  
    - Yani Ioannou *et al.* *(hybrid model between decision forests and convolutional networks)*  
    - [[Paper]](https://arxiv.org/abs/1603.01250)  
  - **"<PUBLICATION>Deep Neural Decision Trees</PUBLICATION>"**, <PUBLICATION>arXiv</PUBLICATION>, 2018  
    - Yongxin Yang, Irene Garcia Morillo, Timothy M Hospedales *(bin each feature of the input instance and determine the leaf node it will arrive)*  
    - [[Paper]](https://arxiv.org/abs/1806.06988) [[Code]](https://github.com/wOOL/DNDT)  
  - **"<PUBLICATION>ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder</PUBLICATION>"**, <CONFERENCE>ICML</CONFERENCE>, 2022  
    - Sangwon Kim, Jaeyeal Nam, Byoung Chul Ko *(transformer version of ProtoTree with expert leaves)*  
    - [[Paper]](https://proceedings.mlr.press/v162/kim22g.html) [[Code]](https://github.com/jumpsnack/ViT-NeT)  

- ***Expert NDTs with architecture search phase***  
  - **"<PUBLICATION>Adaptive Neural Trees</PUBLICATION>"**, <CONFERENCE>ICML</CONFERENCE>, 2019  
    - Ryutaro Tanno *et al.* *(greedily choosing the best option between going deeper and splitting the input space)*  
    - [[Paper]](http://proceedings.mlr.press/v97/tanno19a.html?
```