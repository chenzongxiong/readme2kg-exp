```markdown
\|E\| \geq k} \|E\|^2 + \sum_{VE \,s.t.\, \|E\| < k} \|D\|\|E\|$ </div>                            | <div style="width:500px"> Measure the penalties on tuples in a <DATASET>dataset</DATASET> after k-anonymization, reflecting how indistinguishable they are post-anonymization                                                  </div>                                        | |                        | <EVALMETRIC>Approximation Loss</EVALMETRIC> | <div style="width:300px"> $\mathcal{E}(\hat{\phi}, \mathcal{Z}, f(X)) \triangleq \mathbb{E} [\mathcal{L}(\hat{\phi}, \mathcal{Z}, f(X)) - \mathcal{L}(\phi^*, \mathcal{Z}, f(X))].$  </div>         | <div style="width:500px"> Measure the error caused by randomness added when minimizing the privacy loss as the expected deviation of the randomized explanation from the best local approximation                    </div>                               | |                        | <EVALMETRIC>Explanation Intersection</EVALMETRIC> | <div style="width:300px"> The percentage of bits in the original explanation that is retained in the privatised explanation after using differential privacy </div> | <div style="width:500px"> The higher the better but due to privacy-utility trade-off, this metric should not be 100%.
```