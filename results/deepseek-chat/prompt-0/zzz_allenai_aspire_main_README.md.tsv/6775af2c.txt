```markdown
View example usage and sample document matches here: [`examples/demo-contextualsentence-multim.ipynb`](https://github.com/<PROJECT>allenai</PROJECT>/<PROJECT>aspire</PROJECT>/blob/main/examples/demo-contextualsentence-multim.ipynb)  ##### `<SOFTWARE>SPECTER-CoCite</SOFTWARE>`  The `<SOFTWARE>SPECTER-CoCite</SOFTWARE>` bi-encoder model can be used via the `<SOFTWARE>transformers</SOFTWARE>` library as:  ```<PROGLANG>python</PROGLANG> from <SOFTWARE>transformers</SOFTWARE> import <SOFTWARE>AutoModel</SOFTWARE>, <SOFTWARE>AutoTokenizer</SOFTWARE> aspire_bienc = <SOFTWARE>AutoModel</SOFTWARE>.from_pretrained('<PROJECT>allenai</PROJECT>/<SOFTWARE>aspire-biencoder-compsci-spec</SOFTWARE>') aspire_tok = <SOFTWARE>AutoTokenizer</SOFTWARE>.from_pretrained('<PROJECT>allenai</PROJECT>/<SOFTWARE>aspire-biencoder-compsci-spec</SOFTWARE>') title = "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific "         "Document Similarity" abstract = "We present a new scientific document similarity model based on matching "            "fine-grained aspects of texts." d=[title + aspire_tok.sep_token + abstract] inputs = aspire_tok(d, padding=True, truncation=True, return_tensors="pt", max_length=512) result = aspire_bienc(**inputs) clsrep = result.last_hidden_state[:, 0, :] ```  However, note that the <SOFTWARE>Hugging Face</SOFTWARE> models don't have a set of additional scalar-mix parameters to compute a learned weighted sum of the representations from different layers of the transformer encoder.
```