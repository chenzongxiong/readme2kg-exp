```markdown
The folder structure should be organized as follows before training.  ``` <PROJECT>NuScenes-QA</PROJECT> +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- <DATASET>NuScenes_train_questions.json</DATASET> |   |   +-- <DATASET>NuScenes_val_questions.json</DATASET> |   +-- features/     # downloaded or extracted |   |   +-- <SOFTWARE>CenterPoint</SOFTWARE>/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- <SOFTWARE>BEVDet</SOFTWARE>/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- <SOFTWARE>MSMDFusion</SOFTWARE>/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash <PROGLANG>python</PROGLANG> >= 3.5 <SOFTWARE>CUDA</SOFTWARE> >= 9.0 <SOFTWARE>PyTorch</SOFTWARE> >= 1.4.0 <SOFTWARE>SpaCy</SOFTWARE> == 2.1.0 ```  For the <SOFTWARE>SpaCy</SOFTWARE>, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `<SOFTWARE>CenterPoint</SOFTWARE>` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='<SOFTWARE>CenterPoint</SOFTWARE>' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='<SOFTWARE>CenterPoint</SOFTWARE>' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={<PUBLICATION>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</PUBLICATION>},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={<PUBLICATION>arXiv preprint arXiv:2305.14836</PUBLICATION>},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [<SOFTWARE>MMDetection3D</SOFTWARE>](https://github.com/open-mmlab/<SOFTWARE>mmdetection3d</SOFTWARE>) and [<SOFTWARE>OpenVQA</SOFTWARE>](https://github.com/MILVLG/<SOFTWARE>openvqa</SOFTWARE>) for open sourcing their methods.
```