```markdown
- <DATASET>English test_none</DATASET> dataset (373 instances)<br/>  /datasets/test/<DATASET>tsar2022_en_test_none</DATASET>.tsv    - <DATASET>Spanish test_none</DATASET> dataset (368 instances)<br/>  /datasets/test/<DATASET>tsar2022_es_test_none</DATASET>.tsv    - <DATASET>Portuguese test_none</DATASET> dataset (374 instances)<br/>  /datasets/test/<DATASET>tsar2022_pt_test_none</DATASET>.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - <DATASET>English test_gold</DATASET> dataset (373 instances)<br/>  /datasets/test/<DATASET>tsar2022_en_test_gold</DATASET>.tsv    - <DATASET>Spanish test_gold</DATASET> dataset (368 instances)<br/>  /datasets/test/<DATASET>tsar2022_es_test_gold</DATASET>.tsv    - <DATASET>Portuguese test_gold</DATASET> dataset (374 instances)<br/>  /datasets/test/<DATASET>tsar2022_pt_test_gold</DATASET>.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  <EVALMETRIC>MAP@1</EVALMETRIC>/<EVALMETRIC>Potential@1</EVALMETRIC>/<EVALMETRIC>Precision@1</EVALMETRIC> -  <EVALMETRIC>MAP@3</EVALMETRIC> -  <EVALMETRIC>MAP@5</EVALMETRIC> -  <EVALMETRIC>MAP@10</EVALMETRIC> -  <EVALMETRIC>Potential@3</EVALMETRIC> -  <EVALMETRIC>Potential@5</EVALMETRIC> -  <EVALMETRIC>Potential@10</EVALMETRIC> -  <EVALMETRIC>Accuracy@1@top_gold_1</EVALMETRIC> -  <EVALMETRIC>Accuracy@2@top_gold_1</EVALMETRIC> -  <EVALMETRIC>Accuracy@3@top_gold_1</EVALMETRIC>    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  <EVALMETRIC>Potential@K</EVALMETRIC>  K={1..10}  -  <EVALMETRIC>MAP@K</EVALMETRIC>  K={1..10} -  <EVALMETRIC>Precision@K</EVALMETRIC>  K={1..10}  (macro-average) -  <EVALMETRIC>Recall@K</EVALMETRIC>  K={1..10}     (macro-average) -  <EVALMETRIC>Accuracy@K@top_gold_1</EVALMETRIC>   K={1..10}     ## Evaluation Scripts   ### <SOFTWARE>tsar_eval.py</SOFTWARE>  This script evaluates the following metric:      -  <EVALMETRIC>MAP@1</EVALMETRIC>/<EVALMETRIC>Potential@1</EVALMETRIC>/<EVALMETRIC>Precision@1</EVALMETRIC>     -  <EVALMETRIC>MAP@3</EVALMETRIC>     -  <EVALMETRIC>MAP@5</EVALMETRIC>     -  <EVALMETRIC>MAP@10</EVALMETRIC>     -  <EVALMETRIC>Potential@3</EVALMETRIC>     -  <EVALMETRIC>Potential@5</EVALMETRIC>     -  <EVALMETRIC>Potential@10</EVALMETRIC>     -  <EVALMETRIC>Accuracy@1@top_gold_1</EVALMETRIC>     -  <EVALMETRIC>Accuracy@2@top_gold_1</EVALMETRIC>     -  <EVALMETRIC>Accuracy@3@top_gold_1</EVALMETRIC>          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: <SOFTWARE>tsar_eval.py</SOFTWARE> <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .
```