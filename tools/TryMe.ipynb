{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b957bb-3508-4c00-8abc-9547198a8549",
   "metadata": {},
   "source": [
    "# Read/Write with zongxiong's webanno parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64156087-5576-4ff5-994a-b47b96c01d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from webanno_tsv import webanno_tsv_read_file, webanno_tsv_write, Annotation\n",
    "from dataclasses import dataclass, replace\n",
    "import os\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de617cf-e8a3-4608-8337-2ba134308986",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9568d7b-38cb-48c7-923c-06d22b6041d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  6\n"
     ]
    }
   ],
   "source": [
    "# Read the WebAnno TSV file\n",
    "file_name = '231sm_Low_Resource_KBP_master_README.md.tsv'\n",
    "input_file_path = f'../data/train/{file_name}'  # Replace with the path to the provided WebAnno TSV file\n",
    "ref_doc = webanno_tsv_read_file(input_file_path)\n",
    "print('number of ref doc annotations: ', len(ref_doc.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4339a150-bed3-4aef-a013-f710b772011b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Annotation(tokens=[Token(sentence_idx=1, idx=14, start=86, end=97, text='Few-Shot_ED')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='DATASET', label_id=-1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the first annoatation\n",
    "ref_doc.annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aeb917b-0d34-42f9-8695-5a1298b551d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(idx=1, text='# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the first sentence\n",
    "ref_doc.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c218fb4-bd5d-4dc0-a164-29161d5a338a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(sentence_idx=1, idx=1, start=0, end=1, text='#'),\n",
       " Token(sentence_idx=1, idx=2, start=2, end=18, text='Low_Resource_KBP'),\n",
       " Token(sentence_idx=1, idx=3, start=19, end=28, text='knowledge'),\n",
       " Token(sentence_idx=1, idx=4, start=29, end=34, text='graph'),\n",
       " Token(sentence_idx=1, idx=5, start=35, end=45, text='population'),\n",
       " Token(sentence_idx=1, idx=6, start=46, end=48, text='in'),\n",
       " Token(sentence_idx=1, idx=7, start=49, end=52, text='low'),\n",
       " Token(sentence_idx=1, idx=8, start=53, end=61, text='resource'),\n",
       " Token(sentence_idx=1, idx=9, start=62, end=72, text='conditions'),\n",
       " Token(sentence_idx=1, idx=10, start=75, end=78, text='The'),\n",
       " Token(sentence_idx=1, idx=11, start=79, end=83, text='file'),\n",
       " Token(sentence_idx=1, idx=12, start=84, end=85, text='\"'),\n",
       " Token(sentence_idx=1, idx=13, start=85, end=86, text='*'),\n",
       " Token(sentence_idx=1, idx=14, start=86, end=106, text='Few-Shot_ED.json.zip'),\n",
       " Token(sentence_idx=1, idx=14, start=86, end=97, text='Few-Shot_ED'),\n",
       " Token(sentence_idx=1, idx=15, start=106, end=107, text='*'),\n",
       " Token(sentence_idx=1, idx=16, start=107, end=108, text='\"'),\n",
       " Token(sentence_idx=1, idx=17, start=109, end=111, text='is'),\n",
       " Token(sentence_idx=1, idx=18, start=112, end=115, text='the'),\n",
       " Token(sentence_idx=1, idx=19, start=116, end=117, text='*'),\n",
       " Token(sentence_idx=1, idx=20, start=117, end=118, text='*'),\n",
       " Token(sentence_idx=1, idx=21, start=118, end=119, text='*'),\n",
       " Token(sentence_idx=1, idx=22, start=119, end=127, text='FewEvent'),\n",
       " Token(sentence_idx=1, idx=23, start=127, end=128, text='*'),\n",
       " Token(sentence_idx=1, idx=24, start=128, end=129, text='*'),\n",
       " Token(sentence_idx=1, idx=25, start=129, end=130, text='*'),\n",
       " Token(sentence_idx=1, idx=26, start=131, end=138, text='dataset'),\n",
       " Token(sentence_idx=1, idx=27, start=139, end=142, text='for'),\n",
       " Token(sentence_idx=1, idx=28, start=143, end=146, text='the'),\n",
       " Token(sentence_idx=1, idx=29, start=147, end=152, text='paper'),\n",
       " Token(sentence_idx=1, idx=30, start=153, end=161, text='accepted'),\n",
       " Token(sentence_idx=1, idx=31, start=162, end=164, text='by'),\n",
       " Token(sentence_idx=1, idx=32, start=165, end=169, text='WSDM'),\n",
       " Token(sentence_idx=1, idx=33, start=170, end=174, text='2020'),\n",
       " Token(sentence_idx=1, idx=34, start=175, end=176, text='*'),\n",
       " Token(sentence_idx=1, idx=35, start=176, end=177, text='*'),\n",
       " Token(sentence_idx=1, idx=36, start=177, end=178, text='*'),\n",
       " Token(sentence_idx=1, idx=37, start=178, end=179, text='['),\n",
       " Token(sentence_idx=1, idx=38, start=179, end=180, text='\"'),\n",
       " Token(sentence_idx=1, idx=39, start=180, end=193, text='Meta-Learning'),\n",
       " Token(sentence_idx=1, idx=40, start=194, end=198, text='with'),\n",
       " Token(sentence_idx=1, idx=41, start=199, end=219, text='Dynamic-Memory-Based'),\n",
       " Token(sentence_idx=1, idx=42, start=220, end=232, text='Prototypical'),\n",
       " Token(sentence_idx=1, idx=43, start=233, end=240, text='Network'),\n",
       " Token(sentence_idx=1, idx=44, start=241, end=244, text='for'),\n",
       " Token(sentence_idx=1, idx=45, start=245, end=253, text='Few-Shot'),\n",
       " Token(sentence_idx=1, idx=46, start=254, end=259, text='Event'),\n",
       " Token(sentence_idx=1, idx=47, start=260, end=269, text='Detection'),\n",
       " Token(sentence_idx=1, idx=48, start=269, end=270, text='\"'),\n",
       " Token(sentence_idx=1, idx=49, start=270, end=271, text=']'),\n",
       " Token(sentence_idx=1, idx=50, start=271, end=272, text='('),\n",
       " Token(sentence_idx=1, idx=51, start=272, end=277, text='https'),\n",
       " Token(sentence_idx=1, idx=52, start=277, end=278, text=':'),\n",
       " Token(sentence_idx=1, idx=53, start=278, end=279, text='/'),\n",
       " Token(sentence_idx=1, idx=54, start=279, end=280, text='/'),\n",
       " Token(sentence_idx=1, idx=55, start=280, end=289, text='arxiv.org'),\n",
       " Token(sentence_idx=1, idx=56, start=289, end=290, text='/'),\n",
       " Token(sentence_idx=1, idx=57, start=290, end=293, text='abs'),\n",
       " Token(sentence_idx=1, idx=58, start=293, end=294, text='/'),\n",
       " Token(sentence_idx=1, idx=59, start=294, end=304, text='1910.11621'),\n",
       " Token(sentence_idx=1, idx=60, start=304, end=305, text=')'),\n",
       " Token(sentence_idx=1, idx=61, start=305, end=306, text='*'),\n",
       " Token(sentence_idx=1, idx=62, start=306, end=307, text='*'),\n",
       " Token(sentence_idx=1, idx=63, start=307, end=308, text='*'),\n",
       " Token(sentence_idx=1, idx=64, start=311, end=312, text='#'),\n",
       " Token(sentence_idx=1, idx=65, start=312, end=313, text='#'),\n",
       " Token(sentence_idx=1, idx=66, start=314, end=320, text='Source'),\n",
       " Token(sentence_idx=1, idx=67, start=321, end=323, text='of'),\n",
       " Token(sentence_idx=1, idx=68, start=324, end=327, text='Raw'),\n",
       " Token(sentence_idx=1, idx=69, start=328, end=332, text='Data'),\n",
       " Token(sentence_idx=1, idx=70, start=333, end=334, text='*'),\n",
       " Token(sentence_idx=1, idx=71, start=335, end=337, text='We'),\n",
       " Token(sentence_idx=1, idx=72, start=338, end=343, text='first'),\n",
       " Token(sentence_idx=1, idx=73, start=344, end=349, text='scale'),\n",
       " Token(sentence_idx=1, idx=74, start=350, end=352, text='up'),\n",
       " Token(sentence_idx=1, idx=75, start=353, end=356, text='the'),\n",
       " Token(sentence_idx=1, idx=76, start=357, end=363, text='number'),\n",
       " Token(sentence_idx=1, idx=77, start=364, end=366, text='of'),\n",
       " Token(sentence_idx=1, idx=78, start=367, end=372, text='event'),\n",
       " Token(sentence_idx=1, idx=79, start=373, end=378, text='types'),\n",
       " Token(sentence_idx=1, idx=80, start=379, end=381, text='in'),\n",
       " Token(sentence_idx=1, idx=81, start=382, end=390, text='existing'),\n",
       " Token(sentence_idx=1, idx=82, start=391, end=399, text='datasets'),\n",
       " Token(sentence_idx=1, idx=83, start=399, end=400, text=','),\n",
       " Token(sentence_idx=1, idx=84, start=401, end=410, text='including'),\n",
       " Token(sentence_idx=1, idx=85, start=411, end=414, text='the'),\n",
       " Token(sentence_idx=1, idx=86, start=415, end=416, text='['),\n",
       " Token(sentence_idx=1, idx=87, start=416, end=419, text='ACE'),\n",
       " Token(sentence_idx=1, idx=88, start=419, end=420, text='-'),\n",
       " Token(sentence_idx=1, idx=89, start=420, end=424, text='2005'),\n",
       " Token(sentence_idx=1, idx=90, start=425, end=431, text='corpus'),\n",
       " Token(sentence_idx=1, idx=91, start=431, end=432, text=']'),\n",
       " Token(sentence_idx=1, idx=92, start=432, end=433, text='('),\n",
       " Token(sentence_idx=1, idx=93, start=433, end=437, text='http'),\n",
       " Token(sentence_idx=1, idx=94, start=437, end=438, text=':'),\n",
       " Token(sentence_idx=1, idx=95, start=438, end=439, text='/'),\n",
       " Token(sentence_idx=1, idx=96, start=439, end=440, text='/'),\n",
       " Token(sentence_idx=1, idx=97, start=440, end=462, text='projects.ldc.upenn.edu'),\n",
       " Token(sentence_idx=1, idx=98, start=462, end=463, text='/'),\n",
       " Token(sentence_idx=1, idx=99, start=463, end=466, text='ace'),\n",
       " Token(sentence_idx=1, idx=100, start=466, end=467, text='/'),\n",
       " Token(sentence_idx=1, idx=101, start=467, end=468, text=')'),\n",
       " Token(sentence_idx=1, idx=102, start=468, end=469, text=','),\n",
       " Token(sentence_idx=1, idx=103, start=470, end=473, text='and'),\n",
       " Token(sentence_idx=1, idx=104, start=474, end=475, text='['),\n",
       " Token(sentence_idx=1, idx=105, start=475, end=482, text='TAC-KBP'),\n",
       " Token(sentence_idx=1, idx=106, start=482, end=483, text='-'),\n",
       " Token(sentence_idx=1, idx=107, start=483, end=487, text='2017'),\n",
       " Token(sentence_idx=1, idx=108, start=488, end=493, text='Event'),\n",
       " Token(sentence_idx=1, idx=109, start=494, end=499, text='Track'),\n",
       " Token(sentence_idx=1, idx=110, start=500, end=504, text='Data'),\n",
       " Token(sentence_idx=1, idx=111, start=504, end=505, text=']'),\n",
       " Token(sentence_idx=1, idx=112, start=505, end=506, text='('),\n",
       " Token(sentence_idx=1, idx=113, start=506, end=511, text='https'),\n",
       " Token(sentence_idx=1, idx=114, start=511, end=512, text=':'),\n",
       " Token(sentence_idx=1, idx=115, start=512, end=513, text='/'),\n",
       " Token(sentence_idx=1, idx=116, start=513, end=514, text='/'),\n",
       " Token(sentence_idx=1, idx=117, start=514, end=526, text='tac.nist.gov'),\n",
       " Token(sentence_idx=1, idx=118, start=526, end=527, text='/'),\n",
       " Token(sentence_idx=1, idx=119, start=527, end=531, text='2017'),\n",
       " Token(sentence_idx=1, idx=120, start=531, end=532, text='/'),\n",
       " Token(sentence_idx=1, idx=121, start=532, end=535, text='KBP'),\n",
       " Token(sentence_idx=1, idx=122, start=535, end=536, text='/'),\n",
       " Token(sentence_idx=1, idx=123, start=536, end=541, text='Event'),\n",
       " Token(sentence_idx=1, idx=124, start=541, end=542, text='/'),\n",
       " Token(sentence_idx=1, idx=125, start=542, end=552, text='index.html'),\n",
       " Token(sentence_idx=1, idx=126, start=552, end=553, text=')')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all tokens of a sentence\n",
    "ref_doc.sentence_tokens(ref_doc.sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27114957-e271-4a18-996b-071aaa971994",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(sentence_idx=1, idx=1, start=0, end=1, text='#'),\n",
       " Token(sentence_idx=1, idx=2, start=2, end=18, text='Low_Resource_KBP'),\n",
       " Token(sentence_idx=1, idx=3, start=19, end=28, text='knowledge')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the first 3 tokens\n",
    "ref_doc.tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7413024-0e10-4e7b-adac-3015be91bafe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KBP'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the characters in the document by start, end positions\n",
    "ref_doc.text[15:18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a139ab06-3249-4675-848a-e598329d85e3",
   "metadata": {},
   "source": [
    "### Get all labeled sentences of each entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "572a021a-ae48-4e42-a539-c2da37740f97",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled sentences of CONFERENCE:\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n",
      "Labeled sentences of DATASET:\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n",
      "Labeled sentences of PUBLICATION:\n",
      "# Low_Resource_KBP knowledge graph population in low resource conditions   The file \"*Few-Shot_ED.json.zip*\" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[\"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\"](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)\n"
     ]
    }
   ],
   "source": [
    "# List of all 10 entity types\n",
    "label_list = [\n",
    "    'CONFERENCE', 'DATASET', 'EVALMETRIC', 'LICENSE', 'ONTOLOGY', \n",
    "    'PROGLANG', 'PROJECT', 'PUBLICATION', 'SOFTWARE', 'WORKSHOP'\n",
    "]\n",
    "annotations_by_label = {lb:[] for lb in label_list}\n",
    "\n",
    "for anno in ref_doc.annotations:\n",
    "    annotations_by_label[anno.label].extend(ref_doc.annotation_sentences(annotation=anno))\n",
    "\n",
    "for lb in label_list:\n",
    "    la = annotations_by_label[lb]\n",
    "    if len(la) > 0:\n",
    "        print(f\"Labeled sentences of {lb}:\")\n",
    "        for sent in la:\n",
    "            print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed1232-99c5-44ef-84fe-b858f4d4a5c4",
   "metadata": {},
   "source": [
    "## Write a WebAnno TSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab77d34-2776-45f4-b885-a0b7d5d0c148",
   "metadata": {
    "tags": []
   },
   "source": [
    "### write to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a1ca4a-e9b2-4c50-a969-73ec3ac9a908",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  615\n",
      "number of predicted doc annotations:  616\n",
      "Modified annotations have been written to ../results/val_label+1software/lighterswang_awesome-active-learning-for-medical-image-analysis_main_README.md.tsv\n",
      "number of ref doc annotations:  7\n",
      "number of predicted doc annotations:  8\n",
      "Modified annotations have been written to ../results/val_label+1software/epfl-dlab_llm-grounding-analysis_main_README.md.tsv\n",
      "number of ref doc annotations:  157\n",
      "number of predicted doc annotations:  158\n",
      "Modified annotations have been written to ../results/val_label+1software/obss_jury_main_README.md.tsv\n",
      "number of ref doc annotations:  19\n",
      "number of predicted doc annotations:  20\n",
      "Modified annotations have been written to ../results/val_label+1software/maastrichtlawtech_bsard_master_README.md.tsv\n",
      "number of ref doc annotations:  27\n",
      "number of predicted doc annotations:  28\n",
      "Modified annotations have been written to ../results/val_label+1software/tirtharajdash_VEGNN_master_README.md.tsv\n",
      "number of ref doc annotations:  3\n",
      "number of predicted doc annotations:  4\n",
      "Modified annotations have been written to ../results/val_label+1software/MI2DataLab_nlp_interpretability_framework_master_README.md.tsv\n",
      "number of ref doc annotations:  32\n",
      "number of predicted doc annotations:  33\n",
      "Modified annotations have been written to ../results/val_label+1software/unicamp-dl_cross-lingual-analysis_main_README.md.tsv\n",
      "number of ref doc annotations:  53\n",
      "number of predicted doc annotations:  54\n",
      "Modified annotations have been written to ../results/val_label+1software/kshitij3112_evpsnet_main_README.md.tsv\n",
      "number of ref doc annotations:  33\n",
      "number of predicted doc annotations:  34\n",
      "Modified annotations have been written to ../results/val_label+1software/uvavision_DrillDown_master_README.md.tsv\n",
      "number of ref doc annotations:  75\n",
      "number of predicted doc annotations:  76\n",
      "Modified annotations have been written to ../results/val_label+1software/PanoAsh_ASOD60K_main_README.md.tsv\n",
      "number of ref doc annotations:  59\n",
      "number of predicted doc annotations:  60\n",
      "Modified annotations have been written to ../results/val_label+1software/slds-lmu_wildlife-ml_main_README.md.tsv\n",
      "number of ref doc annotations:  26\n",
      "number of predicted doc annotations:  27\n",
      "Modified annotations have been written to ../results/val_label+1software/andreamust_hamse_ontology_master_README.md.tsv\n",
      "number of ref doc annotations:  83\n",
      "number of predicted doc annotations:  84\n",
      "Modified annotations have been written to ../results/val_label+1software/Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\n",
      "number of ref doc annotations:  27\n",
      "number of predicted doc annotations:  28\n",
      "Modified annotations have been written to ../results/val_label+1software/Leo-Q-316_ImGAGN_main_README.md.tsv\n",
      "number of ref doc annotations:  18\n",
      "number of predicted doc annotations:  19\n",
      "Modified annotations have been written to ../results/val_label+1software/maohaos2_thermometer_main_README.md.tsv\n",
      "number of ref doc annotations:  142\n",
      "number of predicted doc annotations:  143\n",
      "Modified annotations have been written to ../results/val_label+1software/jinyeying_DC-ShadowNet-Hard-and-Soft-Shadow-Removal_main_README.md.tsv\n",
      "number of ref doc annotations:  88\n",
      "number of predicted doc annotations:  89\n",
      "Modified annotations have been written to ../results/val_label+1software/pfnet-research_pfhedge_main_README.md.tsv\n",
      "number of ref doc annotations:  21\n",
      "number of predicted doc annotations:  22\n",
      "Modified annotations have been written to ../results/val_label+1software/HerreraKrachTeichmann_NJODE_master_README.md.tsv\n",
      "number of ref doc annotations:  15\n",
      "number of predicted doc annotations:  16\n",
      "Modified annotations have been written to ../results/val_label+1software/project-miracl_hagrid_main_README.md.tsv\n",
      "number of ref doc annotations:  41\n",
      "number of predicted doc annotations:  42\n",
      "Modified annotations have been written to ../results/val_label+1software/OpenBioLink_ITO_master_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "def add_one_more_annotations_in_files(folder_path=\"../data/val\"): \n",
    "    # Iterate through files in the folder\n",
    "    output_path = '../results/val_label+1software'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            exported_doc = webanno_tsv_read_file(file_path)\n",
    "            tokens_to_annotate = exported_doc.tokens[0:18]\n",
    "            # Example: Add a new annotation for a specific token\n",
    "            # Find the token you want to annotate (e.g., the first token in the first sentence)\n",
    "            new_annotation = utils.make_annotation(tokens=tokens_to_annotate, label='SOFTWARE')\n",
    "            # Add the new annotation to the document\n",
    "            predicted_doc = utils.replace_webanno_annotations(exported_doc, annotations=[*exported_doc.annotations, new_annotation])\n",
    "            # Step 3: Write the modified document to a new TSV file\n",
    "            with open(os.path.join(output_path, file_name), \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(predicted_doc.tsv())\n",
    "\n",
    "            print('number of ref doc annotations: ', len(exported_doc.annotations))\n",
    "            print('number of predicted doc annotations: ', len(predicted_doc.annotations))\n",
    "            print(f\"Modified annotations have been written to {os.path.join(output_path, file_name)}\")\n",
    "\n",
    "add_one_more_annotations_in_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "639a96bf-8a67-48e3-aae8-0e08f40f0aa3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  615\n",
      "number of predicted doc annotations:  290\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/lighterswang_awesome-active-learning-for-medical-image-analysis_main_README.md.tsv\n",
      "number of ref doc annotations:  7\n",
      "number of predicted doc annotations:  100\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/epfl-dlab_llm-grounding-analysis_main_README.md.tsv\n",
      "number of ref doc annotations:  157\n",
      "number of predicted doc annotations:  480\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/obss_jury_main_README.md.tsv\n",
      "number of ref doc annotations:  19\n",
      "number of predicted doc annotations:  140\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/maastrichtlawtech_bsard_master_README.md.tsv\n",
      "number of ref doc annotations:  27\n",
      "number of predicted doc annotations:  430\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/tirtharajdash_VEGNN_master_README.md.tsv\n",
      "number of ref doc annotations:  3\n",
      "number of predicted doc annotations:  60\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/MI2DataLab_nlp_interpretability_framework_master_README.md.tsv\n",
      "number of ref doc annotations:  32\n",
      "number of predicted doc annotations:  220\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/unicamp-dl_cross-lingual-analysis_main_README.md.tsv\n",
      "number of ref doc annotations:  53\n",
      "number of predicted doc annotations:  240\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/kshitij3112_evpsnet_main_README.md.tsv\n",
      "number of ref doc annotations:  33\n",
      "number of predicted doc annotations:  270\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/uvavision_DrillDown_master_README.md.tsv\n",
      "number of ref doc annotations:  75\n",
      "number of predicted doc annotations:  490\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/PanoAsh_ASOD60K_main_README.md.tsv\n",
      "number of ref doc annotations:  59\n",
      "number of predicted doc annotations:  580\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/slds-lmu_wildlife-ml_main_README.md.tsv\n",
      "number of ref doc annotations:  26\n",
      "number of predicted doc annotations:  350\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/andreamust_hamse_ontology_master_README.md.tsv\n",
      "number of ref doc annotations:  83\n",
      "number of predicted doc annotations:  1420\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\n",
      "number of ref doc annotations:  27\n",
      "number of predicted doc annotations:  110\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/Leo-Q-316_ImGAGN_main_README.md.tsv\n",
      "number of ref doc annotations:  18\n",
      "number of predicted doc annotations:  10\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/maohaos2_thermometer_main_README.md.tsv\n",
      "number of ref doc annotations:  142\n",
      "number of predicted doc annotations:  730\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/jinyeying_DC-ShadowNet-Hard-and-Soft-Shadow-Removal_main_README.md.tsv\n",
      "number of ref doc annotations:  88\n",
      "number of predicted doc annotations:  810\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/pfnet-research_pfhedge_main_README.md.tsv\n",
      "number of ref doc annotations:  21\n",
      "number of predicted doc annotations:  430\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/HerreraKrachTeichmann_NJODE_master_README.md.tsv\n",
      "number of ref doc annotations:  15\n",
      "number of predicted doc annotations:  170\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/project-miracl_hagrid_main_README.md.tsv\n",
      "number of ref doc annotations:  41\n",
      "number of predicted doc annotations:  250\n",
      "Modified annotations have been written to ../results/dummy_all_sent_all_label/OpenBioLink_ITO_master_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "# List of all 10 entity types\n",
    "label_list = [\n",
    "    'CONFERENCE', 'DATASET', 'EVALMETRIC', 'LICENSE', 'ONTOLOGY', \n",
    "    'PROGLANG', 'PROJECT', 'PUBLICATION', 'SOFTWARE', 'WORKSHOP'\n",
    "]\n",
    "def dummy_all_sent_all_label(folder_path=\"../data/val\"): # data in\n",
    "    # Iterate through files in the folder\n",
    "    output_path = '../results/dummy_all_sent_all_label'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            exported_doc = webanno_tsv_read_file(file_path)\n",
    "            new_annotations = []\n",
    "            for sent in exported_doc.sentences:\n",
    "                tokens_to_annotate = exported_doc.sentence_tokens(sent)\n",
    "                if len(tokens_to_annotate) > 0:\n",
    "                    for lb in label_list:\n",
    "                        # Example: Add a new annotation for a specific token\n",
    "                        # Find the token you want to annotate (e.g., the first token in the first sentence)\n",
    "                        new_annotation = utils.make_annotation(tokens=tokens_to_annotate, label=lb)\n",
    "                        new_annotations.append(new_annotation)\n",
    "                # Add the new annotation to the document\n",
    "            predicted_doc = utils.replace_webanno_annotations(exported_doc, annotations=new_annotations)\n",
    "            # Step 3: Write the modified document to a new TSV file\n",
    "            with open(os.path.join(output_path, file_name), \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(predicted_doc.tsv())\n",
    "\n",
    "            print('number of ref doc annotations: ', len(exported_doc.annotations))\n",
    "            print('number of predicted doc annotations: ', len(predicted_doc.annotations))\n",
    "            print(f\"Modified annotations have been written to {os.path.join(output_path, file_name)}\")\n",
    "\n",
    "dummy_all_sent_all_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40383618-8daa-4998-b359-ca052387ae9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# software and overlapped software! \n",
    "def dummy_all_doc_2_Software(folder_path=\"../data/val\"): \n",
    "    # Iterate through files in the folder\n",
    "    output_path = '../results/dummy_all_doc_2_Software'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            exported_doc = webanno_tsv_read_file(file_path)\n",
    "            \n",
    "            tokens_to_annotate = exported_doc.tokens\n",
    "            new_annotation1 = utils.make_annotation(tokens=tokens_to_annotate[:1], label='SOFTWARE')\n",
    "            new_annotation2 = utils.make_annotation(tokens=tokens_to_annotate, label='SOFTWARE')\n",
    "            predicted_doc = utils.replace_webanno_annotations(exported_doc, annotations=[new_annotation1, new_annotation2])\n",
    "            # Step 3: Write the modified document to a new TSV file\n",
    "            with open(os.path.join(output_path, file_name), \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(predicted_doc.tsv())\n",
    "\n",
    "            print('number of ref doc annotations: ', len(exported_doc.annotations))\n",
    "            print('number of predicted doc annotations: ', len(predicted_doc.annotations))\n",
    "            print(f\"Modified annotations have been written to {os.path.join(output_path, file_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c89482f-4ad8-48f6-8922-0d68c080ec5b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  615\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/lighterswang_awesome-active-learning-for-medical-image-analysis_main_README.md.tsv\n",
      "number of ref doc annotations:  7\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/epfl-dlab_llm-grounding-analysis_main_README.md.tsv\n",
      "number of ref doc annotations:  157\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/obss_jury_main_README.md.tsv\n",
      "number of ref doc annotations:  19\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/maastrichtlawtech_bsard_master_README.md.tsv\n",
      "number of ref doc annotations:  27\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/tirtharajdash_VEGNN_master_README.md.tsv\n",
      "number of ref doc annotations:  3\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/MI2DataLab_nlp_interpretability_framework_master_README.md.tsv\n",
      "number of ref doc annotations:  32\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/unicamp-dl_cross-lingual-analysis_main_README.md.tsv\n",
      "number of ref doc annotations:  53\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/kshitij3112_evpsnet_main_README.md.tsv\n",
      "number of ref doc annotations:  33\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/uvavision_DrillDown_master_README.md.tsv\n",
      "number of ref doc annotations:  75\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/PanoAsh_ASOD60K_main_README.md.tsv\n",
      "number of ref doc annotations:  59\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/slds-lmu_wildlife-ml_main_README.md.tsv\n",
      "number of ref doc annotations:  26\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/andreamust_hamse_ontology_master_README.md.tsv\n",
      "number of ref doc annotations:  83\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\n",
      "number of ref doc annotations:  27\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/Leo-Q-316_ImGAGN_main_README.md.tsv\n",
      "number of ref doc annotations:  18\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/maohaos2_thermometer_main_README.md.tsv\n",
      "number of ref doc annotations:  142\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/jinyeying_DC-ShadowNet-Hard-and-Soft-Shadow-Removal_main_README.md.tsv\n",
      "number of ref doc annotations:  88\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/pfnet-research_pfhedge_main_README.md.tsv\n",
      "number of ref doc annotations:  21\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/HerreraKrachTeichmann_NJODE_master_README.md.tsv\n",
      "number of ref doc annotations:  15\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/project-miracl_hagrid_main_README.md.tsv\n",
      "number of ref doc annotations:  41\n",
      "number of predicted doc annotations:  2\n",
      "Modified annotations have been written to ../results/dummy_all_doc_2_Software/OpenBioLink_ITO_master_README.md.tsv\n",
      "CPU times: user 59.2 s, sys: 12 ms, total: 59.2 s\n",
      "Wall time: 59.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dummy_all_doc_2_Software()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71752b3a-2ce5-4d5a-ba18-e37c2b9f91a9",
   "metadata": {},
   "source": [
    "### remove all labeled and write to new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3bed42f-6506-4617-9a98-49856791531e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/naver-ai_pcme_main_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/tamlhp_awesome-privex_main_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/obss_jury_main_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/nantiamak_sql4ml_master_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/nokia_codesearch_master_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/vanderschaarlab_prompt-oirl_main_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/nheist_caligraph-for-semrec_main_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/ohadeytan_caffeine_master_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/happyharrycn_actionformer_release_main_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/norlab-ulaval_BorealTC_master_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/roysoumya_knowledge-aware-med-classification_master_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/daijifeng001_TA-FCN_master_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/conversationai_unhealthy-conversations_main_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/OpenBioLink_ITO_master_README.md.tsv\n",
      "number of ref doc annotations:  0\n",
      "number of predicted doc annotations:  0\n",
      "Modified annotations have been written to ../results/empty/poloclub_diffusion-explainer_main_README.md.tsv\n"
     ]
    }
   ],
   "source": [
    "def rm_annotations_in_files(folder_path=\"../data/train\"):\n",
    "    # Iterate through files in the folder\n",
    "    output_path = '../results/empty'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            exported_doc = webanno_tsv_read_file(file_path)\n",
    "            predicted_doc = replace(exported_doc, annotations=[])\n",
    "            with open(os.path.join(output_path, file_name), \"w+\", encoding=\"utf-8\") as f:\n",
    "                f.write(predicted_doc.tsv())\n",
    "\n",
    "            print('number of ref doc annotations: ', len(exported_doc.annotations))\n",
    "            print('number of predicted doc annotations: ', len(predicted_doc.annotations))\n",
    "            print(f\"Modified annotations have been written to {os.path.join(output_path, file_name)}\")\n",
    "            \n",
    "rm_annotations_in_files(\"../data/val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b491ab-c623-475e-8d7f-88cbdfba576f",
   "metadata": {},
   "source": [
    "## Sanitary Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c6f3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 44 annotations\n",
      "Annotation(tokens=[Token(sentence_idx=20, idx=98, start=9756, end=9769, text='International'), Token(sentence_idx=20, idx=99, start=9770, end=9775, text='Joint'), Token(sentence_idx=20, idx=100, start=9776, end=9786, text='Conference'), Token(sentence_idx=20, idx=101, start=9787, end=9789, text='on'), Token(sentence_idx=20, idx=102, start=9790, end=9797, text='Natural'), Token(sentence_idx=20, idx=103, start=9798, end=9806, text='Language'), Token(sentence_idx=20, idx=104, start=9807, end=9817, text='Processing')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='CONFERENCE', label_id=14)\n",
      "Predicted 0 annotations\n",
      "Predicted 7 annotations\n",
      "Annotation(tokens=[Token(sentence_idx=1, idx=105, start=475, end=482, text='TAC-KBP'), Token(sentence_idx=1, idx=106, start=482, end=483, text='-'), Token(sentence_idx=1, idx=107, start=483, end=487, text='2017'), Token(sentence_idx=1, idx=108, start=488, end=493, text='Event'), Token(sentence_idx=1, idx=109, start=494, end=499, text='Track'), Token(sentence_idx=1, idx=110, start=500, end=504, text='Data')], layer='de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity', field='value', label='DATASET', label_id=4)\n",
      "Predicted 0 annotations\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "def verify(ref_doc, predicted_doc):\n",
    "    assert ref_doc.text == predicted_doc.text, 'content changed'\n",
    "    assert len(ref_doc.sentences) == len(predicted_doc.sentences), 'sentences changed'\n",
    "    assert len(ref_doc.tokens) == len(predicted_doc.tokens), 'tokens changed'\n",
    "    for s1, s2 in zip(ref_doc.sentences, predicted_doc.sentences):\n",
    "        assert s1 == s2, f'sentence changed, \\n{s1}\\n{s2}'\n",
    "\n",
    "    for t1, t2 in zip(ref_doc.tokens, predicted_doc.tokens):\n",
    "        assert t1 == t2, f'token changed: \\n{t1}\\n{t2}'\n",
    "\n",
    "    print(f\"Predicted {len(predicted_doc.annotations)} annotations\")\n",
    "    if len(predicted_doc.annotations) > 0:\n",
    "        print(predicted_doc.annotations[-1])\n",
    "\n",
    "for file_path in os.listdir('../data/train'):\n",
    "    ref_doc = webanno_tsv_read_file(f'../data/train/{file_path}')\n",
    "    dummy_predicted_doc = webanno_tsv_read_file(f'../results/dummy/{file_path}')\n",
    "    empty_predicted_doc = webanno_tsv_read_file(f'../results/empty/{file_path}')\n",
    "    verify(ref_doc, dummy_predicted_doc)\n",
    "    verify(ref_doc, empty_predicted_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228d89e-81f3-4c01-ac3d-1f01c830773b",
   "metadata": {},
   "source": [
    "## Try BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24be6bc7-a5aa-4603-8dee-92dd4749dc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "\n",
    "LABELS = [\n",
    "    'CONFERENCE',\n",
    "    'DATASET',\n",
    "    'EVALMETRIC',\n",
    "    'LICENSE',\n",
    "    'ONTOLOGY',\n",
    "    'PROGLANG',\n",
    "    'PROJECT',\n",
    "    'PUBLICATION',\n",
    "    'SOFTWARE',\n",
    "    'WORKSHOP'\n",
    "]\n",
    "\n",
    "\n",
    "def to_char_bio(src_path: str, ref_path: str) -> List[List[str]]:\n",
    "    ref_doc = webanno_tsv_read_file(ref_path)\n",
    "\n",
    "    # Parse the WebAnno TSV file\n",
    "    doc = webanno_tsv_read_file(src_path)\n",
    "    # Initialize a list to store character-level BIO tags\n",
    "    bio_tags_list = []\n",
    "    for target_label in LABELS:\n",
    "        bio_tags = ['#'] * len(doc.text)  # Default to '#' for all characters\n",
    "        # Pick interested sentences and default to 'O'\n",
    "        for annotation in ref_doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "            sentences = doc.annotation_sentences(annotation)\n",
    "            for sentence in sentences:\n",
    "                tokens = doc.sentence_tokens(sentence)\n",
    "                start_char, end_char = tokens[0].start, tokens[-1].end\n",
    "                bio_tags[start_char:end_char] = ['O'] * (end_char-start_char)\n",
    "\n",
    "        for annotation in doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "\n",
    "            start_token, end_token = annotation.tokens[0], annotation.tokens[-1]\n",
    "            start_char = start_token.start\n",
    "            end_char = end_token.end\n",
    "            # Sanity check\n",
    "            if ref_doc.text[start_char:end_char] != annotation.text:\n",
    "                msg = f\"ERROR: src: {src_path}, annotated '{annotation.text}', text: '{ref_doc.text[start_char:end_char]}'\"\n",
    "                print(msg)\n",
    "\n",
    "            # Assign BIO tags to characters in the entity span\n",
    "            if 'I-' in bio_tags[start_char]:\n",
    "                # It's inside other ENTITY, skip it\n",
    "                pass\n",
    "            else:\n",
    "                bio_tags[start_char] = f'B-{label}'  # Beginning of the entity\n",
    "\n",
    "            for i in range(start_char + 1, end_char):\n",
    "                bio_tags[i] = f'I-{label}'  # Inside the entity\n",
    "\n",
    "        # Remove unannotated sentences from bio list.\n",
    "        bio_tags = [x for x in filter(lambda x: x != '#', bio_tags)]\n",
    "        if len(bio_tags) > 0:\n",
    "            bio_tags_list.append(bio_tags)\n",
    "\n",
    "    return bio_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbccf20d-5025-46e6-8b8d-09699fcb9b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_dir = '../data/train'\n",
    "pred_dir = '../results/dummy'\n",
    "\n",
    "ref_file_names = sorted([fp for fp in os.listdir(ref_dir) if os.path.isfile(f'{ref_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "all_ref_bio_tags_list = []\n",
    "for ref_file_name in ref_file_names:\n",
    "    src_path = os.path.join(ref_dir, ref_file_name)\n",
    "    ref_path = src_path\n",
    "    all_ref_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "\n",
    "pred_file_names = sorted([fp for fp in os.listdir(pred_dir) if os.path.isfile(f'{pred_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "all_pred_bio_tags_list = []\n",
    "for idx, ref_file_name in enumerate(ref_file_names):\n",
    "    try:\n",
    "        src_path = os.path.join(pred_dir, ref_file_name)\n",
    "        ref_path = os.path.join(ref_dir, ref_file_name)\n",
    "        all_pred_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "    except FileNotFoundError:\n",
    "        nbr_labels = len(all_ref_bio_tags_list[idx])\n",
    "        pred = []\n",
    "        for label_idx in range(nbr_labels):\n",
    "            pred.append(['O'] * len(all_ref_bio_tags_list[idx][label_idx]))\n",
    "        print(f\"WARN: {ref_file_name} is missing, fill 'O' list as default prediction\")\n",
    "        all_pred_bio_tags_list.append(pred)\n",
    "\n",
    "# Sanity checking\n",
    "for ref_list, pred_list in zip(all_ref_bio_tags_list, all_pred_bio_tags_list):\n",
    "    for ref, pred in zip(ref_list, pred_list):\n",
    "        # print(len(ref), len(pred))\n",
    "        assert len(ref) == len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ee4ad-3d9d-4b45-804b-eb51fa5290f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
