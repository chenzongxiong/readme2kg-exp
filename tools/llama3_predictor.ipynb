{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae8e1d9-35dd-4659-a5aa-9dd8d7303bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "sys.path.append('./readme2kg-exp/src/')\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from termcolor import colored\n",
    "from functools import partial, reduce\n",
    "import operator as op\n",
    "import hashlib\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "from predictor import BasePredictor, LABELS\n",
    "from webanno_tsv import webanno_tsv_read_file, Document, Annotation, Token\n",
    "import utils\n",
    "import cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a3be4a1-90bb-4601-963f-7cfe19c501c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'test_unlabeled'\n",
    "base_path = f'../data/{phase}'\n",
    "file_names = [fp for fp in os.listdir(base_path) if os.path.isfile(os.path.join(base_path, fp)) and fp.endswith('.tsv')]\n",
    "model_name = 'Meta-Llama-3-8B-Instruct'\n",
    "output_folder = f'../results/{model_name}/{phase}'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "143f201a-9d7b-4fcb-9653-902606a2929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\KIT2025\\GitHub\\readme2kg-exp\\src\n",
      "../results/Meta-Llama-3-8B-Instruct/test_unlabeled\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "560ab781-056f-41fe-aad8-6acf457399c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Task:**\n",
      "You are tasked with performing Named Entity Recognition (NER) on the given text. Follow the guidelines strictly to identify and classify entities into their respective categories. Annotate the entities directly in the original text using XML-style tags. Only return the annotated text in Markdown formatâ€”no explanations, introductions, or extra text.\n",
      "\n",
      "\n",
      "**Guidelines:**\n",
      "\n",
      "1. **Entity Classes:**\n",
      "   - **CONFERENCE**: Conference events.\n",
      "     *Definition*:\n",
      "     A formal meeting or gathering focused on a particular field of study or topic.\n",
      "     *Example*:\n",
      "     `<CONFERENCE>International Semantic Web Conference 2019</CONFERENCE>`\n",
      "     `<CONFERENCE>ISWC 2019</CONFERENCE>`\n",
      "     `<CONFERENCE>CVPR2023</CONFERENCE> workshop`\n",
      "\n",
      "   - **DATASET**: Structured collections of data.\n",
      "     *Definition*:\n",
      "     A structured collection of data, organized typically for a specific goal such as analysis, research, or reference.\n",
      "     *Example*:\n",
      "     `<DATASET>Maules Creek</DATASET>`\n",
      "     `Download the <DATASET>USTPO MIT</DATASET> dataset from (https://github.com/wenggong-jin/nips17-rexgen/blob/master/USPTO/data.zip)`\n",
      "\n",
      "   - **EVALMETRIC**: Evaluation metrics for models.\n",
      "     *Definition*:\n",
      "     A quantitative measure used to assess the performance and effectiveness of a statistical or machine learning model.\n",
      "     *Example*:\n",
      "     The evaluation metrics used are `<EVALMETRIC>Precision</EVALMETRIC>`, `<EVALMETRIC>Recall</EVALMETRIC>`, `<EVALMETRIC>F1-Score</EVALMETRIC>`, and `<EVALMETRIC>BLEU Score</EVALMETRIC>`.\n",
      "\n",
      "   - **LICENSE**: Licensing terms.\n",
      "     *Definition*:\n",
      "     Legal terms and conditions for using a particular resource.\n",
      "     *Example*:\n",
      "     Available licenses to use: `<LICENSE>cc-by-3</LICENSE>` and `<LICENSE>CC BY-NC 4.0</LICENSE>`\n",
      "\n",
      "   - **ONTOLOGY**: Semantic frameworks for knowledge representation.\n",
      "     *Definition*:\n",
      "     A framework representing knowledge about a domain, including concepts, entities, properties, and relationships.\n",
      "     *Example*:\n",
      "     The `<ONTOLOGY>Intelligence Task Ontology</ONTOLOGY>` is ...\n",
      "\n",
      "   - **PROGLANG**: Programming languages.\n",
      "     *Definition*:\n",
      "     A formal language used for implementing software.\n",
      "     *Example*:\n",
      "     Programming languages such as `<PROGLANG>Python</PROGLANG>`, `<PROGLANG>PHP</PROGLANG>`, and `<PROGLANG>C++</PROGLANG>`\n",
      "\n",
      "   - **PROJECT**: Scientific or business initiatives.\n",
      "     *Definition*:\n",
      "     A planned initiative aimed at addressing a research question or achieving a specific goal.\n",
      "     *Example*:\n",
      "     The `<PROJECT>Paper With Code</PROJECT>` project (https://<PROJECT>paperswithcode</PROJECT>.com/)\n",
      "\n",
      "   - **PUBLICATION**: Scholarly works.\n",
      "     *Definition*:\n",
      "     A creative work resulting from a publishing process, such as a journal article, conference proceeding, or preprint.\n",
      "     *Example*:\n",
      "     `<PUBLICATION>No Length Left Behind: Enhancing Knowledge Tracing for Modeling Sequences of Excessive or Insufficient Lengths</PUBLICATION>`. In `<PUBLICATION>Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</PUBLICATION>`\n",
      "\n",
      "   - **SOFTWARE**: Software tools or programs.\n",
      "     *Definition*:\n",
      "     Programs or tools designed to perform specific tasks on electronic devices.\n",
      "     *Example*:\n",
      "     You can use the `<SOFTWARE>Protege ontology editor</SOFTWARE>` to explore and edit the resource.\n",
      "\n",
      "   - **WORKSHOP**: Workshop events.\n",
      "     *Definition*:\n",
      "     An educational or hands-on session focused on a specific subject.\n",
      "     *Example*:\n",
      "     Refers to the [Thermal Image Super-Resolution](https://<WORKSHOP>pbvs-workshop</WORKSHOP>.github.io/datasets.html)\n",
      "\n",
      "2. **Annotation Rules:**\n",
      "   - Include the entire proper name but exclude standalone generic descriptors (e.g., exclude \"Dataset\" in \"BookSum Dataset\").\n",
      "   - Use a single-class annotation per entity, based on the context.\n",
      "   - Annotate nested entities separately.\n",
      "   - Include punctuation marks only if part of the entity (e.g., titles with \":\" in \"PAV-SOD: Panoramic Audiovisual Saliency Detection\").\n",
      "   - Annotate entities within URLs (e.g., \"llama\" in `https://ai.meta.com/llama`).\n",
      "\n",
      "3. **Output Format:**\n",
      "   - Return the original content with entities directly annotated using XML-style tags for their respective classes. Example:\n",
      "     - `The <SOFTWARE>Protege Ontology Editor</SOFTWARE> is widely used for creating ontologies.`\n",
      "   - Your output should be in Markdown format with all entities tagged as instructed.\n",
      "\n",
      "---\n",
      "Input Text:\n",
      "\n",
      "{input_text}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_id = 0\n",
    "prompt_template_path = f'../config/deepseek-chat-prompt-0.txt'\n",
    "if os.path.isfile(prompt_template_path):\n",
    "    with open(prompt_template_path, 'r') as fd:\n",
    "        prompt_template = fd.read()\n",
    "else:\n",
    "    prompt_template = ''\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901a66c-ff2d-4d40-a331-15ff80039bb6",
   "metadata": {},
   "source": [
    "# Load Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78df436-8385-4ea4-a549-c746303eccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3d58c-edbc-431f-8b96-48e919e4080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "sentence_text = \"\"\"# DejaVu ## Table of Contents =================    * [Code](#code)     * [Install Requirements](#install-requirements)     * [Usage](#usage)     * [Example](#example)   * [Datasets](#datasets)   * [Deployment and Failure Injection Scripts of Train-Ticket](#deployment-and-failure-injection-scripts-of-train-ticket)   * [Citation](#citation)   * [Supplementary details](#supplementary-details)    ## Paper A preprint version: https://arxiv.org/abs/2207.09021 ## Code ### Install 1.\"\"\"\n",
    "prompt = prompt_template.replace('{input_text}', sentence_text)\n",
    "# original code\n",
    "#prompt = prompt_template.replace('{input_text}', sentence.text)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful NER annotator.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "    \n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=255,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e1bc1ba-089a-4cb9-95ae-4f691968fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction(sentence, tokens, sid_path):\n",
    "    try:\n",
    "        print(f\"Process-{os.getpid()} processing {colored(sentence.text, 'red')} ...\")\n",
    "        prompt = prompt_template.replace('{input_text}', sentence.text)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful NER annotator.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]        \n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "            \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=255,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        result = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "        #print(f\"Process-{os.getpid()} predict {colored(sentence.text, 'cyan')} successfully\")\n",
    "        with open(sid_path, 'w') as file:\n",
    "            file.write(result)\n",
    "    except Exception as ex:\n",
    "        logging.error(f'[do_prediction] got exception: {ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a78fc7e9-36b5-4505-ae93-b6472540e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotation_labels_if_possible(predicted_text):\n",
    "    label_to_text_list = defaultdict(list)\n",
    "    acc_adjusted_pos = 0\n",
    "\n",
    "    matched_labels = {}\n",
    "\n",
    "    for label in LABELS:\n",
    "        regex = f'<{label}>(.*?)</{label}>'\n",
    "        matches = re.finditer(regex, predicted_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        for m in matches:\n",
    "            matched_labels[m.start(1)] = label\n",
    "\n",
    "    for pos in sorted(matched_labels):\n",
    "        label = matched_labels[pos]\n",
    "        regex = f'<{label}>(.*?)</{label}>'\n",
    "        matches = re.finditer(regex, predicted_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        for m in matches:\n",
    "            adjusted_pos = len(label) + 2\n",
    "            label_to_text_list[label].append({\n",
    "                'text': m.group(1),\n",
    "                'start': m.start(1) - adjusted_pos - acc_adjusted_pos,\n",
    "                'end': m.end(1) - adjusted_pos - acc_adjusted_pos,\n",
    "            })\n",
    "            acc_adjusted_pos += adjusted_pos * 2 + 1\n",
    "\n",
    "    return label_to_text_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def post_process(predicted_text, tokens):\n",
    "    cleaned_text = cleaner.Cleaner(predicted_text).clean()\n",
    "    label_to_text_list = extract_annotation_labels_if_possible(cleaned_text)\n",
    "    return label_to_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aefcdd39-2572-4a1e-bc6b-4f5651cb88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, tokens):\n",
    "    path = f'../results/{model_name}/prompt-{prompt_id}/zzz_{file_name}' # NOTE: prefix zzz for directory sorting, non-sense\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    sid = hashlib.sha256(sentence.text.encode()).hexdigest()[:8]\n",
    "    if not os.path.isfile(f'{path}/{sid}.txt'):   # original code\n",
    "    # if os.path.isdir(f'{path}'):\n",
    "        do_prediction(sentence, tokens, f'{path}/{sid}.txt')\n",
    "\n",
    "    with open(f'{path}/{sid}.txt', 'r') as fd:\n",
    "        predicted_text = fd.read()\n",
    "\n",
    "    label_to_text_list = post_process(predicted_text, tokens)\n",
    "    # NOTE: sanity checking\n",
    "    for label, text_list in label_to_text_list.items():\n",
    "        for text in text_list:\n",
    "            if text['text'] != sentence.text[text['start']:text['end']]:\n",
    "                prompt = prompt_template.replace('{input_text}', sentence.text)\n",
    "                #logging.warning(f\"BUG? The predicted text is not exact the same as the original text. \\n\\nPrompt: {prompt}\\nOriginal: {colored(sentence.text, 'green')}\\nGenerated: {colored(text['text'], 'red')}\\n--------------------------------------------------------------------------------\")\n",
    "\n",
    "    span_tokens_to_label_list = []\n",
    "    for label, text_list in label_to_text_list.items():\n",
    "        for text in text_list:\n",
    "            span_tokens_to_label_list.append({\n",
    "                'span_tokens': utils.make_span_tokens(tokens, text['start'], text['end']),\n",
    "                'label': label\n",
    "            })\n",
    "    return span_tokens_to_label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0045624d-f168-44cf-a4c9-f94413793290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_serial(doc: Document):\n",
    "    annotations = []\n",
    "    for sent in doc.sentences:\n",
    "        tokens = doc.sentence_tokens(sent)\n",
    "        span_tokens_to_label_list = predict(sentence=sent, tokens=tokens)\n",
    "        \n",
    "        # create the annotation instances\n",
    "        for span_tokens_to_label in span_tokens_to_label_list:\n",
    "            span_tokens = span_tokens_to_label['span_tokens']\n",
    "            label = span_tokens_to_label['label']\n",
    "            if span_tokens is None:\n",
    "                continue\n",
    "\n",
    "            annotation = utils.make_annotation(tokens=span_tokens, label=label)\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    result = utils.replace_webanno_annotations(doc, annotations=annotations)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6e8a72a-fbcc-43a9-831d-86cf906a5fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/Meta-Llama-3-8B-Instruct/test_unlabeled\n",
      "d:\\KIT2025\\GitHub\\readme2kg-exp\\src\n"
     ]
    }
   ],
   "source": [
    "print(output_folder)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee5349e6-b4b2-49f8-bd0e-a5fc25f83a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 11 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 4 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 3 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 1 annotations\n",
      "WARNING:root:Predicted 2 annotations\n",
      "WARNING:root:Predicted 2 annotations\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_names:\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    ref_doc = webanno_tsv_read_file(file_path)\n",
    "    predicted_doc = call_serial(ref_doc)\n",
    "    # Verify\n",
    "    if ref_doc.text != predicted_doc.text:\n",
    "        #logging.warning('content changed')\n",
    "        pass\n",
    "    if len(ref_doc.sentences) == len(predicted_doc.sentences):\n",
    "        #logging.warning('sentences changed')\n",
    "        pass\n",
    "    if len(ref_doc.tokens) == len(predicted_doc.tokens):\n",
    "        #logging.warning('tokens changed')\n",
    "        pass\n",
    "    for s1, s2 in zip(ref_doc.sentences, predicted_doc.sentences):\n",
    "        if s1 == s2:\n",
    "            #logging.warning(f'sentence changed, \\n{s1}\\n{s2}')\n",
    "            pass\n",
    "\n",
    "    for t1, t2 in zip(ref_doc.tokens, predicted_doc.tokens):\n",
    "        if t1 == t2:\n",
    "            #logging.warning(f'token changed: \\n{t1}\\n{t2}')\n",
    "            pass\n",
    "\n",
    "    logging.warning(f\"Predicted {len(predicted_doc.annotations)} annotations\")\n",
    "    prediction_path = os.path.join(output_folder, file_name)\n",
    "    with open(prediction_path, 'w') as fd:\n",
    "        fd.write(predicted_doc.tsv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c80d22-87b1-4cf4-836e-795ced878a0d",
   "metadata": {},
   "source": [
    "# Scorer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0972bd0-a799-4cf1-b9f1-e1578895855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from webanno_tsv import webanno_tsv_read_file, Document, Annotation\n",
    "from typing import List, Union\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "LABELS = [\n",
    "    'CONFERENCE',\n",
    "    'DATASET',\n",
    "    'EVALMETRIC',\n",
    "    'LICENSE',\n",
    "    'ONTOLOGY',\n",
    "    'PROGLANG',\n",
    "    'PROJECT',\n",
    "    'PUBLICATION',\n",
    "    'SOFTWARE',\n",
    "    'WORKSHOP'\n",
    "]\n",
    "\n",
    "def to_char_bio(src_path: str, ref_path: str) -> List[List[str]]:\n",
    "    ref_doc = webanno_tsv_read_file(ref_path)\n",
    "    # Parse the WebAnno TSV file\n",
    "    doc = webanno_tsv_read_file(src_path)\n",
    "    # Initialize a list to store character-level BIO tags\n",
    "    bio_tags_list = []\n",
    "    for target_label in LABELS:\n",
    "        bio_tags = ['#'] * len(ref_doc.text)  # Default to '#' for all characters\n",
    "        # Pick interested sentences and default them to 'O'\n",
    "        for annotation in ref_doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "            sentences = ref_doc.annotation_sentences(annotation)\n",
    "            for sentence in sentences:\n",
    "                tokens = ref_doc.sentence_tokens(sentence)\n",
    "                start_char, end_char = tokens[0].start, tokens[-1].end\n",
    "                bio_tags[start_char:end_char] = ['O'] * (end_char-start_char)\n",
    "\n",
    "        for annotation in doc.annotations:\n",
    "            label = annotation.label\n",
    "            if label != target_label:\n",
    "                continue\n",
    "\n",
    "            start_token, end_token = annotation.tokens[0], annotation.tokens[-1]\n",
    "            start_char = start_token.start\n",
    "            end_char = end_token.end\n",
    "            # Sanity check\n",
    "            if ref_doc.text[start_char:end_char] != annotation.text:\n",
    "                msg = f\"ERROR: src: {src_path}, annotated '{annotation.text}', text: '{ref_doc.text[start_char:end_char]}'\"\n",
    "                print(msg)\n",
    "\n",
    "            if 'I-' in bio_tags[start_char]:\n",
    "                # Overlapping, it's annotated by another annotations, we connect them as one annotations\n",
    "                pass\n",
    "            else:\n",
    "                if bio_tags[start_char] != '#':\n",
    "                    # Assign BIO tags to characters in the entity span\n",
    "                    bio_tags[start_char] = f'B-{label}'  # Beginning of the entity\n",
    "\n",
    "            for i in range(start_char + 1, end_char):\n",
    "                if bio_tags[i] != '#':\n",
    "                    bio_tags[i] = f'I-{label}'  # Inside the entity\n",
    "\n",
    "        # Remove unannotated sentences from bio list.\n",
    "        bio_tags = [x for x in filter(lambda x: x != '#', bio_tags)]\n",
    "        bio_tags_list.append(bio_tags)\n",
    "\n",
    "    return bio_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aeb3a41d-1c03-40de-a06b-4043c89ec832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    return reduce(lambda x, y: x + y, lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5deab33-5d5e-401b-b440-d2cc989e8e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\KIT2025\\GitHub\\readme2kg-exp\\src\n",
      "WARN: 231sm_Low_Resource_KBP_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: ARM-software_keyword-transformer_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: Cardio-AI_3d-mri-domain-adaptation_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: ChopinSharp_ref-nms_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: James-Durant_fisher-information_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: MELALab_nela-gt-2019_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: allenai_aspire_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: alpiges_LinConGauss_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: anonymous-submission-22_dejavu_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: aspiaspace_earthpt_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: benedekrozemberczki_karateclub_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: claws-lab_awesome-crowd-combat-misinformation_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: daijifeng001_TA-FCN_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: dennlinger_tsar-2022-shared-task_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: dylanashley_story-distiller_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: felixxu35_hamiltoniq_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: freedomintelligence_mllm-bench_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: idt-iti_lightweight-face-detector-pruning_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: iqvianlp_llm-onto-infuse_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: juaml_juharmonize_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: lojzezust_slr_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: lucy3_words_as_gatekeepers_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: microsoft_opendatasheets-framework_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: ml-jku_vnegnn_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: mmp2_megaman_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: nokia_codesearch_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: prasunroy_air-writing_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: qiantianwen_nuscenes-qa_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: salesforce_booksum_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: snap-research_test-time-aggregation-for-cf_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: sosuperic_MeanSum_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: stanfordhci_modelsketchbook_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: tiehangd_Para_DPMM_master_readme.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: uwnetworkslab_netcov_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: vaikkunth_PrivacyFL_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: wenzhengzhang_entqa_main_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv is missing, fill 'O' list as default prediction\n",
      "WARN: zju-vipa_awesome-neural-trees_main_README.md.tsv is missing, fill 'O' list as default prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      " {\n",
      "  \"overall_accuracy\": 0.9559659090909091,\n",
      "  \"overall_macro_precision\": 0.19119318181818182,\n",
      "  \"overall_macro_recall\": 0.2,\n",
      "  \"overall_macro_f1\": 0.1954974582425563,\n",
      "  \"CONFERENCE_macro_precision\": NaN,\n",
      "  \"CONFERENCE_macro_recall\": NaN,\n",
      "  \"CONFERENCE_macro_f1\": NaN,\n",
      "  \"DATASET_macro_precision\": 0.3258739213105163,\n",
      "  \"DATASET_macro_recall\": 0.3333333333333333,\n",
      "  \"DATASET_macro_f1\": 0.3295614229716737,\n",
      "  \"EVALMETRIC_macro_precision\": NaN,\n",
      "  \"EVALMETRIC_macro_recall\": NaN,\n",
      "  \"EVALMETRIC_macro_f1\": NaN,\n",
      "  \"LICENSE_macro_precision\": NaN,\n",
      "  \"LICENSE_macro_recall\": NaN,\n",
      "  \"LICENSE_macro_f1\": NaN,\n",
      "  \"ONTOLOGY_macro_precision\": NaN,\n",
      "  \"ONTOLOGY_macro_recall\": NaN,\n",
      "  \"ONTOLOGY_macro_f1\": NaN,\n",
      "  \"PROGLANG_macro_precision\": NaN,\n",
      "  \"PROGLANG_macro_recall\": NaN,\n",
      "  \"PROGLANG_macro_f1\": NaN,\n",
      "  \"PROJECT_macro_precision\": NaN,\n",
      "  \"PROJECT_macro_recall\": NaN,\n",
      "  \"PROJECT_macro_f1\": NaN,\n",
      "  \"PUBLICATION_macro_precision\": 0.3144163875289874,\n",
      "  \"PUBLICATION_macro_recall\": 0.3333333333333333,\n",
      "  \"PUBLICATION_macro_f1\": 0.3235986342692346,\n",
      "  \"SOFTWARE_macro_precision\": NaN,\n",
      "  \"SOFTWARE_macro_recall\": NaN,\n",
      "  \"SOFTWARE_macro_f1\": NaN,\n",
      "  \"WORKSHOP_macro_precision\": NaN,\n",
      "  \"WORKSHOP_macro_recall\": NaN,\n",
      "  \"WORKSHOP_macro_f1\": NaN\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "d:\\miniconda3\\envs\\embed\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "print(os.getcwd())\n",
    "ref_dir = '../results/Meta-Llama-3-8B-Instruct/test_unlabeled/'\n",
    "pred_dir = '../results/Meta-Llama-3-8B-Instruct/prompt-0/'\n",
    "score_dir = '../results/scores/'\n",
    "\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "os.makedirs(score_dir, exist_ok=True)\n",
    "\n",
    "ref_file_names = sorted([fp for fp in os.listdir(ref_dir) if os.path.isfile(f'{ref_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "\n",
    "if len(ref_file_names) == 0:\n",
    "    raise Exception(\"ERROR: No reference files found, configuration error?\")\n",
    "\n",
    "all_ref_bio_tags_list = []\n",
    "for ref_file_name in ref_file_names:\n",
    "    src_path = os.path.join(ref_dir, ref_file_name)\n",
    "    ref_path = src_path\n",
    "    all_ref_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "\n",
    "pred_file_names = sorted([fp for fp in os.listdir(pred_dir) if os.path.isfile(f'{pred_dir}/{fp}') and fp.endswith('.tsv')])\n",
    "all_pred_bio_tags_list = []\n",
    "for idx, ref_file_name in enumerate(ref_file_names):\n",
    "    try:\n",
    "        src_path = os.path.join(pred_dir, ref_file_name)\n",
    "        ref_path = os.path.join(ref_dir, ref_file_name)\n",
    "        all_pred_bio_tags_list.append(to_char_bio(src_path, ref_path))\n",
    "    except FileNotFoundError:\n",
    "        nbr_labels = len(all_ref_bio_tags_list[idx])\n",
    "        assert nbr_labels == len(LABELS), \"ERROR: reference tags doesn't have ${len(LABELS)} labels.\"\n",
    "        pred = []\n",
    "        for label_idx in range(nbr_labels):\n",
    "            pred.append(['O'] * len(all_ref_bio_tags_list[idx][label_idx]))\n",
    "\n",
    "        print(f\"WARN: {ref_file_name} is missing, fill 'O' list as default prediction\")\n",
    "        all_pred_bio_tags_list.append(pred)\n",
    "# Sanity checking\n",
    "for idx, (ref_list, pred_list) in enumerate(zip(all_ref_bio_tags_list, all_pred_bio_tags_list)):\n",
    "    for label_idx, (ref, pred) in enumerate(zip(ref_list, pred_list)):\n",
    "        assert len(ref) == len(pred), f'ERROR: {ref_file_names[idx]}, label: {LABELS[label_idx]}, reference length: {len(ref)}, prediction length: {len(pred)}'\n",
    "\n",
    "scores = {}\n",
    "################################################################################\n",
    "# Consider whole dataset\n",
    "################################################################################\n",
    "ref_bio_tags_list = flatten(flatten(all_ref_bio_tags_list))\n",
    "pred_bio_tags_list = flatten(flatten(all_pred_bio_tags_list))\n",
    "\n",
    "accuracy = accuracy_score(ref_bio_tags_list, pred_bio_tags_list)\n",
    "scores['overall_accuracy'] = accuracy\n",
    "average = 'macro'\n",
    "ref_bio_tags_list = flatten(flatten(all_ref_bio_tags_list))\n",
    "pred_bio_tags_list = flatten(flatten(all_pred_bio_tags_list))\n",
    "\n",
    "f1 = f1_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "precision = precision_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "recall = recall_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "scores[f\"overall_{average}_precision\"] = precision\n",
    "scores[f\"overall_{average}_recall\"] = recall\n",
    "scores[f\"overall_{average}_f1\"] = f1\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# For each class\n",
    "################################################################################\n",
    "label_to_ref_bio_tags_list = defaultdict(list)\n",
    "label_to_pred_bio_tags_list = defaultdict(list)\n",
    "for ref_bio_tags_list, pred_bio_tags_list in zip(all_ref_bio_tags_list, all_pred_bio_tags_list):\n",
    "    if len(ref_bio_tags_list) != len(LABELS):\n",
    "        print('ERROR: ref bio tags list')\n",
    "    if len(pred_bio_tags_list) != len(LABELS):\n",
    "        print('ERROR: pred bio tags list')\n",
    "\n",
    "    for label, ref_bio_tags, pred_bio_tags in zip(LABELS, ref_bio_tags_list, pred_bio_tags_list):\n",
    "        label_to_ref_bio_tags_list[label].extend(ref_bio_tags)\n",
    "        label_to_pred_bio_tags_list[label].extend(pred_bio_tags)\n",
    "        if len(label_to_ref_bio_tags_list[label]) != len(label_to_pred_bio_tags_list[label]):\n",
    "            print('ERROR: label_to_ref_pred_bio_tags')\n",
    "\n",
    "\n",
    "for label in label_to_ref_bio_tags_list.keys():\n",
    "    ref_bio_tags_list = label_to_ref_bio_tags_list[label]\n",
    "    pred_bio_tags_list = label_to_pred_bio_tags_list[label]\n",
    "    accuracy = accuracy_score(ref_bio_tags_list, pred_bio_tags_list)\n",
    "    f1 = f1_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "    precision = precision_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "    recall = recall_score(ref_bio_tags_list, pred_bio_tags_list, average=average)\n",
    "    scores[f\"{label}_{average}_precision\"] = precision\n",
    "    scores[f\"{label}_{average}_recall\"] = recall\n",
    "    scores[f\"{label}_{average}_f1\"] = f1\n",
    "\n",
    "print(\"Scores:\\n\", json.dumps(scores, indent=2))\n",
    "\n",
    "with open(os.path.join(score_dir, 'Meta-Llama-3-8B-Instruct-scores.json'), 'w') as fd:\n",
    "    json.dump(scores, fd, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
