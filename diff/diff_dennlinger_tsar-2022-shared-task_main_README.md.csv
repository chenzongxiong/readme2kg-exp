sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,# UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?,# <PUBLICATION>UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?</PUBLICATION><WORKSHOP>TSAR-2022</WORKSHOP> Shared Task: Is Compute All We Need for Lexical Simplification?,"# UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?
","# <PROJECT>UniHD</PROJECT> at <WORKSHOP>TSAR-2022 Shared Task</WORKSHOP>: Is Compute All We Need for Lexical Simplification?
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\44b1b36a.txt,0.9940828402366864
3,"We further include modifications made for Spanish and Portuguese, obtaining SotA in those languages as well.","We further include modifications made for Spanish and Portuguese, obtaining SotA in those languages as well.","We further include modifications made for Spanish and Portuguese, obtaining SotA in those languages as well.","We further include modifications made for Spanish and Portuguese, obtaining <EVALMETRIC>SotA</EVALMETRIC> in those languages as well.",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\a27c66e7.txt,1.0
4,"**Find the arXiv version of our paper here: https://arxiv.org/abs/2301.01764**  In case you find these results useful, please consider citing our work in addition to the shared task paper (see below).  ``` @article{aumiller-gertz-2023-unihd, author = {Aumiller, Dennis and Gertz, Michael}, title = {{UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?}}","**Find the arXiv version of our paper here: https://arxiv.org/abs/2301.01764**  In case you find these results useful, please consider citing our work in addition to the shared task paper (see below).  ``` @article{aumiller-gertz-2023-unihd, author = {Aumiller, Dennis and Gertz, Michael}, title = {{<PUBLICATION>UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?</PUBLICATION><WORKSHOP>TSAR-2022</WORKSHOP> Shared Task: Is Compute All We Need for Lexical Simplification?}}","**Find the arXiv version of our paper here: https://arxiv.org/abs/2301.01764**  In case you find these results useful, please consider citing our work in addition to the shared task paper (see below).   @article{aumiller-gertz-2023-unihd, author = {Aumiller, Dennis and Gertz, Michael}, title = {{UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?}}","**Find the arXiv version of our paper here: https://arxiv.org/abs/2301.01764**  In case you find these results useful, please consider citing our work in addition to the shared task paper (see below).   @article{aumiller-gertz-2023-unihd, author = {Aumiller, Dennis and Gertz, Michael}, title = {{<PUBLICATION>UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?</PUBLICATION>}}",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\abdd8d36.txt,0.996078431372549
5,", journal = {CoRR}, volume = {abs/2301.01764}, eprinttype = {arXiv}, eprint = {2301.01764}, url = {https://arxiv.org/abs/2301.01764} } ```   ## Setup for Reproduction To run the results for the respective subsets, you need to provide your own OpenAI API key in a file named `config.py`.",", journal = {CoRR}, volume = {abs/2301.01764}, eprinttype = {arXiv}, eprint = {2301.01764}, url = {https://arxiv.org/abs/2301.01764} } ```   ## Setup for Reproduction To run the results for the respective subsets, you need to provide your own OpenAI API key in a file named `config.py`.",", journal = {CoRR}, volume = {abs/2301.01764}, eprinttype = {arXiv}, eprint = {2301.01764}, url = {https://arxiv.org/abs/2301.01764} }    ## Setup for Reproduction To run the results for the respective subsets, you need to provide your own OpenAI API key in a file named config.py.
",", journal = {<PUBLICATION>CoRR</PUBLICATION>}, volume = {abs/2301.01764}, eprinttype = {arXiv}, eprint = {2301.01764}, url = {https://arxiv.org/abs/2301.01764} }    ## Setup for Reproduction To run the results for the respective subsets, you need to provide your own OpenAI API key in a file named config.py.
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\f35f45c8.txt,0.9859154929577465
6,"A template can be found in `config.py.template`, which already contains the correct variable name and structure.","A template can be found in `config.py.template`, which already contains the correct variable name and structure.","A template can be found in config.py.template, which already contains the correct variable name and structure.","A template can be found in <SOFTWARE>config.py.template</SOFTWARE>, which already contains the correct variable name and structure.",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\df366c00.txt,0.990990990990991
7,Other required packages can be installed with `python3 -m pip install -r requirements.txt` from the main repository folder.,Other required packages can be installed with `<SOFTWARE>python3</SOFTWARE> -m <SOFTWARE>pip</SOFTWARE> install -r requirements.txt` from the main repository folder.,Other required packages can be installed with python3 -m pip install -r requirements.txt from the main repository folder.,Other required packages can be installed with <PROGLANG>python3</PROGLANG> -m pip install -r requirements.txt from the main repository folder.,../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\52d28c27.txt,0.9918032786885246
8,"Obtaining predictions from the API should be possible by running `context_predictor.py` for English results, and the `_es.py` and `\_pt.py` files for Spanish and Portuguese predictions, respectively.","Obtaining predictions from the API should be possible by running `context_predictor.py` for English results, and the `_es.py` and `\_pt.py` files for Spanish and Portuguese predictions, respectively.","Obtaining predictions from the API should be possible by running context_predictor.py for English results, and the _es.py and \_pt.py files for Spanish and Portuguese predictions, respectively.","Obtaining predictions from the API should be possible by running <SOFTWARE>context_predictor.py</SOFTWARE> for English results, and the <SOFTWARE>_es.py</SOFTWARE> and <SOFTWARE>\_pt.py</SOFTWARE> files for Spanish and Portuguese predictions, respectively.",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\b0327313.txt,0.9846938775510204
9,"Our predictions submitted to the workshop can be found either in the subfolder `en/`, or the fiel `UniHD.zip`.","Our predictions submitted to the workshop can be found either in the subfolder `en/`, or the fiel `UniHD.zip`.","Our predictions submitted to the workshop can be found either in the subfolder en/, or the fiel UniHD.zip.","Our predictions submitted to the <WORKSHOP>workshop</WORKSHOP> can be found either in the subfolder en/, or the fiel UniHD.zip.",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\b8610202.txt,0.9814814814814815
10,Later additions of the Spanish and Portuguese predictions are in the respective folders `es/` (for Spanish) and `pt/` (Portuguese).,Later additions of the Spanish and Portuguese predictions are in the respective folders `es/` (for Spanish) and `pt/` (Portuguese).,Later additions of the Spanish and Portuguese predictions are in the respective folders es/ (for Spanish) and pt/ (for Portuguese).,Later additions of the Spanish and Portuguese predictions are in the respective folders es/ (for Spanish) and pt/ (for <PROGLANG>Portuguese</PROGLANG>).,../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\70ac1367.txt,0.9694656488549618
14,"In our experiments, we used the (relatively ""outdated"" `text-davinci-002` model, which may respond differently to prompt variants than later additions","In our experiments, we used the (relatively ""outdated"" `text-davinci-002` model, which may respond differently to prompt variants than later additions","In our experiments, we used the (relatively ""outdated"" text-davinci-002 model, which may respond differently to prompt variants than later additions","In our experiments, we used the (relatively ""outdated"" <SOFTWARE>text-davinci-002</SOFTWARE> model, which may respond differently to prompt variants than later additions",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\41dc25b1.txt,0.9932885906040269
15,.,.,Sec.,Sec.,../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\498ae211.txt,0.4
16,"# Original README: TSAR-2022-Shared-Task Datasets and Evaluation Scripts TSAR2022 Shared Task on Lexical Simplification for English (en), Spanish (es) and Portuguese (pt) - Datasets and Evaluation scripts  Please look at the website of the Shared Task for more details about the Evaluation Benchmark, Guidelines, Registration Form, etc...","# Original README: <WORKSHOP>TSAR-2022</WORKSHOP>-Shared-Task Datasets and Evaluation Scripts <WORKSHOP>TSAR2022</WORKSHOP> Shared Task on Lexical Simplification for English (en), Spanish (es) and Portuguese (pt) - Datasets and Evaluation scripts  Please look at the website of the Shared Task for more details about the Evaluation Benchmark, Guidelines, Registration Form, etc...","# Original README: TSAR-2022-Shared-Task Datasets and Evaluation Scripts TSAR2022 Shared Task on Lexical Simplification for English (en), Spanish (es) and Portuguese (pt) - Datasets and Evaluation scripts  Please look at the website of the Shared Task for more details about the Evaluation Benchmark, Guidelines, Registration Form, etc...
","# Original README: <DATASET>TSAR-2022-Shared-Task</DATASET> Datasets and Evaluation Scripts <PROJECT>TSAR2022 Shared Task on Lexical Simplification for English (en), Spanish (es) and Portuguese (pt)</PROJECT> - Datasets and Evaluation scripts  Please look at the website of the Shared Task for more details about the <EVALMETRIC>Evaluation Benchmark</EVALMETRIC>, Guidelines, Registration Form, etc...
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\947dc8be.txt,0.9985228951255539
17,<br/>[TSAR-2022 Shared-Task website](https://taln.upf.edu/pages/tsar2022-st/)  ## Datasets  There is no training dataset for the TSAR-2022 Shared Task.,<br/>[<WORKSHOP>TSAR-2022</WORKSHOP> Shared-Task website](https://taln.upf.edu/pages/<WORKSHOP>tsar2022</WORKSHOP>-st/)  ## Datasets  There is no training dataset for the <WORKSHOP>TSAR-2022</WORKSHOP> Shared Task.,"<br/>[TSAR-2022 Shared-Task website](https://taln.upf.edu/pages/tsar2022-st/)  ## Datasets  There is no training dataset for the TSAR-2022 Shared Task.
","<br/>[<PROJECT>TSAR-2022 Shared-Task</PROJECT> website](https://taln.upf.edu/pages/<PROJECT>tsar2022-st</PROJECT>/)  ## <DATASET>Datasets</DATASET>  There is no training <DATASET>dataset</DATASET> for the <PROJECT>TSAR-2022 Shared Task</PROJECT>.
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\9f4ea6c7.txt,0.9966996699669967
18,"However, a sample of 10 or 12 instances with gold standard annotations is provided here as a trial/sample dataset.","However, a sample of 10 or 12 instances with gold standard annotations is provided here as a trial/sample dataset.","However, a sample of 10 or 12 instances with gold standard annotations is provided here as a trial/sample dataset.","However, a sample of 10 or 12 instances with gold standard annotations is provided here as a trial/sample <DATASET>dataset</DATASET>.",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\518157a6.txt,1.0
19,"<br/>  <br/>  Format of the files: - Format of the *trial_none* and *test_none* files: <span style=""font-weight:normal"">Sentence&lt;TAB&gt;ComplexWord</span> - Format of the *trial_gold* and *test_gold* files: <span style=""font-weight:normal"">Sentence&lt;TAB&gt;ComplexWord&lt;TAB&gt;Annotation1&lt;TAB&gt;Annotation2&lt;TAB&gt;...","<br/>  <br/>  Format of the files: - Format of the *trial_none* and *test_none* files: <span style=""font-weight:normal"">Sentence&lt;TAB&gt;ComplexWord</span> - Format of the *trial_gold* and *test_gold* files: <span style=""font-weight:normal"">Sentence&lt;TAB&gt;ComplexWord&lt;TAB&gt;Annotation1&lt;TAB&gt;Annotation2&lt;TAB&gt;...","<br/>  <br/>  Format of the files: - Format of the *trial_none* and *test_none* files: <span style=""font-weight:normal"">Sentence&lt;TAB&gt;ComplexWord</span> - Format of the *trial_gold* and *test_gold* files: <span style=""font-weight:normal"">Sentence&lt;TAB&gt;ComplexWord&lt;TAB&gt;Annotation1&lt;TAB&gt;Annotation2&lt;TAB&gt;...
","<br/>  <br/>  Format of the files: - Format of the *<DATASET>trial_none</DATASET>* and *<DATASET>test_none</DATASET>* files: <span style=""font-weight:normal"">Sentence&lt;TAB&gt;ComplexWord</span> - Format of the *<DATASET>trial_gold</DATASET>* and *<DATASET>test_gold</DATASET>* files: <span style=""font-weight:normal"">Sentence&lt;TAB&gt;ComplexWord&lt;TAB&gt;Annotation1&lt;TAB&gt;Annotation2&lt;TAB&gt;...
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\86df254a.txt,0.9984917043740573
20,"&lt;TAB&gt;AnnotationN</span>   ### Trial dataset The trial dataset consists of a set of 10 instances (for English and Portuguese) and 12 instances (for Spanish) of a sentence, a target complex word.","&lt;TAB&gt;AnnotationN</span>   ### Trial dataset The trial dataset consists of a set of 10 instances (for English and Portuguese) and 12 instances (for Spanish) of a sentence, a target complex word.","### Trial dataset The trial dataset consists of a set of 10 instances (for English and Portuguese) and 12 instances (for Spanish) of a sentence, a target complex word.
","### Trial dataset The trial dataset consists of a set of 10 instances (for English and Portuguese) and 12 instances (for Spanish) of a sentence, a target complex word.
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\00ba49b0.txt,0.9100817438692098
21,The *trial_none* files contain only the instances and the *trial_gold* files contain the instances and set of gold annotations,The *trial_none* files contain only the instances and the *trial_gold* files contain the instances and set of gold annotations,The *trial_none* files contain only the instances and the *trial_gold* files contain the instances and set of gold annotations,The *<DATASET>trial_none</DATASET>* files contain only the instances and the *<DATASET>trial_gold</DATASET>* files contain the instances and set of gold annotations,../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\975fb1a2.txt,1.0
22,.,.,Sec.,Sec.,../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\498ae211.txt,0.4
23,- /datasets/trial/tsar2022_en_trial_none.tsv - /datasets/trial/tsar2022_en_trial_gold.tsv - /datasets/trial/tsar2022_es_trial_none.tsv - /datasets/trial/tsar2022_es_trial_gold.tsv - /datasets/trial/tsar2022_pt_trial_none.tsv - /datasets/trial/tsar2022_pt_trial_gold.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words,- /datasets/trial/<WORKSHOP>tsar2022</WORKSHOP>_en_trial_none.tsv - /datasets/trial/<WORKSHOP>tsar2022</WORKSHOP>_en_trial_gold.tsv - /datasets/trial/<WORKSHOP>tsar2022</WORKSHOP>_es_trial_none.tsv - /datasets/trial/<WORKSHOP>tsar2022</WORKSHOP>_es_trial_gold.tsv - /datasets/trial/<WORKSHOP>tsar2022</WORKSHOP>_pt_trial_none.tsv - /datasets/trial/<WORKSHOP>tsar2022</WORKSHOP>_pt_trial_gold.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words,"- /datasets/trial/tsar2022_en_trial_none.tsv - /datasets/trial/tsar2022_en_trial_gold.tsv - /datasets/trial/tsar2022_es_trial_none.tsv - /datasets/trial/tsar2022_es_trial_gold.tsv - /datasets/trial/tsar2022_pt_trial_none.tsv - /datasets/trial/tsar2022_pt_trial_gold.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words
","- /datasets/trial/<DATASET>tsar2022_en_trial_none</DATASET>.tsv - /datasets/trial/<DATASET>tsar2022_en_trial_gold</DATASET>.tsv - /datasets/trial/<DATASET>tsar2022_es_trial_none</DATASET>.tsv - /datasets/trial/<DATASET>tsar2022_es_trial_gold</DATASET>.tsv - /datasets/trial/<DATASET>tsar2022_pt_trial_none</DATASET>.tsv - /datasets/trial/<DATASET>tsar2022_pt_trial_gold</DATASET>.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\2f856695.txt,0.9988165680473373
24,.,.,Sec.,Sec.,../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\498ae211.txt,0.4
25,"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .","- English test_none dataset (373 instances)<br/>  /datasets/test/<WORKSHOP>tsar2022</WORKSHOP>_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/<WORKSHOP>tsar2022</WORKSHOP>_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/<WORKSHOP>tsar2022</WORKSHOP>_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/<WORKSHOP>tsar2022</WORKSHOP>_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/<WORKSHOP>tsar2022</WORKSHOP>_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/<WORKSHOP>tsar2022</WORKSHOP>_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  <EVALMETRIC>MAP@1</EVALMETRIC>/<EVALMETRIC>Potential@1</EVALMETRIC>/<EVALMETRIC>Precision@1</EVALMETRIC> -  <EVALMETRIC>MAP@3</EVALMETRIC> -  <EVALMETRIC>MAP@5</EVALMETRIC> -  <EVALMETRIC>MAP@10</EVALMETRIC> -  <EVALMETRIC>Potential@3</EVALMETRIC> -  <EVALMETRIC>Potential@5</EVALMETRIC> -  <EVALMETRIC>Potential@10</EVALMETRIC> -  <EVALMETRIC>Accuracy@1@top_gold_1</EVALMETRIC> -  <EVALMETRIC>Accuracy@2@top_gold_1</EVALMETRIC> -  <EVALMETRIC>Accuracy@3@top_gold_1</EVALMETRIC>    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  <EVALMETRIC>Potential@K</EVALMETRIC>  K={1..10}  -  <EVALMETRIC>MAP@K</EVALMETRIC>  K={1..10} -  <EVALMETRIC>Precision@K</EVALMETRIC>  K={1..10}  (macro-average) -  <EVALMETRIC>Recall@K</EVALMETRIC>  K={1..10}     (macro-average) -  <EVALMETRIC>Accuracy@K@top_gold_1</EVALMETRIC>   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  <EVALMETRIC>MAP@1</EVALMETRIC>/<EVALMETRIC>Potential@1</EVALMETRIC>/<EVALMETRIC>Precision@1</EVALMETRIC>     -  <EVALMETRIC>MAP@3</EVALMETRIC>     -  <EVALMETRIC>MAP@5</EVALMETRIC>     -  <EVALMETRIC>MAP@10</EVALMETRIC>     -  <EVALMETRIC>Potential@3</EVALMETRIC>     -  <EVALMETRIC>Potential@5</EVALMETRIC>     -  <EVALMETRIC>Potential@10</EVALMETRIC>     -  <EVALMETRIC>Accuracy@1@top_gold_1</EVALMETRIC>     -  <EVALMETRIC>Accuracy@2@top_gold_1</EVALMETRIC>     -  <EVALMETRIC>Accuracy@3@top_gold_1</EVALMETRIC>          Script options and help  ```console Evaluation Script for the <WORKSHOP>TSAR-2022</WORKSHOP> Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console <SOFTWARE>python3</SOFTWARE> .","- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode    usage example  console python3 .
","- <DATASET>English test_none</DATASET> dataset (373 instances)<br/>  /datasets/test/<DATASET>tsar2022_en_test_none</DATASET>.tsv    - <DATASET>Spanish test_none</DATASET> dataset (368 instances)<br/>  /datasets/test/<DATASET>tsar2022_es_test_none</DATASET>.tsv    - <DATASET>Portuguese test_none</DATASET> dataset (374 instances)<br/>  /datasets/test/<DATASET>tsar2022_pt_test_none</DATASET>.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - <DATASET>English test_gold</DATASET> dataset (373 instances)<br/>  /datasets/test/<DATASET>tsar2022_en_test_gold</DATASET>.tsv    - <DATASET>Spanish test_gold</DATASET> dataset (368 instances)<br/>  /datasets/test/<DATASET>tsar2022_es_test_gold</DATASET>.tsv    - <DATASET>Portuguese test_gold</DATASET> dataset (374 instances)<br/>  /datasets/test/<DATASET>tsar2022_pt_test_gold</DATASET>.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  <EVALMETRIC>MAP@1</EVALMETRIC>/<EVALMETRIC>Potential@1</EVALMETRIC>/<EVALMETRIC>Precision@1</EVALMETRIC> -  <EVALMETRIC>MAP@3</EVALMETRIC> -  <EVALMETRIC>MAP@5</EVALMETRIC> -  <EVALMETRIC>MAP@10</EVALMETRIC> -  <EVALMETRIC>Potential@3</EVALMETRIC> -  <EVALMETRIC>Potential@5</EVALMETRIC> -  <EVALMETRIC>Potential@10</EVALMETRIC> -  <EVALMETRIC>Accuracy@1@top_gold_1</EVALMETRIC> -  <EVALMETRIC>Accuracy@2@top_gold_1</EVALMETRIC> -  <EVALMETRIC>Accuracy@3@top_gold_1</EVALMETRIC>    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  <EVALMETRIC>Potential@K</EVALMETRIC>  K={1..10}  -  <EVALMETRIC>MAP@K</EVALMETRIC>  K={1..10} -  <EVALMETRIC>Precision@K</EVALMETRIC>  K={1..10}  (macro-average) -  <EVALMETRIC>Recall@K</EVALMETRIC>  K={1..10}     (macro-average) -  <EVALMETRIC>Accuracy@K@top_gold_1</EVALMETRIC>   K={1..10}     ## Evaluation Scripts   ### <SOFTWARE>tsar_eval.py</SOFTWARE>  This script evaluates the following metric:      -  <EVALMETRIC>MAP@1</EVALMETRIC>/<EVALMETRIC>Potential@1</EVALMETRIC>/<EVALMETRIC>Precision@1</EVALMETRIC>     -  <EVALMETRIC>MAP@3</EVALMETRIC>     -  <EVALMETRIC>MAP@5</EVALMETRIC>     -  <EVALMETRIC>MAP@10</EVALMETRIC>     -  <EVALMETRIC>Potential@3</EVALMETRIC>     -  <EVALMETRIC>Potential@5</EVALMETRIC>     -  <EVALMETRIC>Potential@10</EVALMETRIC>     -  <EVALMETRIC>Accuracy@1@top_gold_1</EVALMETRIC>     -  <EVALMETRIC>Accuracy@2@top_gold_1</EVALMETRIC>     -  <EVALMETRIC>Accuracy@3@top_gold_1</EVALMETRIC>          Script options and help  console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: <SOFTWARE>tsar_eval.py</SOFTWARE> <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode    usage example  console python3 .
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\6ebc6684.txt,0.9977638640429338
26,/tsar_eval.py --gold_file .,/tsar_eval.py --gold_file .,"/tsar_eval.py --gold_file .
","/tsar_eval.py --gold_file .
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\cffe77c7.txt,0.9818181818181818
27,/gold/tsar_es_gold.tsv --predictions_file .,/gold/tsar_es_gold.tsv --predictions_file .,/gold/tsar_es_gold.tsv --predictions_file .,/gold/<DATASET>tsar_es_gold</DATASET>.tsv --predictions_file .,../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\240761f1.txt,1.0
28,/predicted/TEAMNAME_TRACKNAME_RUNNAME.tsv  --output_file .,/predicted/TEAMNAME_TRACKNAME_RUNNAME.tsv  --output_file .,"/predicted/TEAMNAME_TRACKNAME_RUNNAME.tsv  --output_file .
","/predicted/<PROJECT>TEAMNAME</PROJECT>_<PROJECT>TRACKNAME</PROJECT>_<PROJECT>RUNNAME</PROJECT>.tsv  --output_file .
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\8baabb65.txt,0.9914529914529915
29,"/results/TEAMNAME_TRACKNAME_RUNNAME.tsv.eval ```  ## Dataset Compilation and Baselines  A paper describing the compilation of the datasets for English, Portuguese and Spanish that includes several experiments with  two state-of-the-art approaches for Lexical Simplification has been published at this link: https://www.frontiersin.org/articles/10.3389/frai.2022.991242  [Lexical Simplification Benchmarks for English, Portuguese, and Spanish](https://www.frontiersin.org/articles/10.3389/frai.2022.991242).","/results/TEAMNAME_TRACKNAME_RUNNAME.tsv.eval ```  ## Dataset Compilation and Baselines  A paper describing the compilation of the datasets for English, Portuguese and Spanish that includes several experiments with  two state-of-the-art approaches for Lexical Simplification has been published at this link: https://www.frontiersin.org/articles/10.3389/frai.2022.991242  [<PUBLICATION>Lexical Simplification Benchmarks for English, Portuguese, and Spanish</PUBLICATION>](https://www.frontiersin.org/articles/10.3389/frai.2022.991242).","/results/TEAMNAME_TRACKNAME_RUNNAME.tsv.eval   ## Dataset Compilation and Baselines  A paper describing the compilation of the datasets for English, Portuguese and Spanish that includes several experiments with  two state-of-the-art approaches for Lexical Simplification has been published at this link: https://www.frontiersin.org/articles/10.3389/frai.2022.991242  [Lexical Simplification Benchmarks for English, Portuguese, and Spanish](https://www.frontiersin.org/articles/10.3389/frai.2022.991242).
","/results/TEAMNAME_TRACKNAME_RUNNAME.tsv.eval   ## <DATASET>Dataset Compilation and Baselines</DATASET>  A <PUBLICATION>paper</PUBLICATION> describing the compilation of the <DATASET>datasets for English, Portuguese and Spanish</DATASET> that includes several experiments with  two state-of-the-art approaches for <PROJECT>Lexical Simplification</PROJECT> has been published at this link: https://www.frontiersin.org/articles/10.3389/frai.2022.991242  [<PUBLICATION>Lexical Simplification Benchmarks for English, Portuguese, and Spanish</PUBLICATION>](https://www.frontiersin.org/articles/10.3389/frai.2022.991242).
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\9dc81a7a.txt,0.996039603960396
30,"<br/> Sanja Štajner, Daniel Ferrés, Matthew Shardlow, Kai North, Marcos Zampieri and  Horacio Saggion.","<br/> Sanja Štajner, Daniel Ferrés, Matthew Shardlow, Kai North, Marcos Zampieri and  Horacio Saggion.","<br/> Sanja Štajner, Daniel Ferrés, Matthew Shardlow, Kai North, Marcos Zampieri and  Horacio Saggion.
","<br/> <PUBLICATION>Sanja Štajner</PUBLICATION>, <PUBLICATION>Daniel Ferrés</PUBLICATION>, <PUBLICATION>Matthew Shardlow</PUBLICATION>, <PUBLICATION>Kai North</PUBLICATION>, <PUBLICATION>Marcos Zampieri</PUBLICATION> and  <PUBLICATION>Horacio Saggion</PUBLICATION>.
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\79a2a8fd.txt,0.9951219512195122
32,Artif.,Artif.,"Artif.
","Artif.
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\7755fc55.txt,0.9230769230769231
33,Intell.,Intell.,"Intell.
","Intell.
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\91a4ec84.txt,0.9333333333333333
36,<br/> doi: 10.3389/frai.2022.991242  Preprint available at ArXiV: [https://arxiv.org/abs/2209.05301](https://arxiv.org/abs/2209.05301)   ## License  The python scripts follow [AGPL 3.0v license](LICENSE).,<br/> doi: 10.3389/frai.2022.991242  Preprint available at ArXiV: [https://arxiv.org/abs/2209.05301](https://arxiv.org/abs/2209.05301)   ## License  The <PROGLANG>python</PROGLANG> scripts follow [<LICENSE>AGPL 3.0v license</LICENSE>](LICENSE).,"<br/> doi: 10.3389/frai.2022.991242  Preprint available at ArXiV: [https://arxiv.org/abs/2209.05301](https://arxiv.org/abs/2209.05301)   ## License  The python scripts follow [AGPL 3.0v license](LICENSE).
","<br/> doi: 10.3389/frai.2022.991242  Preprint available at ArXiV: [https://arxiv.org/abs/2209.05301](https://arxiv.org/abs/2209.05301)   ## License  The <PROGLANG>python</PROGLANG> scripts follow [<LICENSE>AGPL 3.0v license</LICENSE>](LICENSE).
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\9bf7f4db.txt,0.9975550122249389
37,The datasets (under the /datasets directory) are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [CC-BY-NC-SA-4.0](CC-BY-NC-SA-4.0).  ## Contact https://taln.upf.edu/pages/tsar2022-st/#contact,The datasets (under the /datasets directory) are licensed under a <LICENSE>Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</LICENSE> [<LICENSE>CC-BY-NC-SA-4.0</LICENSE>](<LICENSE>CC-BY-NC-SA-4.0</LICENSE>).  ## Contact https://taln.upf.edu/pages/<WORKSHOP>tsar2022</WORKSHOP>-st/#contact,"Here is the annotated text in Markdown format:

The datasets (under the /datasets directory) are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [CC-BY-NC-SA-4.0](CC-BY-NC-SA-4.0).  ## Contact https://taln.upf.edu/pages/tsar2022-st/#contact
","Here is the annotated text in Markdown format:

The <DATASET>datasets</DATASET> (under the /<DATASET>datasets</DATASET> directory) are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [<LICENSE>CC-BY-NC-SA-4.0</LICENSE>](<LICENSE>CC-BY-NC-SA-4.0</LICENSE>).  ## Contact https://<WORKSHOP>taln.upf.edu/pages/tsar2022-st</WORKSHOP>/#contact
",../results/deepseek-chat/prompt-0/zzz_dennlinger_tsar-2022-shared-task_main_README.md.tsv\b21912d9.txt,0.9077212806026366
