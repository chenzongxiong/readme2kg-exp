sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,"# ActionFormer: Localizing Moments of Actions with Transformers  ## Introduction This code repo implements Actionformer, one of the first Transformer-based model for temporal action localization --- detecting the onsets and offsets of action instances and recognizing their action categories.","# <PUBLICATION>ActionFormer: Localizing Moments of Actions with Transformers</PUBLICATION><SOFTWARE>ActionFormer</SOFTWARE>: Localizing Moments of Actions with Transformers  ## Introduction This code repo implements <SOFTWARE>Actionformer</SOFTWARE>, one of the first Transformer-based model for temporal action localization --- detecting the onsets and offsets of action instances and recognizing their action categories.","# ActionFormer: Localizing Moments of Actions with Transformers  ## Introduction This code repo implements Actionformer, one of the first Transformer-based model for temporal action localization --- detecting the onsets and offsets of action instances and recognizing their action categories.
","# <SOFTWARE>ActionFormer</SOFTWARE>: Localizing Moments of Actions with Transformers  ## Introduction This code repo implements <SOFTWARE>Actionformer</SOFTWARE>, one of the first Transformer-based model for temporal action localization --- detecting the onsets and offsets of action instances and recognizing their action categories.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dc162492.txt,0.9982905982905983
2,"Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% mAP for the first time.","Without bells and whistles, <SOFTWARE>ActionFormer</SOFTWARE> achieves 71.0% <EVALMETRIC>mAP</EVALMETRIC> at <EVALMETRIC>tIoU</EVALMETRIC>=0.5 on <DATASET>THUMOS14</DATASET>, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% <EVALMETRIC>mAP</EVALMETRIC> for the first time.","Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% mAP for the first time.","Without bells and whistles, <SOFTWARE>ActionFormer</SOFTWARE> achieves 71.0% <EVALMETRIC>mAP</EVALMETRIC> at <EVALMETRIC>tIoU</EVALMETRIC>=0.5 on <DATASET>THUMOS14</DATASET>, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% <EVALMETRIC>mAP</EVALMETRIC> for the first time.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c0141251.txt,1.0
3,"Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.56% average mAP) and the more challenging EPIC-Kitchens 100 (+13.5% average mAP over prior works).","Further, <SOFTWARE>ActionFormer</SOFTWARE> demonstrates strong results on <DATASET>ActivityNet 1.3</DATASET> (36.56% average <EVALMETRIC>mAP</EVALMETRIC>) and the more challenging <DATASET>EPIC-Kitchens 100</DATASET> (+13.5% average <EVALMETRIC>mAP</EVALMETRIC> over prior works).","Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.56% average mAP) and the more challenging EPIC-Kitchens 100 (+13.5% average mAP over prior works).","Further, ActionFormer demonstrates strong results on <DATASET>ActivityNet 1.3</DATASET> (36.56% average <EVALMETRIC>mAP</EVALMETRIC>) and the more challenging <DATASET>EPIC-Kitchens 100</DATASET> (+13.5% average <EVALMETRIC>mAP</EVALMETRIC> over prior works).",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\3c5cfe6b.txt,1.0
4,Our paper is accepted to ECCV 2022 and an arXiv version can be found at [this link](https://arxiv.org/abs/2202.07925).,Our paper is accepted to <CONFERENCE>ECCV 2022</CONFERENCE> and an arXiv version can be found at [this link](https://arxiv.org/abs/2202.07925).,"Our paper is accepted to ECCV 2022 and an arXiv version can be found at [this link](https://arxiv.org/abs/2202.07925).
","Our paper is accepted to <CONFERENCE>ECCV 2022</CONFERENCE> and an arXiv version can be found at [this link](https://arxiv.org/abs/2202.07925).
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\a39edd8f.txt,0.9957805907172996
5,"In addition, ActionFormer is the backbone for many winning solutions in the Ego4D Moment Queries Challenge 2022.","In addition, <SOFTWARE>ActionFormer</SOFTWARE> is the backbone for many winning solutions in the <DATASET>Ego4D Moment Queries</DATASET> Challenge 2022.","In addition, ActionFormer is the backbone for many winning solutions in the Ego4D Moment Queries Challenge 2022.","In addition, <SOFTWARE>ActionFormer</SOFTWARE> is the backbone for many winning solutions in the <CONFERENCE>Ego4D Moment Queries Challenge 2022</CONFERENCE>.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1ca1641f.txt,1.0
6,"Our submission in particular is ranked 2nd with a record 21.76% average mAP and 42.54% Recall@1x, tIoU=0.5, nearly three times higher than the official baseline.","Our submission in particular is ranked 2nd with a record 21.76% average <EVALMETRIC>mAP</EVALMETRIC> and 42.54% <EVALMETRIC>Recall@1x</EVALMETRIC>, <EVALMETRIC>tIoU</EVALMETRIC>=0.5, nearly three times higher than the official baseline.","Our submission in particular is ranked 2nd with a record 21.76% average mAP and 42.54% Recall@1x, tIoU=0.5, nearly three times higher than the official baseline.","Our submission in particular is ranked 2nd with a record <EVALMETRIC>21.76% average mAP</EVALMETRIC> and <EVALMETRIC>42.54% Recall@1x</EVALMETRIC>, <EVALMETRIC>tIoU=0.5</EVALMETRIC>, nearly three times higher than the official baseline.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\8196017b.txt,1.0
7,An arXiv version of our tech report can be found at [this link](https://arxiv.org/abs/2211.09074).,An arXiv version of our tech report can be found at [this link](https://arxiv.org/abs/2211.09074).,An arXiv version of our tech report can be found at [this link](https://arxiv.org/abs/2211.09074).,An arXiv version of our tech report can be found at [this link](https://arxiv.org/abs/2211.09074).,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1ea322f2.txt,1.0
8,We invite our audience to try out the code.,We invite our audience to try out the code.,We invite our audience to try out the code.,We invite our audience to try out the code.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\7b22c7c9.txt,1.0
9,"<div align=""center"">   <img src=""teaser.jpg"" width=""600px""/> </div>  Specifically, we adopt a minimalist design and develop a Transformer based model for temporal action localization, inspired by the recent success of Transformers in NLP and vision.","<div align=""center"">   <img src=""teaser.jpg"" width=""600px""/> </div>  Specifically, we adopt a minimalist design and develop a Transformer based model for temporal action localization, inspired by the recent success of <SOFTWARE>Transformers</SOFTWARE> in NLP and vision.","<div align=""center"">   <img src=""teaser.jpg"" width=""600px""/> </div>  Specifically, we adopt a minimalist design and develop a Transformer based model for temporal action localization, inspired by the recent success of Transformers in NLP and vision.
","<div align=""center"">   <img src=""teaser.jpg"" width=""600px""/> </div>  Specifically, we adopt a minimalist design and develop a Transformer based model for temporal action localization, inspired by the recent success of Transformers in NLP and vision.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\4b585ada.txt,0.9979959919839679
10,"Our method, illustrated in the figure, adapts local self-attention to model temporal context in untrimmed videos, classifies every moment in an input video, and regresses their corresponding action boundaries.","Our method, illustrated in the figure, adapts local self-attention to model temporal context in untrimmed videos, classifies every moment in an input video, and regresses their corresponding action boundaries.","Our method, illustrated in the figure, adapts local self-attention to model temporal context in untrimmed videos, classifies every moment in an input video, and regresses their corresponding action boundaries.","Our method, illustrated in the figure, adapts local self-attention to model temporal context in untrimmed videos, classifies every moment in an input video, and regresses their corresponding action boundaries.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\5be1840d.txt,1.0
11,"The result is a deep model that is trained using standard classification and regression loss, and can localize moments of actions in a single shot, without using action proposals or pre-defined anchor windows.","The result is a deep model that is trained using standard classification and regression loss, and can localize moments of actions in a single shot, without using action proposals or pre-defined anchor windows.","The result is a deep model that is trained using standard classification and regression loss, and can localize moments of actions in a single shot, without using action proposals or pre-defined anchor windows.","The result is a deep model that is trained using standard classification and regression loss, and can localize moments of actions in a single shot, without using action proposals or pre-defined anchor windows.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0dbb9ba0.txt,1.0
12,"**Related projects**: > [**SnAG: Scalable and Accurate Video Grounding**](https://arxiv.org/abs/2404.02257) <br> > Fangzhou Mu\*, Sicheng Mo\*, Yin Li <br> > *CVPR 2024* <br> [!","**Related projects**: > [**<PUBLICATION>SnAG: Scalable and Accurate Video Grounding</PUBLICATION>**](https://arxiv.org/abs/2404.02257) <br> > Fangzhou Mu\*, Sicheng Mo\*, Yin Li <br> > *<CONFERENCE>CVPR 2024</CONFERENCE>* <br> [!","**Related projects**: > [**SnAG: Scalable and Accurate Video Grounding**](https://arxiv.org/abs/2404.02257) <br> > Fangzhou Mu\*, Sicheng Mo\*, Yin Li <br> > *CVPR 2024* <br> [!
","**Related <PROJECT>projects</PROJECT>**: > [**<PROJECT>SnAG: Scalable and Accurate Video Grounding</PROJECT>**](https://arxiv.org/abs/2404.02257) <br> > Fangzhou Mu\*, Sicheng Mo\*, Yin Li <br> > *<CONFERENCE>CVPR 2024</CONFERENCE>* <br> [!
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\08058b0d.txt,0.9971830985915493
13,[github](https://img.shields.io/badge/-Github-black?,[github](https://img.shields.io/badge/-Github-black?,"[github](https://img.shields.io/badge/-Github-black?
","[<SOFTWARE>github</SOFTWARE>](https://img.shields.io/badge/-Github-black?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1ea1f68a.txt,0.9904761904761905
14,logo=github)](https://github.com/fmu2/snag_release)  [!,logo=github)](https://github.com/fmu2/snag_release)  [!,"logo=github)](https://github.com/fmu2/snag_release)  [!
","logo=github)](https://github.com/<PROJECT>fmu2/snag_release</PROJECT>)  [!
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\ed788aba.txt,0.990990990990991
15,[github](https://img.shields.io/github/stars/fmu2/snag_release.svg?,[github](https://img.shields.io/github/stars/fmu2/snag_release.svg?,"[github](https://img.shields.io/github/stars/fmu2/snag_release.svg?
","[github](https://img.shields.io/github/stars/fmu2/<PROJECT>snag_release</PROJECT>.svg?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\05487e88.txt,0.9925925925925926
16,style=social)](https://github.com/fmu2/snag_release)  [!,style=social)](https://github.com/fmu2/snag_release)  [!,"style=social)](https://github.com/fmu2/snag_release)  [!
","style=social)](https://github.com/fmu2/<SOFTWARE>snag_release</SOFTWARE>)  [!
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\ad75697a.txt,0.9911504424778761
17,[arXiv](https://img.shields.io/badge/Arxiv-2404.02257-b31b1b.svg?,[arXiv](https://img.shields.io/badge/Arxiv-2404.02257-b31b1b.svg?,"[arXiv](https://img.shields.io/badge/Arxiv-2404.02257-b31b1b.svg?
","[<PUBLICATION>arXiv</PUBLICATION>](https://img.shields.io/badge/Arxiv-2404.02257-b31b1b.svg?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\6ee8d39d.txt,0.9923664122137404
18,logo=arXiv)](https://arxiv.org/abs/2404.02257) <br>  ## Changelog * 11/18/2022: We have released the [tech report](https://arxiv.org/abs/2211.09074) for our submission to the [Ego4D Moment Queries (MQ) Challenge](https://eval.ai/web/challenges/challenge-page/1626/overview).,logo=arXiv)](https://arxiv.org/abs/2404.02257) <br>  ## Changelog * 11/18/2022: We have released the [tech report](https://arxiv.org/abs/2211.09074) for our submission to the [<DATASET>Ego4D Moment Queries</DATASET> (MQ) Challenge](https://eval.ai/web/challenges/challenge-page/1626/overview).,"logo=arXiv)](https://arxiv.org/abs/2404.02257) <br>  ## Changelog * 11/18/2022: We have released the [tech report](https://arxiv.org/abs/2211.09074) for our submission to the [Ego4D Moment Queries (MQ) Challenge](https://eval.ai/web/challenges/challenge-page/1626/overview).
","logo=arXiv)](https://arxiv.org/abs/2404.02257) <br>  ## Changelog * 11/18/2022: We have released the [tech report](https://arxiv.org/abs/2211.09074) for our submission to the [<CONFERENCE>Ego4D Moment Queries (MQ) Challenge</CONFERENCE>](https://eval.ai/web/challenges/challenge-page/1626/overview).
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\902bdbe7.txt,0.9981785063752276
19,"The code repo now includes config files, pre-trained models and results on the Ego4D MQ benchmark","The code repo now includes config files, pre-trained models and results on the <DATASET>Ego4D MQ</DATASET> benchmark","The code repo now includes config files, pre-trained models and results on the Ego4D MQ benchmark","The code repo now includes config files, pre-trained models and results on the <DATASET>Ego4D MQ</DATASET> benchmark",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\511c7879.txt,1.0
20,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
21,* 08/29/2022: Updated arXiv version,* 08/29/2022: Updated arXiv version,* 08/29/2022: Updated arXiv version,* 08/29/2022: Updated <PUBLICATION>arXiv</PUBLICATION> version,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\cede2201.txt,1.0
22,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
23,* 08/01/2022: Updated code repo with latest results on ActivityNet,* 08/01/2022: Updated code repo with latest results on <DATASET>ActivityNet</DATASET>,* 08/01/2022: Updated code repo with latest results on ActivityNet,* 08/01/2022: Updated code repo with latest results on <DATASET>ActivityNet</DATASET>,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\010bfa95.txt,1.0
24,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
25,* 07/08/2022: The paper is accepted to ECCV 2022,* 07/08/2022: The paper is accepted to <CONFERENCE>ECCV 2022</CONFERENCE>,"* 07/08/2022: The paper is accepted to ECCV 2022
","* 07/08/2022: The paper is accepted to <CONFERENCE>ECCV 2022</CONFERENCE>
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1cd24837.txt,0.9896907216494846
26,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
27,* 05/09/2022: Pre-trained models have been updated,* 05/09/2022: Pre-trained models have been updated,* 05/09/2022: Pre-trained models have been updated,* 05/09/2022: Pre-trained models have been updated,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\4129182a.txt,1.0
28,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
29,"* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average mAP on THUMOS14 (>66.0%) and slightly improved results on ActivityNet and EPIC-Kitchens 100.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2.","* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average <EVALMETRIC>mAP</EVALMETRIC> on <DATASET>THUMOS14</DATASET> (>66.0%) and slightly improved results on <DATASET>ActivityNet</DATASET> and <DATASET>EPIC-Kitchens 100</DATASET>.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2.","* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average mAP on THUMOS14 (>66.0%) and slightly improved results on ActivityNet and EPIC-Kitchens 100.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2.","* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average <EVALMETRIC>mAP</EVALMETRIC> on <DATASET>THUMOS14</DATASET> (>66.0%) and slightly improved results on <DATASET>ActivityNet</DATASET> and <DATASET>EPIC-Kitchens 100</DATASET>.   ## Code Overview The structure of this code repo is heavily inspired by <SOFTWARE>Detectron2</SOFTWARE>.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\8ae43482.txt,1.0
30,Some of the main components are * .,Some of the main components are * .,Some of the main components are * .,Some of the main components are * .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\5dedbbd6.txt,1.0
31,/libs/core: Parameter configuration module. * .,/libs/core: Parameter configuration module. * .,/libs/core: Parameter configuration module. * .,/libs/core: <SOFTWARE>Parameter configuration module</SOFTWARE>. * .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1d295d6b.txt,1.0
32,/libs/datasets: Data loader and IO module. * .,/libs/datasets: Data loader and IO module. * .,/libs/datasets: Data loader and IO module. * .,/libs/<DATASET>datasets</DATASET>: Data loader and IO module. * .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1088967f.txt,1.0
33,/libs/modeling: Our main model with all its building blocks. * .,/libs/modeling: Our main model with all its building blocks. * .,/libs/modeling: Our main model with all its building blocks. * .,/libs/<SOFTWARE>modeling</SOFTWARE>: Our main model with all its building blocks. * .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\ea2310ae.txt,1.0
34,"/libs/utils: Utility functions for training, inference, and postprocessing.  ## Installation * Follow INSTALL.md for installing necessary dependencies and compiling the code.  ## Frequently Asked Questions * See FAQ.md.   ## To Reproduce Our Results on THUMOS14 **Download Features and Annotations** * Download *thumos.tar.gz* (`md5sum 375f76ffbf7447af1035e694971ec9b2`) from [this Box link](https://uwmadison.box.com/s/glpuxadymf3gd01m1cj6g5c3bn39qbgr) or [this Google Drive link](https://drive.google.com/file/d/1zt2eoldshf99vJMDuu8jqxda55dCyhZP/view?","/libs/utils: Utility functions for training, inference, and postprocessing.  ## Installation * Follow INSTALL.md for installing necessary dependencies and compiling the code.  ## Frequently Asked Questions * See FAQ.md.   ## To Reproduce Our Results on <DATASET>THUMOS14</DATASET> **Download Features and Annotations** * Download *<DATASET>thumos</DATASET>.tar.gz* (`md5sum 375f76ffbf7447af1035e694971ec9b2`) from [this Box link](https://uwmadison.box.com/s/glpuxadymf3gd01m1cj6g5c3bn39qbgr) or [this <SOFTWARE>Google Drive</SOFTWARE> link](https://drive.google.com/file/d/1zt2eoldshf99vJMDuu8jqxda55dCyhZP/view?","/libs/utils: Utility functions for training, inference, and postprocessing.  ## Installation * Follow INSTALL.md for installing necessary dependencies and compiling the code.  ## Frequently Asked Questions * See FAQ.md.   ## To Reproduce Our Results on THUMOS14 **Download Features and Annotations** * Download *thumos.tar.gz* (md5sum 375f76ffbf7447af1035e694971ec9b2) from [this Box link](https://uwmadison.box.com/s/glpuxadymf3gd01m1cj6g5c3bn39qbgr) or [this Google Drive link](https://drive.google.com/file/d/1zt2eoldshf99vJMDuu8jqxda55dCyhZP/view?
","/libs/utils: Utility functions for training, inference, and postprocessing.  ## Installation * Follow INSTALL.md for installing necessary dependencies and compiling the code.  ## Frequently Asked Questions * See FAQ.md.   ## To Reproduce Our Results on <DATASET>THUMOS14</DATASET> **Download Features and Annotations** * Download *thumos.tar.gz* (md5sum 375f76ffbf7447af1035e694971ec9b2) from [this Box link](https://uwmadison.box.com/s/glpuxadymf3gd01m1cj6g5c3bn39qbgr) or [this Google Drive link](https://drive.google.com/file/d/1zt2eoldshf99vJMDuu8jqxda55dCyhZP/view?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\70b5d6b9.txt,0.9972850678733032
35,usp=sharing) or [this BaiduYun link](https://pan.baidu.com/s/1TgS91LVV-vzFTgIHl1AEGA?,usp=sharing) or [this <SOFTWARE>BaiduYun</SOFTWARE> link](https://pan.baidu.com/s/1TgS91LVV-vzFTgIHl1AEGA?,"usp=sharing) or [this BaiduYun link](https://pan.baidu.com/s/1TgS91LVV-vzFTgIHl1AEGA?
","usp=sharing) or [this BaiduYun link](https://pan.baidu.com/s/1TgS91LVV-vzFTgIHl1AEGA?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\04853979.txt,0.9941520467836257
36,"pwd=74eh). * The file includes I3D features, action annotations in json format (similar to ActivityNet annotation format), and external classification scores.","pwd=74eh). * The file includes I3D features, action annotations in json format (similar to <DATASET>ActivityNet</DATASET> annotation format), and external classification scores.","pwd=74eh). * The file includes I3D features, action annotations in json format (similar to ActivityNet annotation format), and external classification scores.
","pwd=74eh). * The file includes I3D features, action annotations in json format (similar to <DATASET>ActivityNet</DATASET> annotation format), and external classification scores.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fea5d384.txt,0.9968454258675079
37,**Details**: The features are extracted from two-stream I3D models pretrained on Kinetics using clips of `16 frames` at the video frame rate (`~30 fps`) and a stride of `4 frames`.,**Details**: The features are extracted from two-stream I3D models pretrained on Kinetics using clips of `16 frames` at the video frame rate (`~30 fps`) and a stride of `4 frames`.,**Details**: The features are extracted from two-stream I3D models pretrained on Kinetics using clips of 16 frames at the video frame rate (~30 fps) and a stride of 4 frames.,**Details**: The features are extracted from two-stream <SOFTWARE>I3D</SOFTWARE> models pretrained on <DATASET>Kinetics</DATASET> using clips of 16 frames at the video frame rate (~30 fps) and a stride of 4 frames.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1af6e56d.txt,0.9830508474576272
38,This gives one feature vector per `4/30 ~= 0.1333` seconds.,This gives one feature vector per `4/30 ~= 0.1333` seconds.,This gives one feature vector per 4/30 ~= 0.1333 seconds.,This gives one feature vector per 4/30 ~= 0.1333 seconds.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fa1cfa64.txt,0.9827586206896551
39,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fa86218a.txt,1.0
40,/data* (or elsewhere and link to *.,/data* (or elsewhere and link to *.,"/data* (or elsewhere and link to *.
","/data* (or elsewhere and link to *.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\755cba61.txt,0.9859154929577465
41,/data*). * The folder structure should look like ``` This folder │   README.md │,/data*). * The folder structure should look like ``` This folder │   README.md │,"/data*). * The folder structure should look like  This folder │   README.md │
","/data*). * The folder structure should look like  This folder │   README.md │
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\eace0fe7.txt,0.9746835443037974
42,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
43,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
44,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
45,│ └───data/ │    └───thumos/ │    │  └───annotations │    │  └───i3d_features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * Train our ActionFormer with I3D features.,│ └───data/ │    └───thumos/ │    │  └───annotations │    │  └───i3d_features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * Train our <SOFTWARE>ActionFormer</SOFTWARE> with I3D features.,"│ └───data/ │    └───thumos/ │    │  └───annotations │    │  └───i3d_features    │    └───... | └───libs │ │   ...   **Training and Evaluation** * Train our ActionFormer with I3D features.
","│ └───data/ │    └───<DATASET>thumos</DATASET>/ │    │  └───annotations │    │  └───i3d_features    │    └───... | └───libs │ │   ...   **Training and Evaluation** * Train our ActionFormer with I3D features.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\51326d97.txt,0.9894736842105263
46,This will create an experiment folder under *.,This will create an experiment folder under *.,This will create an experiment folder under *.,This will create an experiment folder under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\42ba3464.txt,1.0
47,"/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/<SOFTWARE>ckpt</SOFTWARE>* that stores training config, logs, and checkpoints.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3231b41.txt,1.0
48,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
49,/train.py .,/train.py .,"/train.py .
","/train.py .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c31d2457.txt,0.9565217391304348
50,/configs/thumos_i3d.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,/configs/<DATASET>thumos</DATASET>_i3d.yaml --output reproduce ``` * [Optional] Monitor the training using <SOFTWARE>TensorBoard</SOFTWARE> ```<PROGLANG>shell</PROGLANG> <SOFTWARE>tensorboard</SOFTWARE> --logdir=.,"/configs/thumos_i3d.yaml --output reproduce  * [Optional] Monitor the training using TensorBoard shell tensorboard --logdir=.
","/configs/<DATASET>thumos_i3d</DATASET>.yaml --output reproduce  * [Optional] Monitor the training using <SOFTWARE>TensorBoard</SOFTWARE> shell <SOFTWARE>tensorboard</SOFTWARE> --logdir=.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\b7c6f119.txt,0.9727626459143969
51,/ckpt/thumos_i3d_reproduce/logs ``` * Evaluate the trained model.,/ckpt/<DATASET>thumos</DATASET>_i3d_reproduce/logs ``` * Evaluate the trained model.,"/ckpt/thumos_i3d_reproduce/logs  * Evaluate the trained model.
","/ckpt/thumos_i3d_reproduce/logs  * Evaluate the trained model.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\eec364b9.txt,0.96875
52,The expected average mAP should be around 62.6(%) as in Table 1 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 62.6(%) as in Table 1 of our main paper.,The expected average mAP should be around 62.6(%) as in Table 1 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 62.6(%) as in Table 1 of our main paper.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\b193c6d8.txt,1.0
53,"**With recent commits, the expected average mAP should be higher than 66.0(%)**.","**With recent commits, the expected average <EVALMETRIC>mAP</EVALMETRIC> should be higher than 66.0(%)**.","**With recent commits, the expected average mAP should be higher than 66.0(%).**","**With recent commits, the expected average <EVALMETRIC>mAP</EVALMETRIC> should be higher than 66.0(%).**",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\054261f3.txt,0.9875
54,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
55,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
56,/configs/thumos_i3d.yaml .,/configs/<DATASET>thumos</DATASET>_i3d.yaml .,"/configs/thumos_i3d.yaml .
","/configs/<SOFTWARE>thumos_i3d.yaml</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\514efec1.txt,0.9811320754716981
57,"/ckpt/thumos_i3d_reproduce ``` * Training our model on THUMOS requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.","/ckpt/<DATASET>thumos</DATASET>_i3d_reproduce ``` * Training our model on <DATASET>THUMOS</DATASET> requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.","/ckpt/thumos_i3d_reproduce  * Training our model on THUMOS requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.
","/ckpt/<SOFTWARE>thumos_i3d_reproduce</SOFTWARE>  * Training our model on <DATASET>THUMOS</DATASET> requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\28ec06d7.txt,0.9859154929577465
58,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2825976c.txt,1.0
59,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for THUMOS 14.,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for <DATASET>THUMOS 14</DATASET>.,"**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for THUMOS 14.
","**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for <DATASET>THUMOS 14</DATASET>.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\46dd08d7.txt,0.9949748743718593
60,The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1isG3bc1dG5-llBRFCivJwz_7c_b0XDcY/view?,The model with all training logs can be downloaded from [this <SOFTWARE>Google Drive</SOFTWARE> link](https://drive.google.com/file/d/1isG3bc1dG5-llBRFCivJwz_7c_b0XDcY/view?,The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1isG3bc1dG5-llBRFCivJwz_7c_b0XDcY/view?,The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1isG3bc1dG5-llBRFCivJwz_7c_b0XDcY/view?,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\831c2551.txt,1.0
61,usp=sharing).,usp=sharing).,"usp=sharing).
","usp=sharing).
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\e4c0f51a.txt,0.9629629629629629
62,"To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3fcb2e7.txt,1.0
63,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
64,* Create a folder *.,* Create a folder *.,"To produce the results, create a folder \*.","To produce the results, create a folder \*.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0a48399e.txt,0.5714285714285714
65,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fca77404.txt,1.0
66,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\76bc4db4.txt,1.0
67,/pretrained*). * The folder structure should look like ``` This folder │   README.md │,/pretrained*). * The folder structure should look like ``` This folder │   README.md │,/pretrained*). * The folder structure should look like  This folder │   README.md │,/pretrained*). * The folder structure should look like  This folder │   README.md │,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c24911c3.txt,0.9822485207100592
68,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
69,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
70,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
71,│ └───pretrained/ │    └───thumos_i3d_reproduce/ │    │  └───thumos_reproduce_log.txt │    │  └───thumos_reproduce_results.txt │    │   └───,│ └───pretrained/ │    └───<DATASET>thumos</DATASET>_i3d_reproduce/ │    │  └───<DATASET>thumos</DATASET>_reproduce_log.txt │    │  └───<DATASET>thumos</DATASET>_reproduce_results.txt │    │   └───,"│ └───pretrained/ │    └───thumos_i3d_reproduce/ │    │  └───thumos_reproduce_log.txt │    │  └───thumos_reproduce_results.txt │    │   └───
","│ └───pretrained/ │    └───<DATASET>thumos_i3d_reproduce</DATASET>/ │    │  └───<DATASET>thumos_reproduce_log</DATASET>.txt │    │  └───<DATASET>thumos_reproduce_results</DATASET>.txt │    │   └───
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\24cdb517.txt,0.99644128113879
72,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
73,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
74,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
75,│    └───... | └───libs │ │   ... ``` * The training config is recorded in *.,│    └───... | └───libs │ │   ... ``` * The training config is recorded in *.,│    └───... | └───libs │ │   ...  * The training config is recorded in *.,│    └───... | └───libs │ │   ...  * The training config is recorded in *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\05b39c33.txt,0.9801324503311258
76,/pretrained/thumos_i3d_reproduce/config.txt*. * The training log is located at *.,/pretrained/<DATASET>thumos</DATASET>_i3d_reproduce/config.txt*. * The training log is located at *.,/pretrained/thumos_i3d_reproduce/config.txt*. * The training log is located at *.,/pretrained/thumos_i3d_reproduce/config.txt*. * The training log is located at *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\5ae155d2.txt,1.0
77,/pretrained/thumos_i3d_reproduce/thumos_reproduce_log.txt* and also *.,/pretrained/<DATASET>thumos</DATASET>_i3d_reproduce/thumos_reproduce_log.txt* and also *.,"/pretrained/thumos_i3d_reproduce/thumos_reproduce_log.txt* and also *.
","/pretrained/thumos_i3d_reproduce/<DATASET>thumos_reproduce_log.txt</DATASET>* and also *.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\6e708772.txt,0.9929078014184397
78,/pretrained/thumos_i3d_reproduce/logs*. * The pre-trained model is *.,/pretrained/<DATASET>thumos</DATASET>_i3d_reproduce/logs*. * The pre-trained model is *.,/pretrained/thumos_i3d_reproduce/logs*. * The pre-trained model is *.,/pretrained/thumos_i3d_reproduce/logs*. * The pre-trained model is *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\a188f464.txt,1.0
79,/pretrained/thumos_i3d_reproduce/epoch_034.pth.tar*. * Evaluate the pre-trained model.,/pretrained/<DATASET>thumos</DATASET>_i3d_reproduce/epoch_034.pth.tar*. * Evaluate the pre-trained model.,"/pretrained/thumos_i3d_reproduce/epoch_034.pth.tar*. * Evaluate the pre-trained model.
","/pretrained/thumos_i3d_reproduce/epoch_034.pth.tar*. * Evaluate the pre-trained model.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\a4860d3f.txt,0.9942196531791907
80,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
81,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
82,/configs/thumos_i3d.yaml .,/configs/<DATASET>thumos</DATASET>_i3d.yaml .,"/configs/thumos_i3d.yaml .
","/configs/<SOFTWARE>thumos_i3d.yaml</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\514efec1.txt,0.9811320754716981
83,/pretrained/thumos_i3d_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on ActivityNet 1.3 **Download Features and Annotations** * Download *anet_1.3.tar.gz* (`md5sum c415f50120b9425ee1ede9ac3ce11203`) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?,/pretrained/<DATASET>thumos</DATASET>_i3d_reproduce/ ``` * The results (<EVALMETRIC>mAP</EVALMETRIC> at <EVALMETRIC>tIoUs</EVALMETRIC>) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | <SOFTWARE>ActionFormer</SOFTWARE>      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on <DATASET>ActivityNet 1.3</DATASET> **Download Features and Annotations** * Download *anet_1.3.tar.gz* (`md5sum c415f50120b9425ee1ede9ac3ce11203`) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this <SOFTWARE>Google Drive</SOFTWARE> Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?,"/pretrained/thumos_i3d_reproduce/  * The results (mAP at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on ActivityNet 1.3 **Download Features and Annotations** * Download *anet_1.3.tar.gz* (md5sum c415f50120b9425ee1ede9ac3ce11203) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?
","/pretrained/thumos_i3d_reproduce/  * The results (<EVALMETRIC>mAP</EVALMETRIC> at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on <DATASET>ActivityNet 1.3</DATASET> **Download Features and Annotations** * Download *<DATASET>anet_1.3.tar.gz</DATASET>* (md5sum c415f50120b9425ee1ede9ac3ce11203) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\a1c82b87.txt,0.9952229299363057
84,usp=sharing) or [this BaiduYun Link](https://pan.baidu.com/s/1tw5W8B5YqDvfl-mrlWQvnQ?,usp=sharing) or [this <SOFTWARE>BaiduYun</SOFTWARE> Link](https://pan.baidu.com/s/1tw5W8B5YqDvfl-mrlWQvnQ?,"usp=sharing) or [this BaiduYun Link](https://pan.baidu.com/s/1tw5W8B5YqDvfl-mrlWQvnQ?
","usp=sharing) or [this BaiduYun Link](https://pan.baidu.com/s/1tw5W8B5YqDvfl-mrlWQvnQ?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\779027f8.txt,0.9941520467836257
85,"pwd=xuit). * The file includes TSP features, action annotations in json format (similar to ActivityNet annotation format), and external classification scores.","pwd=xuit). * The file includes TSP features, action annotations in json format (similar to <DATASET>ActivityNet</DATASET> annotation format), and external classification scores.","pwd=xuit). * The file includes TSP features, action annotations in json format (similar to ActivityNet annotation format), and external classification scores.
","pwd=xuit). * The file includes TSP features, action annotations in json format (similar to <DATASET>ActivityNet</DATASET> annotation format), and external classification scores.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\f78423eb.txt,0.9968454258675079
86,"**Details**: The features are extracted from the R(2+1)D-34 model pretrained with TSP on ActivityNet using clips of `16 frames` at a frame rate of `15 fps` and a stride of `16 frames` (*i.e.,* **non-overlapping** clips).","**Details**: The features are extracted from the R(2+1)D-34 model pretrained with TSP on <DATASET>ActivityNet</DATASET> using clips of `16 frames` at a frame rate of `15 fps` and a stride of `16 frames` (*i.e.,* **non-overlapping** clips).","**Details**: The features are extracted from the R(2+1)D-34 model pretrained with TSP on ActivityNet using clips of 16 frames at a frame rate of 15 fps and a stride of 16 frames (*i.e.,* **non-overlapping** clips).","**Details**: The features are extracted from the <SOFTWARE>R(2+1)D-34</SOFTWARE> model pretrained with <DATASET>TSP</DATASET> on <DATASET>ActivityNet</DATASET> using clips of 16 frames at a frame rate of 15 fps and a stride of 16 frames (*i.e.,* **non-overlapping** clips).",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\6ac13e5d.txt,0.815668202764977
87,This gives one feature vector per `16/15 ~= 1.067` seconds.,This gives one feature vector per `16/15 ~= 1.067` seconds.,This gives one feature vector per 16/15 ~= 1.067 seconds.,This gives one feature vector per 16/15 ~= 1.067 seconds.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c838b9c7.txt,0.9827586206896551
88,The features are converted into numpy files for our code.,The features are converted into numpy files for our code.,The features are converted into numpy files for our code.,The features are converted into numpy files for our <PROGLANG>code</PROGLANG>.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c7b35489.txt,1.0
89,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fa86218a.txt,1.0
90,/data* (or elsewhere and link to *.,/data* (or elsewhere and link to *.,"/data* (or elsewhere and link to *.
","/data* (or elsewhere and link to *.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\755cba61.txt,0.9859154929577465
91,/data*). * The folder structure should look like ``` This folder │   README.md │,/data*). * The folder structure should look like ``` This folder │   README.md │,"/data*). * The folder structure should look like  This folder │   README.md │
","/data*). * The folder structure should look like  This folder │   README.md │
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\eace0fe7.txt,0.9746835443037974
92,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
93,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
94,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
95,│ └───data/ │    └───anet_1.3/ │    │  └───annotations │    │  └───tsp_features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * Train our ActionFormer with TSP features.,│ └───data/ │    └───anet_1.3/ │    │  └───annotations │    │  └───tsp_features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * Train our ActionFormer with TSP features.,"│ └───data/ │    └───anet_1.3/ │    │  └───annotations │    │  └───tsp_features    │    └───... | └───libs │ │   ...   **Training and Evaluation** * Train our ActionFormer with TSP features.
","│ └───data/ │    └───<DATASET>anet_1.3</DATASET>/ │    │  └───annotations │    │  └───tsp_features    │    └───... | └───libs │ │   ...   **Training and Evaluation** * Train our <SOFTWARE>ActionFormer</SOFTWARE> with TSP features.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\4271d00d.txt,0.9895833333333334
96,This will create an experiment folder under *.,This will create an experiment folder under *.,This will create an experiment folder under *.,This will create an experiment folder under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\42ba3464.txt,1.0
97,"/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/<SOFTWARE>ckpt</SOFTWARE>* that stores training config, logs, and checkpoints.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3231b41.txt,1.0
98,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
99,/train.py .,/train.py .,"/train.py .
","/train.py .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c31d2457.txt,0.9565217391304348
100,/configs/anet_tsp.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,/configs/anet_tsp.yaml --output reproduce ``` * [Optional] Monitor the training using <SOFTWARE>TensorBoard</SOFTWARE> ```<PROGLANG>shell</PROGLANG> <SOFTWARE>tensorboard</SOFTWARE> --logdir=.,"/configs/anet_tsp.yaml --output reproduce  * [Optional] Monitor the training using TensorBoard shell tensorboard --logdir=.
","/configs/<DATASET>anet_tsp</DATASET>.yaml --output reproduce  * [Optional] Monitor the training using <SOFTWARE>TensorBoard</SOFTWARE> shell tensorboard --logdir=.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\131280ad.txt,0.9723320158102767
101,/ckpt/anet_tsp_reproduce/logs ``` * Evaluate the trained model.,/ckpt/anet_tsp_reproduce/logs ``` * Evaluate the trained model.,"/ckpt/anet_tsp_reproduce/logs  * Evaluate the trained model.
","/ckpt/anet_tsp_reproduce/logs  * Evaluate the trained model.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\a1bf7bde.txt,0.967741935483871
102,The expected average mAP should be around 36.5(%) as in Table 1 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 36.5(%) as in Table 1 of our main paper.,The expected average mAP should be around 36.5(%) as in Table 1 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 36.5(%) as in Table 1 of our main paper.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\61ae5e13.txt,1.0
103,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
104,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
105,/configs/anet_tsp.yaml .,/configs/anet_tsp.yaml .,"/configs/anet_tsp.yaml .
","/configs/<SOFTWARE>anet_tsp.yaml</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2f92cfef.txt,0.9795918367346939
106,"/ckpt/anet_tsp_reproduce ``` * Training our model on ActivityNet requires ~4.6GB GPU memory, yet the inference might require over 10GB GPU memory.","/ckpt/anet_tsp_reproduce ``` * Training our model on <DATASET>ActivityNet</DATASET> requires ~4.6GB GPU memory, yet the inference might require over 10GB GPU memory.","/ckpt/anet_tsp_reproduce  * Training our model on ActivityNet requires ~4.6GB GPU memory, yet the inference might require over 10GB GPU memory.
","/ckpt/anet_tsp_reproduce  * Training our model on <DATASET>ActivityNet</DATASET> requires ~4.6GB GPU memory, yet the inference might require over 10GB GPU memory.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\def50d8a.txt,0.9862068965517241
107,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2825976c.txt,1.0
108,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for ActivityNet 1.3.,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for <DATASET>ActivityNet 1.3</DATASET>.,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for ActivityNet 1.3.,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for <DATASET>ActivityNet 1.3</DATASET>.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\9c0c1799.txt,1.0
109,The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1JKh3w14ngAjgzuuP22BnjhkhIcBSqteJ/view?,The model with all training logs can be downloaded from [this <SOFTWARE>Google Drive</SOFTWARE> link](https://drive.google.com/file/d/1JKh3w14ngAjgzuuP22BnjhkhIcBSqteJ/view?,The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1JKh3w14ngAjgzuuP22BnjhkhIcBSqteJ/view?,The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1JKh3w14ngAjgzuuP22BnjhkhIcBSqteJ/view?,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\77708adc.txt,1.0
110,usp=sharing).,usp=sharing).,"usp=sharing).
","usp=sharing).
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\e4c0f51a.txt,0.9629629629629629
111,"To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3fcb2e7.txt,1.0
112,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
113,* Create a folder *.,* Create a folder *.,"To produce the results, create a folder \*.","To produce the results, create a folder \*.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0a48399e.txt,0.5714285714285714
114,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fca77404.txt,1.0
115,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\76bc4db4.txt,1.0
116,/pretrained*). * The folder structure should look like ``` This folder │   README.md │,/pretrained*). * The folder structure should look like ``` This folder │   README.md │,/pretrained*). * The folder structure should look like  This folder │   README.md │,/pretrained*). * The folder structure should look like  This folder │   README.md │,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c24911c3.txt,0.9822485207100592
117,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
118,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
119,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
120,│ └───pretrained/ │    └───anet_tsp_reproduce/ │    │  └───anet_tsp_reproduce_log.txt │    │  └───anet_tsp_reproduce_results.txt │    │   └───,│ └───pretrained/ │    └───anet_tsp_reproduce/ │    │  └───anet_tsp_reproduce_log.txt │    │  └───anet_tsp_reproduce_results.txt │    │   └───,"│ └───pretrained/ │    └───anet_tsp_reproduce/ │    │  └───anet_tsp_reproduce_log.txt │    │  └───anet_tsp_reproduce_results.txt │    │   └───
","│ └───pretrained/ │    └───<DATASET>anet_tsp_reproduce</DATASET>/ │    │  └───<DATASET>anet_tsp_reproduce_log</DATASET>.txt │    │  └───<DATASET>anet_tsp_reproduce_results</DATASET>.txt │    │   └───
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\075b955b.txt,0.9964912280701754
121,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
122,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
123,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
124,│    └───... | └───libs │ │   ... ``` * The training config is recorded in *.,│    └───... | └───libs │ │   ... ``` * The training config is recorded in *.,│    └───... | └───libs │ │   ...  * The training config is recorded in *.,│    └───... | └───libs │ │   ...  * The training config is recorded in *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\05b39c33.txt,0.9801324503311258
125,/pretrained/anet_tsp_reproduce/config.txt*. * The training log is located at *.,/pretrained/anet_tsp_reproduce/config.txt*. * The training log is located at *.,/pretrained/anet_tsp_reproduce/config.txt*. * The training log is located at *.,/pretrained/anet_tsp_reproduce/config.txt*. * The training log is located at *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\6fb42e55.txt,1.0
126,/pretrained/anet_tsp_reproduce/anet_tsp_reproduce_log.txt* and also *.,/pretrained/anet_tsp_reproduce/anet_tsp_reproduce_log.txt* and also *.,/pretrained/anet_tsp_reproduce/anet_tsp_reproduce_log.txt* and also *.,/pretrained/<SOFTWARE>anet_tsp_reproduce</SOFTWARE>/<SOFTWARE>anet_tsp_reproduce_log.txt</SOFTWARE>* and also *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\59239bea.txt,1.0
127,/pretrained/anet_tsp_reproduce/logs*. * The pre-trained model is *.,/pretrained/anet_tsp_reproduce/logs*. * The pre-trained model is *.,/pretrained/anet_tsp_reproduce/logs*. * The pre-trained model is *.,/pretrained/anet_tsp_reproduce/logs*. * The pre-trained model is *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\afc86196.txt,1.0
128,/pretrained/anet_tsp_reproduce/epoch_014.pth.tar*. * Evaluate the pre-trained model.,/pretrained/anet_tsp_reproduce/epoch_014.pth.tar*. * Evaluate the pre-trained model.,/pretrained/anet_tsp_reproduce/epoch_014.pth.tar*. * Evaluate the pre-trained model.,/pretrained/anet_tsp_reproduce/epoch_014.pth.tar*. * Evaluate the pre-trained model.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\80804bb1.txt,1.0
129,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
130,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
131,/configs/anet_tsp.yaml .,/configs/anet_tsp.yaml .,"/configs/anet_tsp.yaml .
","/configs/<SOFTWARE>anet_tsp.yaml</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2f92cfef.txt,0.9795918367346939
132,/pretrained/anet_tsp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.67 | 37.81 |  8.36 | 36.56 |   **[Optional] Reproducing Our Results with I3D Features**  * Download *anet_1.3_i3d.tar.gz* (`md5sum e649425954e0123401650312dd0d56a7`) from [this Google Drive Link](https://drive.google.com/file/d/16239kUT2Z-j6S6PXIT1b_31OJi35QW_o/view?,/pretrained/anet_tsp_reproduce/ ``` * The results (<EVALMETRIC>mAP</EVALMETRIC> at <EVALMETRIC>tIoU</EVALMETRIC>s) should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | <SOFTWARE>ActionFormer</SOFTWARE>      | 54.67 | 37.81 |  8.36 | 36.56 |   **[Optional] Reproducing Our Results with I3D Features**  * Download *anet_1.3_i3d.tar.gz* (`md5sum e649425954e0123401650312dd0d56a7`) from [this <SOFTWARE>Google Drive</SOFTWARE> Link](https://drive.google.com/file/d/16239kUT2Z-j6S6PXIT1b_31OJi35QW_o/view?,"/pretrained/anet_tsp_reproduce/  * The results (mAP at tIoUs) should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.67 | 37.81 |  8.36 | 36.56 |   **[Optional] Reproducing Our Results with I3D Features**  * Download *anet_1.3_i3d.tar.gz* (md5sum e649425954e0123401650312dd0d56a7) from [this Google Drive Link](https://drive.google.com/file/d/16239kUT2Z-j6S6PXIT1b_31OJi35QW_o/view?
","/pretrained/anet_tsp_reproduce/  * The results (<EVALMETRIC>mAP</EVALMETRIC> at tIoUs) should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | <SOFTWARE>ActionFormer</SOFTWARE>      | 54.67 | 37.81 |  8.36 | 36.56 |   **[Optional] Reproducing Our Results with I3D Features**  * Download *<DATASET>anet_1.3_i3d.tar.gz</DATASET>* (md5sum e649425954e0123401650312dd0d56a7) from [this Google Drive Link](https://drive.google.com/file/d/16239kUT2Z-j6S6PXIT1b_31OJi35QW_o/view?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\20d4e652.txt,0.9936708860759493
133,usp=sharing).,usp=sharing).,"usp=sharing).
","usp=sharing).
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\e4c0f51a.txt,0.9629629629629629
134,**Details**: The features are extracted from the I3D model pretrained on Kinetics using clips of `16 frames` at a frame rate of `25 fps` and a stride of `16 frames`.,**Details**: The features are extracted from the I3D model pretrained on Kinetics using clips of `16 frames` at a frame rate of `25 fps` and a stride of `16 frames`.,**Details**: The features are extracted from the I3D model pretrained on Kinetics using clips of 16 frames at a frame rate of 25 fps and a stride of 16 frames.,**Details**: The features are extracted from the <SOFTWARE>I3D</SOFTWARE> model pretrained on <DATASET>Kinetics</DATASET> using clips of 16 frames at a frame rate of 25 fps and a stride of 16 frames.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\4a053880.txt,0.9814814814814815
135,This gives one feature vector per `16/25 = 0.64` seconds.,This gives one feature vector per `16/25 = 0.64` seconds.,This gives one feature vector per 16/25 = 0.64 seconds.,This gives one feature vector per 16/25 = 0.64 seconds.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\7ca0a8fd.txt,0.9821428571428571
136,The features are converted into numpy files for our code,The features are converted into numpy files for our code,The features are converted into numpy files for our code,The features are converted into numpy files for our <PROGLANG>code</PROGLANG>,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\db2247d9.txt,1.0
137,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
138,* Unpack the file under *.,* Unpack the file under *.,* Unpack the file under *.,* Unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\8dc80b5b.txt,1.0
139,/data* (or elsewhere and link to *.,/data* (or elsewhere and link to *.,"/data* (or elsewhere and link to *.
","/data* (or elsewhere and link to *.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\755cba61.txt,0.9859154929577465
140,"/data*), similar to TSP features","/data*), similar to TSP features","/data*), similar to TSP features","/data*), similar to TSP features",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\ca6d2344.txt,1.0
141,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
142,* Train our ActionFormer with I3D features.,* Train our <SOFTWARE>ActionFormer</SOFTWARE> with I3D features.,* Train our ActionFormer with I3D features.,* Train our <SOFTWARE>ActionFormer</SOFTWARE> with <SOFTWARE>I3D</SOFTWARE> features.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\ef2917fd.txt,1.0
143,This will create an experiment folder under *.,This will create an experiment folder under *.,This will create an experiment folder under *.,This will create an experiment folder under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\42ba3464.txt,1.0
144,"/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/<SOFTWARE>ckpt</SOFTWARE>* that stores training config, logs, and checkpoints.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3231b41.txt,1.0
145,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
146,/train.py .,/train.py .,"/train.py .
","/train.py .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c31d2457.txt,0.9565217391304348
147,/configs/anet_i3d.yaml --output reproduce ```  * Evaluate the trained model.,/configs/anet_i3d.yaml --output reproduce ```  * Evaluate the trained model.,"/configs/anet_i3d.yaml --output reproduce   * Evaluate the trained model.
","/configs/<SOFTWARE>anet_i3d.yaml</SOFTWARE> --output reproduce   * Evaluate the trained model.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\ae0353d7.txt,0.9733333333333334
148,The expected average mAP should be around 36.0(%).,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 36.0(%).,The expected average mAP should be around 36.0(%).,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 36.0(%).,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\5ed23461.txt,1.0
149,This is slightly improved from our paper.,This is slightly improved from our paper.,This is slightly improved from our paper.,This is slightly improved from our <PUBLICATION>paper</PUBLICATION>.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\7c7b7559.txt,1.0
150,The improvement is produced by better training scheme / hyperparameters (see comments in the config file).,The improvement is produced by better training scheme / hyperparameters (see comments in the config file).,The improvement is produced by better training scheme / hyperparameters (see comments in the config file).,The improvement is produced by better training scheme / hyperparameters (see comments in the config file).,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\8eb1c071.txt,1.0
151,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
152,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
153,/configs/anet_i3d.yaml .,/configs/anet_i3d.yaml .,"/configs/anet_i3d.yaml .
","/configs/<SOFTWARE>anet_i3d.yaml</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d60126da.txt,0.9795918367346939
154,/ckpt/anet_i3d_reproduce ```  * The pre-trained model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/152dw2JDoNPssSnaQDaNolQUSFgcHlxe3/view?,/ckpt/anet_i3d_reproduce ```  * The pre-trained model with all training logs can be downloaded from [this <SOFTWARE>Google Drive</SOFTWARE> link](https://drive.google.com/file/d/152dw2JDoNPssSnaQDaNolQUSFgcHlxe3/view?,"/ckpt/anet_i3d_reproduce   * The pre-trained model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/152dw2JDoNPssSnaQDaNolQUSFgcHlxe3/view?
","/ckpt/<SOFTWARE>anet_i3d_reproduce</SOFTWARE>   * The pre-trained model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/152dw2JDoNPssSnaQDaNolQUSFgcHlxe3/view?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\bcc37380.txt,0.9897435897435898
155,usp=sharing).,usp=sharing).,"usp=sharing).
","usp=sharing).
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\e4c0f51a.txt,0.9629629629629629
156,"To produce the results, create a folder *.","To produce the results, create a folder *.","To produce the results, create a folder \*.","To produce the results, create a folder \*.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0a48399e.txt,0.9882352941176471
157,"/pretrained*, unpack the file under *.","/pretrained*, unpack the file under *.","/pretrained*, unpack the file under *.","/pretrained*, unpack the file under *.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\f775c93e.txt,1.0
158,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\76bc4db4.txt,1.0
159,"/pretrained*), and run ```shell python .","/pretrained*), and run ```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .",/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fca77404.txt,0.6666666666666666
160,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
161,/configs/anet_i3d.yaml .,/configs/anet_i3d.yaml .,"/configs/anet_i3d.yaml .
","/configs/<SOFTWARE>anet_i3d.yaml</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d60126da.txt,0.9795918367346939
162,/pretrained/anet_i3d_reproduce/ ```  * The results (mAP at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on EPIC Kitchens 100 **Download Features and Annotations** * Download *epic_kitchens.tar.gz* (`md5sum add9803756afd9a023bc9a9c547e0229`) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?,/pretrained/anet_i3d_reproduce/ ```  * The results (<EVALMETRIC>mAP</EVALMETRIC> at <EVALMETRIC>tIoUs</EVALMETRIC>) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | <SOFTWARE>ActionFormer</SOFTWARE>      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on <DATASET>EPIC Kitchens 100</DATASET> **Download Features and Annotations** * Download *<DATASET>epic_kitchens</DATASET>.tar.gz* (`md5sum add9803756afd9a023bc9a9c547e0229`) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this <SOFTWARE>Google Drive</SOFTWARE> Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?,"/pretrained/anet_i3d_reproduce/   * The results (mAP at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on EPIC Kitchens 100 **Download Features and Annotations** * Download *epic_kitchens.tar.gz* (md5sum add9803756afd9a023bc9a9c547e0229) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?
","/pretrained/anet_i3d_reproduce/   * The results (<EVALMETRIC>mAP</EVALMETRIC> at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on <DATASET>EPIC Kitchens 100</DATASET> **Download Features and Annotations** * Download *<DATASET>epic_kitchens.tar.gz</DATASET>* (md5sum add9803756afd9a023bc9a9c547e0229) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2cf24d00.txt,0.9950248756218906
163,usp=sharing) or [this BaiduYun Link](https://pan.baidu.com/s/15tOdX6Yp4AJ9lFGjbQ8dgg?,usp=sharing) or [this <SOFTWARE>BaiduYun</SOFTWARE> Link](https://pan.baidu.com/s/15tOdX6Yp4AJ9lFGjbQ8dgg?,"usp=sharing) or [this BaiduYun Link](https://pan.baidu.com/s/15tOdX6Yp4AJ9lFGjbQ8dgg?
","usp=sharing) or [this BaiduYun Link](https://pan.baidu.com/s/15tOdX6Yp4AJ9lFGjbQ8dgg?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c2e3b8e1.txt,0.9941520467836257
164,pwd=f3tx). * The file includes SlowFast features as well as action annotations in json format (similar to ActivityNet annotation format).,pwd=f3tx). * The file includes <DATASET>SlowFast</DATASET> features as well as action annotations in json format (similar to <DATASET>ActivityNet</DATASET> annotation format).,"pwd=f3tx). * The file includes SlowFast features as well as action annotations in json format (similar to ActivityNet annotation format).
","pwd=f3tx). * The file includes <SOFTWARE>SlowFast</SOFTWARE> features as well as action annotations in json format (similar to <DATASET>ActivityNet</DATASET> annotation format).
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\55c46c6f.txt,0.9963636363636363
165,**Details**: The features are extracted from the SlowFast model pretrained on the training set of EPIC Kitchens 100 (action classification) using clips of `32 frames` at a frame rate of `30 fps` and a stride of `16 frames`.,**Details**: The features are extracted from the SlowFast model pretrained on the training set of <DATASET>EPIC Kitchens 100</DATASET> (action classification) using clips of `32 frames` at a frame rate of `30 fps` and a stride of `16 frames`.,**Details**: The features are extracted from the SlowFast model pretrained on the training set of EPIC Kitchens 100 (action classification) using clips of 32 frames at a frame rate of 30 fps and a stride of 16 frames.,**Details**: The features are extracted from the <SOFTWARE>SlowFast</SOFTWARE> model pretrained on the training set of <DATASET>EPIC Kitchens 100</DATASET> (action classification) using clips of 32 frames at a frame rate of 30 fps and a stride of 16 frames.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c7d70a05.txt,0.8181818181818182
166,This gives one feature vector per `16/30 ~= 0.5333` seconds.,This gives one feature vector per `16/30 ~= 0.5333` seconds.,This gives one feature vector per 16/30 ~= 0.5333 seconds.,This gives one feature vector per 16/30 ~= 0.5333 seconds.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\ea58b611.txt,0.9830508474576272
167,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fa86218a.txt,1.0
168,/data* (or elsewhere and link to *.,/data* (or elsewhere and link to *.,"/data* (or elsewhere and link to *.
","/data* (or elsewhere and link to *.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\755cba61.txt,0.9859154929577465
169,/data*). * The folder structure should look like ``` This folder │   README.md │,/data*). * The folder structure should look like ``` This folder │   README.md │,"/data*). * The folder structure should look like  This folder │   README.md │
","/data*). * The folder structure should look like  This folder │   README.md │
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\eace0fe7.txt,0.9746835443037974
170,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
171,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
172,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
173,"│ └───data/ │    └───epic_kitchens/ │    │  └───annotations │    │  └───features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * On EPIC Kitchens, we train separate models for nouns and verbs. * To train our ActionFormer on verbs with SlowFast features, use ```shell python .","│ └───data/ │    └───epic_kitchens/ │    │  └───annotations │    │  └───features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * On <DATASET>EPIC Kitchens</DATASET>, we train separate models for nouns and verbs. * To train our <SOFTWARE>ActionFormer</SOFTWARE> on verbs with <DATASET>SlowFast</DATASET> features, use ```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .","│ └───data/ │    └───epic_kitchens/ │    │  └───annotations │    │  └───features    │    └───... | └───libs │ │   ...   **Training and Evaluation** * On EPIC Kitchens, we train separate models for nouns and verbs. * To train our ActionFormer on verbs with SlowFast features, use shell python .
","│ └───data/ │    └───<DATASET>epic_kitchens</DATASET>/ │    │  └───annotations │    │  └───features    │    └───... | └───libs │ │   ...   **Training and Evaluation** * On <DATASET>EPIC Kitchens</DATASET>, we train separate models for nouns and verbs. * To train our <SOFTWARE>ActionFormer</SOFTWARE> on verbs with <SOFTWARE>SlowFast</SOFTWARE> features, use shell python .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\94ed489e.txt,0.9881956155143339
174,/train.py .,/train.py .,"/train.py .
","/train.py .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c31d2457.txt,0.9565217391304348
175,"/configs/epic_slowfast_verb.yaml --output reproduce ``` * To train our ActionFormer on nouns with SlowFast features, use ```shell python .","/configs/epic_slowfast_verb.yaml --output reproduce ``` * To train our ActionFormer on nouns with <DATASET>SlowFast</DATASET> features, use ```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .","/configs/epic_slowfast_verb.yaml --output reproduce  * To train our ActionFormer on nouns with SlowFast features, use shell python .
","/configs/epic_slowfast_verb.yaml --output reproduce  * To train our <PROJECT>ActionFormer</PROJECT> on nouns with <SOFTWARE>SlowFast</SOFTWARE> features, use shell python .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\85f2a524.txt,0.974169741697417
176,/train.py .,/train.py .,"/train.py .
","/train.py .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c31d2457.txt,0.9565217391304348
177,/configs/epic_slowfast_noun.yaml --output reproduce ``` * Evaluate the trained model for verbs.,/configs/epic_slowfast_noun.yaml --output reproduce ``` * Evaluate the trained model for verbs.,"/configs/epic_slowfast_noun.yaml --output reproduce  * Evaluate the trained model for verbs.
","/configs/<SOFTWARE>epic_slowfast_noun.yaml</SOFTWARE> --output reproduce  * Evaluate the trained model for verbs.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\f5084d02.txt,0.9787234042553191
178,The expected average mAP should be around 23.4(%) as in Table 2 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 23.4(%) as in Table 2 of our main paper.,The expected average mAP should be around 23.4(%) as in Table 2 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 23.4(%) as in Table 2 of our main paper.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\9a7a5605.txt,1.0
179,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
180,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
181,/configs/epic_slowfast_verb.yaml .,/configs/epic_slowfast_verb.yaml .,"/configs/epic_slowfast_verb.yaml .
","/configs/<SOFTWARE>epic_slowfast_verb</SOFTWARE>.yaml .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c25b50e4.txt,0.9855072463768116
182,/ckpt/epic_slowfast_verb_reproduce ``` * Evaluate the trained model for nouns.,/ckpt/epic_slowfast_verb_reproduce ``` * Evaluate the trained model for nouns.,"/ckpt/epic_slowfast_verb_reproduce  * Evaluate the trained model for nouns.
","/ckpt/<SOFTWARE>epic_slowfast_verb_reproduce</SOFTWARE>  * Evaluate the trained model for nouns.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\7b937df7.txt,0.974025974025974
183,The expected average mAP should be around 21.9(%) as in Table 2 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 21.9(%) as in Table 2 of our main paper.,The expected average mAP should be around 21.9(%) as in Table 2 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 21.9(%) as in Table 2 of our main paper.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\27da51ce.txt,1.0
184,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
185,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
186,/configs/epic_slowfast_noun.yaml .,/configs/epic_slowfast_noun.yaml .,"/configs/epic_slowfast_noun.yaml .
","/configs/<SOFTWARE>epic_slowfast_noun.yaml</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\4fcd4e91.txt,0.9855072463768116
187,"/ckpt/epic_slowfast_noun_reproduce ``` * Training our model on EPIC Kitchens requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.","/ckpt/epic_slowfast_noun_reproduce ``` * Training our model on <DATASET>EPIC Kitchens</DATASET> requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.","/ckpt/epic_slowfast_noun_reproduce  * Training our model on EPIC Kitchens requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.
","/ckpt/<SOFTWARE>epic_slowfast_noun_reproduce</SOFTWARE>  * Training our model on <DATASET>EPIC Kitchens</DATASET> requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2d0334c0.txt,0.9872611464968153
188,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2825976c.txt,1.0
189,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for EPIC-Kitchens 100.,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for <DATASET>EPIC-Kitchens 100</DATASET>.,"**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for EPIC-Kitchens 100.
","**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for <DATASET>EPIC-Kitchens 100</DATASET>.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\9cb05fa5.txt,0.9953488372093023
190,The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1Ta4ggKSj2YcszSrDbePlHe1ECF1CFKK4/view?,The model with all training logs can be downloaded from [this <SOFTWARE>Google Drive</SOFTWARE> link](https://drive.google.com/file/d/1Ta4ggKSj2YcszSrDbePlHe1ECF1CFKK4/view?,The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1Ta4ggKSj2YcszSrDbePlHe1ECF1CFKK4/view?,The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1Ta4ggKSj2YcszSrDbePlHe1ECF1CFKK4/view?,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\10d463fd.txt,1.0
191,"usp=sharing) (verb), and from this [Google Drive link](https://drive.google.com/file/d/1OTlxeiWj8JE9n1-LsRYogHmqgUdsE5PR/view?","usp=sharing) (verb), and from this [<SOFTWARE>Google Drive</SOFTWARE> link](https://drive.google.com/file/d/1OTlxeiWj8JE9n1-LsRYogHmqgUdsE5PR/view?","usp=sharing) (verb), and from this [Google Drive link](https://drive.google.com/file/d/1OTlxeiWj8JE9n1-LsRYogHmqgUdsE5PR/view?
","usp=sharing) (verb), and from this [Google Drive link](https://drive.google.com/file/d/1OTlxeiWj8JE9n1-LsRYogHmqgUdsE5PR/view?
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\5fc1fe6a.txt,0.9960474308300395
192,usp=sharing) (noun).,usp=sharing) (noun).,"usp=sharing).
","usp=sharing).
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\e4c0f51a.txt,0.7647058823529411
193,"To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3fcb2e7.txt,1.0
194,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
195,* Create a folder *.,* Create a folder *.,"To produce the results, create a folder \*.","To produce the results, create a folder \*.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0a48399e.txt,0.5714285714285714
196,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fca77404.txt,1.0
197,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\76bc4db4.txt,1.0
198,/pretrained*). * The folder structure should look like ``` This folder │   README.md │,/pretrained*). * The folder structure should look like ``` This folder │   README.md │,/pretrained*). * The folder structure should look like  This folder │   README.md │,/pretrained*). * The folder structure should look like  This folder │   README.md │,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c24911c3.txt,0.9822485207100592
199,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
200,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
201,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
202,│ └───pretrained/ │    └───epic_slowfast_verb_reproduce/ │    │  └───epic_slowfast_verb_reproduce_log.txt │    │  └───epic_slowfast_verb_reproduce_results.txt │    │   └───,│ └───pretrained/ │    └───epic_slowfast_verb_reproduce/ │    │  └───epic_slowfast_verb_reproduce_log.txt │    │  └───epic_slowfast_verb_reproduce_results.txt │    │   └───,"│ └───pretrained/ │    └───epic_slowfast_verb_reproduce/ │    │  └───epic_slowfast_verb_reproduce_log.txt │    │  └───epic_slowfast_verb_reproduce_results.txt │    │   └───
","│ └───pretrained/ │    └───<PROJECT>epic_slowfast_verb_reproduce</PROJECT>/ │    │  └───<PROJECT>epic_slowfast_verb_reproduce_log</PROJECT>.txt │    │  └───<PROJECT>epic_slowfast_verb_reproduce_results</PROJECT>.txt │    │   └───
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\92f4c43a.txt,0.9971014492753624
203,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
204,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
205,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
206,│    └───epic_slowfast_noun_reproduce/ │    │  └───epic_slowfast_noun_reproduce_log.txt │    │  └───epic_slowfast_noun_reproduce_results.txt │    │   └───,│    └───epic_slowfast_noun_reproduce/ │    │  └───epic_slowfast_noun_reproduce_log.txt │    │  └───epic_slowfast_noun_reproduce_results.txt │    │   └───,"│    └───epic_slowfast_noun_reproduce/ │    │  └───epic_slowfast_noun_reproduce_log.txt │    │  └───epic_slowfast_noun_reproduce_results.txt │    │   └───
","│    └───<PROJECT>epic_slowfast_noun_reproduce</PROJECT>/ │    │  └───<PROJECT>epic_slowfast_noun_reproduce</PROJECT>_log.txt │    │  └───<PROJECT>epic_slowfast_noun_reproduce</PROJECT>_results.txt │    │   └───
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\7dde69e8.txt,0.9967637540453075
207,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
208,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
209,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
210,│    └───... | └───libs │ │   ... ``` * The training config is recorded in *.,│    └───... | └───libs │ │   ... ``` * The training config is recorded in *.,│    └───... | └───libs │ │   ...  * The training config is recorded in *.,│    └───... | └───libs │ │   ...  * The training config is recorded in *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\05b39c33.txt,0.9801324503311258
211,/pretrained/epic_slowfast_(verb|noun)_reproduce/config.txt*. * The training log is located at *.,/pretrained/epic_slowfast_(verb|noun)_reproduce/config.txt*. * The training log is located at *.,/pretrained/epic_slowfast_(verb|noun)_reproduce/config.txt*. * The training log is located at *.,/pretrained/<SOFTWARE>epic_slowfast_(verb|noun)_reproduce</SOFTWARE>/config.txt*. * The training log is located at *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\e34b65d1.txt,1.0
212,/pretrained/epic_slowfast_(verb|noun)_reproduce/epic_slowfast_(verb|noun)_reproduce_log.txt* and also *.,/pretrained/epic_slowfast_(verb|noun)_reproduce/epic_slowfast_(verb|noun)_reproduce_log.txt* and also *.,"/pretrained/epic_slowfast_(verb|noun)_reproduce/epic_slowfast_(verb|noun)_reproduce_log.txt* and also *.
","/pretrained/<SOFTWARE>epic_slowfast_(verb|noun)_reproduce</SOFTWARE>/<SOFTWARE>epic_slowfast_(verb|noun)_reproduce</SOFTWARE>_log.txt* and also *.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\bfe0b76a.txt,0.9952153110047847
213,/pretrained/epic_slowfast_(verb|noun)_reproduce/logs*. * The pre-trained model is *.,/pretrained/epic_slowfast_(verb|noun)_reproduce/logs*. * The pre-trained model is *.,"/pretrained/epic_slowfast_(verb|noun)_reproduce/logs*. * The pre-trained model is *.
","/pretrained/<SOFTWARE>epic_slowfast_(verb|noun)_reproduce</SOFTWARE>/logs*. * The pre-trained model is *.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\ca3825bf.txt,0.9940828402366864
214,/pretrained/epic_slowfast_(verb|noun)_reproduce/epoch_(020|020).pth.tar*. * Evaluate the pre-trained model for verbs.,/pretrained/epic_slowfast_(verb|noun)_reproduce/epoch_(020|020).pth.tar*. * Evaluate the pre-trained model for verbs.,"/pretrained/epic_slowfast_(verb|noun)_reproduce/epoch_(020|020).pth.tar*. * Evaluate the pre-trained model for verbs.
","/pretrained/<SOFTWARE>epic_slowfast_(verb|noun)_reproduce</SOFTWARE>/epoch_(020|020).pth.tar*. * Evaluate the pre-trained model for verbs.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\8eb80e75.txt,0.9957446808510638
215,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
216,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
217,/configs/epic_slowfast_verb.yaml .,/configs/epic_slowfast_verb.yaml .,"/configs/epic_slowfast_verb.yaml .
","/configs/<SOFTWARE>epic_slowfast_verb</SOFTWARE>.yaml .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c25b50e4.txt,0.9855072463768116
218,/pretrained/epic_slowfast_verb_reproduce/ ``` * Evaluate the pre-trained model for nouns.,/pretrained/epic_slowfast_verb_reproduce/ ``` * Evaluate the pre-trained model for nouns.,"/pretrained/epic_slowfast_verb_reproduce/  * Evaluate the pre-trained model for nouns.
","/pretrained/<SOFTWARE>epic_slowfast_verb_reproduce</SOFTWARE>/  * Evaluate the pre-trained model for nouns.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\a4c82cb7.txt,0.9772727272727273
219,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
220,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
221,/configs/epic_slowfast_noun.yaml .,/configs/epic_slowfast_noun.yaml .,"/configs/epic_slowfast_noun.yaml .
","/configs/<SOFTWARE>epic_slowfast_noun.yaml</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\4fcd4e91.txt,0.9855072463768116
222,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (<EVALMETRIC>mAP</EVALMETRIC> at <EVALMETRIC>tIoUs</EVALMETRIC>) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | <PROJECT>ActionFormer</PROJECT> (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | <SOFTWARE>ActionFormer</SOFTWARE> (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on <DATASET>Ego4D Moment Queries</DATASET> Benchmark **Download Features and Annotations** * Download the official <DATASET>SlowFast</DATASET> and <DATASET>Omnivore</DATASET> features from [the <DATASET>Ego4D</DATASET> website](https://ego4d-data.org/#download) and the official <DATASET>EgoVLP</DATASET> features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,"/pretrained/epic_slowfast_noun_reproduce/  * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).
","/pretrained/epic_slowfast_noun_reproduce/  * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on <DATASET>Ego4D Moment Queries Benchmark</DATASET> **Download Features and Annotations** * Download the official <SOFTWARE>SlowFast</SOFTWARE> and <SOFTWARE>Omnivore</SOFTWARE> features from [the <PROJECT>Ego4D</PROJECT> website](https://<PROJECT>ego4d-data</PROJECT>.org/#download) and the official <SOFTWARE>EgoVLP</SOFTWARE> features from [this link](https://github.com/<PROJECT>showlab</PROJECT>/<SOFTWARE>EgoVLP</SOFTWARE>/issues/1#issuecomment-1219076014).
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\18e4d7cf.txt,0.9971671388101983
223,Please note that we are not authorized to release the features and annotations.,Please note that we are not authorized to release the features and annotations.,Please note that we are not authorized to release the features and annotations.,Please note that we are not authorized to release the features and annotations.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2292d8f0.txt,1.0
224,"Instead, we provide our script for feature and annotation conversion at `.","Instead, we provide our script for feature and annotation conversion at `.","Instead, we provide our script for feature and annotation conversion at .","Instead, we provide our script for feature and annotation conversion at .",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\807138ff.txt,0.9931972789115646
225,/tools/convert_ego4d_trainval.py`.,/tools/convert_ego4d_trainval.py`.,"/tools/convert_ego4d_trainval.py
","/tools/<SOFTWARE>convert_ego4d_trainval.py</SOFTWARE>
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\4411e14a.txt,0.9552238805970149
226,**Details**: All features are extracted at `1.875 fps` from videos at `30 fps`.,**Details**: All features are extracted at `1.875 fps` from videos at `30 fps`.,**Details**: All features are extracted at 1.875 fps from videos at 30 fps.,**Details**: All features are extracted at 1.875 fps from videos at 30 fps.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\a1f82c48.txt,0.974025974025974
227,This gives one feature vector per `~0.5333` seconds.,This gives one feature vector per `~0.5333` seconds.,This gives one feature vector per ~0.5333 seconds.,This gives one feature vector per ~0.5333 seconds.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\80cd52ed.txt,0.9803921568627451
228,Please refer to Ego4D and EgoVLP's documentation for more details on feature extraction.,Please refer to <PROJECT>Ego4D</PROJECT> and <PROJECT>EgoVLP</PROJECT>'s documentation for more details on feature extraction.,Please refer to Ego4D and EgoVLP's documentation for more details on feature extraction.,Please refer to <PROJECT>Ego4D</PROJECT> and <PROJECT>EgoVLP</PROJECT>'s documentation for more details on feature extraction.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\602a2b54.txt,1.0
229,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,**Unpack Features and Annotations** * Unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fa86218a.txt,1.0
230,/data* (or elsewhere and link to *.,/data* (or elsewhere and link to *.,"/data* (or elsewhere and link to *.
","/data* (or elsewhere and link to *.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\755cba61.txt,0.9859154929577465
231,/data*). * The folder structure should look like ``` This folder │   README.md │,/data*). * The folder structure should look like ``` This folder │   README.md │,"/data*). * The folder structure should look like  This folder │   README.md │
","/data*). * The folder structure should look like  This folder │   README.md │
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\eace0fe7.txt,0.9746835443037974
232,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
233,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
234,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
235,│ └───data/ │    └───ego4d/ │    │   └───annotations │    │   └───slowfast_features │    │   └───omnivore_features │    │   └───egovlp_features   │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * We provide config files for training ActionFormer with different feature combinations.,│ └───data/ │    └───ego4d/ │    │   └───annotations │    │   └───slowfast_features │    │   └───omnivore_features │    │   └───egovlp_features   │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * We provide config files for training <SOFTWARE>ActionFormer</SOFTWARE> with different feature combinations.,"│ └───data/ │    └───ego4d/ │    │   └───annotations │    │   └───slowfast_features │    │   └───omnivore_features │    │   └───egovlp_features   │    └───... | └───libs │ │   ...   **Training and Evaluation** * We provide config files for training ActionFormer with different feature combinations.
","│ └───data/ │    └───<DATASET>ego4d</DATASET>/ │    │   └───annotations │    │   └───slowfast_features │    │   └───omnivore_features │    │   └───egovlp_features   │    └───... | └───libs │ │   ...   **Training and Evaluation** * We provide config files for training <SOFTWARE>ActionFormer</SOFTWARE> with different feature combinations.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\389e19e0.txt,0.9933333333333333
236,"For example, training on Omnivore and EgoVLP features will create an experiment folder under *.","For example, training on <DATASET>Omnivore</DATASET> and <DATASET>EgoVLP</DATASET> features will create an experiment folder under *.","For example, training on Omnivore and EgoVLP features will create an experiment folder under *.","For example, training on <DATASET>Omnivore</DATASET> and <DATASET>EgoVLP</DATASET> features will create an experiment folder under *.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\e65b010e.txt,1.0
237,"/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/<SOFTWARE>ckpt</SOFTWARE>* that stores training config, logs, and checkpoints.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3231b41.txt,1.0
238,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
239,/train.py .,/train.py .,"/train.py .
","/train.py .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c31d2457.txt,0.9565217391304348
240,/configs/ego4d_omnivore_egovlp.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,/configs/ego4d_omnivore_egovlp.yaml --output reproduce ``` * [Optional] Monitor the training using <SOFTWARE>TensorBoard</SOFTWARE> ```<PROGLANG>shell</PROGLANG> <SOFTWARE>tensorboard</SOFTWARE> --logdir=.,"/configs/ego4d_omnivore_egovlp.yaml --output reproduce  * [Optional] Monitor the training using TensorBoard shell tensorboard --logdir=.
","/configs/ego4d_omnivore_egovlp.yaml --output reproduce  * [Optional] Monitor the training using <SOFTWARE>TensorBoard</SOFTWARE> shell <SOFTWARE>tensorboard</SOFTWARE> --logdir=.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\23d9dda6.txt,0.974910394265233
241,/ckpt/ego4d_omnivore_egovlp_reproduce/logs ``` * Evaluate the trained model.,/ckpt/ego4d_omnivore_egovlp_reproduce/logs ``` * Evaluate the trained model.,"/ckpt/ego4d_omnivore_egovlp_reproduce/logs  * Evaluate the trained model.
","/ckpt/<SOFTWARE>ego4d_omnivore_egovlp_reproduce</SOFTWARE>/logs  * Evaluate the trained model.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\aae05dde.txt,0.9733333333333334
242,"The expected average mAP and Recall@1x, tIoU=0.5 should be around 22.0(%) and 40.0(%) respectively.","The expected average <EVALMETRIC>mAP</EVALMETRIC> and <EVALMETRIC>Recall@1x</EVALMETRIC>, <EVALMETRIC>tIoU</EVALMETRIC>=0.5 should be around 22.0(%) and 40.0(%) respectively.","The expected average mAP and Recall@1x, tIoU=0.5 should be around 22.0(%) and 40.0(%) respectively.","The expected average <EVALMETRIC>mAP</EVALMETRIC> and <EVALMETRIC>Recall@1x, tIoU=0.5</EVALMETRIC> should be around 22.0(%) and 40.0(%) respectively.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\a09545db.txt,1.0
243,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
244,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
245,/configs/ego4d_omnivore_egovlp.yaml .,/configs/ego4d_omnivore_egovlp.yaml .,"/configs/ego4d_omnivore_egovlp.yaml .
","/configs/<SOFTWARE>ego4d_omnivore_egovlp</SOFTWARE>.yaml .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2e93425b.txt,0.9866666666666667
246,"/ckpt/ego4d_omnivore_egovlp_reproduce ``` * Training our model on Ego4D with all three features requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.","/ckpt/ego4d_omnivore_egovlp_reproduce ``` * Training our model on <DATASET>Ego4D</DATASET> with all three features requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.","/ckpt/ego4d_omnivore_egovlp_reproduce  * Training our model on Ego4D with all three features requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.
","/ckpt/<SOFTWARE>ego4d_omnivore_egovlp_reproduce</SOFTWARE>  * Training our model on <DATASET>Ego4D</DATASET> with all three features requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\7027192c.txt,0.9886363636363636
247,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,We recommend using a GPU with at least 12 GB of memory.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2825976c.txt,1.0
248,**[Optional] Evaluating Our Pre-trained Model**  We also provide pre-trained models for Ego4D trained with all feature combinations.,**[Optional] Evaluating Our Pre-trained Model**  We also provide pre-trained models for <DATASET>Ego4D</DATASET> trained with all feature combinations.,"**[Optional] Evaluating Our Pre-trained Model**  We also provide pre-trained models for Ego4D trained with all feature combinations.
","**[Optional] Evaluating Our Pre-trained Model**  We also provide pre-trained models for <DATASET>Ego4D</DATASET> trained with all feature combinations.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1159b982.txt,0.9962264150943396
249,The models with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/drive/folders/1NpAECS0ZhcCuehXkF9OhLQDPFrNdStJb?,The models with all training logs can be downloaded from [this <SOFTWARE>Google Drive</SOFTWARE> link](https://drive.google.com/drive/folders/1NpAECS0ZhcCuehXkF9OhLQDPFrNdStJb?,The models with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/drive/folders/1NpAECS0ZhcCuehXkF9OhLQDPFrNdStJb?,The models with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/drive/folders/1NpAECS0ZhcCuehXkF9OhLQDPFrNdStJb?,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\9e1a66e1.txt,1.0
250,usp=sharing).,usp=sharing).,"usp=sharing).
","usp=sharing).
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\e4c0f51a.txt,0.9629629629629629
251,"To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below","To evaluate the pre-trained model, please follow the steps listed below",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3fcb2e7.txt,1.0
252,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
253,* Create a folder *.,* Create a folder *.,"To produce the results, create a folder \*.","To produce the results, create a folder \*.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0a48399e.txt,0.5714285714285714
254,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fca77404.txt,1.0
255,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,/pretrained* (or elsewhere and link to *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\76bc4db4.txt,1.0
256,/pretrained*). * An example of the folder structure should look like ``` This folder │   README.md │,/pretrained*). * An example of the folder structure should look like ``` This folder │   README.md │,"/pretrained*). * An example of the folder structure should look like  This folder │   README.md │
","/pretrained*). * An example of the folder structure should look like  This folder │   README.md │
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1d66456c.txt,0.9797979797979798
257,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
258,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
259,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
260,│ └───pretrained/ │    └───ego4d_omnivore_egovlp_reproduce/ │    │   └───ego4d_omnivore_egovlp_reproduce_log.txt │    │   └───ego4d_omnivore_egovlp_reproduce_results.txt │    │   └───,│ └───pretrained/ │    └───<DATASET>ego4d_omnivore_egovlp</DATASET>_reproduce/ │    │   └───<DATASET>ego4d_omnivore_egovlp</DATASET>_reproduce_log.txt │    │   └───<DATASET>ego4d_omnivore_egovlp</DATASET>_reproduce_results.txt │    │   └───,"│ └───pretrained/ │    └───ego4d_omnivore_egovlp_reproduce/ │    │   └───ego4d_omnivore_egovlp_reproduce_log.txt │    │   └───ego4d_omnivore_egovlp_reproduce_results.txt │    │   └───
","│ └───pretrained/ │    └───<PROJECT>ego4d_omnivore_egovlp_reproduce</PROJECT>/ │    │   └───<PROJECT>ego4d_omnivore_egovlp_reproduce</PROJECT>_log.txt │    │   └───<PROJECT>ego4d_omnivore_egovlp_reproduce</PROJECT>_results.txt │    │   └───
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2f579904.txt,0.997275204359673
261,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
262,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
263,.,.,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.16666666666666666
264,│    └───... | └───libs │ │   ... ``` * The training config is recorded in *.,│    └───... | └───libs │ │   ... ``` * The training config is recorded in *.,│    └───... | └───libs │ │   ...  * The training config is recorded in *.,│    └───... | └───libs │ │   ...  * The training config is recorded in *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\05b39c33.txt,0.9801324503311258
265,/pretrained/ego4d_omnivore_egovlp_reproduce/config.txt*. * The training log is located at *.,/pretrained/<DATASET>ego4d_omnivore_egovlp</DATASET>_reproduce/config.txt*. * The training log is located at *.,/pretrained/ego4d_omnivore_egovlp_reproduce/config.txt*. * The training log is located at *.,/pretrained/<SOFTWARE>ego4d_omnivore_egovlp_reproduce</SOFTWARE>/config.txt*. * The training log is located at *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fffe936e.txt,1.0
266,/pretrained/ego4d_omnivore_egovlp_reproduce/ego4d_omnivore_egovlp_reproduce_log.txt* and also *.,/pretrained/<DATASET>ego4d_omnivore_egovlp</DATASET>_reproduce/ego4d_omnivore_egovlp_reproduce_log.txt* and also *.,"/pretrained/ego4d_omnivore_egovlp_reproduce/ego4d_omnivore_egovlp_reproduce_log.txt* and also *.
","/pretrained/<SOFTWARE>ego4d_omnivore_egovlp_reproduce</SOFTWARE>/<SOFTWARE>ego4d_omnivore_egovlp_reproduce</SOFTWARE>_log.txt* and also *.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\5356d8c5.txt,0.9948186528497409
267,/pretrained/ego4d_omnivore_egovlp_reproduce/logs*. * The pre-trained model is *.,/pretrained/<DATASET>ego4d_omnivore_egovlp</DATASET>_reproduce/logs*. * The pre-trained model is *.,"/pretrained/ego4d_omnivore_egovlp_reproduce/logs*. * The pre-trained model is *.
","/pretrained/<SOFTWARE>ego4d_omnivore_egovlp_reproduce</SOFTWARE>/logs*. * The pre-trained model is *.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\56fc880a.txt,0.9937888198757764
268,/pretrained/ego4d_omnivore_egovlp_reproduce/epoch_010.pth.tar*. * Evaluate the pre-trained model.,/pretrained/<DATASET>ego4d_omnivore_egovlp</DATASET>_reproduce/epoch_010.pth.tar*. * Evaluate the pre-trained model.,"/pretrained/ego4d_omnivore_egovlp_reproduce/epoch_010.pth.tar*. * Evaluate the pre-trained model.
","/pretrained/<SOFTWARE>ego4d_omnivore_egovlp_reproduce</SOFTWARE>/epoch_010.pth.tar*. * Evaluate the pre-trained model.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\14fa3d56.txt,0.9948717948717949
269,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,shell python .,shell python .,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.9032258064516129
270,/eval.py .,/eval.py .,"/eval.py .
","<SOFTWARE>/eval.py</SOFTWARE> .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\dbbcd827.txt,0.9523809523809523
271,/configs/ego4d_omnivore_egovlp.yaml .,/configs/ego4d_omnivore_egovlp.yaml .,"/configs/ego4d_omnivore_egovlp.yaml .
","/configs/<SOFTWARE>ego4d_omnivore_egovlp</SOFTWARE>.yaml .
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\2e93425b.txt,0.9866666666666667
272,/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (<EVALMETRIC>mAP</EVALMETRIC> at <EVALMETRIC>tIoUs</EVALMETRIC>) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | <PROJECT>ActionFormer</PROJECT> (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | <SOFTWARE>ActionFormer</SOFTWARE> (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | <SOFTWARE>ActionFormer</SOFTWARE> (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | <PROJECT>ActionFormer</PROJECT> (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | <SOFTWARE>ActionFormer</SOFTWARE> (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | <SOFTWARE>ActionFormer</SOFTWARE> (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | <SOFTWARE>ActionFormer</SOFTWARE> (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | <SOFTWARE>ActionFormer</SOFTWARE> (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | <SOFTWARE>ActionFormer</SOFTWARE> (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | <SOFTWARE>ActionFormer</SOFTWARE> (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | <SOFTWARE>ActionFormer</SOFTWARE> (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | <SOFTWARE>ActionFormer</SOFTWARE> (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,"/pretrained/ego4d_omnivore_egovlp_reproduce/  * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.
","/pretrained/<SOFTWARE>ego4d_omnivore_egovlp_reproduce</SOFTWARE>/  * The results (<EVALMETRIC>mAP</EVALMETRIC> at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | <SOFTWARE>ActionFormer</SOFTWARE> (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | <SOFTWARE>ActionFormer</SOFTWARE> (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | <SOFTWARE>ActionFormer</SOFTWARE> (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | <SOFTWARE>ActionFormer</SOFTWARE> (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | <SOFTWARE>ActionFormer</SOFTWARE> (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | <SOFTWARE>ActionFormer</SOFTWARE> (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (<EVALMETRIC>Recall@1x</EVALMETRIC> at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | <SOFTWARE>ActionFormer</SOFTWARE> (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | <SOFTWARE>ActionFormer</SOFTWARE> (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | <SOFTWARE>ActionFormer</SOFTWARE> (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | <SOFTWARE>ActionFormer</SOFTWARE> (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | <SOFTWARE>ActionFormer</SOFTWARE> (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | <SOFTWARE>ActionFormer</SOFTWARE> (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own <DATASET>Dataset</DATASET> Work in progress.
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\4ca84103.txt,0.998551774076756
273,"Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper. ``` @inproceedings{zhang2022actionformer,   title={ActionFormer: Localizing Moments of Actions with Transformers},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={European Conference on Computer Vision},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} } ```  If you cite our results on Ego4D, please consider citing our tech report in addition to the main paper. ``` @article{mu2022actionformerego4d,   title={Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} } ```  If you are using TSP features, please cite ``` @inproceedings{alwassel2021tsp,   title={{TSP}: Temporally-sensitive pretraining of video encoders for localization tasks},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},   pages={3173--3183},   year={2021} } ```","Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper. ``` @inproceedings{zhang2022actionformer,   title={<PUBLICATION>ActionFormer: Localizing Moments of Actions with Transformers</PUBLICATION>},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={<CONFERENCE>European Conference on Computer Vision</CONFERENCE>},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} } ```  If you cite our results on <DATASET>Ego4D</DATASET>, please consider citing our tech report in addition to the main paper. ``` @article{mu2022actionformerego4d,   title={<PUBLICATION>Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge</PUBLICATION>},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} } ```  If you are using TSP features, please cite ``` @inproceedings{alwassel2021tsp,   title={<PUBLICATION>{TSP}: Temporally-sensitive pretraining of video encoders for localization tasks</PUBLICATION>},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={<PUBLICATION>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</PUBLICATION><WORKSHOP>the IEEE/CVF International Conference on Computer Vision Workshops</WORKSHOP>},   pages={3173--3183},   year={2021} } ```","Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper.  @inproceedings{zhang2022actionformer,   title={ActionFormer: Localizing Moments of Actions with Transformers},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={European Conference on Computer Vision},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} }   If you cite our results on Ego4D, please consider citing our tech report in addition to the main paper.  @article{mu2022actionformerego4d,   title={Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} }   If you are using TSP features, please cite  @inproceedings{alwassel2021tsp,   title={TSP: Temporally-sensitive pretraining of video encoders for localization tasks},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},   pages={3173--3183},   year={2021} } 
","Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper.  @inproceedings{zhang2022actionformer,   title={<PUBLICATION>ActionFormer: Localizing Moments of Actions with Transformers</PUBLICATION>},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={<CONFERENCE>European Conference on Computer Vision</CONFERENCE>},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} }   If you cite our results on Ego4D, please consider citing our tech report in addition to the main paper.  @article{mu2022actionformerego4d,   title={<PUBLICATION>Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge</PUBLICATION>},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} }   If you are using TSP features, please cite  @inproceedings{alwassel2021tsp,   title={<PUBLICATION>TSP: Temporally-sensitive pretraining of video encoders for localization tasks</PUBLICATION>},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={<CONFERENCE>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</CONFERENCE>},   pages={3173--3183},   year={2021} } 
",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\9688548d.txt,0.9909832546157149
