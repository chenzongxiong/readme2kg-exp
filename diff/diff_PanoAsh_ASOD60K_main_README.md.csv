sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
3,"To this end, we propose a new task, panoramic audiovisual salient object detection (PAV-SOD), which aims to segment the objects grasping most of the human attention in 360° panoramic videos reflecting real-life daily scenes.","To this end, we propose a new task, <PROJECT>panoramic audiovisual salient object detection</PROJECT> (<PROJECT>PAV-SOD</PROJECT>), which aims to segment the objects grasping most of the human attention in 360° panoramic videos reflecting real-life daily scenes.","To this end, we propose a new task, panoramic audiovisual salient object detection (`PAV-SOD`), which aims to segment the objects grasping most of the human attention in 360° panoramic videos reflecting real-life daily scenes.","To this end, we propose a new task, panoramic audiovisual salient object detection (`<PUBLICATION>PAV-SOD</PUBLICATION>`), which aims to segment the objects grasping most of the human attention in 360° panoramic videos reflecting real-life daily scenes.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\85f5f838.txt,0.9955555555555555
5,The coarse-to-fine annotations enable multi-perspective analysis regarding PAV-SOD modeling.,The coarse-to-fine annotations enable multi-perspective analysis regarding <SOFTWARE>PAV-SOD</SOFTWARE> modeling.,The coarse-to-fine annotations enable multi-perspective analysis regarding `PAV-SOD` modeling.,The coarse-to-fine annotations enable multi-perspective analysis regarding `<PUBLICATION>PAV-SOD</PUBLICATION>` modeling.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\18886047.txt,0.989247311827957
6,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our PAVS10K.,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our <DATASET>PAVS10K</DATASET>.,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our `PAVS10K`.,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our `<DATASET>PAVS10K</DATASET>`.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\25bec5df.txt,0.9932885906040269
8,"Our CVAE-based audiovisual network, namely CAV-Net, consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network and audiovisual distribution estimation modules.","Our CVAE-based audiovisual network, namely CAV-Net, consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network and audiovisual distribution estimation modules.","Our `CVAE`-based audiovisual network, namely `CAV-Net`, consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network and audiovisual distribution estimation modules.","Our `<SOFTWARE>CVAE</SOFTWARE>`-based audiovisual network, namely `<SOFTWARE>CAV-Net</SOFTWARE>`, consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network and audiovisual distribution estimation modules.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\9d156242.txt,0.9900497512437811
10,"With extensive experimental results, we gain several findings about PAV-SOD challenges and insights towards PAV-SOD model interpretability.","With extensive experimental results, we gain several findings about <PROJECT>PAV-SOD</PROJECT> challenges and insights towards <SOFTWARE>PAV-SOD </SOFTWARE>model interpretability.","With extensive experimental results, we gain several findings about `PAV-SOD` challenges and insights towards `PAV-SOD` model interpretability.","With extensive experimental results, we gain several findings about `<PUBLICATION>PAV-SOD</PUBLICATION>` challenges and insights towards `<PUBLICATION>PAV-SOD</PUBLICATION>` model interpretability.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\dde56107.txt,0.9858156028368794
17,#Img: The number of images/video frames.,#Img: The number of images/video frames.,#`Img`: The number of images/video frames.,#`<DATASET>Img</DATASET>`: The number of images/video frames.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\1b5f2807.txt,0.975609756097561
18,#GT: The number of object-level pixel-wise masks (ground truth for SOD).,#GT: The number of object-level pixel-wise masks (ground truth for SOD).,`#GT: The number of object-level pixel-wise masks (ground truth for SOD).`,`#<DATASET>GT</DATASET>: The number of object-level pixel-wise masks (ground truth for SOD).`,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\2806fbd5.txt,0.9863013698630136
19,Pub. = Publication.,Pub. = Publication.,`Pub. = Publication.`,`<PUBLICATION>Pub.</PUBLICATION> = <PUBLICATION>Publication</PUBLICATION>.`,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\3dacb85e.txt,0.95
23,-Level = Instance-Level Labels.,-Level = Instance-Level Labels.,-Level = `Instance-Level Labels`.,-Level = `<DATASET>Instance-Level Labels</DATASET>`.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\2d70a6a9.txt,0.96875
25,GT = Fixation Maps. † denotes equirectangular images.,GT = Fixation Maps. † denotes equirectangular images.,`GT = Fixation Maps. † denotes equirectangular images.`,`GT = Fixation Maps. † denotes equirectangular images.`,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\3926fa06.txt,0.9814814814814815
32,(b) Instance density (labeled frames per sequence) of each sub-class.,(b) Instance density (labeled frames per sequence) of each sub-class.,(b) `Instance density` (labeled frames per sequence) of each sub-class.,(b) `<EVALMETRIC>Instance density</EVALMETRIC>` (labeled frames per sequence) of each sub-class.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\ffb401b4.txt,0.9857142857142858
36,usp=sharing).  ------  # Dataset Downloads  The whole object-/instance-level ground truth with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/1Whp_ftuXza8-vkjNtICdxdRebcmzcrFi/view?,usp=sharing).  ------  # Dataset Downloads  The whole object-/instance-level ground truth with default split can be downloaded from [<SOFTWARE>Google Drive</SOFTWARE>](https://drive.google.com/file/d/1Whp_ftuXza8-vkjNtICdxdRebcmzcrFi/view?,`usp=sharing).  ------  # Dataset Downloads  The whole object-/instance-level ground truth with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/1Whp_ftuXza8-vkjNtICdxdRebcmzcrFi/view?`,`usp=sharing).  ------  # <DATASET>Dataset Downloads</DATASET>  The whole object-/instance-level ground truth with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/1Whp_ftuXza8-vkjNtICdxdRebcmzcrFi/view?`,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\600e553b.txt,0.9954337899543378
41,usp=sharing)  The audio files (.wav) can be downloaded from [Google Drive](https://drive.google.com/file/d/1-jqDArcm8vBhyku3Xb8HLopG1XmpAS13/view?,usp=sharing)  The audio files (.wav) can be downloaded from [<SOFTWARE>Google Drive</SOFTWARE>](https://drive.google.com/file/d/1-jqDArcm8vBhyku3Xb8HLopG1XmpAS13/view?,`usp=sharing)  The audio files (.wav) can be downloaded from [Google Drive](https://drive.google.com/file/d/1-jqDArcm8vBhyku3Xb8HLopG1XmpAS13/view?`,`usp=sharing)  The audio files (.wav) can be downloaded from [Google Drive](https://drive.google.com/file/d/1-jqDArcm8vBhyku3Xb8HLopG1XmpAS13/view?`,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\04420409.txt,0.9931972789115646
48,> Note: The PAVS10K dataset does not own the copyright of videos.,> Note: The <DATASET>PAVS10K</DATASET> dataset does not own the copyright of videos.,> Note: The `PAVS10K` dataset does not own the copyright of videos.,> Note: The `<DATASET>PAVS10K</DATASET>` dataset does not own the copyright of videos.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\9d62c4d2.txt,0.9848484848484849
49,"Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to PAVS10K.  ------  # Citation          @article{zhang2023pav,       title={PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={ACM Transactions on Multimedia Computing, Communications and Applications},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).","Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to <DATASET>PAVS10K</DATASET>.  ------  # Citation          @article{zhang2023pav,       title={<PUBLICATION>PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection</PUBLICATION><PROJECT>PAV-SOD</PROJECT>: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={<PUBLICATION>ACM Transactions on Multimedia Computing, Communications and Applications</PUBLICATION>},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).","Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to PAVS10K.  ------  # Citation          @article{zhang2023pav},       title={PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={ACM Transactions on Multimedia Computing, Communications and Applications},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).","Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to <DATASET>PAVS10K</DATASET>.  ------  # Citation          @article{zhang2023pav},       title={<PUBLICATION>PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection</PUBLICATION>},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={<PUBLICATION>ACM Transactions on Multimedia Computing, Communications and Applications</PUBLICATION>},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\ec120a26.txt,0.9992872416250891
