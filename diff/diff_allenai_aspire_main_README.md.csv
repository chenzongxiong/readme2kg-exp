sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,"### Aspire Repository accompanying paper for modeling fine grained similarity between documents:   **Title**: ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity""  **Authors**: Sheshera Mysore, Arman Cohan, Tom Hope  **Abstract**: We present a new scientific document similarity model based on matching fine-grained aspects of texts.","### Aspire Repository accompanying paper for modeling fine grained similarity between documents:   **Title**: ""<PUBLICATION>Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity</PUBLICATION>""  **Authors**: Sheshera Mysore, Arman Cohan, Tom Hope  **Abstract**: We present a new scientific document similarity model based on matching fine-grained aspects of texts.","### Aspire Repository accompanying paper for modeling fine grained similarity between documents:   **Title**: ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity""  **Authors**: Sheshera Mysore, Arman Cohan, Tom Hope  **Abstract**: We present a new scientific document similarity model based on matching fine-grained aspects of texts.
","### <PROJECT>Aspire Repository</PROJECT> accompanying paper for modeling fine grained similarity between documents:   **Title**: ""<PUBLICATION>Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity</PUBLICATION>""  **Authors**: Sheshera Mysore, Arman Cohan, Tom Hope  **Abstract**: We present a new scientific document similarity model based on matching fine-grained aspects of texts.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\aec39709.txt,0.9986577181208054
2,"To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations).","To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations).","To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations).","To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of <PUBLICATION>papers</PUBLICATION> that cite multiple <PUBLICATION>papers</PUBLICATION> together (co-citations).",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\3eb38d6b.txt,1.0
6,Our approach improves performance on document similarity tasks in four datasets.,Our approach improves performance on document similarity tasks in four datasets.,Our approach improves performance on document similarity tasks in four datasets.,Our approach improves performance on document similarity tasks in four <DATASET>datasets</DATASET>.,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\eb18167e.txt,1.0
10,[HF Models](#models)     1.,[HF Models](#models)     1.,"[HF Models](#models)     1.
","[<PROJECT>HF Models</PROJECT>](#models)     1.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\71cd8b29.txt,0.9818181818181818
11,[Evaluation Datasets](#evaldata) 1.,[Evaluation Datasets](#evaldata) 1.,"[Evaluation Datasets](#evaldata) 1.
","[Evaluation <DATASET>Datasets</DATASET>](#evaldata) 1.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\0a8ab83e.txt,0.9859154929577465
13,[Repository Contents](#repocontents) 1.,[Repository Contents](#repocontents) 1.,"[Repository Contents](#repocontents) 1.
","[Repository Contents](#repocontents) 1.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\0a9128ac.txt,0.9873417721518988
16,"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.","[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`<SOFTWARE>allenai/aspire-contextualsentence-multim-compsci</SOFTWARE>`](https://huggingface.co/<SOFTWARE>allenai/aspire-contextualsentence-multim-compsci</SOFTWARE>) - [`<SOFTWARE>allenai/aspire-contextualsentence-multim-biomed</SOFTWARE>`](https://huggingface.co/<SOFTWARE>allenai/aspire-contextualsentence-multim-biomed</SOFTWARE>)  `tsAspire`:   - [`<SOFTWARE>allenai/aspire-contextualsentence-singlem-compsci</SOFTWARE>`](https://huggingface.co/<SOFTWARE>allenai/aspire-contextualsentence-singlem-compsci</SOFTWARE>) - [`<SOFTWARE>allenai/aspire-contextualsentence-singlem-biomed</SOFTWARE>`](https://huggingface.co/<SOFTWARE>allenai/aspire-contextualsentence-singlem-biomed</SOFTWARE>)   `SPECTER-CoCite`:   - [`<SOFTWARE>allenai/aspire-biencoder-compsci-spec</SOFTWARE>`](https://huggingface.co/<SOFTWARE>allenai/aspire-biencoder-compsci-spec</SOFTWARE>) - [`<SOFTWARE>allenai/aspire-biencoder-biomed-scib</SOFTWARE>`](https://huggingface.co/<SOFTWARE>allenai/aspire-biencoder-biomed-scib</SOFTWARE>) - [`<SOFTWARE>allenai/aspire-biencoder-biomed-spec</SOFTWARE>`](https://huggingface.co/<SOFTWARE>allenai/aspire-biencoder-biomed-spec</SOFTWARE>)  `cosentbert`:   - [`<SOFTWARE>allenai/aspire-sentence-embedder</SOFTWARE>`](https://huggingface.co/<SOFTWARE>allenai/aspire-sentence-embedder</SOFTWARE>)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `<SOFTWARE>transformers</SOFTWARE>` library and some additional code to compute contextual sentence vectors as:  ```<PROGLANG>python</PROGLANG> from <SOFTWARE>transformers</SOFTWARE> import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = '<SOFTWARE>allenai/aspire-contextualsentence-singlem-compsci</SOFTWARE>' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `<SOFTWARE>transformers</SOFTWARE>` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.","[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  otAspire:  - [allenai/aspire-contextualsentence-multim-compsci](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [allenai/aspire-contextualsentence-multim-biomed](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  tsAspire:   - [allenai/aspire-contextualsentence-singlem-compsci](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [allenai/aspire-contextualsentence-singlem-biomed](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   SPECTER-CoCite:   - [allenai/aspire-biencoder-compsci-spec](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [allenai/aspire-biencoder-biomed-scib](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [allenai/aspire-biencoder-biomed-spec](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  cosentbert:   - [allenai/aspire-sentence-embedder](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### tsAspire  The tsAspire multi-vector model trained for single matches across documents can be used via the transformers library and some additional code to compute contextual sentence vectors as:  python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs)   ##### otAspire  The otAspire multi-vector model trained for _multiple_ matching across documents can be used via the transformers library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.
","[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  <SOFTWARE>otAspire</SOFTWARE>:  - [allenai/aspire-contextualsentence-multim-compsci](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [allenai/aspire-contextualsentence-multim-biomed](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  <SOFTWARE>tsAspire</SOFTWARE>:   - [allenai/aspire-contextualsentence-singlem-compsci](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [allenai/aspire-contextualsentence-singlem-biomed](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   <SOFTWARE>SPECTER-CoCite</SOFTWARE>:   - [allenai/aspire-biencoder-compsci-spec](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [allenai/aspire-biencoder-biomed-scib](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [allenai/aspire-biencoder-biomed-spec](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  <SOFTWARE>cosentbert</SOFTWARE>:   - [allenai/aspire-sentence-embedder](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### <SOFTWARE>tsAspire</SOFTWARE>  The <SOFTWARE>tsAspire</SOFTWARE> multi-vector model trained for single matches across documents can be used via the <SOFTWARE>transformers</SOFTWARE> library and some additional code to compute contextual sentence vectors as:  <PROGLANG>python</PROGLANG> from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs)   ##### <SOFTWARE>otAspire</SOFTWARE>  The <SOFTWARE>otAspire</SOFTWARE> multi-vector model trained for _multiple_ matching across documents can be used via the <SOFTWARE>transformers</SOFTWARE> library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\dc65aae3.txt,0.9461274845784784
17,"View example usage and sample document matches here: [`examples/demo-contextualsentence-multim.ipynb`](https://github.com/allenai/aspire/blob/main/examples/demo-contextualsentence-multim.ipynb)  ##### `SPECTER-CoCite`  The `SPECTER-CoCite` bi-encoder model can be used via the `transformers` library as:  ```python from transformers import AutoModel, AutoTokenizer aspire_bienc = AutoModel.from_pretrained('allenai/aspire-biencoder-compsci-spec') aspire_tok = AutoTokenizer.from_pretrained('allenai/aspire-biencoder-compsci-spec') title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific ""         ""Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching ""            ""fine-grained aspects of texts."" d=[title + aspire_tok.sep_token + abstract] inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) result = aspire_bienc(**inputs) clsrep = result.last_hidden_state[:, 0, :] ```  However, note that the Hugging Face models don't have a set of additional scalar-mix parameters to compute a learned weighted sum of the representations from different layers of the transformer encoder.","View example usage and sample document matches here: [`examples/demo-contextualsentence-multim.ipynb`](https://github.com/allenai/aspire/blob/main/examples/demo-contextualsentence-multim.ipynb)  ##### `SPECTER-CoCite`  The `SPECTER-CoCite` bi-encoder model can be used via the `transformers` library as:  ```<PROGLANG>python</PROGLANG> from transformers import AutoModel, AutoTokenizer aspire_bienc = AutoModel.from_pretrained('allenai/aspire-biencoder-compsci-spec') aspire_tok = AutoTokenizer.from_pretrained('allenai/aspire-biencoder-compsci-spec') title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific ""         ""Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching ""            ""fine-grained aspects of texts."" d=[title + aspire_tok.sep_token + abstract] inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) result = aspire_bienc(**inputs) clsrep = result.last_hidden_state[:, 0, :] ```  However, note that the Hugging Face models don't have a set of additional scalar-mix parameters to compute a learned weighted sum of the representations from different layers of the transformer encoder.","View example usage and sample document matches here: [examples/demo-contextualsentence-multim.ipynb](https://github.com/allenai/aspire/blob/main/examples/demo-contextualsentence-multim.ipynb)  ##### SPECTER-CoCite  The SPECTER-CoCite bi-encoder model can be used via the transformers library as:  python from transformers import AutoModel, AutoTokenizer aspire_bienc = AutoModel.from_pretrained('allenai/aspire-biencoder-compsci-spec') aspire_tok = AutoTokenizer.from_pretrained('allenai/aspire-biencoder-compsci-spec') title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific ""         ""Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching ""            ""fine-grained aspects of texts."" d=[title + aspire_tok.sep_token + abstract] inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) result = aspire_bienc(**inputs) clsrep = result.last_hidden_state[:, 0, :]   However, note that the Hugging Face models don't have a set of additional scalar-mix parameters to compute a learned weighted sum of the representations from different layers of the transformer encoder.
","View example usage and sample document matches here: [examples/demo-contextualsentence-multim.ipynb](https://github.com/<PROJECT>allenai</PROJECT>/<PROJECT>aspire</PROJECT>/blob/main/examples/demo-contextualsentence-multim.ipynb)  ##### <SOFTWARE>SPECTER-CoCite</SOFTWARE>  The <SOFTWARE>SPECTER-CoCite</SOFTWARE> bi-encoder model can be used via the <SOFTWARE>transformers</SOFTWARE> library as:  <PROGLANG>python</PROGLANG> from <SOFTWARE>transformers</SOFTWARE> import <SOFTWARE>AutoModel</SOFTWARE>, <SOFTWARE>AutoTokenizer</SOFTWARE> aspire_bienc = <SOFTWARE>AutoModel</SOFTWARE>.from_pretrained('<PROJECT>allenai</PROJECT>/<SOFTWARE>aspire-biencoder-compsci-spec</SOFTWARE>') aspire_tok = <SOFTWARE>AutoTokenizer</SOFTWARE>.from_pretrained('<PROJECT>allenai</PROJECT>/<SOFTWARE>aspire-biencoder-compsci-spec</SOFTWARE>') title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific ""         ""Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching ""            ""fine-grained aspects of texts."" d=[title + aspire_tok.sep_token + abstract] inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) result = aspire_bienc(**inputs) clsrep = result.last_hidden_state[:, 0, :]   However, note that the <SOFTWARE>Hugging Face</SOFTWARE> models don't have a set of additional scalar-mix parameters to compute a learned weighted sum of the representations from different layers of the transformer encoder.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\6775af2c.txt,0.9835095137420719
18,These are used in our paper and are important for performance in some datasets.,These are used in our paper and are important for performance in some datasets.,These are used in our paper and are important for performance in some datasets.,These are used in our <PUBLICATION>paper</PUBLICATION> and are important for performance in some <DATASET>datasets</DATASET>.,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\7fac2ce5.txt,1.0
19,"Obtain the model zip files:  - [`aspire-biencoder-biomed-scib-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-scib-full.zip) - [`aspire-biencoder-biomed-spec-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-spec-full.zip) - [`aspire-biencoder-compsci-spec-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip)  ```bash wget -O aspire-biencoder-compsci-spec-full.zip https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip unzip aspire-biencoder-compsci-spec-full.zip ```  Now it may be used as:  ```python  import os, json, codecs, torch from transformers import AutoTokenizer from examples.ex_aspire_bienc import AspireBiEnc  # Directory where zipped model was downloaded and unzipped. model_path = '.","Obtain the model zip files:  - [`aspire-biencoder-biomed-scib-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-scib-full.zip) - [`aspire-biencoder-biomed-spec-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-spec-full.zip) - [`aspire-biencoder-compsci-spec-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip)  ```<PROGLANG>bash</PROGLANG> wget -O aspire-biencoder-compsci-spec-full.zip https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip unzip aspire-biencoder-compsci-spec-full.zip ```  Now it may be used as:  ```<PROGLANG>python</PROGLANG>  import os, json, codecs, torch from transformers import AutoTokenizer from examples.ex_aspire_bienc import AspireBiEnc  # Directory where zipped model was downloaded and unzipped. model_path = '.","Obtain the model zip files:  
- [aspire-biencoder-biomed-scib-full](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-scib-full.zip)  
- [aspire-biencoder-biomed-spec-full](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-spec-full.zip)  
- [aspire-biencoder-compsci-spec-full](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip)  

bash  
wget -O aspire-biencoder-compsci-spec-full.zip https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip  
unzip aspire-biencoder-compsci-spec-full.zip  
  

Now it may be used as:  

python  
import os, json, codecs, torch  
from transformers import AutoTokenizer  
from examples.ex_aspire_bienc import AspireBiEnc  

# Directory where zipped model was downloaded and unzipped.  
model_path = '.","Obtain the model zip files:  
- [<SOFTWARE>aspire-biencoder-biomed-scib-full</SOFTWARE>](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-scib-full.zip)  
- [<SOFTWARE>aspire-biencoder-biomed-spec-full</SOFTWARE>](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-spec-full.zip)  
- [<SOFTWARE>aspire-biencoder-compsci-spec-full</SOFTWARE>](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip)  

bash  
wget -O <SOFTWARE>aspire-biencoder-compsci-spec-full</SOFTWARE>.zip https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip  
unzip <SOFTWARE>aspire-biencoder-compsci-spec-full</SOFTWARE>.zip  
  

Now it may be used as:  

python  
import os, json, codecs, torch  
from transformers import AutoTokenizer  
from examples.ex_aspire_bienc import <SOFTWARE>AspireBiEnc</SOFTWARE>  

# Directory where zipped model was downloaded and unzipped.  
model_path = '.",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\e1c349ba.txt,0.8040885860306644
20,"/aspire-biencoder-compsci-spec-full'  # Load hyperparameters from disk. with codecs.open(os.path.join(model_path, 'run_info.json'), 'r') as fp:     hparams = json.load(fp)     model_hparams = hparams['all_hparams']  # Initialize the tokenizer and model. aspire_tok = AutoTokenizer.from_pretrained(model_hparams['base-pt-layer']) aspire_bienc = AspireBiEnc(model_hparams)  # Load model parameters from disk. model_fname = os.path.join(model_path, 'model_cur_best.pt') aspire_bienc.load_state_dict(torch.load(model_fname))  # Encode example input. title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific ""         ""Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching ""            ""fine-grained aspects of texts."" d = [title + aspire_tok.sep_token + abstract]  inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) clsrep = aspire_bienc.forward(inputs)  ```   #### Evaluation Datasets <a name=""evaldata""></a>  The paper uses the following evaluation datasets:  - RELISH was created in [Brown et al. 2019](https://academic.oup.com/database/article/doi/10.1093/database/baz085/5608006?","/aspire-biencoder-compsci-spec-full'  # Load hyperparameters from disk. with codecs.open(os.path.join(model_path, 'run_info.json'), 'r') as fp:     hparams = json.load(fp)     model_hparams = hparams['all_hparams']  # Initialize the tokenizer and model. aspire_tok = AutoTokenizer.from_pretrained(model_hparams['base-pt-layer']) aspire_bienc = AspireBiEnc(model_hparams)  # Load model parameters from disk. model_fname = os.path.join(model_path, 'model_cur_best.pt') aspire_bienc.load_state_dict(torch.load(model_fname))  # Encode example input. title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific ""         ""Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching ""            ""fine-grained aspects of texts."" d = [title + aspire_tok.sep_token + abstract]  inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) clsrep = aspire_bienc.forward(inputs)  ```   #### Evaluation Datasets <a name=""evaldata""></a>  The paper uses the following evaluation datasets:  - <DATASET>RELISH</DATASET> was created in [Brown et al. 2019](https://academic.oup.com/database/article/doi/10.1093/database/baz085/5608006?","/aspire-biencoder-compsci-spec-full'  # Load hyperparameters from disk. with codecs.open(os.path.join(model_path, 'run_info.json'), 'r') as fp:     hparams = json.load(fp)     model_hparams = hparams['all_hparams']  # Initialize the tokenizer and model. aspire_tok = AutoTokenizer.from_pretrained(model_hparams['base-pt-layer']) aspire_bienc = AspireBiEnc(model_hparams)  # Load model parameters from disk. model_fname = os.path.join(model_path, 'model_cur_best.pt') aspire_bienc.load_state_dict(torch.load(model_fname))  # Encode example input. title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching fine-grained aspects of texts."" d = [title + aspire_tok.sep_token + abstract]  inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) clsrep = aspire_bienc.forward(inputs)     #### Evaluation Datasets <a name=""evaldata""></a>  The paper uses the following evaluation datasets:  - RELISH was created in [Brown et al. 2019](https://academic.oup.com/database/article/doi/10.1093/database/baz085/5608006?
","/aspire-biencoder-compsci-spec-full'  # Load hyperparameters from disk. with codecs.open(os.path.join(model_path, 'run_info.json'), 'r') as fp:     hparams = json.load(fp)     model_hparams = hparams['all_hparams']  # Initialize the tokenizer and model. aspire_tok = AutoTokenizer.from_pretrained(model_hparams['base-pt-layer']) aspire_bienc = AspireBiEnc(model_hparams)  # Load model parameters from disk. model_fname = os.path.join(model_path, 'model_cur_best.pt') aspire_bienc.load_state_dict(torch.load(model_fname))  # Encode example input. title = ""<PUBLICATION>Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity</PUBLICATION>"" abstract = ""We present a new scientific document similarity model based on matching fine-grained aspects of texts."" d = [title + aspire_tok.sep_token + abstract]  inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) clsrep = aspire_bienc.forward(inputs)     #### Evaluation Datasets <a name=""evaldata""></a>  The paper uses the following evaluation datasets:  - <DATASET>RELISH</DATASET> was created in [<PUBLICATION>Brown et al. 2019</PUBLICATION>](https://academic.oup.com/database/article/doi/10.1093/database/baz085/5608006?
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\bb3dfd33.txt,0.9877066553624417
22,While I wasn't able to access the link in the publication.,While I wasn't able to access the link in the publication.,While I wasn't able to access the link in the publication.,While I wasn't able to access the link in the <PUBLICATION>publication</PUBLICATION>.,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\cc64b6b1.txt,1.0
23,I was able to obtain a copy of the dataset from: [link](http://pubannotation.org/projects/RELISH-DB).,I was able to obtain a copy of the dataset from: [link](http://pubannotation.org/projects/RELISH-DB).,I was able to obtain a copy of the RELISH-DB dataset from: [link](http://pubannotation.org/projects/RELISH-DB).,I was able to obtain a copy of the <DATASET>RELISH-DB</DATASET> dataset from: [link](http://pubannotation.org/projects/<DATASET>RELISH-DB</DATASET>).,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\7dea159d.txt,0.9528301886792453
24,Dataset splits are created in `pre_proc_relish.py`,Dataset splits are created in `pre_proc_relish.py`,"Dataset splits are created in pre_proc_relish.py
","<DATASET>Dataset splits</DATASET> are created in <PROGLANG>pre_proc_relish.py</PROGLANG>
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\73d88dcb.txt,0.9696969696969697
25,.,.,".
.
",".
.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\cdb4ee2a.txt,0.4
27,"The versions of the dataset used may be accessed here: [query topics](https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml), [relevance annotations](https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt), and the metadata for papers is obtained from the [CORD-19](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html) dataset in the [2021-06-21](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-06-21/metadata.csv) release.","The versions of the dataset used may be accessed here: [query topics](https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml), [relevance annotations](https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt), and the metadata for papers is obtained from the [<DATASET>CORD-19</DATASET>](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html) dataset in the [2021-06-21](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-06-21/metadata.csv) release.","The versions of the dataset used may be accessed here: [query topics](https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml), [relevance annotations](https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt), and the metadata for papers is obtained from the CORD-19 dataset in the 2021-06-21 release.","The versions of the <DATASET>dataset</DATASET> used may be accessed here: [query topics](https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml), [relevance annotations](https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt), and the metadata for papers is obtained from the <DATASET>CORD-19</DATASET> <DATASET>dataset</DATASET> in the <DATASET>2021-06-21</DATASET> release.",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\93350fec.txt,0.7104930467762326
28,"The function `get_qbe_pools` in `pre_proc_treccovid.py`, converts the dataset in its original form to the reformulated form, TRECCOVID-RF, used in the paper.","The function `get_qbe_pools` in `pre_proc_<DATASET>treccovid</DATASET>.py`, converts the dataset in its original form to the reformulated form, <DATASET>TRECCOVID</DATASET>-RF, used in the paper.","The function get_qbe_pools in pre_proc_treccovid.py, converts the dataset in its original form to the reformulated form, TRECCOVID-RF, used in the paper.","The function <PROGLANG>get_qbe_pools</PROGLANG> in <PROGLANG>pre_proc_treccovid.py</PROGLANG>, converts the <DATASET>dataset</DATASET> in its original form to the reformulated form, <DATASET>TRECCOVID-RF</DATASET>, used in the <PUBLICATION>paper</PUBLICATION>.",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\837cc195.txt,0.9870967741935484
29,Dataset splits are created in `pre_proc_treccovid.py`,Dataset splits are created in `pre_proc_<DATASET>treccovid</DATASET>.py`,"Dataset splits are created in pre_proc_treccovid.py
","<DATASET>Dataset splits</DATASET> are created in <PROGLANG>pre_proc_treccovid.py</PROGLANG>
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\a7eafeed.txt,0.9714285714285714
30,.,.,".
.
",".
.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\cdb4ee2a.txt,0.4
31,- SciDocs is obtained from: [link](https://github.com/allenai/scidocs).,- <DATASET>SciDocs</DATASET> is obtained from: [link](https://github.com/allenai/<DATASET>scidocs</DATASET>).,- SciDocs is obtained from: [link](https://github.com/allenai/scidocs).,- <PROJECT>SciDocs</PROJECT> is obtained from: [link](https://github.com/allenai/<PROJECT>scidocs</PROJECT>).,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\4dd43e69.txt,1.0
32,The dataset splits supplied alongside the original dataset are used as is,The dataset splits supplied alongside the original dataset are used as is,The dataset splits supplied alongside the original dataset are used as is,The <DATASET>dataset splits</DATASET> supplied alongside the original <DATASET>dataset</DATASET> are used as is,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\d48dc390.txt,1.0
33,.,.,".
.
",".
.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\cdb4ee2a.txt,0.4
34,- CSFCube is obtained from: [link](https://github.com/iesl/CSFCube).,- <DATASET>CSFCube</DATASET> is obtained from: [link](https://github.com/iesl/<DATASET>CSFCube</DATASET>).,- CSFCube is obtained from: [link](https://github.com/iesl/CSFCube).,- <PROJECT>CSFCube</PROJECT> is obtained from: [link](https://github.com/iesl/<PROJECT>CSFCube</PROJECT>).,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\be4f659e.txt,1.0
35,The dataset splits supplied alongside the original dataset are used as is.,The dataset splits supplied alongside the original dataset are used as is.,The dataset splits supplied alongside the original dataset are used as is.,The <DATASET>dataset splits</DATASET> supplied alongside the original <DATASET>dataset</DATASET> are used as is.,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\ff112d86.txt,1.0
36,"Complete evaluation datasets used in the paper can be downloaded here: [`datasets/datasets.md`](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name=""repocontents""></a>      ├── bin     ├── config     │             └── models_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid_models         │             │             ├── disent_models.py         │             │             ├── pair_distances.py         │             │             └── sentsim_models.py         │             ├── main_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre_process             ├── extract_entities.py             ├── pp_settings.py             ├── pre_proc_cocits.py             ├── pre_proc_gorc.py             ├── pre_proc_relish.py             ├── pre_proc_scidocs.py             ├── pre_proc_treccovid.py             ├── pp_gen_nearest.py             └── pre_proc_buildreps.py   **The repository is organized broadly as:**  `src/pre_process/`: Scripts to 1) generate gather and filter co-citations data from the [S2ORC](https://github.com/allenai/s2orc) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets.","Complete evaluation datasets used in the paper can be downloaded here: [`datasets/datasets.md`](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name=""repocontents""></a>      ├── bin     ├── config     │             └── models_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid_models         │             │             ├── disent_models.py         │             │             ├── pair_distances.py         │             │             └── sentsim_models.py         │             ├── main_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre_process             ├── extract_entities.py             ├── pp_settings.py             ├── pre_proc_cocits.py             ├── pre_proc_gorc.py             ├── pre_proc_relish.py             ├── pre_proc_scidocs.py             ├── pre_proc_treccovid.py             ├── pp_gen_nearest.py             └── pre_proc_buildreps.py   **The repository is organized broadly as:**  `src/pre_process/`: Scripts to 1) generate gather and filter co-citations data from the [<DATASET>S2ORC</DATASET>](https://github.com/allenai/<DATASET>s2orc</DATASET>) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets.","Complete evaluation datasets used in the paper can be downloaded here: [datasets/datasets.md](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name=""repocontents""></a>      ├── bin     ├── config     │             └── models_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid_models         │             │             ├── disent_models.py         │             │             ├── pair_distances.py         │             │             └── sentsim_models.py         │             ├── main_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre_process             ├── extract_entities.py             ├── pp_settings.py             ├── pre_proc_cocits.py             ├── pre_proc_gorc.py             ├── pre_proc_relish.py             ├── pre_proc_scidocs.py             ├── pre_proc_treccovid.py             ├── pp_gen_nearest.py             └── pre_proc_buildreps.py   **The repository is organized broadly as:**  src/pre_process/: Scripts to 1) generate gather and filter co-citations data from the [S2ORC](https://github.com/allenai/s2orc) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets.
","Complete evaluation <DATASET>datasets</DATASET> used in the paper can be downloaded here: [<DATASET>datasets/datasets.md</DATASET>](https://github.com/allenai/aspire/blob/main/<DATASET>datasets</DATASET>/<DATASET>datasets.md</DATASET>)   ### Repository Contents <a name=""repocontents""></a>      ├── bin     ├── config     │             └── models_config     │                 ├── <DATASET>s2orcbiomed</DATASET>     │                 ├── <DATASET>s2orccompsci</DATASET>     │                 └── <DATASET>s2orcscidocs</DATASET>     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── <SOFTWARE>datasets.py</SOFTWARE>         │             │             ├── <SOFTWARE>metrics.py</SOFTWARE>         │             │             ├── <SOFTWARE>models.py</SOFTWARE>         │             │             └── <SOFTWARE>utils.py</SOFTWARE>         │             └── <SOFTWARE>evaluate.py</SOFTWARE>         ├── learning         │             ├── facetid_models         │             │             ├── <SOFTWARE>disent_models.py</SOFTWARE>         │             │             ├── <SOFTWARE>pair_distances.py</SOFTWARE>         │             │             └── <SOFTWARE>sentsim_models.py</SOFTWARE>         │             ├── <SOFTWARE>main_fsim.py</SOFTWARE>         │             ├── <SOFTWARE>batchers.py</SOFTWARE>         │             └── <SOFTWARE>trainer.py</SOFTWARE>         └── pre_process             ├── <SOFTWARE>extract_entities.py</SOFTWARE>             ├── <SOFTWARE>pp_settings.py</SOFTWARE>             ├── <SOFTWARE>pre_proc_cocits.py</SOFTWARE>             ├── <SOFTWARE>pre_proc_gorc.py</SOFTWARE>             ├── <SOFTWARE>pre_proc_relish.py</SOFTWARE>             ├── <SOFTWARE>pre_proc_scidocs.py</SOFTWARE>             ├── <SOFTWARE>pre_proc_treccovid.py</SOFTWARE>             ├── <SOFTWARE>pp_gen_nearest.py</SOFTWARE>             └── <SOFTWARE>pre_proc_buildreps.py</SOFTWARE>   **The repository is organized broadly as:**  <SOFTWARE>src/pre_process/</SOFTWARE>: Scripts to 1) generate gather and filter co-citations data from the [<DATASET>S2ORC</DATASET>](https://github.com/allenai/<DATASET>s2orc</DATASET>) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation <DATASET>datasets</DATASET> into apt formats for use with models 4) extract NER entities from <DATASET>datasets</DATASET>.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\d57685e3.txt,0.9985767150583547
37,"`src/learning/`: Classes for implementing models, training, batching data, and a main script to train and save the model.","`src/learning/`: Classes for implementing models, training, batching data, and a main script to train and save the model.","src/learning/: Classes for implementing models, training, batching data, and a main script to train and save the model.
","src/learning/: Classes for implementing models, training, batching data, and a main script to train and save the model.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\cd06cca8.txt,0.9875518672199171
38,`src/evaluation/`: Scripts to evaluate model performances on various evaluation datasets.,`src/evaluation/`: Scripts to evaluate model performances on various evaluation datasets.,"src/evaluation/: Scripts to evaluate model performances on various evaluation datasets.
","src/evaluation/: Scripts to evaluate model performances on various evaluation <DATASET>datasets</DATASET>.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\25bad68c.txt,0.9830508474576272
39,See `src/evaluation/evaluate.md` for help.,See `src/evaluation/evaluate.md` for help.,"See src/evaluation/evaluate.md for help.
","See src/evaluation/evaluate.md for help.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\84dc66dc.txt,0.963855421686747
40,`config/models_config`: JSON files with hyper-parameters for models in the paper consumed by code in `src/learning/`.,`config/models_config`: JSON files with hyper-parameters for models in the paper consumed by code in `src/learning/`.,"config/models_config: JSON files with hyper-parameters for models in the paper consumed by code in src/learning/.
","config/models_config: JSON files with hyper-parameters for models in the paper consumed by code in <PROGLANG>src/learning/</PROGLANG>.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\34cee3f4.txt,0.9783549783549783
41,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.","Since we evaluate on datasets in the Biomedical (<DATASET>RELISH</DATASET>, <DATASET>TRECCOVID</DATASET>-RF), Computer Science (<DATASET>CSFCube</DATASET>), and mixed domains (<DATASET>SciDocs</DATASET>) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.","Here is the annotated text in Markdown format:

Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named s2orcbiomed, s2orccompsci, and s2orcscidocs contain config files for the models trained for each domain.","Here is the annotated text in Markdown format:

Since we evaluate on datasets in the Biomedical (<DATASET>RELISH</DATASET>, <DATASET>TRECCOVID-RF</DATASET>), Computer Science (<DATASET>CSFCube</DATASET>), and mixed domains (<DATASET>SciDocs</DATASET>) we train separate models for these domains, the sub-directories named s2orcbiomed, s2orccompsci, and s2orcscidocs contain config files for the models trained for each domain.",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\97fef831.txt,0.8936170212765957
42,`bin`: Shell scripts to call the scripts in all the `src` sub-directories with appropriate command line arguments.,`bin`: Shell scripts to call the scripts in all the `src` sub-directories with appropriate command line arguments.,"bin: Shell scripts to call the scripts in all the src sub-directories with appropriate command line arguments.
","bin: Shell scripts to call the scripts in all the src sub-directories with appropriate command line arguments.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\29bcf345.txt,0.9777777777777777
43,`scripts`: Miscellaneous glue code.,`scripts`: Miscellaneous glue code.,"scripts: Miscellaneous glue code.
","<PROGLANG>scripts</PROGLANG>: Miscellaneous glue code.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\46f17c4d.txt,0.9565217391304348
44,**The following files are the main entry points into the repository:**  `src/learning/main_fsim.py`: The main script called from `bin/learning/run_main_fsim-ddp.sh` to initialize and train a model.,**The following files are the main entry points into the repository:**  `src/learning/main_fsim.py`: The main script called from `bin/learning/run_main_fsim-ddp.sh` to initialize and train a model.,**The following files are the main entry points into the repository:**  src/learning/main_fsim.py: The main script called from bin/learning/run_main_fsim-ddp.sh to initialize and train a model.,**The following files are the main entry points into the repository:**  src/learning/<SOFTWARE>main_fsim.py</SOFTWARE>: The main script called from bin/learning/<SOFTWARE>run_main_fsim-ddp.sh</SOFTWARE> to initialize and train a model.,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\27065dde.txt,0.9897435897435898
45,The models consume json config files in `config/models_config/{<domain>}`.,The models consume json config files in `config/models_config/{<domain>}`.,The models consume json config files in config/models_config/{<domain>}.,The models consume json config files in config/models_config/{<domain>}.,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\1c66584f.txt,0.9863013698630136
46,A mapping from the model names/classes/configs in the repository to the models reported in the paper is as follows:  `src/evaluation/evaluate.py`: Contain code to generate rankings over the evaluation datasets.,A mapping from the model names/classes/configs in the repository to the models reported in the paper is as follows:  `src/evaluation/evaluate.py`: Contain code to generate rankings over the evaluation datasets.,"A mapping from the model names/classes/configs in the repository to the models reported in the paper is as follows:  src/evaluation/evaluate.py: Contain code to generate rankings over the evaluation datasets.
","A mapping from the model names/classes/configs in the repository to the models reported in the paper is as follows:  <SOFTWARE>src/evaluation/evaluate.py</SOFTWARE>: Contain code to generate rankings over the evaluation datasets.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\dd6686c9.txt,0.9928400954653938
49,"`src/pre_process/pre_proc_gorc.py:` Code to gather full text articles from the [S2ORC](https://github.com/allenai/s2orc) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers).","`src/pre_process/pre_proc_gorc.py:` Code to gather full text articles from the [<DATASET>S2ORC</DATASET>](https://github.com/allenai/<DATASET>s2orc</DATASET>) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers).","src/pre_process/pre_proc_gorc.py: Code to gather full text articles from the [S2ORC](https://github.com/allenai/s2orc) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers).
","src/pre_process/pre_proc_gorc.py: Code to gather full text articles from the [<DATASET>S2ORC</DATASET>](https://github.com/allenai/<DATASET>s2orc</DATASET>) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers).
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\7f89a092.txt,0.9942418426103646
51,`src/pre_process/pre_proc_cocits.py:` Generate training data for the models reported in the paper.,`src/pre_process/pre_proc_cocits.py:` Generate training data for the models reported in the paper.,"src/pre_process/pre_proc_cocits.py: Generate training data for the models reported in the paper.
","<SOFTWARE>src/pre_process/pre_proc_cocits.py</SOFTWARE>: Generate training data for the models reported in the paper.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\54bf646e.txt,0.9846153846153847
53,These are the `filter_cocitation_sentences` and `filter_cocitation_papers` functions respectively.,These are the `filter_cocitation_sentences` and `filter_cocitation_papers` functions respectively.,These are the filter_cocitation_sentences and filter_cocitation_papers functions respectively.,These are the <SOFTWARE>filter_cocitation_sentences</SOFTWARE> and <SOFTWARE>filter_cocitation_papers</SOFTWARE> functions respectively.,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\1a579ab7.txt,0.9791666666666666
54,Functions listed under `write_examples` generate training positive pairs for various models (negatives are generated with in-batch negative sampling).,Functions listed under `write_examples` generate training positive pairs for various models (negatives are generated with in-batch negative sampling).,Functions listed under write_examples generate training positive pairs for various models (negatives are generated with in-batch negative sampling).,Functions listed under <SOFTWARE>write_examples</SOFTWARE> generate training positive pairs for various models (negatives are generated with in-batch negative sampling).,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\953893e8.txt,0.9932885906040269
55,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.","`src/pre_process/pre_proc_{<DATASET>relish</DATASET>/<DATASET>scidocs</DATASET>/<DATASET>treccovid</DATASET>}.py`: Pre-process the evaluation datasets (<DATASET>RELISH</DATASET>, <DATASET>TRECCOVID</DATASET>, and <DATASET>SciDocs</DATASET>) into a format consumed by trained models and evaluation scripts.","src/pre_process/pre_proc_{relish/scidocs/treccovid}.py: Pre-process the evaluation RELISH, TRECCOVID, and SciDocs into a format consumed by trained models and evaluation scripts.
","src/pre_process/pre_proc_{relish/scidocs/treccovid}.py: Pre-process the evaluation <DATASET>RELISH</DATASET>, <DATASET>TRECCOVID</DATASET>, and <DATASET>SciDocs</DATASET> into a format consumed by trained models and evaluation scripts.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\7d627d21.txt,0.9621621621621622
57,Details about each dataset are as follows:  `src/pre_process/extract_entities.py`:  Use PURE's Entity Model () to extract named entities from abstracts.,Details about each dataset are as follows:  `src/pre_process/extract_entities.py`:  Use PURE's Entity Model () to extract named entities from abstracts.,"Details about each dataset are as follows:  src/pre_process/extract_entities.py:  Use PURE's Entity Model () to extract named entities from abstracts.
","Details about each <DATASET>dataset</DATASET> are as follows:  src/pre_process/<PROGLANG>extract_entities.py</PROGLANG>:  Use <SOFTWARE>PURE's Entity Model</SOFTWARE> () to extract named entities from abstracts.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\89483a24.txt,0.9900990099009901
59,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.","For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [<DATASET>Semantic Scholar Open Research Corpus</DATASET>](https://github.com/allenai/<DATASET>s2orc</DATASET>) (<DATASET>S2ORC</DATASET>) and the evaluation datasets <DATASET>RELISH</DATASET> (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), <DATASET>TRECCOVID</DATASET>, <DATASET>SciDocs</DATASET>, and <DATASET>CSFCube</DATASET> linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={<PUBLICATION>Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity</PUBLICATION>},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1.","For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under config/models_config/{<domain>}  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        cosentbert        |  facetid_models.sentsim_models.SentBERTWrapper | | ICTSentBert              |        ictsentbert        |  facetid_models.sentsim_models.ICTBERTWrapper | | SPECTER-CoCite              |        hparam_opt/cospecter-best/hparam_opt/cospecter-specinit-best        |  facetid_models.disent_models.MySPECTER  | | tsAspire                    |        hparam_opt/sbalisentbienc-sup-best        |        facetid_models.disent_models.WordSentAbsSupAlignBiEnc   | | otAspire                    |        hparam_opt/miswordbienc-otstuni-best        |      facetid_models.disent_models.WordSentAlignBiEnc   | | ts+otAspire                 |        hparam_opt/sbalisentbienc-otuni-best        |        facetid_models.disent_models.WordSentAbsSupAlignBiEnc   | | maxAspire                 |          hparam_opt/miswordbienc-l2max-best      |        facetid_models.disent_models.WordSentAlignBiEnc | | absAspire                 |          hparam_opt/sbalisentbienc-sup-absali-best      |        facetid_models.disent_models.WordSentAbsSupAlignBiEnc   | | attAspire                 |          hparam_opt/miswordbienc-cdatt-best      |        facetid_models.disent_models.WordSentAlignBiEnc   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} }    ### TODOs <a name=""todos""></a>  1.
","For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under config/models_config/{<domain>}  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        cosentbert        |  facetid_models.sentsim_models.SentBERTWrapper | | ICTSentBert              |        ictsentbert        |  facetid_models.sentsim_models.ICTBERTWrapper | | SPECTER-CoCite              |        hparam_opt/cospecter-best/hparam_opt/cospecter-specinit-best        |  facetid_models.disent_models.MySPECTER  | | tsAspire                    |        hparam_opt/sbalisentbienc-sup-best        |        facetid_models.disent_models.WordSentAbsSupAlignBiEnc   | | otAspire                    |        hparam_opt/miswordbienc-otstuni-best        |      facetid_models.disent_models.WordSentAlignBiEnc   | | ts+otAspire                 |        hparam_opt/sbalisentbienc-otuni-best        |        facetid_models.disent_models.WordSentAbsSupAlignBiEnc   | | maxAspire                 |          hparam_opt/miswordbienc-l2max-best      |        facetid_models.disent_models.WordSentAlignBiEnc | | absAspire                 |          hparam_opt/sbalisentbienc-sup-absali-best      |        facetid_models.disent_models.WordSentAbsSupAlignBiEnc   | | attAspire                 |          hparam_opt/miswordbienc-cdatt-best      |        facetid_models.disent_models.WordSentAlignBiEnc   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [<DATASET>Semantic Scholar Open Research Corpus</DATASET>](https://github.com/allenai/s2orc) (<DATASET>S2ORC</DATASET>) and the evaluation datasets <DATASET>RELISH</DATASET> (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), <DATASET>TRECCOVID</DATASET>, <DATASET>SciDocs</DATASET>, and <DATASET>CSFCube</DATASET> linked above. (2) The pre-trained models of [<SOFTWARE>SPECTER</SOFTWARE>](https://github.com/allenai/specter). (3) The software packages: [<SOFTWARE>GeomLoss</SOFTWARE>](https://www.kernel-operations.io/geomloss/index.html) and [<SOFTWARE>sentence-transformers</SOFTWARE>](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [<PUBLICATION>Aspire paper</PUBLICATION>](https://arxiv.org/pdf/2004.07180.pdf) as:    bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} }    ### TODOs <a name=""todos""></a>  1.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\85fc4df0.txt,0.9106350125458406
61,(in-progress)     - Currently released models are _per-domain_ models for computer science and biomedical papers which were used in the paper.,(in-progress)     - Currently released models are _per-domain_ models for computer science and biomedical papers which were used in the paper.,(in-progress)     - Currently released models are _per-domain_ models for computer science and biomedical papers which were used in the paper.,(in-progress)     - Currently released models are _per-domain_ models for computer science and biomedical papers which were used in the <PUBLICATION>paper</PUBLICATION>.,../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\ea609cf8.txt,1.0
64,.,.,".
.
",".
.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\cdb4ee2a.txt,0.4
65,"- Co-citation data used to train the above model will also be released, this is co-citation pairs on the order of a few million pairs of papers. 3.","- Co-citation data used to train the above model will also be released, this is co-citation pairs on the order of a few million pairs of papers. 3.","- Co-citation data used to train the above model will also be released, this is co-citation pairs on the order of a few million pairs of papers. 3.  

- Co-citation data used to train the above model will also be released, this is co-citation pairs on the order of a few million pairs of papers. 3.
","- Co-citation data used to train the above model will also be released, this is co-citation pairs on the order of a few million pairs of papers. 3.  

- Co-citation data used to train the above model will also be released, this is co-citation pairs on the order of a few million pairs of papers. 3.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\0675fcc9.txt,0.6591928251121076
67,.,.,".
.
",".
.
",../results/deepseek-chat/prompt-0/zzz_allenai_aspire_main_README.md.tsv\cdb4ee2a.txt,0.4
