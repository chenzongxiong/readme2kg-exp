sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,"# Awesome Neural Tree Papers <img class=""emoji"" alt="":art:"" height=""30"" width=""30"" src=""tree.png""> Selected papers and possible corresponding codes in our review paper **""A Survey of Neural Trees"" [[arXiv Version]](https://arxiv.org/abs/2209.03415)**  *If you find there is a missed paper or a possible mistake in our survey, please feel free to email me or pull a request here.","# Awesome Neural Tree Papers <img class=""emoji"" alt="":art:"" height=""30"" width=""30"" src=""tree.png""> Selected papers and possible corresponding codes in our review paper **""<PUBLICATION>A Survey of Neural Trees</PUBLICATION>"" [[arXiv Version]](https://arxiv.org/abs/2209.03415)**  *If you find there is a missed paper or a possible mistake in our survey, please feel free to email me or pull a request here.","# Awesome Neural Tree Papers <img class=""emoji"" alt="":art:"" height=""30"" width=""30"" src=""tree.png""> Selected papers and possible corresponding codes in our review paper **""A Survey of Neural Trees"" [[arXiv Version]](https://arxiv.org/abs/2209.03415)**  *If you find there is a missed paper or a possible mistake in our survey, please feel free to email me or pull a request here.","# Awesome Neural Tree Papers <img class=""emoji"" alt="":art:"" height=""30"" width=""30"" src=""tree.png""> Selected papers and possible corresponding codes in our review paper **""<PUBLICATION>A Survey of Neural Trees</PUBLICATION>"" [[arXiv Version]](https://arxiv.org/abs/2209.03415)**  *If you find there is a missed paper or a possible mistake in our survey, please feel free to email me or pull a request here.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\088d5f63.txt,1.0
2,I am more than glad to receive your advice.,I am more than glad to receive your advice.,I am more than glad to receive your advice.,I am more than glad to receive your advice.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\c4422211.txt,1.0
3,Thanks!,Thanks!,Thanks!,Thanks!,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\1aa7b1c1.txt,1.0
4,"*  ## Introduction Neural trees (NTs) refer to a school of methods that combine neural networks (NNs) and decision trees (DTs), for which we present a comprehensive review in this survey.","*  ## Introduction Neural trees (NTs) refer to a school of methods that combine neural networks (NNs) and decision trees (DTs), for which we present a comprehensive review in this survey.","*  ## Introduction Neural trees (NTs) refer to a school of methods that combine neural networks (NNs) and decision trees (DTs), for which we present a comprehensive review in this survey.","*  ## Introduction <PROGLANG>Neural trees</PROGLANG> (NTs) refer to a school of methods that combine <PROGLANG>neural networks</PROGLANG> (NNs) and <PROGLANG>decision trees</PROGLANG> (DTs), for which we present a comprehensive review in this survey.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\057344b5.txt,1.0
5,Our keynote is to identify how these approaches enhance the model interpretability and suggest possible solutions to the remaining challenges.,Our keynote is to identify how these approaches enhance the model interpretability and suggest possible solutions to the remaining challenges.,Our keynote is to identify how these approaches enhance the model interpretability and suggest possible solutions to the remaining challenges.,Our keynote is to identify how these approaches enhance the model interpretability and suggest possible solutions to the remaining challenges.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\53413657.txt,1.0
6,"Besides, we provide a discussion about other considerations like conditional computation and promising directions towards this field, in hope to advance the practice of NTs.  ## *News!","Besides, we provide a discussion about other considerations like conditional computation and promising directions towards this field, in hope to advance the practice of NTs.  ## *News!","Besides, we provide a discussion about other considerations like conditional computation and promising directions towards this field, in hope to advance the practice of NTs.  ## *News!","Besides, we provide a discussion about other considerations like conditional computation and promising directions towards this field, in hope to advance the practice of NTs.  ## *News!",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\7d1fd332.txt,1.0
7,"* **New work of NTs and possible changes in this survey will be notified here.**  - Add ""ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction"" [[Paper]](https://arxiv.org/pdf/2210.03745.pdf), an interesting work that combines prototype learning, soft decision trees and Graph Neural Networks for compound property prediction","* **New work of NTs and possible changes in this survey will be notified here.**  - Add ""<PUBLICATION>ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction</PUBLICATION>"" [[Paper]](https://arxiv.org/pdf/2210.03745.pdf), an interesting work that combines prototype learning, soft decision trees and Graph Neural Networks for compound property prediction","* **New work of NTs and possible changes in this survey will be notified here.**  - Add `ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction` [[Paper]](https://arxiv.org/pdf/2210.03745.pdf), an interesting work that combines prototype learning, soft decision trees and Graph Neural Networks for compound property prediction","* **New work of NTs and possible changes in this survey will be notified here.**  - Add `<PUBLICATION>ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction</PUBLICATION>` [[Paper]](https://arxiv.org/pdf/2210.03745.pdf), an interesting work that combines prototype learning, soft decision trees and Graph Neural Networks for compound property prediction",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\4d976c20.txt,0.9943820224719101
8,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\cdb4ee2a.txt,1.0
9,"- Add ""Large-Scale Category Structure Aware Image Categorization"" [[Paper]](https://proceedings.neurips.cc/paper/2011/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf) & ""Leveraging Class Hierarchies with Metric-Guided Prototype Learning"" [[Paper]](https://arxiv.org/pdf/2007.03047.pdf), two methods incorporating the class hierarchy into the regulariser of the general objective function in section 3.","- Add ""<PUBLICATION>Large-Scale Category Structure Aware Image Categorization</PUBLICATION>"" [[Paper]](https://proceedings.neurips.cc/paper/2011/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf) & ""<PUBLICATION>Leveraging Class Hierarchies with Metric-Guided Prototype Learning</PUBLICATION>"" [[Paper]](https://arxiv.org/pdf/2007.03047.pdf), two methods incorporating the class hierarchy into the regulariser of the general objective function in section 3.","- Add `Large-Scale Category Structure Aware Image Categorization` [[Paper]](https://proceedings.neurips.cc/paper/2011/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf) & `Leveraging Class Hierarchies with Metric-Guided Prototype Learning` [[Paper]](https://arxiv.org/pdf/2007.03047.pdf), two methods incorporating the class hierarchy into the regulariser of the general objective function in section 3.","- Add `<PUBLICATION>Large-Scale Category Structure Aware Image Categorization</PUBLICATION>` [[Paper]](https://proceedings.neurips.cc/paper/2011/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf) & `<PUBLICATION>Leveraging Class Hierarchies with Metric-Guided Prototype Learning</PUBLICATION>` [[Paper]](https://arxiv.org/pdf/2007.03047.pdf), two methods incorporating the class hierarchy into the regulariser of the general objective function in section 3.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\90483941.txt,0.9900249376558603
10,"Although a regulariser is certainly feasible in utilize the class hierarchy, it is curiously rare in practice and these two methods are the only two examples we know of. - Add ""Distillation Decision Tree"" [[Paper]](https://arxiv.org/pdf/2206.04661.pdf), which interprets the black-box model by distilling its knowledge into decision trees, belonging to section 2.2.  ### Citation  If you find this survey useful for your research, please consider citing ``` @article{li2022survey,   title={A Survey of Neural Trees},   author={Li, Haoling and Song, Jie and Xue, Mengqi and Zhang, Haofei and Ye, Jingwen and Cheng, Lechao and Song, Mingli},   journal={arXiv preprint arXiv:2209.03415},   year={2022} } ```  Thanks!","Although a regulariser is certainly feasible in utilize the class hierarchy, it is curiously rare in practice and these two methods are the only two examples we know of. - Add ""<PUBLICATION>Distillation Decision Tree</PUBLICATION>"" [[Paper]](https://arxiv.org/pdf/2206.04661.pdf), which interprets the black-box model by distilling its knowledge into decision trees, belonging to section 2.2.  ### Citation  If you find this survey useful for your research, please consider citing ``` @article{li2022survey,   title={<PUBLICATION>A Survey of Neural Trees</PUBLICATION>},   author={Li, Haoling and Song, Jie and Xue, Mengqi and Zhang, Haofei and Ye, Jingwen and Cheng, Lechao and Song, Mingli},   journal={arXiv preprint arXiv:2209.03415},   year={2022} } ```  Thanks!","Although a regulariser is certainly feasible in utilize the class hierarchy, it is curiously rare in practice and these two methods are the only two examples we know of. - Add ""Distillation Decision Tree"" [[Paper]](https://arxiv.org/pdf/2206.04661.pdf), which interprets the black-box model by distilling its knowledge into decision trees, belonging to section 2.2.  ### Citation  If you find this survey useful for your research, please consider citing ``` @article{li2022survey,   title={A Survey of Neural Trees},   author={Li, Haoling and Song, Jie and Xue, Mengqi and Zhang, Haofei and Ye, Jingwen and Cheng, Lechao and Song, Mingli},   journal={arXiv preprint arXiv:2209.03415},   year={2022} } ```  Thanks!","Although a regulariser is certainly feasible in utilize the class hierarchy, it is curiously rare in practice and these two methods are the only two examples we know of. - Add ""<PUBLICATION>Distillation Decision Tree</PUBLICATION>"" [[Paper]](https://arxiv.org/pdf/2206.04661.pdf), which interprets the black-box model by distilling its knowledge into decision trees, belonging to section 2.2.  ### Citation  If you find this survey useful for your research, please consider citing ``` @article{li2022survey,   title={<PUBLICATION>A Survey of Neural Trees</PUBLICATION>},   author={Li, Haoling and Song, Jie and Xue, Mengqi and Zhang, Haofei and Ye, Jingwen and Cheng, Lechao and Song, Mingli},   journal={<PUBLICATION>arXiv preprint arXiv:2209.03415</PUBLICATION>},   year={2022} } ```  Thanks!",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\1a5a23d8.txt,1.0
11,"## A Taxonomy of Current Methods In this survey, we present a thorough taxonomy for NTs that expresses the gradual integration and co-evolution of NNs and DTs.","## A Taxonomy of Current Methods In this survey, we present a thorough taxonomy for NTs that expresses the gradual integration and co-evolution of NNs and DTs.","## A Taxonomy of Current Methods In this survey, we present a thorough taxonomy for NTs that expresses the gradual integration and co-evolution of NNs and DTs.","## A Taxonomy of Current Methods In this survey, we present a thorough taxonomy for NTs that expresses the gradual integration and co-evolution of NNs and DTs.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\9993dc9a.txt,1.0
12,<p align='center'>     </br>     <img src='outline_final.png' width='1000'> </p>  ### 1.,<p align='center'>     </br>     <img src='outline_final.png' width='1000'> </p>  ### 1.,<p align='center'>     </br>     <img src='outline_final.png' width='1000'> </p>  ### 1.,<p align='center'>     </br>     <img src='outline_final.png' width='1000'> </p>  ### 1.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\6ba4926e.txt,1.0
13,Non-hybrid: NNs and DTs Cooperated Approaches.,Non-hybrid: NNs and DTs Cooperated Approaches.,`Non-hybrid: NNs and DTs Cooperated Approaches.`,`Non-hybrid: NNs and DTs Cooperated Approaches.`,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\03df9309.txt,0.9787234042553191
14,These approaches are the very first to combine NNs and DTs.,These approaches are the very first to combine NNs and DTs.,These approaches are the very first to combine NNs and DTs.,These approaches are the very first to combine <PROGLANG>NNs</PROGLANG> and <PROGLANG>DTs</PROGLANG>.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\1fe55437.txt,1.0
15,They employ DTs as auxiliaries for NNs as well as using NNs as tools to improve the design of DTs.,They employ DTs as auxiliaries for NNs as well as using NNs as tools to improve the design of DTs.,They employ `DTs` as auxiliaries for `NNs` as well as using `NNs` as tools to improve the design of `DTs`.,They employ `<SOFTWARE>DTs</SOFTWARE>` as auxiliaries for `<SOFTWARE>NNs</SOFTWARE>` as well as using `<SOFTWARE>NNs</SOFTWARE>` as tools to improve the design of `<SOFTWARE>DTs</SOFTWARE>`.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\154a928e.txt,0.9607843137254902
16,"In these approaches, ""neural"" and ""tree"" are separated, *i.e.*, one of NN and DT is assigned to accomplish a specific task, while the other performs as its assistant or interpreter.","In these approaches, ""neural"" and ""tree"" are separated, *i.e.*, one of NN and DT is assigned to accomplish a specific task, while the other performs as its assistant or interpreter.","In these approaches, ""neural"" and ""tree"" are separated, *i.e.*, one of NN and DT is assigned to accomplish a specific task, while the other performs as its assistant or interpreter.","In these approaches, ""neural"" and ""tree"" are separated, *i.e.*, one of NN and DT is assigned to accomplish a specific task, while the other performs as its assistant or interpreter.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\b62d4a1f.txt,1.0
17,It can  date back to the early 1990s when DTs were supposed to provide structural priors for NNs or extract rules from a  trained NN.,It can  date back to the early 1990s when DTs were supposed to provide structural priors for NNs or extract rules from a  trained NN.,It can  date back to the early 1990s when DTs were supposed to provide structural priors for NNs or extract rules from a  trained NN.,It can  date back to the early 1990s when DTs were supposed to provide structural priors for NNs or extract rules from a  trained NN.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\69b7a952.txt,1.0
18,"They are perceived implicit combinations of NNs and DTs, because NNs and DTs still operate on their  own paradigms and no hybrid model is produced.  #### 1.1 DTs as Structural Priors of NNs.","They are perceived implicit combinations of NNs and DTs, because NNs and DTs still operate on their  own paradigms and no hybrid model is produced.  #### 1.1 DTs as Structural Priors of NNs.","They are perceived implicit combinations of NNs and DTs, because NNs and DTs still operate on their own paradigms and no hybrid model is produced. #### 1.1 DTs as Structural Priors of NNs.","They are perceived implicit combinations of NNs and DTs, because NNs and DTs still operate on their own paradigms and no hybrid model is produced. #### 1.1 DTs as Structural Priors of NNs.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\27b248da.txt,0.9947089947089947
19,"Adopt DTs to approximate the target concept of NNs in terms of logical descriptions, *i.e.*, use DTs to design NNs  with structural priors","Adopt DTs to approximate the target concept of NNs in terms of logical descriptions, *i.e.*, use DTs to design NNs  with structural priors","Adopt DTs to approximate the target concept of NNs in terms of logical descriptions, *i.e.*, use DTs to design NNs with structural priors","Adopt <PROGLANG>DTs</PROGLANG> to approximate the target concept of <PROGLANG>NNs</PROGLANG> in terms of logical descriptions, *i.e.*, use <PROGLANG>DTs</PROGLANG> to design <PROGLANG>NNs</PROGLANG> with structural priors",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\65edc2bf.txt,0.9963636363636363
20,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\cdb4ee2a.txt,1.0
21,"- ***Entropy net: the first practice***   - **""Entropy nets: from decision trees to neural networks""**, Proceedings of the IEEE, 1990     - I.K.","- ***Entropy net: the first practice***   - **""<PUBLICATION>Entropy nets: from decision trees to neural networks</PUBLICATION>""**, <PUBLICATION>Proceedings of the IEEE</PUBLICATION>, 1990     - I.K.","- ***Entropy net: the first practice***   - **""Entropy nets: from decision trees to neural networks""**, Proceedings of the IEEE, 1990     - I.K.","- ***<PUBLICATION>Entropy net: the first practice</PUBLICATION>***   - **""<PUBLICATION>Entropy nets: from decision trees to neural networks</PUBLICATION>""**, <PUBLICATION>Proceedings of the IEEE</PUBLICATION>, 1990     - I.K.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\b0dceff7.txt,1.0
22,"Sethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/58346/)   - **""Layered neural net design through decision trees""**, ISCAS, 1990     - I.K.","Sethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/58346/)   - **""<PUBLICATION>Layered neural net design through decision trees</PUBLICATION>""**, <CONFERENCE>ISCAS</CONFERENCE>, 1990     - I.K.","Sethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/58346/)   - **""Layered neural net design through decision trees""**, ISCAS, 1990     - I.K.","Sethi     - [[<PUBLICATION>Paper</PUBLICATION>]](https://ieeexplore.ieee.org/abstract/document/58346/)   - **""<PUBLICATION>Layered neural net design through decision trees</PUBLICATION>""**, <CONFERENCE>ISCAS</CONFERENCE>, 1990     - I.K.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\4164713b.txt,1.0
23,"Sethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/112298)   - **""Comparison between entropy net and decision tree classifiers""**, IJCNN, 1990     - I.K.","Sethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/112298)   - **""<PUBLICATION>Comparison between entropy net and decision tree classifiers</PUBLICATION>""**, <CONFERENCE>IJCNN</CONFERENCE>, 1990     - I.K.","Sethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/112298)   - **""Comparison between entropy net and decision tree classifiers""**, IJCNN, 1990     - I.K.","Sethi     - [[<PUBLICATION>Paper</PUBLICATION>]](https://ieeexplore.ieee.org/abstract/document/112298)   - **""<PUBLICATION>Comparison between entropy net and decision tree classifiers</PUBLICATION>""**, <CONFERENCE>IJCNN</CONFERENCE>, 1990     - I.K.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\61905690.txt,1.0
24,"Sethi, Mike Otten     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/5726783)  An example for the original [entropy net](https://ieeexplore.ieee.org/abstract/document/58346/):  <p align='center'>     </br>     <img src='shortened_structural_prior.png' width='1000'> </p>  - ***ID3-based algorithm that converts DTs into hidden layers:***   - **""A machine learning method for generation of a neural network architecture: a continuous ID3 algorithm""**, IEEE Transactions on Neural Networks, 1992     - K.J.","Sethi, Mike Otten     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/5726783)  An example for the original [entropy net](https://ieeexplore.ieee.org/abstract/document/58346/):  <p align='center'>     </br>     <img src='shortened_structural_prior.png' width='1000'> </p>  - ***ID3-based algorithm that converts DTs into hidden layers:***   - **""<PUBLICATION>A machine learning method for generation of a neural network architecture: a continuous ID3 algorithm</PUBLICATION>""**, IEEE Transactions on Neural Networks, 1992     - K.J.","Sethi, Mike Otten     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/5726783)  An example for the original [entropy net](https://ieeexplore.ieee.org/abstract/document/58346/):  <p align='center'>     </br>     <img src='shortened_structural_prior.png' width='1000'> </p>  - ***ID3-based algorithm that converts DTs into hidden layers:***   - **""A machine learning method for generation of a neural network architecture: a continuous ID3 algorithm""**, IEEE Transactions on Neural Networks, 1992     - K.J.","Sethi, Mike Otten     - [[<PUBLICATION>Paper</PUBLICATION>]](https://ieeexplore.ieee.org/abstract/document/5726783)  An example for the original [entropy net](https://ieeexplore.ieee.org/abstract/document/58346/):  <p align='center'>     </br>     <img src='shortened_structural_prior.png' width='1000'> </p>  - ***ID3-based algorithm that converts DTs into hidden layers:***   - **""A machine learning method for generation of a neural network architecture: a continuous ID3 algorithm""**, <PUBLICATION>IEEE Transactions on Neural Networks</PUBLICATION>, 1992     - K.J.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\7c75a04b.txt,1.0
25,"Cios, N.","Cios, N.","`Cios, N.`","`Cios, N.`",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\145161a2.txt,0.8888888888888888
26,"Liu     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/125869)  - ***Soft thresholding of entropy net:***   - **""Neural implementation of tree classifiers""**, IEEE Transactions on Systems, Man, and Cybernetics, 1995     - I.K.","Liu     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/125869)  - ***Soft thresholding of entropy net:***   - **""<PUBLICATION>Neural implementation of tree classifiers</PUBLICATION>""**, IEEE Transactions on Systems, Man, and Cybernetics, 1995     - I.K.","Liu     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/125869)  - ***Soft thresholding of entropy net:***   - **""Neural implementation of tree classifiers""**, IEEE Transactions on Systems, Man, and Cybernetics, 1995     - I.K.","Liu     - [[<PUBLICATION>Paper</PUBLICATION>]](https://ieeexplore.ieee.org/abstract/document/125869)  - ***Soft thresholding of entropy net:***   - **""<PUBLICATION>Neural implementation of tree classifiers</PUBLICATION>""**, <PUBLICATION>IEEE Transactions on Systems, Man, and Cybernetics</PUBLICATION>, 1995     - I.K.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\80acad9d.txt,1.0
27,"Sethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/398685)  - ***Oblique trees version of entropy net:***   - **""A mapping from linear tree classifiers to neural net classifiers""**, ICNN, 1994     - Youngtae Park     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/374145)   - **""Fast Training Algorithms for Multi-layer Neural Nets""**, IEEE Transactions on Neural Networks, 1991     - Richard P.","Sethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/398685)  - ***Oblique trees version of entropy net:***   - **""<PUBLICATION>A mapping from linear tree classifiers to neural net classifiers</PUBLICATION>""**, <CONFERENCE>ICNN</CONFERENCE>, 1994     - Youngtae Park     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/374145)   - **""<PUBLICATION>Fast Training Algorithms for Multi-layer Neural Nets</PUBLICATION>""**, IEEE Transactions on Neural Networks, 1991     - Richard P.","Sethi     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/398685)  - ***Oblique trees version of entropy net:***   - **""A mapping from linear tree classifiers to neural net classifiers""**, ICNN, 1994     - Youngtae Park     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/374145)   - **""Fast Training Algorithms for Multi-layer Neural Nets""**, IEEE Transactions on Neural Networks, 1991     - Richard P.","Sethi     - [[<PUBLICATION>Paper</PUBLICATION>]](https://ieeexplore.ieee.org/abstract/document/398685)  - ***Oblique trees version of entropy net:***   - **""A mapping from linear tree classifiers to neural net classifiers""**, <CONFERENCE>ICNN, 1994</CONFERENCE>     - Youngtae Park     - [[<PUBLICATION>Paper</PUBLICATION>]](https://ieeexplore.ieee.org/abstract/document/374145)   - **""Fast Training Algorithms for Multi-layer Neural Nets""**, <PUBLICATION>IEEE Transactions on Neural Networks, 1991</PUBLICATION>     - Richard P.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\ede138a7.txt,1.0
28,Brent     - [[Paper]](https://citeseerx.ist.psu.edu/viewdoc/download?,Brent     - [[Paper]](https://citeseerx.ist.psu.edu/viewdoc/download?,Brent     - [[Paper]](https://citeseerx.ist.psu.edu/viewdoc/download?,Brent     - [[<PUBLICATION>Paper</PUBLICATION>]](https://citeseerx.ist.psu.edu/viewdoc/download?,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\80db028c.txt,1.0
29,"doi=10.1.1.122.4561&rep=rep1&type=pdf)  - ***A knowledge acquisition system that concerns about the knowledge form DTs and NNs prefer:***   - **""Implementation and refinement of decision trees using neural networks for hybrid knowledge acquisition""**,  Artificial Intelligence in Engineering, 1995     - Katsuhiko Tsujino, Shogo Nishida     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/0954181095000054)  - ***Create a disjunctive normal form formula for each class and reformulate the first layer of entropy net:***   - **""Initializing neural networks using decision trees""**, published by Rutgers University, 1990     - Arunava Banerjee     - [[Paper]](https://scholarship.libraries.rutgers.edu/esploro/outputs/technicalDocumentation/Initializing-neural-networks-using-decision-trees/991031549998404646)   - **""Initialization of neural networks by means of decision trees""**, Knowledge-Based Systems, 1995     - Irena Ivanova, Miroslav Kubat     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/0950705196819174)   - **""On mapping decision trees and neural networks""**, Knowledge-Based Systems, 1999     - R.SetionoW, K.Leow     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S095070519900009X)  #### 1.2 DTs for Approximating NNs.","doi=10.1.1.122.4561&rep=rep1&type=pdf)  - ***A knowledge acquisition system that concerns about the knowledge form DTs and NNs prefer:***   - **""<PUBLICATION>Implementation and refinement of decision trees using neural networks for hybrid knowledge acquisition</PUBLICATION>""**,  Artificial Intelligence in Engineering, 1995     - Katsuhiko Tsujino, Shogo Nishida     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/0954181095000054)  - ***Create a disjunctive normal form formula for each class and reformulate the first layer of entropy net:***   - **""<PUBLICATION>Initializing neural networks using decision trees</PUBLICATION>""**, published by Rutgers University, 1990     - Arunava Banerjee     - [[Paper]](https://scholarship.libraries.rutgers.edu/esploro/outputs/technicalDocumentation/Initializing-neural-networks-using-decision-trees/991031549998404646)   - **""<PUBLICATION>Initialization of neural networks by means of decision trees</PUBLICATION>""**, Knowledge-Based Systems, 1995     - Irena Ivanova, Miroslav Kubat     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/0950705196819174)   - **""<PUBLICATION>On mapping decision trees and neural networks</PUBLICATION>""**, Knowledge-Based Systems, 1999     - R.SetionoW, K.Leow     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S095070519900009X)  #### 1.2 DTs for Approximating NNs.","doi=10.1.1.122.4561&rep=rep1&type=pdf)  - ***A knowledge acquisition system that concerns about the knowledge form DTs and NNs prefer:***   - **""Implementation and refinement of decision trees using neural networks for hybrid knowledge acquisition""**, Artificial Intelligence in Engineering, 1995     - Katsuhiko Tsujino, Shogo Nishida     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/0954181095000054)  - ***Create a disjunctive normal form formula for each class and reformulate the first layer of entropy net:***   - **""Initializing neural networks using decision trees""**, published by Rutgers University, 1990     - Arunava Banerjee     - [[Paper]](https://scholarship.libraries.rutgers.edu/esploro/outputs/technicalDocumentation/Initializing-neural-networks-using-decision-trees/991031549998404646)   - **""Initialization of neural networks by means of decision trees""**, Knowledge-Based Systems, 1995     - Irena Ivanova, Miroslav Kubat     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/0950705196819174)   - **""On mapping decision trees and neural networks""**, Knowledge-Based Systems, 1999     - R.SetionoW, K.Leow     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S095070519900009X)  #### 1.2 DTs for Approximating NNs.","doi=10.1.1.122.4561&rep=rep1&type=pdf)  - ***A knowledge acquisition system that concerns about the knowledge form DTs and NNs prefer:***   - **""<PUBLICATION>Implementation and refinement of decision trees using neural networks for hybrid knowledge acquisition</PUBLICATION>""**, <PUBLICATION>Artificial Intelligence in Engineering</PUBLICATION>, 1995     - Katsuhiko Tsujino, Shogo Nishida     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/0954181095000054)  - ***Create a disjunctive normal form formula for each class and reformulate the first layer of entropy net:***   - **""<PUBLICATION>Initializing neural networks using decision trees</PUBLICATION>""**, published by <PUBLICATION>Rutgers University</PUBLICATION>, 1990     - Arunava Banerjee     - [[Paper]](https://scholarship.libraries.rutgers.edu/esploro/outputs/technicalDocumentation/Initializing-neural-networks-using-decision-trees/991031549998404646)   - **""<PUBLICATION>Initialization of neural networks by means of decision trees</PUBLICATION>""**, <PUBLICATION>Knowledge-Based Systems</PUBLICATION>, 1995     - Irena Ivanova, Miroslav Kubat     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/0950705196819174)   - **""<PUBLICATION>On mapping decision trees and neural networks</PUBLICATION>""**, <PUBLICATION>Knowledge-Based Systems</PUBLICATION>, 1999     - R.SetionoW, K.Leow     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S095070519900009X)  #### 1.2 DTs for Approximating NNs.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\b931bba7.txt,0.9996122528111672
30,"Apply DTs to interpret trained NNs by approximating the network (mostly in terms of input-output mapping) and mimic the decision boundaries implicitly learned by the hidden layers.  ##### *1.2.1 Pedagogical Techniques.* Treat NNs as black-box approaches, and extract the input-output rule directly form NNs regardless of its intermediate layers.","Apply DTs to interpret trained NNs by approximating the network (mostly in terms of input-output mapping) and mimic the decision boundaries implicitly learned by the hidden layers.  ##### *1.2.1 Pedagogical Techniques.* Treat NNs as black-box approaches, and extract the input-output rule directly form NNs regardless of its intermediate layers.","Apply DTs to interpret trained NNs by approximating the network (mostly in terms of input-output mapping) and mimic the decision boundaries implicitly learned by the hidden layers.  ##### *1.2.1 Pedagogical Techniques.* Treat NNs as black-box approaches, and extract the input-output rule directly form NNs regardless of its intermediate layers.","Apply <PROGLANG>DTs</PROGLANG> to interpret trained <PROGLANG>NNs</PROGLANG> by approximating the network (mostly in terms of input-output mapping) and mimic the decision boundaries implicitly learned by the hidden layers.  ##### *1.2.1 Pedagogical Techniques.* Treat <PROGLANG>NNs</PROGLANG> as black-box approaches, and extract the input-output rule directly form <PROGLANG>NNs</PROGLANG> regardless of its intermediate layers.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\42563d0f.txt,1.0
31,"A description of pedagogical techniques:  <p align='center'>     </br>     <img src='shortened_rule_extract.png' width='1000'> </p>  - ***TREPAN: the first practice of DT-based interpretation***   - **""Extracting Tree-Structured Representations of Trained Networks""**, NeurIPS, 1995     - Mark Craven, Jude Shavlik     - [[Paper]](https://proceedings.neurips.cc/paper/1995/hash/45f31d16b1058d586fc3be7207b58053-Abstract.html)   - **""Using Sampling and Queries to Extract Rules from Trained Neural Networks""**, Machine Learning Proceedings, 1994     - Mark Craven, Jude Shavlik     - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500131)    - ***Remove insignificant input neurons from the trained network before inducing the DT***   - **""Decision Tree Extraction using Trained Neural Network""**, SMARTGREENS, 2020     - Nikola Vasilev *et al.*     - [[Paper]](https://pdfs.semanticscholar.org/9528/ece5f2a9c71fac8ac258d6812ff5cc75da63.pdf)  - ***Turn the ùëö-of-ùëõ   type splits into standard splitting tests such as C4.5***   - **""Decision Tree Extraction from Trained Neural Networks""**, AAAI, 2004     - Darren Dancey *et al.* *(CART tree)*     - [[Paper]](https://www.aaai.org/Papers/FLAIRS/2004/Flairs04-089.pdf)   - **""Converting a trained neural network to a decision tree DecText - decision tree extractor""**, Dissertation, 2000     - Olcay Boz *(propose discretization algorithms to handle continuous features)*     - [[Paper]](https://www.proquest.com/openview/11cd66e5b5aab652403e370ca7bf1eef/1?","A description of pedagogical techniques:  <p align='center'>     </br>     <img src='shortened_rule_extract.png' width='1000'> </p>  - ***TREPAN: the first practice of DT-based interpretation***   - **""<PUBLICATION>Extracting Tree-Structured Representations of Trained Networks</PUBLICATION>""**, <CONFERENCE>NeurIPS</CONFERENCE>, 1995     - Mark Craven, Jude Shavlik     - [[Paper]](https://proceedings.neurips.cc/paper/1995/hash/45f31d16b1058d586fc3be7207b58053-Abstract.html)   - **""<PUBLICATION>Using Sampling and Queries to Extract Rules from Trained Neural Networks</PUBLICATION>""**, Machine Learning Proceedings, 1994     - Mark Craven, Jude Shavlik     - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500131)    - ***Remove insignificant input neurons from the trained network before inducing the DT***   - **""<PUBLICATION>Decision Tree Extraction using Trained Neural Network</PUBLICATION>""**, <CONFERENCE>SMARTGREENS</CONFERENCE>, 2020     - Nikola Vasilev *et al.*     - [[Paper]](https://pdfs.semanticscholar.org/9528/ece5f2a9c71fac8ac258d6812ff5cc75da63.pdf)  - ***Turn the ùëö-of-ùëõ   type splits into standard splitting tests such as C4.5***   - **""<PUBLICATION>Decision Tree Extraction from Trained Neural Networks</PUBLICATION>""**, <CONFERENCE>AAAI</CONFERENCE>, 2004     - Darren Dancey *et al.* *(CART tree)*     - [[Paper]](https://www.aaai.org/Papers/FLAIRS/2004/Flairs04-089.pdf)   - **""<PUBLICATION>Converting a trained neural network to a decision tree DecText - decision tree extractor</PUBLICATION>""**, Dissertation, 2000     - Olcay Boz *(propose discretization algorithms to handle continuous features)*     - [[Paper]](https://www.proquest.com/openview/11cd66e5b5aab652403e370ca7bf1eef/1?","A description of pedagogical techniques:  <p align='center'>     </br>     <img src='shortened_rule_extract.png' width='1000'> </p>  - ***TREPAN: the first practice of DT-based interpretation***   - **""Extracting Tree-Structured Representations of Trained Networks""**, NeurIPS, 1995     - Mark Craven, Jude Shavlik     - [[Paper]](https://proceedings.neurips.cc/paper/1995/hash/45f31d16b1058d586fc3be7207b58053-Abstract.html)   - **""Using Sampling and Queries to Extract Rules from Trained Neural Networks""**, Machine Learning Proceedings, 1994     - Mark Craven, Jude Shavlik     - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500131)    - ***Remove insignificant input neurons from the trained network before inducing the DT***   - **""Decision Tree Extraction using Trained Neural Network""**, SMARTGREENS, 2020     - Nikola Vasilev *et al.*     - [[Paper]](https://pdfs.semanticscholar.org/9528/ece5f2a9c71fac8ac258d6812ff5cc75da63.pdf)  - ***Turn the ùëö-of-ùëõ   type splits into standard splitting tests such as C4.5***   - **""Decision Tree Extraction from Trained Neural Networks""**, AAAI, 2004     - Darren Dancey *et al.* *(CART tree)*     - [[Paper]](https://www.aaai.org/Papers/FLAIRS/2004/Flairs04-089.pdf)   - **""Converting a trained neural network to a decision tree DecText - decision tree extractor""**, Dissertation, 2000     - Olcay Boz *(propose discretization algorithms to handle continuous features)*     - [[Paper]](https://www.proquest.com/openview/11cd66e5b5aab652403e370ca7bf1eef/1?","A description of pedagogical techniques:  <p align='center'>     </br>     <img src='shortened_rule_extract.png' width='1000'> </p>  - ***<PROJECT>TREPAN</PROJECT>: the first practice of DT-based interpretation***   - **""<PUBLICATION>Extracting Tree-Structured Representations of Trained Networks</PUBLICATION>""**, <CONFERENCE>NeurIPS</CONFERENCE>, 1995     - Mark Craven, Jude Shavlik     - [[Paper]](https://proceedings.neurips.cc/paper/1995/hash/45f31d16b1058d586fc3be7207b58053-Abstract.html)   - **""<PUBLICATION>Using Sampling and Queries to Extract Rules from Trained Neural Networks</PUBLICATION>""**, <PUBLICATION>Machine Learning Proceedings</PUBLICATION>, 1994     - Mark Craven, Jude Shavlik     - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500131)    - ***Remove insignificant input neurons from the trained network before inducing the DT***   - **""<PUBLICATION>Decision Tree Extraction using Trained Neural Network</PUBLICATION>""**, <CONFERENCE>SMARTGREENS</CONFERENCE>, 2020     - Nikola Vasilev *et al.*     - [[Paper]](https://pdfs.semanticscholar.org/9528/ece5f2a9c71fac8ac258d6812ff5cc75da63.pdf)  - ***Turn the ùëö-of-ùëõ   type splits into standard splitting tests such as <SOFTWARE>C4.5</SOFTWARE>***   - **""<PUBLICATION>Decision Tree Extraction from Trained Neural Networks</PUBLICATION>""**, <CONFERENCE>AAAI</CONFERENCE>, 2004     - Darren Dancey *et al.* *(<SOFTWARE>CART tree</SOFTWARE>)*     - [[Paper]](https://www.aaai.org/Papers/FLAIRS/2004/Flairs04-089.pdf)   - **""<PUBLICATION>Converting a trained neural network to a decision tree DecText - decision tree extractor</PUBLICATION>""**, <PUBLICATION>Dissertation</PUBLICATION>, 2000     - Olcay Boz *(propose discretization algorithms to handle continuous features)*     - [[Paper]](https://www.proquest.com/openview/11cd66e5b5aab652403e370ca7bf1eef/1?",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\66862709.txt,1.0
32,"pq-origsite=gscholar&cbl=18750&diss=y)   - **""Extracting decision trees from trained neural networks""**, Pattern Recognition, 2020     - Olcay Boz      - [[Paper]](https://dl.acm.org/doi/abs/10.1145/775047.775113)   - **""NeC4.5: neural ensemble based C4.5""**, IEEE Transactions on Knowledge and Data Engineering, 2004     - Zhi-Hua Zhou, Yuan Jiang *(use NN ensembles)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1294896)   - **""Extracting decision trees from trained neural networks""**, Pattern Recognition, 1999     - R.Krishnan, *et al.* *(adapt genetic algorithms to generate artificial examples)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0031320398001812)  - ***generalize the TREPAN algorithm***   - **""An investigation of TREPAN utilising a continuous oracle model""**, Data Analysis Techniques and Strategies, 2011      - William A Young *et al.* *(develop DTs derived from continuous-based models)*     - [[Paper]](https://www.researchgate.net/profile/William-Young-10/publication/227441144_An_investigation_of_TREPAN_utilising_a_continuous_oracle_model/links/00b495258560f7e8d3000000/An-investigation-of-TREPAN-utilising-a-continuous-oracle-model.pdf?","pq-origsite=gscholar&cbl=18750&diss=y)   - **""<PUBLICATION>Extracting decision trees from trained neural networks</PUBLICATION>""**, Pattern Recognition, 2020     - Olcay Boz      - [[Paper]](https://dl.acm.org/doi/abs/10.1145/775047.775113)   - **""<PUBLICATION>NeC4.5: neural ensemble based C4.5</PUBLICATION>""**, IEEE Transactions on Knowledge and Data Engineering, 2004     - Zhi-Hua Zhou, Yuan Jiang *(use NN ensembles)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1294896)   - **""<PUBLICATION>Extracting decision trees from trained neural networks</PUBLICATION>""**, Pattern Recognition, 1999     - R.Krishnan, *et al.* *(adapt genetic algorithms to generate artificial examples)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0031320398001812)  - ***generalize the TREPAN algorithm***   - **""<PUBLICATION>An investigation of TREPAN utilising a continuous oracle model</PUBLICATION>""**, Data Analysis Techniques and Strategies, 2011      - William A Young *et al.* *(develop DTs derived from continuous-based models)*     - [[Paper]](https://www.researchgate.net/profile/William-Young-10/publication/227441144_An_investigation_of_TREPAN_utilising_a_continuous_oracle_model/links/00b495258560f7e8d3000000/An-investigation-of-TREPAN-utilising-a-continuous-oracle-model.pdf?","- **""Extracting decision trees from trained neural networks""**, Pattern Recognition, 2020
  - Olcay Boz
  - [[Paper]](https://dl.acm.org/doi/abs/10.1145/775047.775113)
- **""NeC4.5: neural ensemble based C4.5""**, IEEE Transactions on Knowledge and Data Engineering, 2004
  - Zhi-Hua Zhou, Yuan Jiang *(use NN ensembles)*
  - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1294896)
- **""Extracting decision trees from trained neural networks""**, Pattern Recognition, 1999
  - R.Krishnan, *et al.* *(adapt genetic algorithms to generate artificial examples)*
  - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0031320398001812)
- ***generalize the TREPAN algorithm***
- **""An investigation of TREPAN utilising a continuous oracle model""**, Data Analysis Techniques and Strategies, 2011
  - William A Young *et al.* *(develop DTs derived from continuous-based models)*
  - [[Paper]](https://www.researchgate.net/profile/William-Young-10/publication/227441144_An_investigation_of_TREPAN_utilising_a_continuous_oracle_model/links/00b495258560f7e8d3000000/An-investigation-of-TREPAN-utilising-a-continuous-oracle-model.pdf?","- **""<PUBLICATION>Extracting decision trees from trained neural networks</PUBLICATION>""**, <PUBLICATION>Pattern Recognition</PUBLICATION>, 2020
  - Olcay Boz
  - [[Paper]](https://dl.acm.org/doi/abs/10.1145/775047.775113)
- **""<PUBLICATION>NeC4.5: neural ensemble based C4.5</PUBLICATION>""**, <PUBLICATION>IEEE Transactions on Knowledge and Data Engineering</PUBLICATION>, 2004
  - Zhi-Hua Zhou, Yuan Jiang *(use NN ensembles)*
  - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1294896)
- **""<PUBLICATION>Extracting decision trees from trained neural networks</PUBLICATION>""**, <PUBLICATION>Pattern Recognition</PUBLICATION>, 1999
  - R.Krishnan, *et al.* *(adapt genetic algorithms to generate artificial examples)*
  - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0031320398001812)
- ***generalize the TREPAN algorithm***
- **""<PUBLICATION>An investigation of TREPAN utilising a continuous oracle model</PUBLICATION>""**, <PUBLICATION>Data Analysis Techniques and Strategies</PUBLICATION>, 2011
  - William A Young *et al.* *(develop DTs derived from continuous-based models)*
  - [[Paper]](https://www.researchgate.net/profile/William-Young-10/publication/227441144_An_investigation_of_TREPAN_utilising_a_continuous_oracle_model/links/00b495258560f7e8d3000000/An-investigation-of-TREPAN-utilising-a-continuous-oracle-model.pdf?",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\9902fb34.txt,0.9617346938775511
33,"_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)   - **""Trepan-Plus: An Extension of a Decision Tree Extraction Algorithm Utilizing Artificial Neural Networks""**, Intelligent Engineering Systems Through Artificial Neural Networks, 2007     - M Rangwala *et al.* *(extend TREPAN to multi-class regression problems)*     - [[Paper]](https://d1wqtxts1xzle7.cloudfront.net/47121083/TREPAN-PLUS_AN_EXTENSION_OF_A_DECISION_T20160708-16395-1sj4ete-with-cover-page-v2.pdf?","_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)   - **""<PUBLICATION>Trepan-Plus: An Extension of a Decision Tree Extraction Algorithm Utilizing Artificial Neural Networks</PUBLICATION>""**, Intelligent Engineering Systems Through Artificial Neural Networks, 2007     - M Rangwala *et al.* *(extend TREPAN to multi-class regression problems)*     - [[Paper]](https://d1wqtxts1xzle7.cloudfront.net/47121083/TREPAN-PLUS_AN_EXTENSION_OF_A_DECISION_T20160708-16395-1sj4ete-with-cover-page-v2.pdf?","_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)   - **""Trepan-Plus: An Extension of a Decision Tree Extraction Algorithm Utilizing Artificial Neural Networks""**, Intelligent Engineering Systems Through Artificial Neural Networks, 2007     - M Rangwala *et al.* *(extend TREPAN to multi-class regression problems)*     - [[Paper]](https://d1wqtxts1xzle7.cloudfront.net/47121083/TREPAN-PLUS_AN_EXTENSION_OF_A_DECISION_T20160708-16395-1sj4ete-with-cover-page-v2.pdf?","_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)   - **""<PUBLICATION>Trepan-Plus: An Extension of a Decision Tree Extraction Algorithm Utilizing Artificial Neural Networks</PUBLICATION>""**, <PUBLICATION>Intelligent Engineering Systems Through Artificial Neural Networks</PUBLICATION>, 2007     - M Rangwala *et al.* *(extend TREPAN to multi-class regression problems)*     - [[Paper]](https://d1wqtxts1xzle7.cloudfront.net/47121083/TREPAN-PLUS_AN_EXTENSION_OF_A_DECISION_T20160708-16395-1sj4ete-with-cover-page-v2.pdf?",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\25ca4e39.txt,1.0
34,"Expires=1662897159&Signature=C2Z0VL3cIkc56vlIci5ERXC7VADyi9tEqIlvJ3CqHzaLB9bfY9Tma8UVsZpu1BXI1l24paZ0iJArqUUzEqrTvum7s93dAUL-pKmmBKausyKiAw3qj2Bx18I5jzDOat2TCKA6wd2GbiexuK7gtlfNgeu03fBg0Rx2UWt~2KW6RNMeamZinhzp1FCrSzH4fFRqhTuc2~UgajTzsaUlJHbJef-s9u2wOiwqNiEUAfwN8zG0tIEt-fE0GEUdSC9AzBVXLpodLTadDzUZODX7lzUX~o00-ZHxKl7VOnbi0wQ8OZw087zKQjqQNOw1ik16zfKMamZW~YS6e38aUgud0sOrBg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)   - **""Extracting fuzzy symbolic representation from artificial neural networks""**, NAFIPS, 1999     - Maciej Faifer *et al.* *(use fuzzy representation during the tree-induction process)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/781764)   - **""DT+GNN: A Fully Explainable Graph Neural Network using Decision Trees""**, arXiv, 2022     - Peter M√ºller *et al.* *(incorporate DTs into graph neural networks)*     - [[Paper]](https://arxiv.org/abs/2205.13234)  - ***NNs perform significance analysis to select attributes***   - **""ANN-DT: an algorithm for extraction of decision trees from artificial neural networks""**, IEEE Transactions on Neural Networks, 1999     - Gregor PJ Schmitz *et al.*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/809084)   ##### *1.2.2 Decompositional Techniques.* Decompositional techniques concern units in the hidden layers.","Expires=1662897159&Signature=C2Z0VL3cIkc56vlIci5ERXC7VADyi9tEqIlvJ3CqHzaLB9bfY9Tma8UVsZpu1BXI1l24paZ0iJArqUUzEqrTvum7s93dAUL-pKmmBKausyKiAw3qj2Bx18I5jzDOat2TCKA6wd2GbiexuK7gtlfNgeu03fBg0Rx2UWt~2KW6RNMeamZinhzp1FCrSzH4fFRqhTuc2~UgajTzsaUlJHbJef-s9u2wOiwqNiEUAfwN8zG0tIEt-fE0GEUdSC9AzBVXLpodLTadDzUZODX7lzUX~o00-ZHxKl7VOnbi0wQ8OZw087zKQjqQNOw1ik16zfKMamZW~YS6e38aUgud0sOrBg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)   - **""<PUBLICATION>Extracting fuzzy symbolic representation from artificial neural networks</PUBLICATION>""**, <CONFERENCE>NAFIPS</CONFERENCE>, 1999     - Maciej Faifer *et al.* *(use fuzzy representation during the tree-induction process)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/781764)   - **""<PUBLICATION>DT+GNN: A Fully Explainable Graph Neural Network using Decision Trees</PUBLICATION>""**, arXiv, 2022     - Peter M√ºller *et al.* *(incorporate DTs into graph neural networks)*     - [[Paper]](https://arxiv.org/abs/2205.13234)  - ***NNs perform significance analysis to select attributes***   - **""<PUBLICATION>ANN-DT: an algorithm for extraction of decision trees from artificial neural networks</PUBLICATION>""**, IEEE Transactions on Neural Networks, 1999     - Gregor PJ Schmitz *et al.*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/809084)   ##### *1.2.2 Decompositional Techniques.* Decompositional techniques concern units in the hidden layers.","- **""Extracting fuzzy symbolic representation from artificial neural networks""**, NAFIPS, 1999
  - Maciej Faifer *et al.* *(use fuzzy representation during the tree-induction process)*
  - [[Paper]](https://ieeexplore.ieee.org/abstract/document/781764)
- **""DT+GNN: A Fully Explainable Graph Neural Network using Decision Trees""**, arXiv, 2022
  - Peter M√ºller *et al.* *(incorporate DTs into graph neural networks)*
  - [[Paper]](https://arxiv.org/abs/2205.13234)
- ***NNs perform significance analysis to select attributes***
- **""ANN-DT: an algorithm for extraction of decision trees from artificial neural networks""**, IEEE Transactions on Neural Networks, 1999
  - Gregor PJ Schmitz *et al.*
  - [[Paper]](https://ieeexplore.ieee.org/abstract/document/809084)
##### *1.2.2 Decompositional Techniques.* Decompositional techniques concern units in the hidden layers.","- **""<PUBLICATION>Extracting fuzzy symbolic representation from artificial neural networks</PUBLICATION>""**, <CONFERENCE>NAFIPS</CONFERENCE>, 1999
  - Maciej Faifer *et al.* *(use fuzzy representation during the tree-induction process)*
  - [[Paper]](https://ieeexplore.ieee.org/abstract/document/781764)
- **""<PUBLICATION>DT+GNN: A Fully Explainable Graph Neural Network using Decision Trees</PUBLICATION>""**, <PUBLICATION>arXiv</PUBLICATION>, 2022
  - Peter M√ºller *et al.* *(incorporate DTs into graph neural networks)*
  - [[Paper]](https://arxiv.org/abs/2205.13234)
- ***NNs perform significance analysis to select attributes***
- **""<PUBLICATION>ANN-DT: an algorithm for extraction of decision trees from artificial neural networks</PUBLICATION>""**, <PUBLICATION>IEEE Transactions on Neural Networks</PUBLICATION>, 1999
  - Gregor PJ Schmitz *et al.*
  - [[Paper]](https://ieeexplore.ieee.org/abstract/document/809084)
##### *1.2.2 Decompositional Techniques.* Decompositional techniques concern units in the hidden layers.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\a9505c06.txt,0.7928011075219197
35,"They extract rules from the trained NN at the level of  individual neurons, thus gaining insight into their inner structures","They extract rules from the trained NN at the level of  individual neurons, thus gaining insight into their inner structures","They extract rules from the trained NN at the level of individual neurons, thus gaining insight into their inner structures","They extract rules from the trained NN at the level of individual neurons, thus gaining insight into their inner structures",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\35d2258b.txt,0.9959514170040485
36,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\cdb4ee2a.txt,1.0
37,"- ***Extract axis-aligned trees***   - **""Rule extraction from neural networks via decision tree induction""**, IJCNN, 2001     - Makoto Sato, Hiroshi Tsukimoto *(CRED algorithm that deals with NNs with one hidden layer)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/938448)   - **""DeepRED ‚Äì Rule Extraction from Deep Neural Networks""**, International Conference on Discovery Science, 2016     - Jan Ruben Zilke, Eneldo Loza Menc√≠a, Frederik Janssen *(Extend CRED algorithm by deriving intermediate rules for every additional hidden layer)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-46307-0_29)   - **""Eclectic rule extraction from Neural Networks using aggregated Decision Trees""**, ICECE, 2012     - MD.","- ***Extract axis-aligned trees***   - **""<PUBLICATION>Rule extraction from neural networks via decision tree induction</PUBLICATION>""**, <CONFERENCE>IJCNN</CONFERENCE>, 2001     - Makoto Sato, Hiroshi Tsukimoto *(CRED algorithm that deals with NNs with one hidden layer)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/938448)   - **""<PUBLICATION>DeepRED ‚Äì Rule Extraction from Deep Neural Networks</PUBLICATION>""**, <CONFERENCE>International Conference on Discovery Science</CONFERENCE>, 2016     - Jan Ruben Zilke, Eneldo Loza Menc√≠a, Frederik Janssen *(Extend CRED algorithm by deriving intermediate rules for every additional hidden layer)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-46307-0_29)   - **""<PUBLICATION>Eclectic rule extraction from Neural Networks using aggregated Decision Trees</PUBLICATION>""**, <CONFERENCE>ICECE</CONFERENCE>, 2012     - MD.","- ***Extract axis-aligned trees***  
  - **""Rule extraction from neural networks via decision tree induction""**, IJCNN, 2001  
    - Makoto Sato, Hiroshi Tsukimoto *(CRED algorithm that deals with NNs with one hidden layer)*  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/938448)  
  - **""DeepRED ‚Äì Rule Extraction from Deep Neural Networks""**, International Conference on Discovery Science, 2016  
    - Jan Ruben Zilke, Eneldo Loza Menc√≠a, Frederik Janssen *(Extend CRED algorithm by deriving intermediate rules for every additional hidden layer)*  
    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-46307-0_29)  
  - **""Eclectic rule extraction from Neural Networks using aggregated Decision Trees""**, ICECE, 2012  
    - MD.","- ***Extract axis-aligned trees***  
  - **""<PUBLICATION>Rule extraction from neural networks via decision tree induction</PUBLICATION>""**, <CONFERENCE>IJCNN</CONFERENCE>, 2001  
    - Makoto Sato, Hiroshi Tsukimoto *(CRED algorithm that deals with NNs with one hidden layer)*  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/938448)  
  - **""<PUBLICATION>DeepRED ‚Äì Rule Extraction from Deep Neural Networks</PUBLICATION>""**, <CONFERENCE>International Conference on Discovery Science</CONFERENCE>, 2016  
    - Jan Ruben Zilke, Eneldo Loza Menc√≠a, Frederik Janssen *(Extend CRED algorithm by deriving intermediate rules for every additional hidden layer)*  
    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-46307-0_29)  
  - **""<PUBLICATION>Eclectic rule extraction from Neural Networks using aggregated Decision Trees</PUBLICATION>""**, <CONFERENCE>ICECE</CONFERENCE>, 2012  
    - MD.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\0ca0e72d.txt,0.9894039735099338
38,"Ridwan Al Iqbal *(Simplify rules by logical simplification)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6471502)  - ***Extract oblique and multivariate trees***   - **""NeuroLinear: From neural networks to oblique decision rules""**, Neurocomputing, 1997      - Rudy Setiono, Huan Liu *(oblique trees)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0925231297000386)   - **""Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees""**, 2020     - Duy T Nguyen, Kathryn E Kasmarik, Hussein A Abbass *(multivariate trees)*     - [[Paper]](http://arxiv-export-lb.library.cornell.edu/abs/2003.04675)  ##### *1.2.3 Eclectic Techniques.* Typically, eclectic techniques model the relationship between the last hidden layer and the outputs, then infer the input-output relationship from the magnitudes of the weights in a NN.","Ridwan Al Iqbal *(Simplify rules by logical simplification)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6471502)  - ***Extract oblique and multivariate trees***   - **""<PUBLICATION>NeuroLinear: From neural networks to oblique decision rules</PUBLICATION>""**, Neurocomputing, 1997      - Rudy Setiono, Huan Liu *(oblique trees)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0925231297000386)   - **""<PUBLICATION>Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees</PUBLICATION>""**, 2020     - Duy T Nguyen, Kathryn E Kasmarik, Hussein A Abbass *(multivariate trees)*     - [[Paper]](http://arxiv-export-lb.library.cornell.edu/abs/2003.04675)  ##### *1.2.3 Eclectic Techniques.* Typically, eclectic techniques model the relationship between the last hidden layer and the outputs, then infer the input-output relationship from the magnitudes of the weights in a NN.","Ridwan Al Iqbal *(Simplify rules by logical simplification)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6471502)  - ***Extract oblique and multivariate trees***   - **""NeuroLinear: From neural networks to oblique decision rules""**, Neurocomputing, 1997      - Rudy Setiono, Huan Liu *(oblique trees)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0925231297000386)   - **""Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees""**, 2020     - Duy T Nguyen, Kathryn E Kasmarik, Hussein A Abbass *(multivariate trees)*     - [[Paper]](http://arxiv-export-lb.library.cornell.edu/abs/2003.04675)  ##### *1.2.3 Eclectic Techniques.* Typically, eclectic techniques model the relationship between the last hidden layer and the outputs, then infer the input-output relationship from the magnitudes of the weights in a NN.","Ridwan Al Iqbal *(Simplify rules by logical simplification)*     - [[<PUBLICATION>Paper</PUBLICATION>]](https://ieeexplore.ieee.org/abstract/document/6471502)  - ***Extract oblique and multivariate trees***   - **""<PUBLICATION>NeuroLinear: From neural networks to oblique decision rules</PUBLICATION>""**, <PUBLICATION>Neurocomputing</PUBLICATION>, 1997      - Rudy Setiono, Huan Liu *(oblique trees)*     - [[<PUBLICATION>Paper</PUBLICATION>]](https://www.sciencedirect.com/science/article/abs/pii/S0925231297000386)   - **""<PUBLICATION>Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees</PUBLICATION>""**, 2020     - Duy T Nguyen, Kathryn E Kasmarik, Hussein A Abbass *(multivariate trees)*     - [[<PUBLICATION>Paper</PUBLICATION>]](http://arxiv-export-lb.library.cornell.edu/abs/2003.04675)  ##### *1.2.3 Eclectic Techniques.* Typically, eclectic techniques model the relationship between the last hidden layer and the outputs, then infer the input-output relationship from the magnitudes of the weights in a NN.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\12eeee84.txt,1.0
39,"However, few of them are based on DTs.","However, few of them are based on DTs.","However, few of them are based on DTs.","However, few of them are based on <PROGLANG>DTs</PROGLANG>.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\761ff215.txt,1.0
40,"Therefore, this survey comprises approaches that extract **partial knowledge** contained in the hidden layers and do not demand their approximation target is the input-output relationship  - ***Extended C-Net algorithm***   - **""Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees""**, arXiv, 2020     - Duy T Nguyen, Kathryn E Kasmarik, Hussein A Abbass *(a typical eclectic technique)*     - [[Paper]](http://arxiv-export-lb.library.cornell.edu/abs/2003.04675)  - ***Generalized eclectic techniques***   - **""Global Model Interpretation Via Recursive Partitioning""**, HPCC/SmartCity/DSS, 2018     - Chengliang Yang, Anand Rangarajan, Sanjay Ranka *(CART tree learned from the contribution matrix and applied to scene understanding tasks)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8622994) [[Code]](https://github.com/west-gates/GIRP)   - **""Interpreting CNNs via Decision Trees""**, CVPR, 2019     - Quanshi Zhang *et al.* *(encodes all potential decision modes of the CNN in a coarse-to-fine manner)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Interpreting_CNNs_via_Decision_Trees_CVPR_2019_paper.html)  ##### *1.2.4 Optional: DTs for Regularizing NNs.* These approaches train NNs that resemble compact DTs through crafted regularization terms, so that they can be well-approximated by small DTs","Therefore, this survey comprises approaches that extract **partial knowledge** contained in the hidden layers and do not demand their approximation target is the input-output relationship  - ***Extended C-Net algorithm***   - **""<PUBLICATION>Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees</PUBLICATION>""**, arXiv, 2020     - Duy T Nguyen, Kathryn E Kasmarik, Hussein A Abbass *(a typical eclectic technique)*     - [[Paper]](http://arxiv-export-lb.library.cornell.edu/abs/2003.04675)  - ***Generalized eclectic techniques***   - **""<PUBLICATION>Global Model Interpretation Via Recursive Partitioning</PUBLICATION>""**, <CONFERENCE>HPCC/SmartCity/DSS</CONFERENCE>, 2018     - Chengliang Yang, Anand Rangarajan, Sanjay Ranka *(CART tree learned from the contribution matrix and applied to scene understanding tasks)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8622994) [[Code]](https://github.com/west-gates/GIRP)   - **""<PUBLICATION>Interpreting CNNs via Decision Trees</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2019     - Quanshi Zhang *et al.* *(encodes all potential decision modes of the CNN in a coarse-to-fine manner)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Interpreting_CNNs_via_Decision_Trees_CVPR_2019_paper.html)  ##### *1.2.4 Optional: DTs for Regularizing NNs.* These approaches train NNs that resemble compact DTs through crafted regularization terms, so that they can be well-approximated by small DTs","Therefore, this survey comprises approaches that extract **partial knowledge** contained in the hidden layers and do not demand their approximation target is the input-output relationship  - ***Extended C-Net algorithm***   - **""Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees""**, arXiv, 2020     - Duy T Nguyen, Kathryn E Kasmarik, Hussein A Abbass *(a typical eclectic technique)*     - [[Paper]](http://arxiv-export-lb.library.cornell.edu/abs/2003.04675)  - ***Generalized eclectic techniques***   - **""Global Model Interpretation Via Recursive Partitioning""**, HPCC/SmartCity/DSS, 2018     - Chengliang Yang, Anand Rangarajan, Sanjay Ranka *(CART tree learned from the contribution matrix and applied to scene understanding tasks)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8622994) [[Code]](https://github.com/west-gates/GIRP)   - **""Interpreting CNNs via Decision Trees""**, CVPR, 2019     - Quanshi Zhang *et al.* *(encodes all potential decision modes of the CNN in a coarse-to-fine manner)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Interpreting_CNNs_via_Decision_Trees_CVPR_2019_paper.html)  ##### *1.2.4 Optional: DTs for Regularizing NNs.* These approaches train NNs that resemble compact DTs through crafted regularization terms, so that they can be well-approximated by small DTs","Therefore, this survey comprises approaches that extract **partial knowledge** contained in the hidden layers and do not demand their approximation target is the input-output relationship  - ***<SOFTWARE>Extended C-Net algorithm</SOFTWARE>***   - **""<PUBLICATION>Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees</PUBLICATION>""**, <PUBLICATION>arXiv</PUBLICATION>, 2020     - Duy T Nguyen, Kathryn E Kasmarik, Hussein A Abbass *(a typical eclectic technique)*     - [[Paper]](http://arxiv-export-lb.library.cornell.edu/abs/2003.04675)  - ***Generalized eclectic techniques***   - **""<PUBLICATION>Global Model Interpretation Via Recursive Partitioning</PUBLICATION>""**, <CONFERENCE>HPCC/SmartCity/DSS</CONFERENCE>, 2018     - Chengliang Yang, Anand Rangarajan, Sanjay Ranka *(CART tree learned from the contribution matrix and applied to scene understanding tasks)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8622994) [[Code]](https://github.com/west-gates/<PROJECT>GIRP</PROJECT>)   - **""<PUBLICATION>Interpreting CNNs via Decision Trees</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2019     - Quanshi Zhang *et al.* *(encodes all potential decision modes of the CNN in a coarse-to-fine manner)*     - [[Paper]](https://openaccess.thecvf.com/content_<CONFERENCE>CVPR</CONFERENCE>_2019/html/Zhang_Interpreting_CNNs_via_Decision_Trees_<CONFERENCE>CVPR</CONFERENCE>_2019_paper.html)  ##### *1.2.4 Optional: DTs for Regularizing NNs.* These approaches train NNs that resemble compact DTs through crafted regularization terms, so that they can be well-approximated by small DTs",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\1fec168d.txt,1.0
41,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\cdb4ee2a.txt,1.0
42,"- ***Tree regularization***   - **""Beyond Sparsity: Tree Regularization of Deep Models for Interpretability""**, AAAI, 2018      - Mike Wu *et al.* *(a complexity penalty function that aims to optimize the deep model for interpretability and human-simulatability)*     - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/11501) [[Code]](https://github.com/dtak/tree-regularization-public)   - **""Regional Tree Regularization for Interpretability in Deep Neural Networks""**, AAAI, 2020     - Mike Wu *et al.","- ***Tree regularization***   - **""<PUBLICATION>Beyond Sparsity: Tree Regularization of Deep Models for Interpretability</PUBLICATION>""**, <CONFERENCE>AAAI</CONFERENCE>, 2018      - Mike Wu *et al.* *(a complexity penalty function that aims to optimize the deep model for interpretability and human-simulatability)*     - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/11501) [[Code]](https://github.com/dtak/tree-regularization-public)   - **""<PUBLICATION>Regional Tree Regularization for Interpretability in Deep Neural Networks</PUBLICATION>""**, <CONFERENCE>AAAI</CONFERENCE>, 2020     - Mike Wu *et al.","- ***Tree regularization***  
  - **""Beyond Sparsity: Tree Regularization of Deep Models for Interpretability""**, AAAI, 2018  
  - Mike Wu *et al.* *(a complexity penalty function that aims to optimize the deep model for interpretability and human-simulatability)*  
  - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/11501) [[Code]](https://github.com/dtak/tree-regularization-public)  
  - **""Regional Tree Regularization for Interpretability in Deep Neural Networks""**, AAAI, 2020  
  - Mike Wu *et al.","- ***Tree regularization***  
  - **""<PUBLICATION>Beyond Sparsity: Tree Regularization of Deep Models for Interpretability</PUBLICATION>""**, <CONFERENCE>AAAI</CONFERENCE>, 2018  
  - Mike Wu *et al.* *(a complexity penalty function that aims to optimize the deep model for interpretability and human-simulatability)*  
  - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/11501) [[Code]](https://github.com/dtak/tree-regularization-public)  
  - **""<PUBLICATION>Regional Tree Regularization for Interpretability in Deep Neural Networks</PUBLICATION>""**, <CONFERENCE>AAAI</CONFERENCE>, 2020  
  - Mike Wu *et al.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\f2e8adf1.txt,0.9893307468477207
43,"(extended work of tree regularization that encourage a deep model to be approximated by several separate DTs specific to pre-defined regions of the input space)*     - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/6112)  - ***L1-orthogonal regularization***   - **""Enhancing Decision Tree Based Interpretation of Deep Neural Networks through L1-Orthogonal Regularization""**, ICMLA, 2019     - Nina Schaaf, Marco Huber, Johannes Maucher *(so that they do not need a surrogate network to estimate the tree loss)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8999213)  #### 1.3 NNs for Designing DTs.","(extended work of tree regularization that encourage a deep model to be approximated by several separate DTs specific to pre-defined regions of the input space)*     - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/6112)  - ***L1-orthogonal regularization***   - **""<PUBLICATION>Enhancing Decision Tree Based Interpretation of Deep Neural Networks through L1-Orthogonal Regularization</PUBLICATION>""**, <CONFERENCE>ICMLA</CONFERENCE>, 2019     - Nina Schaaf, Marco Huber, Johannes Maucher *(so that they do not need a surrogate network to estimate the tree loss)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8999213)  #### 1.3 NNs for Designing DTs.","(extended work of tree regularization that encourage a deep model to be approximated by several separate DTs specific to pre-defined regions of the input space)*     - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/6112)  - ***L1-orthogonal regularization***   - **""Enhancing Decision Tree Based Interpretation of Deep Neural Networks through L1-Orthogonal Regularization""**, ICMLA, 2019     - Nina Schaaf, Marco Huber, Johannes Maucher *(so that they do not need a surrogate network to estimate the tree loss)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8999213)  #### 1.3 NNs for Designing DTs.","(extended work of tree regularization that encourage a deep model to be approximated by several separate DTs specific to pre-defined regions of the input space)*     - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/6112)  - ***L1-orthogonal regularization***   - **""<PUBLICATION>Enhancing Decision Tree Based Interpretation of Deep Neural Networks through L1-Orthogonal Regularization</PUBLICATION>""**, <CONFERENCE>ICMLA</CONFERENCE>, 2019     - Nina Schaaf, Marco Huber, Johannes Maucher *(so that they do not need a surrogate network to estimate the tree loss)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8999213)  #### 1.3 NNs for Designing DTs.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\1c899260.txt,1.0
44,"NNs specialized for designing more reasonable DTs and do not benefit from it, which is rare in practice","NNs specialized for designing more reasonable DTs and do not benefit from it, which is rare in practice","NNs specialized for designing more reasonable DTs and do not benefit from it, which is rare in practice","NNs specialized for designing more reasonable DTs and do not benefit from it, which is rare in practice",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\6cf91f76.txt,1.0
45,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\cdb4ee2a.txt,1.0
46,"- ***Induce DTs from more valid data filtered by the trained NNs***   - **""An integrated approach of neural network and decision tree to classification""**, ICMLC, 2005     - Xiao-Ye Wang, Xiu-Xia Liang, Ji-Zhou Sun     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1527283)  ### 2.","- ***Induce DTs from more valid data filtered by the trained NNs***   - **""<PUBLICATION>An integrated approach of neural network and decision tree to classification</PUBLICATION>""**, <CONFERENCE>ICMLC</CONFERENCE>, 2005     - Xiao-Ye Wang, Xiu-Xia Liang, Ji-Zhou Sun     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1527283)  ### 2.","- ***Induce DTs from more valid data filtered by the trained NNs***   - **""An integrated approach of neural network and decision tree to classification""**, ICMLC, 2005     - Xiao-Ye Wang, Xiu-Xia Liang, Ji-Zhou Sun     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1527283)  ### 2.","- ***Induce DTs from more valid data filtered by the trained NNs***   - **""<PUBLICATION>An integrated approach of neural network and decision tree to classification</PUBLICATION>""**, <CONFERENCE>ICMLC</CONFERENCE>, 2005     - Xiao-Ye Wang, Xiu-Xia Liang, Ji-Zhou Sun     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1527283)  ### 2.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\537fd669.txt,1.0
47,Semi-hybrid: NNs Leveraging Class Hierarchies.,Semi-hybrid: NNs Leveraging Class Hierarchies.,`Semi-hybrid: NNs Leveraging Class Hierarchies.`,`<PUBLICATION>Semi-hybrid: NNs Leveraging Class Hierarchies</PUBLICATION>.`,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\a7c65209.txt,0.9787234042553191
48,"DT algorithms have two key ideas: ""decision"" and ""tree"".","DT algorithms have two key ideas: ""decision"" and ""tree"".","DT algorithms have two key ideas: ""decision"" and ""tree"".","DT algorithms have two key ideas: ""decision"" and ""tree"".",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\b74ef088.txt,1.0
49,"We view that ""decision"" is decision branches implemented by routing functions, and ""tree"" is the model topology that identifies a class hierarchy.","We view that ""decision"" is decision branches implemented by routing functions, and ""tree"" is the model topology that identifies a class hierarchy.","We view that ""decision"" is decision branches implemented by routing functions, and ""tree"" is the model topology that identifies a class hierarchy.","We view that ""decision"" is decision branches implemented by routing functions, and ""tree"" is the model topology that identifies a class hierarchy.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\91c872f9.txt,1.0
50,"This survey tries to include NNs that draw on a part of ideas from DTs, either the class hierarchy or the decision branches, but not both.","This survey tries to include NNs that draw on a part of ideas from DTs, either the class hierarchy or the decision branches, but not both.","This survey tries to include NNs that draw on a part of ideas from DTs, either the class hierarchy or the decision branches, but not both.","This survey tries to include NNs that draw on a part of ideas from DTs, either the class hierarchy or the decision branches, but not both.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\c7c9b569.txt,1.0
51,These  approaches are considered to be half and implicit integration of NNs and DTs.,These  approaches are considered to be half and implicit integration of NNs and DTs.,These approaches are considered to be half and implicit integration of NNs and DTs.,These approaches are considered to be half and implicit integration of NNs and DTs.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\632a6a88.txt,0.9940119760479041
52,"They borrow some inherent ideas from DTs for NNs, instead of designing a NN that works like a DT.","They borrow some inherent ideas from DTs for NNs, instead of designing a NN that works like a DT.","They borrow some inherent ideas from DTs for NNs, instead of designing a NN that works like a DT.","They borrow some inherent ideas from <PROGLANG>DTs</PROGLANG> for <PROGLANG>NNs</PROGLANG>, instead of designing a <PROGLANG>NN</PROGLANG> that works like a <PROGLANG>DT</PROGLANG>.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\b2cb3e25.txt,1.0
53,"However, we only concern about the former in our taxonomy,  *i.e.*, NNs leveraging class hierarchies.","However, we only concern about the former in our taxonomy,  *i.e.*, NNs leveraging class hierarchies.","However, we only concern about the former in our taxonomy,  *i.e.*, NNs leveraging class hierarchies.","However, we only concern about the former in our taxonomy,  *i.e.*, NNs leveraging class hierarchies.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\d60ba70b.txt,1.0
54,"These approaches can not implement the class hierarchy in the network structure due to the absence of decision branches, so they resort to incorporating the hierarchical relations into a NN directly.","These approaches can not implement the class hierarchy in the network structure due to the absence of decision branches, so they resort to incorporating the hierarchical relations into a NN directly.","These approaches can not implement the class hierarchy in the network structure due to the absence of decision branches, so they resort to incorporating the hierarchical relations into a NN directly.","These approaches can not implement the class hierarchy in the network structure due to the absence of decision branches, so they resort to incorporating the hierarchical relations into a NN directly.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\003be231.txt,1.0
55,"There are three lines of research: hierarchical architecture, hierarchical loss function and label embedding based methods, corresponding to different strategies of employing class hierarchies.","There are three lines of research: hierarchical architecture, hierarchical loss function and label embedding based methods, corresponding to different strategies of employing class hierarchies.","There are three lines of research: hierarchical architecture, hierarchical loss function and label embedding based methods, corresponding to different strategies of employing class hierarchies.","There are three lines of research: hierarchical architecture, hierarchical loss function and label embedding based methods, corresponding to different strategies of employing class hierarchies.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\ccde0171.txt,1.0
56,Overview of NNs exploiting class hierarchies:   <p align='center'>     </br>     <img src='class_hierarchy.png' width='1000'> </p>  Functions and notations in the figure are adopted from [Bertinetto *et al.*](https://openaccess.thecvf.com/content_CVPR_2020/html/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.html):  #### 2.1 Hierarchical Architecture.,Overview of NNs exploiting class hierarchies:   <p align='center'>     </br>     <img src='class_hierarchy.png' width='1000'> </p>  Functions and notations in the figure are adopted from [Bertinetto *et al.*](https://openaccess.thecvf.com/content_CVPR_2020/html/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.html):  #### 2.1 Hierarchical Architecture.,Overview of NNs exploiting class hierarchies:   <p align='center'>     </br>     <img src='class_hierarchy.png' width='1000'> </p>  Functions and notations in the figure are adopted from [Bertinetto *et al.*](https://openaccess.thecvf.com/content_CVPR_2020/html/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.html):  #### 2.1 Hierarchical Architecture.,Overview of NNs exploiting class hierarchies:   <p align='center'>     </br>     <img src='class_hierarchy.png' width='1000'> </p>  Functions and notations in the figure are adopted from [Bertinetto *et al.*](https://openaccess.thecvf.com/content_<PUBLICATION>CVPR_2020</PUBLICATION>/html/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_<PUBLICATION>CVPR_2020</PUBLICATION>_paper.html):  #### 2.1 Hierarchical Architecture.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\a5ecaabc.txt,1.0
57,"These methods attempt to incorporate the class hierarchy into the architecture of the classifier, so that the networks are designed to be branched and each branch is tasked to identify the concept abstraction at one specific level of the class hierarchy","These methods attempt to incorporate the class hierarchy into the architecture of the classifier, so that the networks are designed to be branched and each branch is tasked to identify the concept abstraction at one specific level of the class hierarchy","These methods attempt to incorporate the class hierarchy into the architecture of the classifier, so that the networks are designed to be branched and each branch is tasked to identify the concept abstraction at one specific level of the class hierarchy","These methods attempt to incorporate the class hierarchy into the architecture of the classifier, so that the networks are designed to be branched and each branch is tasked to identify the concept abstraction at one specific level of the class hierarchy",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\7c528ccb.txt,1.0
58,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\cdb4ee2a.txt,1.0
59,"- ***Hierarchical multi-label classification using local neural networks***   - **""An integrated approach of neural network and decision tree to classification""**, ICMLC, 2014     - Ricardo Cerri, Rodrigo C Barros, Andr√© CPLF De Carvalho     - [[Paper]](https://www.sciencedirect.com/science/article/pii/S0022000013000718)   - **""Reduction strategies for hierarchical multi-label classification in protein function prediction""**, BMC Bioinformatics, 2016     - Ricardo Cerri *et al.*     - [[Paper]](https://link.springer.com/article/10.1186/s12859-016-1232-1)   - **""HD-CNN: Hierarchical Deep Convolutional Neural Networks for Large Scale Visual Recognition""**, ICCV, 2015     - Zhicheng Yan *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.html) [[Code]](https://github.com/Changgang-Zheng/HD-CNN)   - **""Your ""Flamingo"" is My ""Bird"": Fine-Grained, or Not""**, CVPR, 2021     - Dongliang Chang *et al.* *(disentangle coarse and fine features)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Chang_Your_Flamingo_is_My_Bird_Fine-Grained_or_Not_CVPR_2021_paper.html) [[Code]](https://github.com/PRIS-CV/Fine-Grained-or-Not)  - ***Branch the network after a shared trunk***   - **""Learning to Make Better Mistakes: Semantics-aware Visual Food Recognition""**, ACM international conference on Multimedia, 2016      - Hui Wu *et al.* *(a single network backbone shared by multiple fully-connected layers)*     - [[Paper]](https://dl.acm.org/doi/abs/10.1145/2964284.2967205)   - **""Do Convolutional Neural Networks Learn Class Hierarchy?""","- ***Hierarchical multi-label classification using local neural networks***   - **""<PUBLICATION>An integrated approach of neural network and decision tree to classification</PUBLICATION>""**, <CONFERENCE>ICMLC</CONFERENCE>, 2014     - Ricardo Cerri, Rodrigo C Barros, Andr√© CPLF De Carvalho     - [[Paper]](https://www.sciencedirect.com/science/article/pii/S0022000013000718)   - **""<PUBLICATION>Reduction strategies for hierarchical multi-label classification in protein function prediction</PUBLICATION>""**, BMC Bioinformatics, 2016     - Ricardo Cerri *et al.*     - [[Paper]](https://link.springer.com/article/10.1186/s12859-016-1232-1)   - **""<PUBLICATION>HD-CNN: Hierarchical Deep Convolutional Neural Networks for Large Scale Visual Recognition</PUBLICATION>""**, <CONFERENCE>ICCV</CONFERENCE>, 2015     - Zhicheng Yan *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.html) [[Code]](https://github.com/Changgang-Zheng/HD-CNN)   - **""<PUBLICATION>Your ""Flamingo"" is My ""Bird"": Fine-Grained, or Not</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2021     - Dongliang Chang *et al.* *(disentangle coarse and fine features)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Chang_Your_Flamingo_is_My_Bird_Fine-Grained_or_Not_CVPR_2021_paper.html) [[Code]](https://github.com/PRIS-CV/Fine-Grained-or-Not)  - ***Branch the network after a shared trunk***   - **""<PUBLICATION>Learning to Make Better Mistakes: Semantics-aware Visual Food Recognition</PUBLICATION>""**, <CONFERENCE>ACM international conference on Multimedia</CONFERENCE>, 2016      - Hui Wu *et al.* *(a single network backbone shared by multiple fully-connected layers)*     - [[Paper]](https://dl.acm.org/doi/abs/10.1145/2964284.2967205)   - **""<PUBLICATION>Do Convolutional Neural Networks Learn Class Hierarchy?</PUBLICATION>""","- ***Hierarchical multi-label classification using local neural networks***  
  - **""An integrated approach of neural network and decision tree to classification""**, ICMLC, 2014  
    - Ricardo Cerri, Rodrigo C Barros, Andr√© CPLF De Carvalho  
    - [[Paper]](https://www.sciencedirect.com/science/article/pii/S0022000013000718)  
  - **""Reduction strategies for hierarchical multi-label classification in protein function prediction""**, BMC Bioinformatics, 2016  
    - Ricardo Cerri *et al.*  
    - [[Paper]](https://link.springer.com/article/10.1186/s12859-016-1232-1)  
  - **""HD-CNN: Hierarchical Deep Convolutional Neural Networks for Large Scale Visual Recognition""**, ICCV, 2015  
    - Zhicheng Yan *et al.*  
    - [[Paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.html) [[Code]](https://github.com/Changgang-Zheng/HD-CNN)  
  - **""Your ""Flamingo"" is My ""Bird"": Fine-Grained, or Not""**, CVPR, 2021  
    - Dongliang Chang *et al.* *(disentangle coarse and fine features)*  
    - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Chang_Your_Flamingo_is_My_Bird_Fine-Grained_or_Not_CVPR_2021_paper.html) [[Code]](https://github.com/PRIS-CV/Fine-Grained-or-Not)  
- ***Branch the network after a shared trunk***  
  - **""Learning to Make Better Mistakes: Semantics-aware Visual Food Recognition""**, ACM international conference on Multimedia, 2016  
    - Hui Wu *et al.* *(a single network backbone shared by multiple fully-connected layers)*  
    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/2964284.2967205)  
  - **""Do Convolutional Neural Networks Learn Class Hierarchy?""","- ***Hierarchical multi-label classification using local neural networks***  
  - **""<PUBLICATION>An integrated approach of neural network and decision tree to classification</PUBLICATION>""**, <CONFERENCE>ICMLC</CONFERENCE>, 2014  
    - Ricardo Cerri, Rodrigo C Barros, Andr√© CPLF De Carvalho  
    - [[Paper]](https://www.sciencedirect.com/science/article/pii/S0022000013000718)  
  - **""<PUBLICATION>Reduction strategies for hierarchical multi-label classification in protein function prediction</PUBLICATION>""**, <PUBLICATION>BMC Bioinformatics</PUBLICATION>, 2016  
    - Ricardo Cerri *et al.*  
    - [[Paper]](https://link.springer.com/article/10.1186/s12859-016-1232-1)  
  - **""<PUBLICATION>HD-CNN: Hierarchical Deep Convolutional Neural Networks for Large Scale Visual Recognition</PUBLICATION>""**, <CONFERENCE>ICCV</CONFERENCE>, 2015  
    - Zhicheng Yan *et al.*  
    - [[Paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.html) [[Code]](https://github.com/Changgang-Zheng/HD-CNN)  
  - **""<PUBLICATION>Your ""Flamingo"" is My ""Bird"": Fine-Grained, or Not</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2021  
    - Dongliang Chang *et al.* *(disentangle coarse and fine features)*  
    - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Chang_Your_Flamingo_is_My_Bird_Fine-Grained_or_Not_CVPR_2021_paper.html) [[Code]](https://github.com/PRIS-CV/Fine-Grained-or-Not)  
- ***Branch the network after a shared trunk***  
  - **""<PUBLICATION>Learning to Make Better Mistakes: Semantics-aware Visual Food Recognition</PUBLICATION>""**, <CONFERENCE>ACM international conference on Multimedia</CONFERENCE>, 2016  
    - Hui Wu *et al.* *(a single network backbone shared by multiple fully-connected layers)*  
    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/2964284.2967205)  
  - **""<PUBLICATION>Do Convolutional Neural Networks Learn Class Hierarchy?</PUBLICATION>""",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\1e6300f8.txt,0.9902557856272838
60,"**, TVCG, 2017     - Alsallakh Bilal *et al.* *(deep CNN with branches at intermediate layers to fit the coarser-grained labels)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8017618)   - **""Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding""**, ACM international conference on Multimedia, 2018     - Tianshui Chen *et al.* *(introduce an attention mechanism to incorporate the coarse-grained results for learning finer-grained features)*     - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3240508.3240523)  - ***Interactions between different branches***   - **""Label Hierarchy Transition: Modeling Class Hierarchies to Enhance Deep Classifiers""**, arXiv, 2021     - Renzhen Wang, *et al* *(propose label hierarchy transition matrices whose column vectors represent the conditional label distributions of classes between two adjacent hierarchies)*     - [[Paper]](https://arxiv.org/abs/2112.02353)   - **""Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification""**, CVPR, 2022     - Jingzhou Chen *et al.* *(propose the hierarchical residual network in which granularity-specific features from parent levels are added to features in children levels.)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Label_Relation_Graphs_Enhanced_Hierarchical_Residual_Network_for_Hierarchical_Multi-Granularity_CVPR_2022_paper.html) [[Code]](https://github.com/MonsterZhZh/HRN)  #### 2.2 Hierarchical Loss Function.","**, <CONFERENCE>TVCG</CONFERENCE>, 2017     - Alsallakh Bilal *et al.* *(deep CNN with branches at intermediate layers to fit the coarser-grained labels)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8017618)   - **""<PUBLICATION>Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding</PUBLICATION>""**, <CONFERENCE>ACM international conference on Multimedia</CONFERENCE>, 2018     - Tianshui Chen *et al.* *(introduce an attention mechanism to incorporate the coarse-grained results for learning finer-grained features)*     - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3240508.3240523)  - ***Interactions between different branches***   - **""<PUBLICATION>Label Hierarchy Transition: Modeling Class Hierarchies to Enhance Deep Classifiers</PUBLICATION>""**, arXiv, 2021     - Renzhen Wang, *et al* *(propose label hierarchy transition matrices whose column vectors represent the conditional label distributions of classes between two adjacent hierarchies)*     - [[Paper]](https://arxiv.org/abs/2112.02353)   - **""<PUBLICATION>Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2022     - Jingzhou Chen *et al.* *(propose the hierarchical residual network in which granularity-specific features from parent levels are added to features in children levels.)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Label_Relation_Graphs_Enhanced_Hierarchical_Residual_Network_for_Hierarchical_Multi-Granularity_CVPR_2022_paper.html) [[Code]](https://github.com/MonsterZhZh/HRN)  #### 2.2 Hierarchical Loss Function.","- **""Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding""**, ACM international conference on Multimedia, 2018     - Tianshui Chen *et al.* *(introduce an attention mechanism to incorporate the coarse-grained results for learning finer-grained features)*     - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3240508.3240523)  - ***Interactions between different branches***   - **""Label Hierarchy Transition: Modeling Class Hierarchies to Enhance Deep Classifiers""**, arXiv, 2021     - Renzhen Wang, *et al* *(propose label hierarchy transition matrices whose column vectors represent the conditional label distributions of classes between two adjacent hierarchies)*     - [[Paper]](https://arxiv.org/abs/2112.02353)   - **""Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification""**, CVPR, 2022     - Jingzhou Chen *et al.* *(propose the hierarchical residual network in which granularity-specific features from parent levels are added to features in children levels.)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Label_Relation_Graphs_Enhanced_Hierarchical_Residual_Network_for_Hierarchical_Multi-Granularity_CVPR_2022_paper.html) [[Code]](https://github.com/MonsterZhZh/HRN)  #### 2.2 Hierarchical Loss Function.","- **""Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding""**, <PUBLICATION>ACM international conference on Multimedia</PUBLICATION>, 2018     - Tianshui Chen *et al.* *(introduce an attention mechanism to incorporate the coarse-grained results for learning finer-grained features)*     - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3240508.3240523)  - ***Interactions between different branches***   - **""Label Hierarchy Transition: Modeling Class Hierarchies to Enhance Deep Classifiers""**, <PUBLICATION>arXiv</PUBLICATION>, 2021     - Renzhen Wang, *et al* *(propose label hierarchy transition matrices whose column vectors represent the conditional label distributions of classes between two adjacent hierarchies)*     - [[Paper]](https://arxiv.org/abs/2112.02353)   - **""Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification""**, <CONFERENCE>CVPR</CONFERENCE>, 2022     - Jingzhou Chen *et al.* *(propose the hierarchical residual network in which granularity-specific features from parent levels are added to features in children levels.)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Label_Relation_Graphs_Enhanced_Hierarchical_Residual_Network_for_Hierarchical_Multi-Granularity_CVPR_2022_paper.html) [[Code]](https://github.com/MonsterZhZh/HRN)  #### 2.2 Hierarchical Loss Function.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\522245f2.txt,0.9296847939037063
61,"Incorporate the class hierarchy into loss functions, which exploits the underlying hierarchical relationships and produce predictions coherent with the pre-defined class hierarchy","Incorporate the class hierarchy into loss functions, which exploits the underlying hierarchical relationships and produce predictions coherent with the pre-defined class hierarchy","Incorporate the class hierarchy into loss functions, which exploits the underlying hierarchical relationships and produce predictions coherent with the pre-defined class hierarchy","Incorporate the class hierarchy into loss functions, which exploits the underlying hierarchical relationships and produce predictions coherent with the pre-defined class hierarchy",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\93d0a538.txt,1.0
62,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\cdb4ee2a.txt,1.0
63,"- ***Predict at each grain separately, *i.e.*, first take a model pre-trained on one level, then tune it using labels from other levels***   - **""Coherent Hierarchical Multi-Label Classification Networks""**, NIPS, 2020     - Eleonora Giunchiglia, Thomas Lukasiewicz     - [[Paper]](https://proceedings.neurips.cc/paper/2020/hash/6dd4e10e3296fa63738371ec0d5df818-Abstract.html) [[Code]](https://github.com/EGiunchiglia/C-HMCNN)   - **""Learning Hierarchical Visual Representations in Deep Neural Networks Using Hierarchical Linguistic Labels""**, arXiv, 2018     - Joshua C Peterson *et al.*     - [[Paper]](https://arxiv.org/abs/1805.07647)  - ***Utilize the hierarchical constraints directly***   - **""Learning hierarchical similarity metrics""**, CVPR, 2012     - Nakul Verma *et al.* *(probabilistic nearest-neighbor classification based framework)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6247938)   - **""Discriminative Transfer Learning with Tree-based Priors""**, NIPS, 2013     - Nitish Srivastava, Russ R Salakhutdinov *(DNN with hierarchical priors over the parameters of the classification layer)*     - [[Paper]](https://proceedings.neurips.cc/paper/2013/hash/9ac403da7947a183884c18a67d3aa8de-Abstract.html)   - **""Large-Scale Object Classification Using Label Relation Graphs""**, ECCV, 2018     - Jia Deng *et al.* *(encodes semantic relations into a directed acyclic graph and compute a loss defined on it)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_4) [[Code]](https://github.com/kylemin/HEX-graph)   - **""Making Better Mistakes: Leveraging Class Hierarchies With Deep Networks""**, CVPR, 2020     - Luca Bertinetto *et al.* *(incorporate class hierarchy into the cross-entropy loss)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.html)  #### 2.3 Label Embedding.","- ***Predict at each grain separately, *i.e.*, first take a model pre-trained on one level, then tune it using labels from other levels***   - **""<PUBLICATION>Coherent Hierarchical Multi-Label Classification Networks</PUBLICATION>""**, <CONFERENCE>NIPS</CONFERENCE>, 2020     - Eleonora Giunchiglia, Thomas Lukasiewicz     - [[Paper]](https://proceedings.neurips.cc/paper/2020/hash/6dd4e10e3296fa63738371ec0d5df818-Abstract.html) [[Code]](https://github.com/EGiunchiglia/C-HMCNN)   - **""<PUBLICATION>Learning Hierarchical Visual Representations in Deep Neural Networks Using Hierarchical Linguistic Labels</PUBLICATION>""**, arXiv, 2018     - Joshua C Peterson *et al.*     - [[Paper]](https://arxiv.org/abs/1805.07647)  - ***Utilize the hierarchical constraints directly***   - **""<PUBLICATION>Learning hierarchical similarity metrics</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2012     - Nakul Verma *et al.* *(probabilistic nearest-neighbor classification based framework)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6247938)   - **""<PUBLICATION>Discriminative Transfer Learning with Tree-based Priors</PUBLICATION>""**, <CONFERENCE>NIPS</CONFERENCE>, 2013     - Nitish Srivastava, Russ R Salakhutdinov *(DNN with hierarchical priors over the parameters of the classification layer)*     - [[Paper]](https://proceedings.neurips.cc/paper/2013/hash/9ac403da7947a183884c18a67d3aa8de-Abstract.html)   - **""<PUBLICATION>Large-Scale Object Classification Using Label Relation Graphs</PUBLICATION>""**, <CONFERENCE>ECCV</CONFERENCE>, 2018     - Jia Deng *et al.* *(encodes semantic relations into a directed acyclic graph and compute a loss defined on it)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_4) [[Code]](https://github.com/kylemin/HEX-graph)   - **""<PUBLICATION>Making Better Mistakes: Leveraging Class Hierarchies With Deep Networks</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2020     - Luca Bertinetto *et al.* *(incorporate class hierarchy into the cross-entropy loss)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.html)  #### 2.3 Label Embedding.","- ***Predict at each grain separately, *i.e.*, first take a model pre-trained on one level, then tune it using labels from other levels***  
  - **""Coherent Hierarchical Multi-Label Classification Networks""**, NIPS, 2020  
    - Eleonora Giunchiglia, Thomas Lukasiewicz  
    - [[Paper]](https://proceedings.neurips.cc/paper/2020/hash/6dd4e10e3296fa63738371ec0d5df818-Abstract.html) [[Code]](https://github.com/EGiunchiglia/C-HMCNN)  
  - **""Learning Hierarchical Visual Representations in Deep Neural Networks Using Hierarchical Linguistic Labels""**, arXiv, 2018  
    - Joshua C Peterson *et al.*  
    - [[Paper]](https://arxiv.org/abs/1805.07647)  
- ***Utilize the hierarchical constraints directly***  
  - **""Learning hierarchical similarity metrics""**, CVPR, 2012  
    - Nakul Verma *et al.* *(probabilistic nearest-neighbor classification based framework)*  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6247938)  
  - **""Discriminative Transfer Learning with Tree-based Priors""**, NIPS, 2013  
    - Nitish Srivastava, Russ R Salakhutdinov *(DNN with hierarchical priors over the parameters of the classification layer)*  
    - [[Paper]](https://proceedings.neurips.cc/paper/2013/hash/9ac403da7947a183884c18a67d3aa8de-Abstract.html)  
  - **""Large-Scale Object Classification Using Label Relation Graphs""**, ECCV, 2018  
    - Jia Deng *et al.* *(encodes semantic relations into a directed acyclic graph and compute a loss defined on it)*  
    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_4) [[Code]](https://github.com/kylemin/HEX-graph)  
  - **""Making Better Mistakes: Leveraging Class Hierarchies With Deep Networks""**, CVPR, 2020  
    - Luca Bertinetto *et al.* *(incorporate class hierarchy into the cross-entropy loss)*  
    - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.html)  
#### 2.3 Label Embedding.","- ***Predict at each grain separately, *i.e.*, first take a model pre-trained on one level, then tune it using labels from other levels***  
  - **""<PUBLICATION>Coherent Hierarchical Multi-Label Classification Networks</PUBLICATION>""**, <CONFERENCE>NIPS</CONFERENCE>, 2020  
    - Eleonora Giunchiglia, Thomas Lukasiewicz  
    - [[Paper]](https://proceedings.neurips.cc/paper/2020/hash/6dd4e10e3296fa63738371ec0d5df818-Abstract.html) [[Code]](https://github.com/EGiunchiglia/C-HMCNN)  
  - **""<PUBLICATION>Learning Hierarchical Visual Representations in Deep Neural Networks Using Hierarchical Linguistic Labels</PUBLICATION>""**, <PUBLICATION>arXiv</PUBLICATION>, 2018  
    - Joshua C Peterson *et al.*  
    - [[Paper]](https://arxiv.org/abs/1805.07647)  
- ***Utilize the hierarchical constraints directly***  
  - **""<PUBLICATION>Learning hierarchical similarity metrics</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2012  
    - Nakul Verma *et al.* *(probabilistic nearest-neighbor classification based framework)*  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6247938)  
  - **""<PUBLICATION>Discriminative Transfer Learning with Tree-based Priors</PUBLICATION>""**, <CONFERENCE>NIPS</CONFERENCE>, 2013  
    - Nitish Srivastava, Russ R Salakhutdinov *(DNN with hierarchical priors over the parameters of the classification layer)*  
    - [[Paper]](https://proceedings.neurips.cc/paper/2013/hash/9ac403da7947a183884c18a67d3aa8de-Abstract.html)  
  - **""<PUBLICATION>Large-Scale Object Classification Using Label Relation Graphs</PUBLICATION>""**, <CONFERENCE>ECCV</CONFERENCE>, 2018  
    - Jia Deng *et al.* *(encodes semantic relations into a directed acyclic graph and compute a loss defined on it)*  
    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_4) [[Code]](https://github.com/kylemin/HEX-graph)  
  - **""<PUBLICATION>Making Better Mistakes: Leveraging Class Hierarchies With Deep Networks</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2020  
    - Luca Bertinetto *et al.* *(incorporate class hierarchy into the cross-entropy loss)*  
    - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.html)  
#### 2.3 Label Embedding.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\3dbf526e.txt,0.9903455284552846
64,These approaches aim to encode the class hierarchy into embeddings whose relative locations or possible interactions represent the semantic relationships,These approaches aim to encode the class hierarchy into embeddings whose relative locations or possible interactions represent the semantic relationships,These approaches aim to encode the class hierarchy into embeddings whose relative locations or possible interactions represent the semantic relationships,These approaches aim to encode the class hierarchy into embeddings whose relative locations or possible interactions represent the semantic relationships,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\1a2b18d8.txt,1.0
65,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\cdb4ee2a.txt,1.0
66,"- ***DeViSE method that utilizes unannotated Wikipedia text***   - **""DeViSE: A Deep Visual-Semantic Embedding Model""**, NIPS, 2013     - Andrea Frome *et al.*     - [[Paper]](https://proceedings.neurips.cc/paper/2013/hash/7cce53cf90577442771720a370c3c723-Abstract.html) [[Code]](https://github.com/jean4599/DeViSE)  - ***Embeddings whose pair-wise dot products correspond to semantic similarity between classes***   - **""Hierarchy-Based Image Embeddings for Semantic Image Retrieval""**, WACV, 2019     - Bj√∂rn Barz, Joachim Denzler     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8658633) [[Code]](https://github.com/cvjena/semantic-embeddings)  - **Employ the entailment cones to learn order-preserving embeddings**   - **""Hierarchical Image Classification Using Entailment Cone Embeddings""**, CVPR, 2020     - Ankit Dhall *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_CVPRW_2020/html/w50/Dhall_Hierarchical_Image_Classification_Using_Entailment_Cone_Embeddings_CVPRW_2020_paper.html) [[Code]](https://github.com/ankitdhall/learning_embeddings)  - ***Label embeddings in zero-shot classification***   - **""Latent Embeddings for Zero-Shot Classification""**, CVPR, 2016     - Yongqin Xian *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2016/html/Xian_Latent_Embeddings_for_CVPR_2016_paper.html)   - **""Evaluation of Output Embeddings for Fine-Grained Image Classification""**, CVPR, 2015     - Zeynep Akata *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2015/html/Akata_Evaluation_of_Output_2015_CVPR_paper.html)   ### 3.","- ***DeViSE method that utilizes unannotated Wikipedia text***   - **""<PUBLICATION>DeViSE: A Deep Visual-Semantic Embedding Model</PUBLICATION>""**, <CONFERENCE>NIPS</CONFERENCE>, 2013     - Andrea Frome *et al.*     - [[Paper]](https://proceedings.neurips.cc/paper/2013/hash/7cce53cf90577442771720a370c3c723-Abstract.html) [[Code]](https://github.com/jean4599/DeViSE)  - ***Embeddings whose pair-wise dot products correspond to semantic similarity between classes***   - **""<PUBLICATION>Hierarchy-Based Image Embeddings for Semantic Image Retrieval</PUBLICATION>""**, <CONFERENCE>WACV</CONFERENCE>, 2019     - Bj√∂rn Barz, Joachim Denzler     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8658633) [[Code]](https://github.com/cvjena/semantic-embeddings)  - **Employ the entailment cones to learn order-preserving embeddings**   - **""<PUBLICATION>Hierarchical Image Classification Using Entailment Cone Embeddings</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2020     - Ankit Dhall *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_CVPRW_2020/html/w50/Dhall_Hierarchical_Image_Classification_Using_Entailment_Cone_Embeddings_CVPRW_2020_paper.html) [[Code]](https://github.com/ankitdhall/learning_embeddings)  - ***Label embeddings in zero-shot classification***   - **""<PUBLICATION>Latent Embeddings for Zero-Shot Classification</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2016     - Yongqin Xian *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2016/html/Xian_Latent_Embeddings_for_CVPR_2016_paper.html)   - **""<PUBLICATION>Evaluation of Output Embeddings for Fine-Grained Image Classification</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2015     - Zeynep Akata *et al.*     - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2015/html/Akata_Evaluation_of_Output_2015_CVPR_paper.html)   ### 3.","- ***DeViSE method that utilizes unannotated Wikipedia text***  
  - **""DeViSE: A Deep Visual-Semantic Embedding Model""**, NIPS, 2013  
    - Andrea Frome *et al.*  
    - [[Paper]](https://proceedings.neurips.cc/paper/2013/hash/7cce53cf90577442771720a370c3c723-Abstract.html) [[Code]](https://github.com/jean4599/DeViSE)  
- ***Embeddings whose pair-wise dot products correspond to semantic similarity between classes***  
  - **""Hierarchy-Based Image Embeddings for Semantic Image Retrieval""**, WACV, 2019  
    - Bj√∂rn Barz, Joachim Denzler  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8658633) [[Code]](https://github.com/cvjena/semantic-embeddings)  
- **Employ the entailment cones to learn order-preserving embeddings**  
  - **""Hierarchical Image Classification Using Entailment Cone Embeddings""**, CVPR, 2020  
    - Ankit Dhall *et al.*  
    - [[Paper]](https://openaccess.thecvf.com/content_CVPRW_2020/html/w50/Dhall_Hierarchical_Image_Classification_Using_Entailment_Cone_Embeddings_CVPRW_2020_paper.html) [[Code]](https://github.com/ankitdhall/learning_embeddings)  
- ***Label embeddings in zero-shot classification***  
  - **""Latent Embeddings for Zero-Shot Classification""**, CVPR, 2016  
    - Yongqin Xian *et al.*  
    - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2016/html/Xian_Latent_Embeddings_for_CVPR_2016_paper.html)  
  - **""Evaluation of Output Embeddings for Fine-Grained Image Classification""**, CVPR, 2015  
    - Zeynep Akata *et al.*  
    - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2015/html/Akata_Evaluation_of_Output_2015_CVPR_paper.html)  
### 3.","- ***DeViSE method that utilizes unannotated Wikipedia text***  
  - **""<PUBLICATION>DeViSE: A Deep Visual-Semantic Embedding Model</PUBLICATION>""**, <CONFERENCE>NIPS</CONFERENCE>, 2013  
    - Andrea Frome *et al.*  
    - [[Paper]](https://proceedings.neurips.cc/paper/2013/hash/7cce53cf90577442771720a370c3c723-Abstract.html) [[Code]](https://github.com/jean4599/DeViSE)  
- ***Embeddings whose pair-wise dot products correspond to semantic similarity between classes***  
  - **""<PUBLICATION>Hierarchy-Based Image Embeddings for Semantic Image Retrieval</PUBLICATION>""**, <CONFERENCE>WACV</CONFERENCE>, 2019  
    - Bj√∂rn Barz, Joachim Denzler  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/8658633) [[Code]](https://github.com/cvjena/semantic-embeddings)  
- **Employ the entailment cones to learn order-preserving embeddings**  
  - **""<PUBLICATION>Hierarchical Image Classification Using Entailment Cone Embeddings</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2020  
    - Ankit Dhall *et al.*  
    - [[Paper]](https://openaccess.thecvf.com/content_CVPRW_2020/html/w50/Dhall_Hierarchical_Image_Classification_Using_Entailment_Cone_Embeddings_CVPRW_2020_paper.html) [[Code]](https://github.com/ankitdhall/learning_embeddings)  
- ***Label embeddings in zero-shot classification***  
  - **""<PUBLICATION>Latent Embeddings for Zero-Shot Classification</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2016  
    - Yongqin Xian *et al.*  
    - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2016/html/Xian_Latent_Embeddings_for_CVPR_2016_paper.html)  
  - **""<PUBLICATION>Evaluation of Output Embeddings for Fine-Grained Image Classification</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2015  
    - Zeynep Akata *et al.*  
    - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2015/html/Akata_Evaluation_of_Output_2015_CVPR_paper.html)  
### 3.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\217c4270.txt,0.9891270580925753
67,Hybrid: Neural Decision Trees.,Hybrid: Neural Decision Trees.,`Hybrid: Neural Decision Trees.`,`Hybrid: <PUBLICATION>Neural Decision Trees</PUBLICATION>.`,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\a6fd89c3.txt,0.967741935483871
68,Neural Decision Trees (NDTs) are hybrid NN models that implement both the class hierarchy and the decision branches.,Neural Decision Trees (NDTs) are hybrid NN models that implement both the class hierarchy and the decision branches.,Neural Decision Trees (NDTs) are hybrid NN models that implement both the class hierarchy and the decision branches.,Neural Decision Trees (NDTs) are hybrid NN models that implement both the class hierarchy and the decision branches.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\f3f69d46.txt,1.0
69,"Their core idea is to exploit NNs in the tree design by making the routing functions differentiable, thus allowing gradient descent-based methods to optimize.  #### 3.1 Neural Decision Trees with Class Hierarchies.","Their core idea is to exploit NNs in the tree design by making the routing functions differentiable, thus allowing gradient descent-based methods to optimize.  #### 3.1 Neural Decision Trees with Class Hierarchies.","Their core idea is to exploit NNs in the tree design by making the routing functions differentiable, thus allowing gradient descent-based methods to optimize.  #### 3.1 Neural Decision Trees with Class Hierarchies.","Their core idea is to exploit NNs in the tree design by making the routing functions differentiable, thus allowing gradient descent-based methods to optimize.  #### 3.1 <PUBLICATION>Neural Decision Trees with Class Hierarchies</PUBLICATION>.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\0e2a945e.txt,1.0
70,"If a NDT satisfies this property, it means each internal node is assigned a specific and intermediate classification task.","If a NDT satisfies this property, it means each internal node is assigned a specific and intermediate classification task.","If a NDT satisfies this property, it means each internal node is assigned a specific and intermediate classification task.","If a NDT satisfies this property, it means each internal node is assigned a specific and intermediate classification task.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\0adc744f.txt,1.0
71,"This ""divide-and-conquer"" strategy and stepwise inference process make the model more interpretable, because each node in the tree is responsible and distinguishable from other nodes.  ##### *3.1.1 Data-Driven NDTs.* Data-driven methods employ data-dependent heuristics to perform local optimization.","This ""divide-and-conquer"" strategy and stepwise inference process make the model more interpretable, because each node in the tree is responsible and distinguishable from other nodes.  ##### *3.1.1 Data-Driven NDTs.* Data-driven methods employ data-dependent heuristics to perform local optimization.","This ""divide-and-conquer"" strategy and stepwise inference process make the model more interpretable, because each node in the tree is responsible and distinguishable from other nodes.  ##### *3.1.1 Data-Driven NDTs.* Data-driven methods employ data-dependent heuristics to perform local optimization.","This ""divide-and-conquer"" strategy and stepwise inference process make the model more interpretable, because each node in the tree is responsible and distinguishable from other nodes.  ##### *3.1.1 <DATASET>Data-Driven NDTs</DATASET>.* Data-driven methods employ data-dependent heuristics to perform local optimization.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\613dd3c5.txt,1.0
72,The resultant tree will lead to  a recursive partitioning of the input space ùëã  through a cascade of tests and define the output of each leaf in terms of examples falling within it.,The resultant tree will lead to  a recursive partitioning of the input space ùëã  through a cascade of tests and define the output of each leaf in terms of examples falling within it.,The resultant tree will lead to  a recursive partitioning of the input space ùëã  through a cascade of tests and define the output of each leaf in terms of examples falling within it.,The resultant tree will lead to  a recursive partitioning of the input space ùëã  through a cascade of tests and define the output of each leaf in terms of examples falling within it.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\18c2e485.txt,1.0
73,"A comparison between NDTs according to whether it implements a class hierarchy and whether it is data-driven: <p align='center'>     </br>     <img src='NDT_hierarchy.png' width='1000'> </p>  - ***Informativeness-related splitting functions with NN-based routers***   - **""Neural trees-using neural nets in a tree classifier structure""**, ICASSP, 1991     - J-E Stromberg, Jalel Zrida, Alf Isaksson *(train a small NN at each internal node, which singles out one unique class that gains the most class purity)*     - [[Paper]](https://www.computer.org/csdl/proceedings-article/icassp/1991/0003137/12OmNzdoMSH)   - **""Evolutionary design of neural network tree-integration of decision tree, neural network and GA""**, CEC, 2001     - Qiangfu Zhao *(use genetic algorithms to maximize the information gain ratio at each internal node)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/934395)   - **""Hybrid decision tree""**, Knowledge-Based Systems, 2002     - Zhi-Hua Zhou, Zhao-Qian Chen *(Divide the instance space into ordered attributes and unordered attributes)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0950705102000382)   - **""Classification trees with neural network feature extraction""**, CVPR, 1992     - Heng Guo, Saul B Gelfand *(find two clusters that minimizes a Gini impurity criterion, then find a good split through back-propagation)*     - [[Paper]](https://www.computer.org/csdl/proceedings-article/cvpr/1992/00223275/12OmNzBOilt)  - ***Decrease the classification error at each node to be extended***   - **""Growing and pruning neural tree networks""**, IEEE Transactions on Computers, 1993     - A Sakar *et al.* *(Divide the instance space into ordered attributes and unordered attributes)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/210172)  - ***Grows during competitive learning***   - **""Competitive neural trees for pattern classification""**,  IEEE Transactions on Neural Networks, 1998     - Sven Behnke, Nicolaos B Karayiannis     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/728387)  - ***Map examples into the label embedding space and predicts using a NDT***   - **""Label Embedding Trees for Large Multi-Class Tasks""**, NIPS, 2010     - Samy Bengio, Jason Weston, David Grangier     - [[Paper]](https://proceedings.neurips.cc/paper/2010/hash/06138bc5af6023646ede0e1f7c1eac75-Abstract.html)   - **""Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition""**, NIPS, 2011     - Jia Deng, Sanjeev Satheesh, Alexander Berg, Fei Li     - [[Paper]](https://proceedings.neurips.cc/paper/2011/hash/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html)  - ***Apply data-driven architectures to fuzzy NDTs***   - **""Globally optimal fuzzy decision trees for classification and regression""**, IEEE Transactions on Pattern Analysis and Machine Intelligence, 1999     - Alberto Su√°rez, James F Lutsko *(superimpose a fuzzy structure over the skeleton of a CART tree and introduce a global optimization algorithm)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/817409)   - **""Budding Trees""**, ICPR, 2014     - Ozan Irsoy, Olcay Taner Yildiz, Ethem Alpaydin *(a FDT model that can be dynamically adjusted)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6977328) [[Code]](https://github.com/oir/budding-tree)   - **""Convolutional Decision Trees for Feature Learning and Segmentation""**, GCPR, 2014    - Dmitry Laptev, Joachim M Buhmann *(applies fuzzy NDTs to image segmentation by extracting the most informative and interpretable features)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-11752-2_8)   - **""Neural Decision Trees""**, arXiv, 2017     - Randall Balestriero *(map similar inputs to the same hash value)*     - [[Paper]](https://arxiv.org/abs/1702.07360)   - **""Neuro-fuzzy decision trees""**, Neural Systems, 2006     - Rajen B Bhatt, M Gopal *(employs fuzzy ID3 algorithm for DT-renovation)*     - [[Paper]](https://www.worldscientific.com/doi/abs/10.1142/s0129065706000470)  - ***Incremental learning***   - **""Hybrid decision tree""**, Knowledge-Based Systems, 2022     - Zhi-Hua Zhou, Zhao-Qian Chen *(two example-incremental tasks, one hypothesis-driven constructive induction mechanism)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0950705102000382)   - **""Soft decision trees""**, ICPR, 2012     - Ozan Irsoy, Olcay Taner Yƒ±ldƒ±z, Ethem Alpaydƒ±n *(sigmoid-based FDT whose splits are made by checking if there is an improvement over the validation set)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6460506)   - **""A Neural Tree with Partial Incremental Learning Capability""**, ICMLC, 2007     - Mu-Chun Su, Hsu-Hsun Lo *(choose a target class at each internal node and train a small NN to separate the positive patterns from negative ones)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/4370106)  ##### *3.1.2 Bigot NDTs.* A bigot NDT tends to have a pre-defined structure and determine leaves (pure classes or fixed class distributions) by priors or algorithms such that induce the class hierarchy","A comparison between NDTs according to whether it implements a class hierarchy and whether it is data-driven: <p align='center'>     </br>     <img src='NDT_hierarchy.png' width='1000'> </p>  - ***Informativeness-related splitting functions with NN-based routers***   - **""<PUBLICATION>Neural trees-using neural nets in a tree classifier structure</PUBLICATION>""**, <CONFERENCE>ICASSP</CONFERENCE>, 1991     - J-E Stromberg, Jalel Zrida, Alf Isaksson *(train a small NN at each internal node, which singles out one unique class that gains the most class purity)*     - [[Paper]](https://www.computer.org/csdl/proceedings-article/icassp/1991/0003137/12OmNzdoMSH)   - **""<PUBLICATION>Evolutionary design of neural network tree-integration of decision tree, neural network and GA</PUBLICATION>""**, <CONFERENCE>CEC</CONFERENCE>, 2001     - Qiangfu Zhao *(use genetic algorithms to maximize the information gain ratio at each internal node)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/934395)   - **""<PUBLICATION>Hybrid decision tree</PUBLICATION>""**, Knowledge-Based Systems, 2002     - Zhi-Hua Zhou, Zhao-Qian Chen *(Divide the instance space into ordered attributes and unordered attributes)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0950705102000382)   - **""<PUBLICATION>Classification trees with neural network feature extraction</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 1992     - Heng Guo, Saul B Gelfand *(find two clusters that minimizes a Gini impurity criterion, then find a good split through back-propagation)*     - [[Paper]](https://www.computer.org/csdl/proceedings-article/cvpr/1992/00223275/12OmNzBOilt)  - ***Decrease the classification error at each node to be extended***   - **""<PUBLICATION>Growing and pruning neural tree networks</PUBLICATION>""**, IEEE Transactions on Computers, 1993     - A Sakar *et al.* *(Divide the instance space into ordered attributes and unordered attributes)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/210172)  - ***Grows during competitive learning***   - **""<PUBLICATION>Competitive neural trees for pattern classification</PUBLICATION>""**,  IEEE Transactions on Neural Networks, 1998     - Sven Behnke, Nicolaos B Karayiannis     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/728387)  - ***Map examples into the label embedding space and predicts using a NDT***   - **""<PUBLICATION>Label Embedding Trees for Large Multi-Class Tasks</PUBLICATION>""**, <CONFERENCE>NIPS</CONFERENCE>, 2010     - Samy Bengio, Jason Weston, David Grangier     - [[Paper]](https://proceedings.neurips.cc/paper/2010/hash/06138bc5af6023646ede0e1f7c1eac75-Abstract.html)   - **""<PUBLICATION>Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</PUBLICATION>""**, <CONFERENCE>NIPS</CONFERENCE>, 2011     - Jia Deng, Sanjeev Satheesh, Alexander Berg, Fei Li     - [[Paper]](https://proceedings.neurips.cc/paper/2011/hash/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html)  - ***Apply data-driven architectures to fuzzy NDTs***   - **""<PUBLICATION>Globally optimal fuzzy decision trees for classification and regression</PUBLICATION>""**, IEEE Transactions on Pattern Analysis and Machine Intelligence, 1999     - Alberto Su√°rez, James F Lutsko *(superimpose a fuzzy structure over the skeleton of a CART tree and introduce a global optimization algorithm)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/817409)   - **""<PUBLICATION>Budding Trees</PUBLICATION>""**, <CONFERENCE>ICPR</CONFERENCE>, 2014     - Ozan Irsoy, Olcay Taner Yildiz, Ethem Alpaydin *(a FDT model that can be dynamically adjusted)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6977328) [[Code]](https://github.com/oir/budding-tree)   - **""<PUBLICATION>Convolutional Decision Trees for Feature Learning and Segmentation</PUBLICATION>""**, <CONFERENCE>GCPR</CONFERENCE>, 2014    - Dmitry Laptev, Joachim M Buhmann *(applies fuzzy NDTs to image segmentation by extracting the most informative and interpretable features)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-11752-2_8)   - **""<PUBLICATION>Neural Decision Trees</PUBLICATION>""**, arXiv, 2017     - Randall Balestriero *(map similar inputs to the same hash value)*     - [[Paper]](https://arxiv.org/abs/1702.07360)   - **""<PUBLICATION>Neuro-fuzzy decision trees</PUBLICATION>""**, Neural Systems, 2006     - Rajen B Bhatt, M Gopal *(employs fuzzy ID3 algorithm for DT-renovation)*     - [[Paper]](https://www.worldscientific.com/doi/abs/10.1142/s0129065706000470)  - ***Incremental learning***   - **""<PUBLICATION>Hybrid decision tree</PUBLICATION>""**, Knowledge-Based Systems, 2022     - Zhi-Hua Zhou, Zhao-Qian Chen *(two example-incremental tasks, one hypothesis-driven constructive induction mechanism)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0950705102000382)   - **""<PUBLICATION>Soft decision trees</PUBLICATION>""**, <CONFERENCE>ICPR</CONFERENCE>, 2012     - Ozan Irsoy, Olcay Taner Yƒ±ldƒ±z, Ethem Alpaydƒ±n *(sigmoid-based FDT whose splits are made by checking if there is an improvement over the validation set)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6460506)   - **""<PUBLICATION>A Neural Tree with Partial Incremental Learning Capability</PUBLICATION>""**, <CONFERENCE>ICMLC</CONFERENCE>, 2007     - Mu-Chun Su, Hsu-Hsun Lo *(choose a target class at each internal node and train a small NN to separate the positive patterns from negative ones)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/4370106)  ##### *3.1.2 Bigot NDTs.* A bigot NDT tends to have a pre-defined structure and determine leaves (pure classes or fixed class distributions) by priors or algorithms such that induce the class hierarchy","A comparison between NDTs according to whether it implements a class hierarchy and whether it is data-driven: <p align='center'>     </br>     <img src='NDT_hierarchy.png' width='1000'> </p>  - ***Informativeness-related splitting functions with NN-based routers***   - **""Neural trees-using neural nets in a tree classifier structure""**, ICASSP, 1991     - J-E Stromberg, Jalel Zrida, Alf Isaksson *(train a small NN at each internal node, which singles out one unique class that gains the most class purity)*     - [[Paper]](https://www.computer.org/csdl/proceedings-article/icassp/1991/0003137/12OmNzdoMSH)   - **""Evolutionary design of neural network tree-integration of decision tree, neural network and GA""**, CEC, 2001     - Qiangfu Zhao *(use genetic algorithms to maximize the information gain ratio at each internal node)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/934395)   - **""Hybrid decision tree""**, Knowledge-Based Systems, 2002     - Zhi-Hua Zhou, Zhao-Qian Chen *(Divide the instance space into ordered attributes and unordered attributes)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0950705102000382)   - **""Classification trees with neural network feature extraction""**, CVPR, 1992     - Heng Guo, Saul B Gelfand *(find two clusters that minimizes a Gini impurity criterion, then find a good split through back-propagation)*     - [[Paper]](https://www.computer.org/csdl/proceedings-article/cvpr/1992/00223275/12OmNzBOilt)  - ***Decrease the classification error at each node to be extended***   - **""Growing and pruning neural tree networks""**, IEEE Transactions on Computers, 1993     - A Sakar *et al.* *(Divide the instance space into ordered attributes and unordered attributes)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/210172)  - ***Grows during competitive learning***   - **""Competitive neural trees for pattern classification""**,  IEEE Transactions on Neural Networks, 1998     - Sven Behnke, Nicolaos B Karayiannis     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/728387)  - ***Map examples into the label embedding space and predicts using a NDT***   - **""Label Embedding Trees for Large Multi-Class Tasks""**, NIPS, 2010     - Samy Bengio, Jason Weston, David Grangier     - [[Paper]](https://proceedings.neurips.cc/paper/2010/hash/06138bc5af6023646ede0e1f7c1eac75-Abstract.html)   - **""Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition""**, NIPS, 2011     - Jia Deng, Sanjeev Satheesh, Alexander Berg, Fei Li     - [[Paper]](https://proceedings.neurips.cc/paper/2011/hash/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html)  - ***Apply data-driven architectures to fuzzy NDTs***   - **""Globally optimal fuzzy decision trees for classification and regression""**, IEEE Transactions on Pattern Analysis and Machine Intelligence, 1999     - Alberto Su√°rez, James F Lutsko *(superimpose a fuzzy structure over the skeleton of a CART tree and introduce a global optimization algorithm)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/817409)   - **""Budding Trees""**, ICPR, 2014     - Ozan Irsoy, Olcay Taner Yildiz, Ethem Alpaydin *(a FDT model that can be dynamically adjusted)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6977328) [[Code]](https://github.com/oir/budding-tree)   - **""Convolutional Decision Trees for Feature Learning and Segmentation""**, GCPR, 2014    - Dmitry Laptev, Joachim M Buhmann *(applies fuzzy NDTs to image segmentation by extracting the most informative and interpretable features)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-11752-2_8)   - **""Neural Decision Trees""**, arXiv, 2017     - Randall Balestriero *(map similar inputs to the same hash value)*     - [[Paper]](https://arxiv.org/abs/1702.07360)   - **""Neuro-fuzzy decision trees""**, Neural Systems, 2006     - Rajen B Bhatt, M Gopal *(employs fuzzy ID3 algorithm for DT-renovation)*     - [[Paper]](https://www.worldscientific.com/doi/abs/10.1142/s0129065706000470)  - ***Incremental learning***   - **""Hybrid decision tree""**, Knowledge-Based Systems, 2022     - Zhi-Hua Zhou, Zhao-Qian Chen *(two example-incremental tasks, one hypothesis-driven constructive induction mechanism)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0950705102000382)   - **""Soft decision trees""**, ICPR, 2012     - Ozan Irsoy, Olcay Taner Yƒ±ldƒ±z, Ethem Alpaydƒ±n *(sigmoid-based FDT whose splits are made by checking if there is an improvement over the validation set)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6460506)   - **""A Neural Tree with Partial Incremental Learning Capability""**, ICMLC, 2007     - Mu-Chun Su, Hsu-Hsun Lo *(choose a target class at each internal node and train a small NN to separate the positive patterns from negative ones)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/4370106)  ##### *3.1.2 Bigot NDTs.* A bigot NDT tends to have a pre-defined structure and determine leaves (pure classes or fixed class distributions) by priors or algorithms such that induce the class hierarchy","A comparison between NDTs according to whether it implements a class hierarchy and whether it is data-driven: <p align='center'>     </br>     <img src='NDT_hierarchy.png' width='1000'> </p>  - ***Informativeness-related splitting functions with NN-based routers***   - **""<PUBLICATION>Neural trees-using neural nets in a tree classifier structure</PUBLICATION>""**, <CONFERENCE>ICASSP</CONFERENCE>, 1991     - J-E Stromberg, Jalel Zrida, Alf Isaksson *(train a small NN at each internal node, which singles out one unique class that gains the most class purity)*     - [[Paper]](https://www.computer.org/csdl/proceedings-article/icassp/1991/0003137/12OmNzdoMSH)   - **""<PUBLICATION>Evolutionary design of neural network tree-integration of decision tree, neural network and GA</PUBLICATION>""**, <CONFERENCE>CEC</CONFERENCE>, 2001     - Qiangfu Zhao *(use genetic algorithms to maximize the information gain ratio at each internal node)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/934395)   - **""<PUBLICATION>Hybrid decision tree</PUBLICATION>""**, <PUBLICATION>Knowledge-Based Systems</PUBLICATION>, 2002     - Zhi-Hua Zhou, Zhao-Qian Chen *(Divide the instance space into ordered attributes and unordered attributes)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0950705102000382)   - **""<PUBLICATION>Classification trees with neural network feature extraction</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 1992     - Heng Guo, Saul B Gelfand *(find two clusters that minimizes a Gini impurity criterion, then find a good split through back-propagation)*     - [[Paper]](https://www.computer.org/csdl/proceedings-article/cvpr/1992/00223275/12OmNzBOilt)  - ***Decrease the classification error at each node to be extended***   - **""<PUBLICATION>Growing and pruning neural tree networks</PUBLICATION>""**, <PUBLICATION>IEEE Transactions on Computers</PUBLICATION>, 1993     - A Sakar *et al.* *(Divide the instance space into ordered attributes and unordered attributes)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/210172)  - ***Grows during competitive learning***   - **""<PUBLICATION>Competitive neural trees for pattern classification</PUBLICATION>""**,  <PUBLICATION>IEEE Transactions on Neural Networks</PUBLICATION>, 1998     - Sven Behnke, Nicolaos B Karayiannis     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/728387)  - ***Map examples into the label embedding space and predicts using a NDT***   - **""<PUBLICATION>Label Embedding Trees for Large Multi-Class Tasks</PUBLICATION>""**, <CONFERENCE>NIPS</CONFERENCE>, 2010     - Samy Bengio, Jason Weston, David Grangier     - [[Paper]](https://proceedings.neurips.cc/paper/2010/hash/06138bc5af6023646ede0e1f7c1eac75-Abstract.html)   - **""<PUBLICATION>Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</PUBLICATION>""**, <CONFERENCE>NIPS</CONFERENCE>, 2011     - Jia Deng, Sanjeev Satheesh, Alexander Berg, Fei Li     - [[Paper]](https://proceedings.neurips.cc/paper/2011/hash/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html)  - ***Apply data-driven architectures to fuzzy NDTs***   - **""<PUBLICATION>Globally optimal fuzzy decision trees for classification and regression</PUBLICATION>""**, <PUBLICATION>IEEE Transactions on Pattern Analysis and Machine Intelligence</PUBLICATION>, 1999     - Alberto Su√°rez, James F Lutsko *(superimpose a fuzzy structure over the skeleton of a CART tree and introduce a global optimization algorithm)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/817409)   - **""<PUBLICATION>Budding Trees</PUBLICATION>""**, <CONFERENCE>ICPR</CONFERENCE>, 2014     - Ozan Irsoy, Olcay Taner Yildiz, Ethem Alpaydin *(a FDT model that can be dynamically adjusted)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6977328) [[Code]](https://github.com/oir/budding-tree)   - **""<PUBLICATION>Convolutional Decision Trees for Feature Learning and Segmentation</PUBLICATION>""**, <CONFERENCE>GCPR</CONFERENCE>, 2014    - Dmitry Laptev, Joachim M Buhmann *(applies fuzzy NDTs to image segmentation by extracting the most informative and interpretable features)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-319-11752-2_8)   - **""<PUBLICATION>Neural Decision Trees</PUBLICATION>""**, <PUBLICATION>arXiv</PUBLICATION>, 2017     - Randall Balestriero *(map similar inputs to the same hash value)*     - [[Paper]](https://arxiv.org/abs/1702.07360)   - **""<PUBLICATION>Neuro-fuzzy decision trees</PUBLICATION>""**, <PUBLICATION>Neural Systems</PUBLICATION>, 2006     - Rajen B Bhatt, M Gopal *(employs fuzzy ID3 algorithm for DT-renovation)*     - [[Paper]](https://www.worldscientific.com/doi/abs/10.1142/s0129065706000470)  - ***Incremental learning***   - **""<PUBLICATION>Hybrid decision tree</PUBLICATION>""**, <PUBLICATION>Knowledge-Based Systems</PUBLICATION>, 2022     - Zhi-Hua Zhou, Zhao-Qian Chen *(two example-incremental tasks, one hypothesis-driven constructive induction mechanism)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0950705102000382)   - **""<PUBLICATION>Soft decision trees</PUBLICATION>""**, <CONFERENCE>ICPR</CONFERENCE>, 2012     - Ozan Irsoy, Olcay Taner Yƒ±ldƒ±z, Ethem Alpaydƒ±n *(sigmoid-based FDT whose splits are made by checking if there is an improvement over the validation set)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6460506)   - **""<PUBLICATION>A Neural Tree with Partial Incremental Learning Capability</PUBLICATION>""**, <CONFERENCE>ICMLC</CONFERENCE>, 2007     - Mu-Chun Su, Hsu-Hsun Lo *(choose a target class at each internal node and train a small NN to separate the positive patterns from negative ones)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/4370106)  ##### *3.1.2 Bigot NDTs.* A bigot NDT tends to have a pre-defined structure and determine leaves (pure classes or fixed class distributions) by priors or algorithms such that induce the class hierarchy",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\8f446d25.txt,1.0
74,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\cdb4ee2a.txt,1.0
75,"- ***Bigot NDTs with pure leaves***   - **""Structure-driven induction of decision tree classifiers through neural learning""**, Pattern Recognition, 1997     - Ishwar K Sethi *et al.* *(randomly initialized leaf classes)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0031320397000058)   - **""NBDT: Neural-Backed Decision Trees""**, ICCV, 2020     - Alvin Wan *et al.* *(assign a specific concept to each terminal and intermediate node by WordNet)*     - [[Paper]](https://arxiv.org/abs/2004.00221) [[Code]](https://github.com/alvinwan/nbdt-pytorch-image-models)  - ***Bigot NDTs with determined leaf distributions***   - **""Distilling a Neural Network Into a Soft Decision Tree""**, AI*IA, 2017     - Nicholas Frosst, Geoffrey Hinton *(jointly optimize leaves and other parameters via back-propagation)*     - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)   - **""Policy-gradient Methods for Decision Trees""**, ESANN, 2016     - Aur√©lia L√©on, Ludovic Denoyer *(use Monte Carlo approximation to expect the gradient of the objective function)*     - [[Paper]](http://www.smart-labex.fr/publications/pdf/56eab488c05ef.pdf)   - **""Deep Neural Decision Forests""**, ICCV, 2015     - Peter Kontschieder *et al.* *(propose a derivative-free strategy to solely optimize the leaf parameters, as a convex optimization problem)*     - [[Paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.html) [[Code]](https://github.com/jingxil/Neural-Decision-Forests)   - **""Deep Regression Forests for Age Estimation""**, CVPR, 2018     - Wei Shen *et al.* *(regression forests for age estimation)*     - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Regression_Forests_CVPR_2018_paper.html) [[Code]](https://github.com/Sakura03/age_trans)   - **""Neural Decision Forests for Semantic Image Labelling""**, CVPR, 2014     - Samuel Rota Bulo, Peter Kontschieder *(decision forests for semantic image labelling)*     - [[Paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Bulo_Neural_Decision_Forests_2014_CVPR_paper.html)   - **""Neural Prototype Trees for Interpretable Fine-Grained Image Recognition""**, CVPR, 2021     - WMeike Nauta, Ron van Bree, Christin Seifert *(prototype-based ante-hoc methods)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Nauta_Neural_Prototype_Trees_for_Interpretable_Fine-Grained_Image_Recognition_CVPR_2021_paper.html?","- ***Bigot NDTs with pure leaves***   - **""<PUBLICATION>Structure-driven induction of decision tree classifiers through neural learning</PUBLICATION>""**, Pattern Recognition, 1997     - Ishwar K Sethi *et al.* *(randomly initialized leaf classes)*     - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0031320397000058)   - **""<PUBLICATION>NBDT: Neural-Backed Decision Trees</PUBLICATION>""**, <CONFERENCE>ICCV</CONFERENCE>, 2020     - Alvin Wan *et al.* *(assign a specific concept to each terminal and intermediate node by WordNet)*     - [[Paper]](https://arxiv.org/abs/2004.00221) [[Code]](https://github.com/alvinwan/nbdt-pytorch-image-models)  - ***Bigot NDTs with determined leaf distributions***   - **""<PUBLICATION>Distilling a Neural Network Into a Soft Decision Tree</PUBLICATION>""**, <CONFERENCE>AI*IA</CONFERENCE>, 2017     - Nicholas Frosst, Geoffrey Hinton *(jointly optimize leaves and other parameters via back-propagation)*     - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)   - **""<PUBLICATION>Policy-gradient Methods for Decision Trees</PUBLICATION>""**, <CONFERENCE>ESANN</CONFERENCE>, 2016     - Aur√©lia L√©on, Ludovic Denoyer *(use Monte Carlo approximation to expect the gradient of the objective function)*     - [[Paper]](http://www.smart-labex.fr/publications/pdf/56eab488c05ef.pdf)   - **""<PUBLICATION>Deep Neural Decision Forests</PUBLICATION>""**, <CONFERENCE>ICCV</CONFERENCE>, 2015     - Peter Kontschieder *et al.* *(propose a derivative-free strategy to solely optimize the leaf parameters, as a convex optimization problem)*     - [[Paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.html) [[Code]](https://github.com/jingxil/Neural-Decision-Forests)   - **""<PUBLICATION>Deep Regression Forests for Age Estimation</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2018     - Wei Shen *et al.* *(regression forests for age estimation)*     - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Regression_Forests_CVPR_2018_paper.html) [[Code]](https://github.com/Sakura03/age_trans)   - **""<PUBLICATION>Neural Decision Forests for Semantic Image Labelling</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2014     - Samuel Rota Bulo, Peter Kontschieder *(decision forests for semantic image labelling)*     - [[Paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Bulo_Neural_Decision_Forests_2014_CVPR_paper.html)   - **""<PUBLICATION>Neural Prototype Trees for Interpretable Fine-Grained Image Recognition</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2021     - WMeike Nauta, Ron van Bree, Christin Seifert *(prototype-based ante-hoc methods)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Nauta_Neural_Prototype_Trees_for_Interpretable_Fine-Grained_Image_Recognition_CVPR_2021_paper.html?","- ***Bigot NDTs with pure leaves***  
  - **""Structure-driven induction of decision tree classifiers through neural learning""**, Pattern Recognition, 1997  
    - Ishwar K Sethi *et al.* *(randomly initialized leaf classes)*  
    - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0031320397000058)  
  - **""NBDT: Neural-Backed Decision Trees""**, ICCV, 2020  
    - Alvin Wan *et al.* *(assign a specific concept to each terminal and intermediate node by WordNet)*  
    - [[Paper]](https://arxiv.org/abs/2004.00221) [[Code]](https://github.com/alvinwan/nbdt-pytorch-image-models)  
- ***Bigot NDTs with determined leaf distributions***  
  - **""Distilling a Neural Network Into a Soft Decision Tree""**, AI*IA, 2017  
    - Nicholas Frosst, Geoffrey Hinton *(jointly optimize leaves and other parameters via back-propagation)*  
    - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)  
  - **""Policy-gradient Methods for Decision Trees""**, ESANN, 2016  
    - Aur√©lia L√©on, Ludovic Denoyer *(use Monte Carlo approximation to expect the gradient of the objective function)*  
    - [[Paper]](http://www.smart-labex.fr/publications/pdf/56eab488c05ef.pdf)  
  - **""Deep Neural Decision Forests""**, ICCV, 2015  
    - Peter Kontschieder *et al.* *(propose a derivative-free strategy to solely optimize the leaf parameters, as a convex optimization problem)*  
    - [[Paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.html) [[Code]](https://github.com/jingxil/Neural-Decision-Forests)  
  - **""Deep Regression Forests for Age Estimation""**, CVPR, 2018  
    - Wei Shen *et al.* *(regression forests for age estimation)*  
    - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Regression_Forests_CVPR_2018_paper.html) [[Code]](https://github.com/Sakura03/age_trans)  
  - **""Neural Decision Forests for Semantic Image Labelling""**, CVPR, 2014  
    - Samuel Rota Bulo, Peter Kontschieder *(decision forests for semantic image labelling)*  
    - [[Paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Bulo_Neural_Decision_Forests_2014_CVPR_paper.html)  
  - **""Neural Prototype Trees for Interpretable Fine-Grained Image Recognition""**, CVPR, 2021  
    - WMeike Nauta, Ron van Bree, Christin Seifert *(prototype-based ante-hoc methods)*  
    - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Nauta_Neural_Prototype_Trees_for_Interpretable_Fine-Grained_Image_Recognition_CVPR_2021_paper.html?","- ***Bigot NDTs with pure leaves***  
  - **""<PUBLICATION>Structure-driven induction of decision tree classifiers through neural learning</PUBLICATION>""**, <PUBLICATION>Pattern Recognition</PUBLICATION>, 1997  
    - Ishwar K Sethi *et al.* *(randomly initialized leaf classes)*  
    - [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0031320397000058)  
  - **""<PUBLICATION>NBDT: Neural-Backed Decision Trees</PUBLICATION>""**, <CONFERENCE>ICCV</CONFERENCE>, 2020  
    - Alvin Wan *et al.* *(assign a specific concept to each terminal and intermediate node by WordNet)*  
    - [[Paper]](https://arxiv.org/abs/2004.00221) [[Code]](https://github.com/alvinwan/nbdt-pytorch-image-models)  
- ***Bigot NDTs with determined leaf distributions***  
  - **""<PUBLICATION>Distilling a Neural Network Into a Soft Decision Tree</PUBLICATION>""**, <CONFERENCE>AI*IA</CONFERENCE>, 2017  
    - Nicholas Frosst, Geoffrey Hinton *(jointly optimize leaves and other parameters via back-propagation)*  
    - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)  
  - **""<PUBLICATION>Policy-gradient Methods for Decision Trees</PUBLICATION>""**, <CONFERENCE>ESANN</CONFERENCE>, 2016  
    - Aur√©lia L√©on, Ludovic Denoyer *(use Monte Carlo approximation to expect the gradient of the objective function)*  
    - [[Paper]](http://www.smart-labex.fr/publications/pdf/56eab488c05ef.pdf)  
  - **""<PUBLICATION>Deep Neural Decision Forests</PUBLICATION>""**, <CONFERENCE>ICCV</CONFERENCE>, 2015  
    - Peter Kontschieder *et al.* *(propose a derivative-free strategy to solely optimize the leaf parameters, as a convex optimization problem)*  
    - [[Paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.html) [[Code]](https://github.com/jingxil/Neural-Decision-Forests)  
  - **""<PUBLICATION>Deep Regression Forests for Age Estimation</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2018  
    - Wei Shen *et al.* *(regression forests for age estimation)*  
    - [[Paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Regression_Forests_CVPR_2018_paper.html) [[Code]](https://github.com/Sakura03/age_trans)  
  - **""<PUBLICATION>Neural Decision Forests for Semantic Image Labelling</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2014  
    - Samuel Rota Bulo, Peter Kontschieder *(decision forests for semantic image labelling)*  
    - [[Paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Bulo_Neural_Decision_Forests_2014_CVPR_paper.html)  
  - **""<PUBLICATION>Neural Prototype Trees for Interpretable Fine-Grained Image Recognition</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2021  
    - WMeike Nauta, Ron van Bree, Christin Seifert *(prototype-based ante-hoc methods)*  
    - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Nauta_Neural_Prototype_Trees_for_Interpretable_Fine-Grained_Image_Recognition_CVPR_2021_paper.html?",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\f3085c81.txt,0.9904015670910872
76,"ref=https://githubhelp.com) [[Code]](https://github.com/M-Nauta/ProtoTree)   - **""Learn decision trees with deep visual primitives""**, SSRN, 2022     - Mengqi Xue, *et al*     - [[Paper]](https://papers.ssrn.com/sol3/papers.cfm?","ref=https://githubhelp.com) [[Code]](https://github.com/M-Nauta/ProtoTree)   - **""<PUBLICATION>Learn decision trees with deep visual primitives</PUBLICATION>""**, <CONFERENCE>SSRN</CONFERENCE>, 2022     - Mengqi Xue, *et al*     - [[Paper]](https://papers.ssrn.com/sol3/papers.cfm?","ref=https://githubhelp.com) [[Code]](https://github.com/M-Nauta/ProtoTree)   - **""Learn decision trees with deep visual primitives""**, SSRN, 2022     - Mengqi Xue, *et al*     - [[Paper]](https://papers.ssrn.com/sol3/papers.cfm?","ref=https://githubhelp.com) [[Code]](https://github.com/M-Nauta/ProtoTree)   - **""Learn decision trees with deep visual primitives""**, <PUBLICATION>SSRN</PUBLICATION>, 2022     - Mengqi Xue, *et al*     - [[Paper]](https://papers.ssrn.com/sol3/papers.cfm?",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\de085114.txt,1.0
77,"abstract_id=4210199)   - ***Bigot NDTs for knowledge distillation***   - **""Tree-Like Decision Distillation""**, CVPR, 2021     - Jie Song *et al.* *(layer-wise dissect the decision process of a DNN)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Song_Tree-Like_Decision_Distillation_CVPR_2021_paper.html)   - **""Tree-Like Branching Network for Multi-class Classification""**, LNNS, 2021     - Mengqi Xue, Jie Song, Li Sun, Mingli Song *(mine the underlying category relationships from a trained teacher network and determines the appropriate layers on which specialized branches grow)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-93247-3_18)   - **""Distilling a Neural Network Into a Soft Decision Tree""**, AI*IA, 2017     - Nicholas Frosst, Geoffrey Hinton *(use a trained NN to provide soft targets for training a fuzzy NDT)*     - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)   - **""TNT: An Interpretable Tree-Network-Tree Learning Framework using Knowledge Distillation""**, Entropy, 2020     - Jiawei Li *et al.* *(transfer knowledge between tree models and DNNs)*     - [[Paper]](https://www.mdpi.com/1099-4300/22/11/1203)   - **""KDExplainer: A Task-oriented Attention Model for Explaining Knowledge Distillation""**, arXiv, 2021     - Mengqi Xue *et al.*     - [[Paper]](https://arxiv.org/abs/2105.04181)  #### 3.2 Expert NDTs (NDTs without Class Hierarchies) NDTs without class hierarchies restrain themselves little and perform arbitrary predictions at the leaves.","abstract_id=4210199)   - ***Bigot NDTs for knowledge distillation***   - **""<PUBLICATION>Tree-Like Decision Distillation</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2021     - Jie Song *et al.* *(layer-wise dissect the decision process of a DNN)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Song_Tree-Like_Decision_Distillation_CVPR_2021_paper.html)   - **""<PUBLICATION>Tree-Like Branching Network for Multi-class Classification</PUBLICATION>""**, <CONFERENCE>LNNS</CONFERENCE>, 2021     - Mengqi Xue, Jie Song, Li Sun, Mingli Song *(mine the underlying category relationships from a trained teacher network and determines the appropriate layers on which specialized branches grow)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-93247-3_18)   - **""<PUBLICATION>Distilling a Neural Network Into a Soft Decision Tree</PUBLICATION>""**, <CONFERENCE>AI*IA</CONFERENCE>, 2017     - Nicholas Frosst, Geoffrey Hinton *(use a trained NN to provide soft targets for training a fuzzy NDT)*     - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)   - **""<PUBLICATION>TNT: An Interpretable Tree-Network-Tree Learning Framework using Knowledge Distillation</PUBLICATION>""**, Entropy, 2020     - Jiawei Li *et al.* *(transfer knowledge between tree models and DNNs)*     - [[Paper]](https://www.mdpi.com/1099-4300/22/11/1203)   - **""<PUBLICATION>KDExplainer: A Task-oriented Attention Model for Explaining Knowledge Distillation</PUBLICATION>""**, arXiv, 2021     - Mengqi Xue *et al.*     - [[Paper]](https://arxiv.org/abs/2105.04181)  #### 3.2 Expert NDTs (NDTs without Class Hierarchies) NDTs without class hierarchies restrain themselves little and perform arbitrary predictions at the leaves.","abstract_id=4210199)   - ***Bigot NDTs for knowledge distillation***   - **""Tree-Like Decision Distillation""**, CVPR, 2021     - Jie Song *et al.* *(layer-wise dissect the decision process of a DNN)*     - [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Song_Tree-Like_Decision_Distillation_CVPR_2021_paper.html)   - **""Tree-Like Branching Network for Multi-class Classification""**, LNNS, 2021     - Mengqi Xue, Jie Song, Li Sun, Mingli Song *(mine the underlying category relationships from a trained teacher network and determines the appropriate layers on which specialized branches grow)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-93247-3_18)   - **""Distilling a Neural Network Into a Soft Decision Tree""**, AI*IA, 2017     - Nicholas Frosst, Geoffrey Hinton *(use a trained NN to provide soft targets for training a fuzzy NDT)*     - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)   - **""TNT: An Interpretable Tree-Network-Tree Learning Framework using Knowledge Distillation""**, Entropy, 2020     - Jiawei Li *et al.* *(transfer knowledge between tree models and DNNs)*     - [[Paper]](https://www.mdpi.com/1099-4300/22/11/1203)   - **""KDExplainer: A Task-oriented Attention Model for Explaining Knowledge Distillation""**, arXiv, 2021     - Mengqi Xue *et al.*     - [[Paper]](https://arxiv.org/abs/2105.04181)  #### 3.2 Expert NDTs (NDTs without Class Hierarchies) NDTs without class hierarchies restrain themselves little and perform arbitrary predictions at the leaves.","abstract_id=4210199)   - ***Bigot NDTs for knowledge distillation***   - **""<PUBLICATION>Tree-Like Decision Distillation</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2021     - Jie Song *et al.* *(layer-wise dissect the decision process of a DNN)*     - [[Paper]](https://openaccess.thecvf.com/content/<CONFERENCE>CVPR</CONFERENCE>2021/html/Song_Tree-Like_Decision_Distillation_<CONFERENCE>CVPR</CONFERENCE>_2021_paper.html)   - **""<PUBLICATION>Tree-Like Branching Network for Multi-class Classification</PUBLICATION>""**, <PUBLICATION>LNNS</PUBLICATION>, 2021     - Mengqi Xue, Jie Song, Li Sun, Mingli Song *(mine the underlying category relationships from a trained teacher network and determines the appropriate layers on which specialized branches grow)*     - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-93247-3_18)   - **""<PUBLICATION>Distilling a Neural Network Into a Soft Decision Tree</PUBLICATION>""**, <CONFERENCE>AI*IA</CONFERENCE>, 2017     - Nicholas Frosst, Geoffrey Hinton *(use a trained NN to provide soft targets for training a fuzzy NDT)*     - [[Paper]](https://arxiv.org/abs/1711.09784) [[Code]](https://github.com/kimhc6028/soft-decision-tree)   - **""<PUBLICATION>TNT: An Interpretable Tree-Network-Tree Learning Framework using Knowledge Distillation</PUBLICATION>""**, <PUBLICATION>Entropy</PUBLICATION>, 2020     - Jiawei Li *et al.* *(transfer knowledge between tree models and DNNs)*     - [[Paper]](https://www.mdpi.com/1099-4300/22/11/1203)   - **""<PUBLICATION>KDExplainer: A Task-oriented Attention Model for Explaining Knowledge Distillation</PUBLICATION>""**, <PUBLICATION>arXiv</PUBLICATION>, 2021     - Mengqi Xue *et al.*     - [[Paper]](https://arxiv.org/abs/2105.04181)  #### 3.2 Expert NDTs (NDTs without Class Hierarchies) NDTs without class hierarchies restrain themselves little and perform arbitrary predictions at the leaves.",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\b8ecc33e.txt,1.0
78,Their  leaf nodes are usually classifiers rather than determined classes or distributions,Their  leaf nodes are usually classifiers rather than determined classes or distributions,Their  leaf nodes are usually classifiers rather than determined classes or distributions,Their  leaf nodes are usually classifiers rather than determined classes or distributions,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\9c7cbcfb.txt,1.0
79,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\cdb4ee2a.txt,1.0
80,"- ***Hierarchical Mixtures of Experts (HME)***   - **""Hierarchical Mixtures of Experts and the EM Algorithm""**, Neural computation, 1994      - Michael I Jordan, Robert A Jacobs *(the original HME, a tree-structured model for regression and classification.)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6796382)   - **""Classification using hierarchical mixtures of experts""**, NNSP, 1994     - Steve R Waterhouse, Anthony J Robinson *(each leaf expert is non-linear and performs multi-way classification)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/366050)   - **""Bayesian Hierarchical Mixtures of Experts""**, arXiv, 2012     - Christopher M Bishop, Naonori Ueda, Steve Waterhouse *(bayesian treatments of the HME model to prevent the severe overfitting caused by maximum likelihood)*     - [[Paper]](https://arxiv.org/abs/1212.2447)     - ***Generalized HMEs in advanced frameworks***   - **""Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization""**, CVPR, 2020     - Ruyi Ji *et al.* *(incorporate convolutional operations along edges and use attention transformer modules to capture discriminative features)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ji_Attention_Convolutional_Binary_Neural_Tree_for_Fine-Grained_Visual_Categorization_CVPR_2020_paper.html) [[Code]](https://isrc.iscas.ac.cn/gitlab/research/acnet)   - **""NDT: Neual Decision Tree Towards Fully Functioned Neural Graph""**, arXiv, 2017     - Han Xiao *(reformulate the non-differentiable information gain in the form of Dirac symbol and approximate it as a continuous function)*     - [[Paper]](https://arxiv.org/abs/1712.05934)   - **""Decision Forests, Convolutional Networks and the Models in-Between""**, arXiv, 2016     - Yani Ioannou *et al.* *(hybrid model between decision forests and convolutional networks)*     - [[Paper]](https://arxiv.org/abs/1603.01250)   - **""Deep Neural Decision Trees""**, arXiv, 2018     - Yongxin Yang, Irene Garcia Morillo, Timothy M Hospedales *(bin each feature of the input instance and determine the leaf node it will arrive)*     - [[Paper]](https://arxiv.org/abs/1806.06988) [[Code]](https://github.com/wOOL/DNDT)   - **""ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder""**, ICML, 2022     - Sangwon Kim, Jaeyeal Nam, Byoung Chul Ko *(transformer version of ProtoTree with expert leaves)*     - [[Paper]](https://proceedings.mlr.press/v162/kim22g.html) [[Code]](https://github.com/jumpsnack/ViT-NeT)  - ***Expert NDTs with architecture search phase***   - **""Adaptive Neural Trees""**, ICML, 2019     - Ryutaro Tanno *et al.* *(greedily choosing the best option between going deeper and splitting the input space)*     - [[Paper]](http://proceedings.mlr.press/v97/tanno19a.html?","- ***Hierarchical Mixtures of Experts (HME)***   - **""<PUBLICATION>Hierarchical Mixtures of Experts and the EM Algorithm</PUBLICATION>""**, Neural computation, 1994      - Michael I Jordan, Robert A Jacobs *(the original HME, a tree-structured model for regression and classification.)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6796382)   - **""<PUBLICATION>Classification using hierarchical mixtures of experts</PUBLICATION>""**, <CONFERENCE>NNSP</CONFERENCE>, 1994     - Steve R Waterhouse, Anthony J Robinson *(each leaf expert is non-linear and performs multi-way classification)*     - [[Paper]](https://ieeexplore.ieee.org/abstract/document/366050)   - **""<PUBLICATION>Bayesian Hierarchical Mixtures of Experts</PUBLICATION>""**, arXiv, 2012     - Christopher M Bishop, Naonori Ueda, Steve Waterhouse *(bayesian treatments of the HME model to prevent the severe overfitting caused by maximum likelihood)*     - [[Paper]](https://arxiv.org/abs/1212.2447)     - ***Generalized HMEs in advanced frameworks***   - **""<PUBLICATION>Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2020     - Ruyi Ji *et al.* *(incorporate convolutional operations along edges and use attention transformer modules to capture discriminative features)*     - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ji_Attention_Convolutional_Binary_Neural_Tree_for_Fine-Grained_Visual_Categorization_CVPR_2020_paper.html) [[Code]](https://isrc.iscas.ac.cn/gitlab/research/acnet)   - **""<PUBLICATION>NDT: Neual Decision Tree Towards Fully Functioned Neural Graph</PUBLICATION>""**, arXiv, 2017     - Han Xiao *(reformulate the non-differentiable information gain in the form of Dirac symbol and approximate it as a continuous function)*     - [[Paper]](https://arxiv.org/abs/1712.05934)   - **""<PUBLICATION>Decision Forests, Convolutional Networks and the Models in-Between</PUBLICATION>""**, arXiv, 2016     - Yani Ioannou *et al.* *(hybrid model between decision forests and convolutional networks)*     - [[Paper]](https://arxiv.org/abs/1603.01250)   - **""<PUBLICATION>Deep Neural Decision Trees</PUBLICATION>""**, arXiv, 2018     - Yongxin Yang, Irene Garcia Morillo, Timothy M Hospedales *(bin each feature of the input instance and determine the leaf node it will arrive)*     - [[Paper]](https://arxiv.org/abs/1806.06988) [[Code]](https://github.com/wOOL/DNDT)   - **""<PUBLICATION>ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder</PUBLICATION>""**, <CONFERENCE>ICML</CONFERENCE>, 2022     - Sangwon Kim, Jaeyeal Nam, Byoung Chul Ko *(transformer version of ProtoTree with expert leaves)*     - [[Paper]](https://proceedings.mlr.press/v162/kim22g.html) [[Code]](https://github.com/jumpsnack/ViT-NeT)  - ***Expert NDTs with architecture search phase***   - **""<PUBLICATION>Adaptive Neural Trees</PUBLICATION>""**, <CONFERENCE>ICML</CONFERENCE>, 2019     - Ryutaro Tanno *et al.* *(greedily choosing the best option between going deeper and splitting the input space)*     - [[Paper]](http://proceedings.mlr.press/v97/tanno19a.html?","- ***Hierarchical Mixtures of Experts (HME)***  
  - **""Hierarchical Mixtures of Experts and the EM Algorithm""**, Neural computation, 1994  
    - Michael I Jordan, Robert A Jacobs *(the original HME, a tree-structured model for regression and classification.)*  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6796382)  
  - **""Classification using hierarchical mixtures of experts""**, NNSP, 1994  
    - Steve R Waterhouse, Anthony J Robinson *(each leaf expert is non-linear and performs multi-way classification)*  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/366050)  
  - **""Bayesian Hierarchical Mixtures of Experts""**, arXiv, 2012  
    - Christopher M Bishop, Naonori Ueda, Steve Waterhouse *(bayesian treatments of the HME model to prevent the severe overfitting caused by maximum likelihood)*  
    - [[Paper]](https://arxiv.org/abs/1212.2447)  

- ***Generalized HMEs in advanced frameworks***  
  - **""Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization""**, CVPR, 2020  
    - Ruyi Ji *et al.* *(incorporate convolutional operations along edges and use attention transformer modules to capture discriminative features)*  
    - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ji_Attention_Convolutional_Binary_Neural_Tree_for_Fine-Grained_Visual_Categorization_CVPR_2020_paper.html) [[Code]](https://isrc.iscas.ac.cn/gitlab/research/acnet)  
  - **""NDT: Neual Decision Tree Towards Fully Functioned Neural Graph""**, arXiv, 2017  
    - Han Xiao *(reformulate the non-differentiable information gain in the form of Dirac symbol and approximate it as a continuous function)*  
    - [[Paper]](https://arxiv.org/abs/1712.05934)  
  - **""Decision Forests, Convolutional Networks and the Models in-Between""**, arXiv, 2016  
    - Yani Ioannou *et al.* *(hybrid model between decision forests and convolutional networks)*  
    - [[Paper]](https://arxiv.org/abs/1603.01250)  
  - **""Deep Neural Decision Trees""**, arXiv, 2018  
    - Yongxin Yang, Irene Garcia Morillo, Timothy M Hospedales *(bin each feature of the input instance and determine the leaf node it will arrive)*  
    - [[Paper]](https://arxiv.org/abs/1806.06988) [[Code]](https://github.com/wOOL/DNDT)  
  - **""ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder""**, ICML, 2022  
    - Sangwon Kim, Jaeyeal Nam, Byoung Chul Ko *(transformer version of ProtoTree with expert leaves)*  
    - [[Paper]](https://proceedings.mlr.press/v162/kim22g.html) [[Code]](https://github.com/jumpsnack/ViT-NeT)  

- ***Expert NDTs with architecture search phase***  
  - **""Adaptive Neural Trees""**, ICML, 2019  
    - Ryutaro Tanno *et al.* *(greedily choosing the best option between going deeper and splitting the input space)*  
    - [[Paper]](http://proceedings.mlr.press/v97/tanno19a.html?","- ***<PROJECT>Hierarchical Mixtures of Experts (HME)</PROJECT>***  
  - **""<PUBLICATION>Hierarchical Mixtures of Experts and the EM Algorithm</PUBLICATION>""**, <PUBLICATION>Neural computation</PUBLICATION>, 1994  
    - Michael I Jordan, Robert A Jacobs *(the original HME, a tree-structured model for regression and classification.)*  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6796382)  
  - **""<PUBLICATION>Classification using hierarchical mixtures of experts</PUBLICATION>""**, <CONFERENCE>NNSP</CONFERENCE>, 1994  
    - Steve R Waterhouse, Anthony J Robinson *(each leaf expert is non-linear and performs multi-way classification)*  
    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/366050)  
  - **""<PUBLICATION>Bayesian Hierarchical Mixtures of Experts</PUBLICATION>""**, <PUBLICATION>arXiv</PUBLICATION>, 2012  
    - Christopher M Bishop, Naonori Ueda, Steve Waterhouse *(bayesian treatments of the HME model to prevent the severe overfitting caused by maximum likelihood)*  
    - [[Paper]](https://arxiv.org/abs/1212.2447)  

- ***Generalized HMEs in advanced frameworks***  
  - **""<PUBLICATION>Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization</PUBLICATION>""**, <CONFERENCE>CVPR</CONFERENCE>, 2020  
    - Ruyi Ji *et al.* *(incorporate convolutional operations along edges and use attention transformer modules to capture discriminative features)*  
    - [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ji_Attention_Convolutional_Binary_Neural_Tree_for_Fine-Grained_Visual_Categorization_CVPR_2020_paper.html) [[Code]](https://isrc.iscas.ac.cn/gitlab/research/acnet)  
  - **""<PUBLICATION>NDT: Neual Decision Tree Towards Fully Functioned Neural Graph</PUBLICATION>""**, <PUBLICATION>arXiv</PUBLICATION>, 2017  
    - Han Xiao *(reformulate the non-differentiable information gain in the form of Dirac symbol and approximate it as a continuous function)*  
    - [[Paper]](https://arxiv.org/abs/1712.05934)  
  - **""<PUBLICATION>Decision Forests, Convolutional Networks and the Models in-Between</PUBLICATION>""**, <PUBLICATION>arXiv</PUBLICATION>, 2016  
    - Yani Ioannou *et al.* *(hybrid model between decision forests and convolutional networks)*  
    - [[Paper]](https://arxiv.org/abs/1603.01250)  
  - **""<PUBLICATION>Deep Neural Decision Trees</PUBLICATION>""**, <PUBLICATION>arXiv</PUBLICATION>, 2018  
    - Yongxin Yang, Irene Garcia Morillo, Timothy M Hospedales *(bin each feature of the input instance and determine the leaf node it will arrive)*  
    - [[Paper]](https://arxiv.org/abs/1806.06988) [[Code]](https://github.com/wOOL/DNDT)  
  - **""<PUBLICATION>ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder</PUBLICATION>""**, <CONFERENCE>ICML</CONFERENCE>, 2022  
    - Sangwon Kim, Jaeyeal Nam, Byoung Chul Ko *(transformer version of ProtoTree with expert leaves)*  
    - [[Paper]](https://proceedings.mlr.press/v162/kim22g.html) [[Code]](https://github.com/jumpsnack/ViT-NeT)  

- ***Expert NDTs with architecture search phase***  
  - **""<PUBLICATION>Adaptive Neural Trees</PUBLICATION>""**, <CONFERENCE>ICML</CONFERENCE>, 2019  
    - Ryutaro Tanno *et al.* *(greedily choosing the best option between going deeper and splitting the input space)*  
    - [[Paper]](http://proceedings.mlr.press/v97/tanno19a.html?",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\6e3e47c8.txt,0.989365473236441
81,"utm_campaign=piqcy&utm_medium=email&utm_source=Revue%20newsletter) [[Code]](https://github.com/rtanno21609/AdaptiveNeuralTrees)   - **""Self-Born Wiring for Neural Trees""**, ICCV, 2021     - Ying Chen *et al.* *(NDTs that are self-born from a large search space)*     - [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Self-Born_Wiring_for_Neural_Trees_ICCV_2021_paper.html) [[Code]](https://github.com/HollyLee2000/SeBoW-paddle)","utm_campaign=piqcy&utm_medium=email&utm_source=Revue%20newsletter) [[Code]](https://github.com/rtanno21609/AdaptiveNeuralTrees)   - **""<PUBLICATION>Self-Born Wiring for Neural Trees</PUBLICATION>""**, <CONFERENCE>ICCV</CONFERENCE>, 2021     - Ying Chen *et al.* *(NDTs that are self-born from a large search space)*     - [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Self-Born_Wiring_for_Neural_Trees_ICCV_2021_paper.html) [[Code]](https://github.com/HollyLee2000/SeBoW-paddle)","utm_campaign=piqcy&utm_medium=email&utm_source=Revue%20newsletter) [[Code]](https://github.com/rtanno21609/AdaptiveNeuralTrees)   - **""Self-Born Wiring for Neural Trees""**, ICCV, 2021     - Ying Chen *et al.* *(NDTs that are self-born from a large search space)*     - [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Self-Born_Wiring_for_Neural_Trees_ICCV_2021_paper.html) [[Code]](https://github.com/HollyLee2000/SeBoW-paddle)","utm_campaign=piqcy&utm_medium=email&utm_source=Revue%20newsletter) [[Code]](https://github.com/rtanno21609/AdaptiveNeuralTrees)   - **""<PUBLICATION>Self-Born Wiring for Neural Trees</PUBLICATION>""**, <CONFERENCE>ICCV</CONFERENCE>, 2021     - Ying Chen *et al.* *(NDTs that are self-born from a large search space)*     - [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Self-Born_Wiring_for_Neural_Trees_ICCV_2021_paper.html) [[Code]](https://github.com/HollyLee2000/SeBoW-paddle)",../results/deepseek-chat/prompt-0/zzz_zju-vipa_awesome-neural-trees_main_README.md.tsv\d5ceed03.txt,1.0
