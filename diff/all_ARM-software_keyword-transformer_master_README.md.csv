sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,"# Keyword Transformer: A Self-Attention Model for Keyword Spotting  <img src=""kwt.png"" alt=""drawing"" width=""200""/>  This is the official repository for the paper [Keyword Transformer: A Self-Attention Model for Keyword Spotting](https://arxiv.org/abs/2104.00769), presented at Interspeech 2021.","# <PUBLICATION>Keyword Transformer: A Self-Attention Model for Keyword Spotting</PUBLICATION>  <img src=""kwt.png"" alt=""drawing"" width=""200""/>  This is the official repository for the paper [<PUBLICATION>Keyword Transformer: A Self-Attention Model for Keyword Spotting</PUBLICATION>](https://arxiv.org/abs/2104.00769), presented at <CONFERENCE>Interspeech 2021</CONFERENCE>.","# Keyword Transformer: A Self-Attention Model for Keyword Spotting  <img src=""kwt.png"" alt=""drawing"" width=""200""/>  This is the official repository for the paper Keyword Transformer: A Self-Attention Model for Keyword Spotting, presented at Interspeech 2021.","# <SOFTWARE>Keyword Transformer</SOFTWARE>: A Self-Attention Model for Keyword Spotting  <img src=""kwt.png"" alt=""drawing"" width=""200""/>  This is the official repository for the paper <PUBLICATION>Keyword Transformer: A Self-Attention Model for Keyword Spotting</PUBLICATION>, presented at <CONFERENCE>Interspeech 2021</CONFERENCE>.",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\095aee3f.txt,0.9347826086956522
2,"Consider citing our paper if you find this work useful.  ``` @inproceedings{berg21_interspeech,   author={Axel Berg and Mark O’Connor and Miguel Tairum Cruz},   title={{Keyword Transformer: A Self-Attention Model for Keyword Spotting}},   year=2021,   booktitle={Proc.","Consider citing our paper if you find this work useful.  ``` @inproceedings{berg21_<CONFERENCE>interspeech</CONFERENCE>,   author={Axel Berg and Mark O’Connor and Miguel Tairum Cruz},   title={{<PUBLICATION>Keyword Transformer: A Self-Attention Model for Keyword Spotting</PUBLICATION>}},   year=2021,   booktitle={<PUBLICATION>Proc.</PUBLICATION>","Consider citing our paper if you find this work useful.  ``` @inproceedings{berg21_interspeech,   author={Axel Berg and Mark O’Connor and Miguel Tairum Cruz},   title={{Keyword Transformer: A Self-Attention Model for Keyword Spotting}},   year=2021,   booktitle={Proc. Interspeech","Consider citing our paper if you find this work useful.  ``` @inproceedings{berg21_interspeech,   author={Axel Berg and Mark O’Connor and Miguel Tairum Cruz},   title={{<PUBLICATION>Keyword Transformer: A Self-Attention Model for Keyword Spotting</PUBLICATION>}},   year=2021,   booktitle={Proc. <CONFERENCE>Interspeech</CONFERENCE>",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\9589ff3f.txt,0.9781021897810219
3,"Interspeech 2021},   pages={4249--4253},   doi={10.21437/Interspeech.2021-1286} } ```  ## Setup  ### Download Google Speech Commands  There are two versions of the dataset, V1 and V2.","<PUBLICATION></PUBLICATION><CONFERENCE>Interspeech 2021</CONFERENCE>},   pages={4249--4253},   doi={10.21437/<CONFERENCE>Interspeech</CONFERENCE>.2021-1286} } ```  ## Setup  ### Download <DATASET>Google Speech Commands</DATASET>  There are two versions of the dataset, V1 and V2.","Interspeech 2021},   pages={4249--4253},   doi={10.21437/Interspeech.2021-1286} } ```  ## Setup  ### Download Google Speech Commands  There are two versions of the dataset, V1 and V2.","<CONFERENCE>Interspeech 2021</CONFERENCE>},   pages={4249--4253},   doi={10.21437/Interspeech.2021-1286} } ```  ## Setup  ### Download <DATASET>Google Speech Commands</DATASET>  There are two versions of the dataset, V1 and V2.",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\6f6c5f06.txt,1.0
4,"To download and extract dataset V2, run:  ```shell wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz mkdir data2 mv .","To download and extract dataset V2, run:  ```shell <SOFTWARE>wget</SOFTWARE> https://storage.googleapis.com/download.<SOFTWARE>tensorflow</SOFTWARE>.org/data/<DATASET>speech_commands_v0.02</DATASET>.tar.gz mkdir data2 mv .","To download and extract dataset V2, run:  ```shell wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz mkdir data2 mv .","To download and extract dataset <DATASET>V2</DATASET>, run:  ```shell wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz mkdir data2 mv .",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\12a3bd88.txt,1.0
5,/speech_commands_v0.02.tar.gz .,/<DATASET>speech_commands_v0.02</DATASET>.tar.gz .,`/speech_commands_v0.02.tar.gz .`,`/speech_commands_<DATASET>v0.02</DATASET>.tar.gz .`,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\3c4f8e95.txt,0.96875
6,/data2 cd .,/data2 cd .,/data2 cd .,/data2 cd .,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\39e7d754.txt,1.0
7,/data2 tar -xf .,/data2 tar -xf .,/data2 tar -xf .,/data2 tar -xf .,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\a923f9e7.txt,1.0
8,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .,/<DATASET>speech_commands_v0.02</DATASET>.tar.gz cd ../ ```  And similarly for V1:  ```shell <DATASET>wget</DATASET> http://download.<SOFTWARE>tensorflow</SOFTWARE>.org/data/<DATASET>speech_commands_v0.01</DATASET>.tar.gz mkdir data1 mv .,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/<DATASET>speech_commands_v0.01</DATASET>.tar.gz mkdir data1 mv .,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\88cf003d.txt,1.0
9,/speech_commands_v0.01.tar.gz .,/<DATASET>speech_commands_v0.01</DATASET>.tar.gz .,`/speech_commands_v0.01.tar.gz .`,`/speech_commands_v0.01.tar.gz .`,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\ba2aa19d.txt,0.96875
10,/data1 cd .,/data1 cd .,/data1 cd .,/data1 cd .,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\5527fa8f.txt,1.0
11,/data1 tar -xf .,/data1 tar -xf .,/data1 tar -xf .,/data1 tar -xf .,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\4eb5a671.txt,1.0
12,/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .,/<DATASET>speech_commands_v0.01</DATASET>.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell <SOFTWARE>pip</SOFTWARE> install <SOFTWARE>virtualenv</SOFTWARE> <SOFTWARE>virtualenv</SOFTWARE> --system-site-packages -p <SOFTWARE>python3</SOFTWARE> .,/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .,/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p <PROGLANG>python3</PROGLANG> .,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\506038da.txt,1.0
13,/venv3 source .,/venv3 source .,/venv3 source .,/venv3 source .,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\d9c9fec1.txt,1.0
14,"/venv3/bin/activate ```  To install dependencies, run  ```shell pip install -r requirements.txt ```  Tested using Tensorflow 2.4.0rc1 with CUDA 11.","/venv3/bin/activate ```  To install dependencies, run  ```shell <SOFTWARE>pip</SOFTWARE> install -r requirements.txt ```  Tested using <SOFTWARE>Tensorflow 2.4.0rc1</SOFTWARE> with <SOFTWARE>CUDA 11</SOFTWARE>.","/venv3/bin/activate ```  To install dependencies, run  ```shell pip install -r requirements.txt ```  Tested using `Tensorflow 2.4.0rc1` with `CUDA 11`.","/venv3/bin/activate ```  To install dependencies, run  ```shell pip install -r requirements.txt ```  Tested using `<SOFTWARE>Tensorflow 2.4.0rc1</SOFTWARE>` with `<SOFTWARE>CUDA 11</SOFTWARE>`.",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\3c89a33a.txt,0.9865771812080537
15,**Note**: Installing the correct Tensorflow version is important for reproducibility!,**Note**: Installing the correct <SOFTWARE>Tensorflow</SOFTWARE> version is important for reproducibility!,**Note**: Installing the correct `Tensorflow` version is important for reproducibility!,**Note**: Installing the correct `<SOFTWARE>Tensorflow</SOFTWARE>` version is important for reproducibility!,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\d8f39846.txt,0.9883720930232558
16,Using more recent versions of Tensorflow results in small accuracy differences each time the model is evaluated.,Using more recent versions of <SOFTWARE>Tensorflow</SOFTWARE> results in small accuracy differences each time the model is evaluated.,Using more recent versions of `Tensorflow` results in small accuracy differences each time the model is evaluated.,Using more recent versions of `<SOFTWARE>Tensorflow</SOFTWARE>` results in small accuracy differences each time the model is evaluated.,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\8b290e09.txt,0.9911504424778761
17,"This might be due to a change in how the random seed generator is implemented, and therefore changes the sampling of the ""unknown""  keyword class.  ## Model The Keyword-Transformer model is defined [here](kws_streaming/models/kws_transformer.py).","This might be due to a change in how the random seed generator is implemented, and therefore changes the sampling of the ""unknown""  keyword class.  ## Model The Keyword-Transformer model is defined [here](kws_streaming/models/kws_transformer.py).","This might be due to a change in how the random seed generator is implemented, and therefore changes the sampling of the ""unknown""  keyword class.  ## Model The `Keyword-Transformer` model is defined [here](kws_streaming/models/kws_transformer.py).","This might be due to a change in how the random seed generator is implemented, and therefore changes the sampling of the ""unknown""  keyword class.  ## Model The `<SOFTWARE>Keyword-Transformer</SOFTWARE>` model is defined [here](kws_streaming/models/kws_transformer.py).",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\47a37cdb.txt,0.9959514170040485
18,"It takes the mel scale spectrogram as input, which has shape 98 x 40 using the default settings, corresponding to the 98 time windows with 40 frequency coefficients.","It takes the mel scale spectrogram as input, which has shape 98 x 40 using the default settings, corresponding to the 98 time windows with 40 frequency coefficients.","It takes the mel scale spectrogram as input, which has shape 98 x 40 using the default settings, corresponding to the 98 time windows with 40 frequency coefficients.","It takes the mel scale spectrogram as input, which has shape 98 x 40 using the default settings, corresponding to the 98 time windows with 40 frequency coefficients.",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\eddb6f83.txt,1.0
19,"There are three variants of the Keyword-Transformer model:  * **Time-domain attention**: each time-window is treated as a patch, self-attention is computed between time-windows * **Frequency-domain attention**: each frequency is treated as a patch self-attention is computed between frequencies * **Combination of both**: The signal is fed into both a time- and a frequency-domain transformer and the outputs are combined * **Patch-wise attention**: Similar to the vision transformer, it extracts rectangular patches from the spectrogram, so attention happens both in the time and frequency domain simultaneously.  ## Training a model from scratch To train KWT-3 from scratch on Speech Commands V2, run    ```shell sh train.sh ```  Please note that the train directory (given by the argument  `--train_dir`) cannot exist prior to start script.","There are three variants of the Keyword-Transformer model:  * **Time-domain attention**: each time-window is treated as a patch, self-attention is computed between time-windows * **Frequency-domain attention**: each frequency is treated as a patch self-attention is computed between frequencies * **Combination of both**: The signal is fed into both a time- and a frequency-domain transformer and the outputs are combined * **Patch-wise attention**: Similar to the vision transformer, it extracts rectangular patches from the spectrogram, so attention happens both in the time and frequency domain simultaneously.  ## Training a model from scratch To train KWT-3 from scratch on <DATASET>Speech Commands V2</DATASET>, run    ```shell sh train.sh ```  Please note that the train directory (given by the argument  `--train_dir`) cannot exist prior to start script.","There are three variants of the Keyword-Transformer model:  * **Time-domain attention**: each time-window is treated as a patch, self-attention is computed between time-windows * **Frequency-domain attention**: each frequency is treated as a patch self-attention is computed between frequencies * **Combination of both**: The signal is fed into both a time- and a frequency-domain transformer and the outputs are combined * **Patch-wise attention**: Similar to the vision transformer, it extracts rectangular patches from the spectrogram, so attention happens both in the time and frequency domain simultaneously.  ## Training a model from scratch To train KWT-3 from scratch on Speech Commands V2, run    ```shell sh train.sh ```  Please note that the train directory (given by the argument  `--train_dir`) cannot exist prior to start script.","There are three variants of the Keyword-Transformer model:  * **Time-domain attention**: each time-window is treated as a patch, self-attention is computed between time-windows * **Frequency-domain attention**: each frequency is treated as a patch self-attention is computed between frequencies * **Combination of both**: The signal is fed into both a time- and a frequency-domain transformer and the outputs are combined * **Patch-wise attention**: Similar to the vision transformer, it extracts rectangular patches from the spectrogram, so attention happens both in the time and frequency domain simultaneously.  ## Training a model from scratch To train <SOFTWARE>KWT-3</SOFTWARE> from scratch on <DATASET>Speech Commands V2</DATASET>, run    ```shell sh train.sh ```  Please note that the train directory (given by the argument  `--train_dir`) cannot exist prior to start script.",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\568dae15.txt,1.0
20,"The model-specific arguments for KWT are:  ```shell --num_layers 12 \ #number of sequential transformer encoders --heads 3 \ #number of attentions heads --d_model 192 \ #embedding dimension --mlp_dim 768 \ #mlp-dimension --dropout1 0. \ #dropout in mlp/multi-head attention blocks --attention_type 'time' \ #attention type: 'time', 'freq', 'both' or 'patch' --patch_size '1,40' \ #spectrogram patch_size, if patch attention is used --prenorm False \ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (Att-MH-RNN), similar to the approach in [DeIT](https://github.com/facebookresearch/deit).","The model-specific arguments for <SOFTWARE>KWT</SOFTWARE> are:  ```shell --num_layers 12 \ #number of sequential transformer encoders --heads 3 \ #number of attentions heads --d_model 192 \ #embedding dimension --mlp_dim 768 \ #mlp-dimension --dropout1 0. \ #dropout in mlp/multi-head attention blocks --attention_type 'time' \ #attention type: 'time', 'freq', 'both' or 'patch' --patch_size '1,40' \ #spectrogram patch_size, if patch attention is used --prenorm False \ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (<SOFTWARE>Att-MH-RNN</SOFTWARE>), similar to the approach in [<SOFTWARE>DeIT</SOFTWARE>](https://github.com/facebookresearch/<SOFTWARE>deit</SOFTWARE>).","The model-specific arguments for KWT are:  ```shell --num_layers 12 \ #number of sequential transformer encoders --heads 3 \ #number of attentions heads --d_model 192 \ #embedding dimension --mlp_dim 768 \ #mlp-dimension --dropout1 0. \ #dropout in mlp/multi-head attention blocks --attention_type 'time' \ #attention type: 'time', 'freq', 'both' or 'patch' --patch_size '1,40' \ #spectrogram patch_size, if patch attention is used --prenorm False \ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (Att-MH-RNN), similar to the approach in [DeIT](https://github.com/facebookresearch/deit).","The model-specific arguments for <SOFTWARE>KWT</SOFTWARE> are:  ```shell --num_layers 12 \ #number of sequential transformer encoders --heads 3 \ #number of attentions heads --d_model 192 \ #embedding dimension --mlp_dim 768 \ #mlp-dimension --dropout1 0. \ #dropout in mlp/multi-head attention blocks --attention_type 'time' \ #attention type: 'time', 'freq', 'both' or 'patch' --patch_size '1,40' \ #spectrogram patch_size, if patch attention is used --prenorm False \ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (<SOFTWARE>Att-MH-RNN</SOFTWARE>), similar to the approach in [<SOFTWARE>DeIT</SOFTWARE>](https://github.com/facebookresearch/deit).",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\52e90fb0.txt,1.0
21,"To train KWT-3 with hard distillation from a pre-trained model, run  ```shell sh distill.sh ```  ## Run inference using a pre-trained model  Pre-trained weights for KWT-3, KWT-2 and KWT-1 are provided in .","To train KWT-3 with hard distillation from a pre-trained model, run  ```shell sh distill.sh ```  ## Run inference using a pre-trained model  Pre-trained weights for KWT-3, KWT-2 and KWT-1 are provided in .","To train KWT-3 with hard distillation from a pre-trained model, run  ```shell sh distill.sh ```  ## Run inference using a pre-trained model  Pre-trained weights for KWT-3, KWT-2 and KWT-1 are provided in .","To train <SOFTWARE>KWT-3</SOFTWARE> with hard distillation from a pre-trained model, run  ```shell sh distill.sh ```  ## Run inference using a pre-trained model  Pre-trained weights for <SOFTWARE>KWT-3</SOFTWARE>, <SOFTWARE>KWT-2</SOFTWARE> and <SOFTWARE>KWT-1</SOFTWARE> are provided in .",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\42afb631.txt,1.0
22,/models_data_v2_12_labels.,/models_data_v2_12_labels.,/models_data_v2_12_labels.,/models_data_v2_12_labels.,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\08adc2ea.txt,1.0
23,"|Model name|embedding dim|mlp-dim|heads|depth|#params|V2-12 accuracy|pre-trained| |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:| |KWT-1|64|128|1|12|607K|97.7|[here](models_data_v2_12_labels/kwt1)| |KWT-2|128|256|2|12|2.4M|98.2|[here](models_data_v2_12_labels/kwt2)| |KWT-3|192|768|3|12|5.5M|98.7|[here](models_data_v2_12_labels/kwt3)|  To perform inference on Google Speech Commands v2 with 12 labels, run  ```shell sh eval.sh ```  ## Acknowledgements  The code heavily borrows from the [KWS streaming work](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.","|Model name|embedding dim|mlp-dim|heads|depth|#params|V2-12 accuracy|pre-trained| |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:| |KWT-1|64|128|1|12|607K|97.7|[here](models_data_v2_12_labels/kwt1)| |KWT-2|128|256|2|12|2.4M|98.2|[here](models_data_v2_12_labels/kwt2)| |KWT-3|192|768|3|12|5.5M|98.7|[here](models_data_v2_12_labels/kwt3)|  To perform inference on <DATASET>Google Speech Commands v2</DATASET> with 12 labels, run  ```shell sh eval.sh ```  ## Acknowledgements  The code heavily borrows from the [KWS streaming work](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.","|Model name|embedding dim|mlp-dim|heads|depth|#params|V2-12 accuracy|pre-trained| |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:| |KWT-1|64|128|1|12|607K|97.7|[here](models_data_v2_12_labels/kwt1)| |KWT-2|128|256|2|12|2.4M|98.2|[here](models_data_v2_12_labels/kwt2)| |KWT-3|192|768|3|12|5.5M|98.7|[here](models_data_v2_12_labels/kwt3)|  To perform inference on Google Speech Commands v2 with 12 labels, run  ```shell sh eval.sh ```  ## Acknowledgements  The code heavily borrows from the KWS streaming work by Google Research.","|Model name|embedding dim|mlp-dim|heads|depth|#params|V2-12 accuracy|pre-trained| |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:| |KWT-1|64|128|1|12|607K|97.7|[here](models_data_v2_12_labels/kwt1)| |KWT-2|128|256|2|12|2.4M|98.2|[here](models_data_v2_12_labels/kwt2)| |KWT-3|192|768|3|12|5.5M|98.7|[here](models_data_v2_12_labels/kwt3)|  To perform inference on <DATASET>Google Speech Commands v2</DATASET> with 12 labels, run  ```shell sh eval.sh ```  ## Acknowledgements  The code heavily borrows from the <SOFTWARE>KWS streaming work</SOFTWARE> by <PROJECT>Google Research</PROJECT>.",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\fcb63bec.txt,0.9148550724637681
24,"For a more detailed description of the code structure, see the original authors' [README](kws_streaming/README.md).","For a more detailed description of the code structure, see the original authors' [README](kws_streaming/README.md).","For a more detailed description of the code structure, see the original authors' [README](kws_streaming/README.md).","For a more detailed description of the code structure, see the original authors' [README](<SOFTWARE>kws_streaming</SOFTWARE>/README.md).",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\ebd339a9.txt,1.0
25,We also exploit training techniques from [DeiT](https://github.com/facebookresearch/deit).,We also exploit training techniques from [<PROJECT>DeiT</PROJECT>](https://github.com/facebookresearch/<PROJECT>deit</PROJECT>).,We also exploit training techniques from [DeiT](https://github.com/facebookresearch/deit).,We also exploit training techniques from [<SOFTWARE>DeiT</SOFTWARE>](https://github.com/facebookresearch/<SOFTWARE>deit</SOFTWARE>).,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\027e43da.txt,1.0
26,We thank the authors for sharing their code.,We thank the authors for sharing their code.,We thank the authors for sharing their code.,We thank the authors for sharing their code.,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\5dfd584e.txt,1.0
27,Please consider citing them as well if you use our code.  ## License  The source files in this repository are released under the [Apache 2.0](LICENSE.txt) license.,Please consider citing them as well if you use our code.  ## License  The source files in this repository are released under the [<LICENSE>Apache 2.0</LICENSE>](LICENSE.txt) license.,Please consider citing them as well if you use our code.  ## License  The source files in this repository are released under the `Apache 2.0` license.,Please consider citing them as well if you use our code.  ## License  The source files in this repository are released under the `<LICENSE>Apache 2.0</LICENSE>` license.,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\b89716b4.txt,0.9456869009584664
28,Some source files are derived from the [KWS streaming repository](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.,Some source files are derived from the [<SOFTWARE>KWS streaming repository</SOFTWARE>](https://github.com/google-research/google-research/tree/master/<SOFTWARE>kws_streaming</SOFTWARE>) by Google Research.,Some source files are derived from the [KWS streaming repository](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.,Some source files are derived from the [<SOFTWARE>KWS streaming</SOFTWARE> repository](https://github.com/<PROJECT>google-research</PROJECT>/<PROJECT>google-research</PROJECT>/tree/master/<SOFTWARE>kws_streaming</SOFTWARE>) by <PROJECT>Google Research</PROJECT>.,../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\17f25f38.txt,1.0
29,"These are also released under the Apache 2.0 license, the text of which can be seen in the LICENSE file on their repository.","These are also released under the <LICENSE>Apache 2.0</LICENSE> license, the text of which can be seen in the LICENSE file on their repository.","These are also released under the `Apache 2.0` license, the text of which can be seen in the LICENSE file on their repository.","These are also released under the `<LICENSE>Apache 2.0</LICENSE>` license, the text of which can be seen in the LICENSE file on their repository.",../results/deepseek-chat/prompt-0/zzz_ARM-software_keyword-transformer_master_README.md.tsv\ddfecc43.txt,0.992
