sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,# [AAAI 2024] NuScenes-QA  Official repository for the AAAI 2024 paper **[NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://arxiv.org/pdf/2305.14836.pdf)**.  !,# [<CONFERENCE>AAAI 2024</CONFERENCE>] <SOFTWARE>NuScenes-QA</SOFTWARE>  Official repository for the <CONFERENCE>AAAI 2024</CONFERENCE> paper **[<PUBLICATION>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</PUBLICATION><SOFTWARE>NuScenes-QA</SOFTWARE>: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://arxiv.org/pdf/2305.14836.pdf)**.  !,"# [AAAI 2024] NuScenes-QA  Official repository for the AAAI 2024 paper **[NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://arxiv.org/pdf/2305.14836.pdf)**.  !
","# [<CONFERENCE>AAAI 2024</CONFERENCE>] <DATASET>NuScenes-QA</DATASET>  Official repository for the <CONFERENCE>AAAI 2024</CONFERENCE> paper **[<PUBLICATION>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</PUBLICATION>](https://arxiv.org/pdf/2305.14836.pdf)**.  !
",../results/deepseek-chat/prompt-0/zzz_qiantianwen_nuscenes-qa_main_README.md.tsv\db650f99.txt,0.9976580796252927
2,[DataConstruction](docs/data_construction.png)  ## :fire: News  - `2024.11.01`  CenterPoint feature released,[DataConstruction](docs/data_construction.png)  ## :fire: News  - `2024.11.01`  CenterPoint feature released,"[DataConstruction](docs/data_construction.png)  ## :fire: News  - 2024.11.01  CenterPoint feature released
","[<PROJECT>DataConstruction</PROJECT>](docs/data_construction.png)  ## :fire: News  - 2024.11.01  <SOFTWARE>CenterPoint</SOFTWARE> feature released
",../results/deepseek-chat/prompt-0/zzz_qiantianwen_nuscenes-qa_main_README.md.tsv\2cdae496.txt,0.986046511627907
3,.,.,- 2024.10.11  Training and Testing code released. - 2023.12.09  Our paper is accepted by AAAI 2024!,- 2024.10.11  Training and Testing code released. - 2023.12.09  Our paper is accepted by <CONFERENCE>AAAI 2024</CONFERENCE>!,../results/deepseek-chat/prompt-0/zzz_qiantianwen_nuscenes-qa_main_README.md.tsv\c6c082e0.txt,0.02
4,- `2024.10.11`  Training and Testing code released. - `2023.12.09`  Our paper is accepted by AAAI 2024!,- `2024.10.11`  Training and Testing code released. - `2023.12.09`  Our paper is accepted by <CONFERENCE>AAAI 2024</CONFERENCE>!,- 2024.10.11  Training and Testing code released. - 2023.12.09  Our paper is accepted by AAAI 2024!,- 2024.10.11  Training and Testing code released. - 2023.12.09  Our paper is accepted by <CONFERENCE>AAAI 2024</CONFERENCE>!,../results/deepseek-chat/prompt-0/zzz_qiantianwen_nuscenes-qa_main_README.md.tsv\c6c082e0.txt,0.9801980198019802
5,"- `2023.09.04`  Our NuScenes-QA dataset v1.0 released.  ## :hourglass_flowing_sand: To Do  - [x] Release question & anwswer data - [x] Release visual feature - [x] Release training and testing code  ## :running: Getting Started  ### Data Preparation  We have released our question-answer annotations, please download it from [HERE](https://drive.google.com/drive/folders/1jIkICT23wZWZYPrWCa0x-ubjpClSzOuU?","- `2023.09.04`  Our <DATASET>NuScenes-QA</DATASET> dataset v1.0 released.  ## :hourglass_flowing_sand: To Do  - [x] Release question & anwswer data - [x] Release visual feature - [x] Release training and testing code  ## :running: Getting Started  ### Data Preparation  We have released our question-answer annotations, please download it from [HERE](https://drive.google.com/drive/folders/1jIkICT23wZWZYPrWCa0x-ubjpClSzOuU?","- 2023.09.04  Our NuScenes-QA dataset v1.0 released.  ## :hourglass_flowing_sand: To Do  - [x] Release question & anwswer data - [x] Release visual feature - [x] Release training and testing code  ## :running: Getting Started  ### Data Preparation  We have released our question-answer annotations, please download it from [HERE](https://drive.google.com/drive/folders/1jIkICT23wZWZYPrWCa0x-ubjpClSzOuU?
","- 2023.09.04  Our <DATASET>NuScenes-QA</DATASET> dataset v1.0 released.  ## :hourglass_flowing_sand: To Do  - [x] Release question & anwswer data - [x] Release visual feature - [x] Release training and testing code  ## :running: Getting Started  ### Data Preparation  We have released our question-answer annotations, please download it from [HERE](https://drive.google.com/drive/folders/1jIkICT23wZWZYPrWCa0x-ubjpClSzOuU?
",../results/deepseek-chat/prompt-0/zzz_qiantianwen_nuscenes-qa_main_README.md.tsv\04e366bf.txt,0.9962917181705809
6,usp=sharing).,usp=sharing).,"For specific details on feature extraction, you can refer to the **Visual Feature Extraction** and **Object Embedding** sections of our paper.","For specific details on feature extraction, you can refer to the **<PUBLICATION>Visual Feature Extraction</PUBLICATION>** and **<PUBLICATION>Object Embedding</PUBLICATION>** sections of our paper.",../results/deepseek-chat/prompt-0/zzz_qiantianwen_nuscenes-qa_main_README.md.tsv\82442bb8.txt,0.12903225806451613
7,"For the visual data, you can download **CenterPoint** feature that we have extracted from [HERE](https://drive.google.com/file/d/1TNsK6cpQ4pd9fH1s7WFxTSXilrT2uNYb/view?","For the visual data, you can download **CenterPoint** feature that we have extracted from [HERE](https://drive.google.com/file/d/1TNsK6cpQ4pd9fH1s7WFxTSXilrT2uNYb/view?","For the visual data, you can download CenterPoint feature that we have extracted from [HERE](https://drive.google.com/file/d/1TNsK6cpQ4pd9fH1s7WFxTSXilrT2uNYb/view?","For the visual data, you can download <SOFTWARE>CenterPoint</SOFTWARE> feature that we have extracted from [HERE](https://drive.google.com/file/d/1TNsK6cpQ4pd9fH1s7WFxTSXilrT2uNYb/view?",../results/deepseek-chat/prompt-0/zzz_qiantianwen_nuscenes-qa_main_README.md.tsv\2e6e99d2.txt,0.9879518072289156
8,usp=sharing).,usp=sharing).,"For specific details on feature extraction, you can refer to the **Visual Feature Extraction** and **Object Embedding** sections of our paper.","For specific details on feature extraction, you can refer to the **<PUBLICATION>Visual Feature Extraction</PUBLICATION>** and **<PUBLICATION>Object Embedding</PUBLICATION>** sections of our paper.",../results/deepseek-chat/prompt-0/zzz_qiantianwen_nuscenes-qa_main_README.md.tsv\82442bb8.txt,0.12903225806451613
10,"For specific details on feature extraction, you can refer to the **Visual Feature Extraction** and **Object Embedding** sections of our paper.","For specific details on feature extraction, you can refer to the **Visual Feature Extraction** and **Object Embedding** sections of our paper.","For specific details on feature extraction, you can refer to the **Visual Feature Extraction** and **Object Embedding** sections of our paper.","For specific details on feature extraction, you can refer to the **<PUBLICATION>Visual Feature Extraction</PUBLICATION>** and **<PUBLICATION>Object Embedding</PUBLICATION>** sections of our paper.",../results/deepseek-chat/prompt-0/zzz_qiantianwen_nuscenes-qa_main_README.md.tsv\82442bb8.txt,1.0
11,"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.","The folder structure should be organized as follows before training.  ``` <DATASET>NuScenes-QA</DATASET> +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```<PROGLANG>bash</PROGLANG> <SOFTWARE>python >= 3.5</SOFTWARE> <SOFTWARE>CUDA >= 9.0</SOFTWARE> <SOFTWARE>PyTorch >= 1.4.0</SOFTWARE> <SOFTWARE>SpaCy == 2.1.0</SOFTWARE> ```  For the <SOFTWARE>SpaCy</SOFTWARE>, you can install it by:  ```<PROGLANG>bash</PROGLANG> wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz <SOFTWARE>pip</SOFTWARE> install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```<PROGLANG>bash</PROGLANG> <SOFTWARE>python3</SOFTWARE> run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```<PROGLANG>bash</PROGLANG> outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```<PROGLANG>bash</PROGLANG> <SOFTWARE>python3</SOFTWARE> run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={<PUBLICATION>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</PUBLICATION>},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [<SOFTWARE>MMDetection3D</SOFTWARE>](https://github.com/open-mmlab/mmdetection3d) and [<SOFTWARE>OpenVQA</SOFTWARE>](https://github.com/MILVLG/openvqa) for open sourcing their methods.","The folder structure should be organized as follows before training.   NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py   ### Installation  The following packages are required to build the project:  bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0   For the SpaCy, you can install it by:  bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz   ### Training   The following script will start training a man_small model with CenterPoint feature on 2 GPUs:  bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1'   All checkpoint files and the training logs will be saved to the following paths respectively:  bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt   ### Testing  For testing, you can use the following script:  bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl'   The evaluation results and the answers for all questions will ba saved to the following paths respectively:  bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt   ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with twqian19@fudan.edu.cn.   ## :book: Citation If you find our paper and project useful, please consider citing: bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} }   ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.
","The folder structure should be organized as follows before training.   <PROJECT>NuScenes-QA</PROJECT> +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- <DATASET>NuScenes_train_questions.json</DATASET> |   |   +-- <DATASET>NuScenes_val_questions.json</DATASET> |   +-- features/     # downloaded or extracted |   |   +-- <SOFTWARE>CenterPoint</SOFTWARE>/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- <SOFTWARE>BEVDet</SOFTWARE>/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- <SOFTWARE>MSMDFusion</SOFTWARE>/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py   ### Installation  The following packages are required to build the project:  bash <PROGLANG>python</PROGLANG> >= 3.5 <SOFTWARE>CUDA</SOFTWARE> >= 9.0 <SOFTWARE>PyTorch</SOFTWARE> >= 1.4.0 <SOFTWARE>SpaCy</SOFTWARE> == 2.1.0   For the <SOFTWARE>SpaCy</SOFTWARE>, you can install it by:  bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz   ### Training   The following script will start training a man_small model with <SOFTWARE>CenterPoint</SOFTWARE> feature on 2 GPUs:  bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='<SOFTWARE>CenterPoint</SOFTWARE>' --GPU='0, 1'   All checkpoint files and the training logs will be saved to the following paths respectively:  bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt   ### Testing  For testing, you can use the following script:  bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='<SOFTWARE>CenterPoint</SOFTWARE>' --CKPT_PATH'path/to/ckpt.pkl'   The evaluation results and the answers for all questions will ba saved to the following paths respectively:  bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt   ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with twqian19@fudan.edu.cn.   ## :book: Citation If you find our paper and project useful, please consider citing: bibtex @article{qian2023nuscenes,   title={<PUBLICATION>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</PUBLICATION>},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={<PUBLICATION>arXiv preprint arXiv:2305.14836</PUBLICATION>},   year={2023} }   ## Acknowlegement  We sincerely thank the authors of [<SOFTWARE>MMDetection3D</SOFTWARE>](https://github.com/open-mmlab/<SOFTWARE>mmdetection3d</SOFTWARE>) and [<SOFTWARE>OpenVQA</SOFTWARE>](https://github.com/MILVLG/<SOFTWARE>openvqa</SOFTWARE>) for open sourcing their methods.
",../results/deepseek-chat/prompt-0/zzz_qiantianwen_nuscenes-qa_main_README.md.tsv\3c1a1d48.txt,0.9845201238390093
