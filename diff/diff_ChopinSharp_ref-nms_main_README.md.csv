sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
3,Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<dataset>_<split_by>.txt` - selected GloVe feature: `std_glove_<dataset>_<split_by>.npy` - referring expression database: `std_refdb_<dataset>_<split_by>.json` - critical objects database: `std_ctxdb_<dataset>_<split_by>.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset refcoco --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset refcoco --split-by unc ```  We use tensorboard to monitor the training process.,Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<dataset>_<split_by>.txt` - selected GloVe feature: `std_glove_<dataset>_<split_by>.npy` - referring expression database: `std_refdb_<dataset>_<split_by>.json` - critical objects database: `std_ctxdb_<dataset>_<split_by>.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset <DATASET>refcoco</DATASET> --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset <DATASET>refcoco</DATASET> --split-by unc ```  We use tensorboard to monitor the training process.,Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_refcoco_split_by.txt` - selected GloVe feature: `std_glove_refcoco_split_by.npy` - referring expression database: `std_refdb_refcoco_split_by.json` - critical objects database: `std_ctxdb_refcoco_split_by.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset refcoco --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset refcoco --split-by unc ```  We use tensorboard to monitor the training process.,Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<DATASET>refcoco</DATASET>_<DATASET>split_by</DATASET>.txt` - selected GloVe feature: `std_glove_<DATASET>refcoco</DATASET>_<DATASET>split_by</DATASET>.npy` - referring expression database: `std_refdb_<DATASET>refcoco</DATASET>_<DATASET>split_by</DATASET>.json` - critical objects database: `std_ctxdb_<DATASET>refcoco</DATASET>_<DATASET>split_by</DATASET>.json`   ## Train **Train with binary XE loss:** ``` <PROGLANG>PYTHONPATH</PROGLANG>=$PWD <PROGLANG>python</PROGLANG> tools/train_att_vanilla.py --dataset <DATASET>refcoco</DATASET> --split-by <DATASET>unc</DATASET> ```  **Train with ranking loss:** ``` <PROGLANG>PYTHONPATH</PROGLANG>=$PWD <PROGLANG>python</PROGLANG> tools/train_att_rank.py --dataset <DATASET>refcoco</DATASET> --split-by <DATASET>unc</DATASET> ```  We use <SOFTWARE>tensorboard</SOFTWARE> to monitor the training process.,../results/deepseek-chat/prompt-0/zzz_ChopinSharp_ref-nms_main_README.md.tsv\332d33ae.txt,0.8968481375358166
4,The log file can be found in `tb` folder.  ## Evaluate Recall **Save Ref-NMS proposals:** ``` PYTHONPATH=$PWD python tools/save_ref_nms_proposals.py --dataset refcoco --split-by unc --tid <tid> --m <loss_type> ``` `<loss_type>` can be either `att_vanilla` for binary XE loss or `att_rank` for rank loss.,The log file can be found in `tb` folder.  ## Evaluate <EVALMETRIC>Recall</EVALMETRIC> **Save Ref-NMS proposals:** ``` PYTHONPATH=$PWD python tools/save_ref_nms_proposals.py --dataset <DATASET>refcoco</DATASET> --split-by unc --tid <tid> --m <loss_type> ``` `<loss_type>` can be either `att_vanilla` for binary XE loss or `att_rank` for rank loss.,The log file can be found in `tb` folder.  ## Evaluate `Recall` **Save Ref-NMS proposals:** ``` PYTHONPATH=$PWD python tools/save_ref_nms_proposals.py --dataset `refcoco` --split-by unc --tid <tid> --m <loss_type> ``` `<loss_type>` can be either `att_vanilla` for binary XE loss or `att_rank` for rank loss.,The log file can be found in `tb` folder.  ## Evaluate `<EVALMETRIC>Recall</EVALMETRIC>` **Save Ref-NMS proposals:** ``` PYTHONPATH=$PWD python tools/save_ref_nms_proposals.py --dataset `<DATASET>refcoco</DATASET>` --split-by unc --tid <tid> --m <loss_type> ``` `<loss_type>` can be either `att_vanilla` for binary XE loss or `att_rank` for rank loss.,../results/deepseek-chat/prompt-0/zzz_ChopinSharp_ref-nms_main_README.md.tsv\48976f3c.txt,0.9704918032786886
5,**Evaluate recall on referent object:** ``` PYTHONPATH=$PWD python tools/eval_proposal_hit_rate.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ``` `conf` parameter is the score threshold used to filter Ref-NMS proposals.,**Evaluate <EVALMETRIC>recall</EVALMETRIC> on referent object:** ``` PYTHONPATH=$PWD python tools/eval_proposal_hit_rate.py --m <loss_type> --dataset <DATASET>refcoco</DATASET> --split-by unc --tid <tid> --conf <conf> ``` `conf` parameter is the score threshold used to filter Ref-NMS proposals.,**Evaluate `recall` on referent object:** ``` PYTHONPATH=$PWD python tools/eval_proposal_hit_rate.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ``` `conf` parameter is the score threshold used to filter Ref-NMS proposals.,**Evaluate `<EVALMETRIC>recall</EVALMETRIC>` on referent object:** ``` PYTHONPATH=$PWD <PROGLANG>python</PROGLANG> tools/eval_proposal_hit_rate.py --m <loss_type> --<DATASET>dataset refcoco</DATASET> --split-by unc --tid <tid> --conf <conf> ``` `conf` parameter is the score threshold used to filter Ref-NMS proposals.,../results/deepseek-chat/prompt-0/zzz_ChopinSharp_ref-nms_main_README.md.tsv\e81355cb.txt,0.9722222222222222
6,It should be picked properly so that the recall of the referent is high while the number of proposals per expression is around 8-10.,It should be picked properly so that the recall of the referent is high while the number of proposals per expression is around 8-10.,It should be picked properly so that the `recall` of the referent is high while the number of proposals per expression is around 8-10.,It should be picked properly so that the `<EVALMETRIC>recall</EVALMETRIC>` of the referent is high while the number of proposals per expression is around 8-10.,../results/deepseek-chat/prompt-0/zzz_ChopinSharp_ref-nms_main_README.md.tsv\7284d2ae.txt,0.9924812030075187
7,**Evaluate recall on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset refcoco --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.,**Evaluate <EVALMETRIC>recall</EVALMETRIC> on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_<EVALMETRIC>recall</EVALMETRIC>.py --m <loss_type> --dataset <DATASET>refcoco</DATASET> --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset <DATASET>refcoco</DATASET> --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.,**Evaluate `recall` on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset `refcoco` --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset `refcoco` --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>__<split_by>_<top_N>.json`.,**Evaluate `<EVALMETRIC>recall</EVALMETRIC>` on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset `<DATASET>refcoco</DATASET>` --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset `<DATASET>refcoco</DATASET>` --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.,../results/deepseek-chat/prompt-0/zzz_ChopinSharp_ref-nms_main_README.md.tsv\2d7f0c9e.txt,0.9487418452935694
8,We provide altered version of [MAttNet](https://github.com/ChopinSharp/MAttNet) and [CM-A-E](https://github.com/ChopinSharp/CM-Erase-REG) for downstream REG task evaluation.,We provide altered version of [<SOFTWARE>MAttNet</SOFTWARE>](https://github.com/ChopinSharp/<SOFTWARE>MAttNet</SOFTWARE>) and [<SOFTWARE>CM-A-E</SOFTWARE>](https://github.com/ChopinSharp/<SOFTWARE>CM-Erase-REG</SOFTWARE>) for downstream REG task evaluation.,We provide altered version of `[MAttNet](https://github.com/ChopinSharp/MAttNet)` and `[CM-A-E](https://github.com/ChopinSharp/CM-Erase-REG)` for downstream REG task evaluation.,We provide altered version of `<SOFTWARE>[MAttNet](https://github.com/ChopinSharp/MAttNet)</SOFTWARE>` and `<SOFTWARE>[CM-A-E](https://github.com/ChopinSharp/CM-Erase-REG)</SOFTWARE>` for downstream REG task evaluation.,../results/deepseek-chat/prompt-0/zzz_ChopinSharp_ref-nms_main_README.md.tsv\145d5699.txt,0.9885714285714285
14,"usp=sharing) [[Baidu Disk]](https://pan.baidu.com/s/1G4k7APKSUs-_5StXoYaNrA) (extraction code: 5a9r)  ## Citation ``` @inproceedings{chen2021ref,   title={Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding},   author={Chen, Long and Ma, Wenbo and Xiao, Jun and Zhang, Hanwang and Chang, Shih-Fu},   booktitle={AAAI},   year={2021} } ```","usp=sharing) [[<SOFTWARE>Baidu Disk</SOFTWARE>]](https://pan.baidu.com/s/1G4k7APKSUs-_5StXoYaNrA) (extraction code: 5a9r)  ## Citation ``` @inproceedings{chen2021ref,   title={<PUBLICATION>Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding</PUBLICATION>},   author={Chen, Long and Ma, Wenbo and Xiao, Jun and Zhang, Hanwang and Chang, Shih-Fu},   booktitle={<CONFERENCE>AAAI</CONFERENCE>},   year={2021} } ```","usp=sharing) [[Baidu Disk]](https://pan.baidu.com/s/1G4k7APKSUs-_5StXoYaNrA) (extraction code: 5a9r)  ## Citation ``` @inproceedings{chen2021ref,   title={Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding},   author={Chen, Long and Ma, Wenbo and Xiao, Jun and Zhang, Hanwang and Chang, Shih-Fu},   booktitle={AAAI},   year={2021} }","usp=sharing) [[Baidu Disk]](https://pan.baidu.com/s/1G4k7APKSUs-_5StXoYaNrA) (extraction code: 5a9r)  ## Citation ``` @inproceedings{chen2021ref,   title={<PUBLICATION>Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding</PUBLICATION>},   author={Chen, Long and Ma, Wenbo and Xiao, Jun and Zhang, Hanwang and Chang, Shih-Fu},   booktitle={<CONFERENCE>AAAI</CONFERENCE>},   year={2021} }",../results/deepseek-chat/prompt-0/zzz_ChopinSharp_ref-nms_main_README.md.tsv\19aba8c5.txt,0.994535519125683
