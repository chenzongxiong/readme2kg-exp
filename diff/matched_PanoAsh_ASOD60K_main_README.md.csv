sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,# [PAV-SOD: A New Task Towards Panoramic Audiovisual Saliency Detection (TOMM 2022)](https://drive.google.com/file/d/1-1RcARcbz4pACFzkjXcp6MP8R9CGScqI/view?,# [<PUBLICATION>PAV-SOD: A New Task Towards Panoramic Audiovisual Saliency Detection</PUBLICATION><PROJECT>PAV-SOD</PROJECT>: A New Task Towards Panoramic Audiovisual Saliency Detection (<PUBLICATION>TOMM 2022</PUBLICATION>)](https://drive.google.com/file/d/1-1RcARcbz4pACFzkjXcp6MP8R9CGScqI/view?,"# [PAV-SOD: A New Task Towards Panoramic Audiovisual Saliency Detection (TOMM 2022)](https://drive.google.com/file/d/1-1RcARcbz4pACFzkjXcp6MP8R9CGScqI/view?
","# [<PUBLICATION>PAV-SOD: A New Task Towards Panoramic Audiovisual Saliency Detection</PUBLICATION> (<PUBLICATION>TOMM 2022</PUBLICATION>)](https://drive.google.com/file/d/1-1RcARcbz4pACFzkjXcp6MP8R9CGScqI/view?
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\9a4e532c.txt,0.9968051118210862
2,"usp=sharing)  Object-level audiovisual saliency detection in 360° panoramic real-life dynamic scenes is important for exploring and modeling human perception in immersive environments, also for aiding the development of virtual, augmented and mixed reality applications in the fields of such as education, social network, entertainment and training.","usp=sharing)  Object-level audiovisual saliency detection in 360° panoramic real-life dynamic scenes is important for exploring and modeling human perception in immersive environments, also for aiding the development of virtual, augmented and mixed reality applications in the fields of such as education, social network, entertainment and training.","usp=sharing)  Object-level audiovisual saliency detection in 360° panoramic real-life dynamic scenes is important for exploring and modeling human perception in immersive environments, also for aiding the development of virtual, augmented and mixed reality applications in the fields of such as education, social network, entertainment and training.","usp=sharing)  Object-level audiovisual saliency detection in 360° panoramic real-life dynamic scenes is important for exploring and modeling human perception in immersive environments, also for aiding the development of virtual, augmented and mixed reality applications in the fields of such as education, social network, entertainment and training.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\d87e9b85.txt,1.0
3,"To this end, we propose a new task, panoramic audiovisual salient object detection (PAV-SOD), which aims to segment the objects grasping most of the human attention in 360° panoramic videos reflecting real-life daily scenes.","To this end, we propose a new task, <PROJECT>panoramic audiovisual salient object detection</PROJECT> (<PROJECT>PAV-SOD</PROJECT>), which aims to segment the objects grasping most of the human attention in 360° panoramic videos reflecting real-life daily scenes.","To this end, we propose a new task, panoramic audiovisual salient object detection (PAV-SOD), which aims to segment the objects grasping most of the human attention in 360° panoramic videos reflecting real-life daily scenes.","To this end, we propose a new task, panoramic audiovisual salient object detection (<PUBLICATION>PAV-SOD</PUBLICATION>), which aims to segment the objects grasping most of the human attention in 360° panoramic videos reflecting real-life daily scenes.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\85f5f838.txt,1.0
4,"To support the task, we collect PAVS10K, the first panoramic video dataset for audiovisual salient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting PAV-SOD, and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks.","To support the task, we collect <DATASET>PAVS10K</DATASET>, the first panoramic video dataset for audiovisual salient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting <PROJECT>PAV-SOD</PROJECT>, and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks.","Here is the annotated text in Markdown format:

To support the task, we collect PAVS10K, the first panoramic video dataset for audiovisual salient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting PAV-SOD, and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks.
","Here is the annotated text in Markdown format:

To support the task, we collect <DATASET>PAVS10K</DATASET>, the first panoramic video dataset for audiovisual salient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting <SOFTWARE>PAV-SOD</SOFTWARE>, and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks.
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\0225e3d2.txt,0.9463307776560789
5,The coarse-to-fine annotations enable multi-perspective analysis regarding PAV-SOD modeling.,The coarse-to-fine annotations enable multi-perspective analysis regarding <SOFTWARE>PAV-SOD</SOFTWARE> modeling.,The coarse-to-fine annotations enable multi-perspective analysis regarding PAV-SOD modeling.,The coarse-to-fine annotations enable multi-perspective analysis regarding <PUBLICATION>PAV-SOD</PUBLICATION> modeling.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\18886047.txt,1.0
6,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our PAVS10K.,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our <DATASET>PAVS10K</DATASET>.,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our PAVS10K.,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our <DATASET>PAVS10K</DATASET>.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\25bec5df.txt,1.0
7,"Besides, we propose a new baseline network, which takes advantage of both visual and audio cues of 360° video frames by using a new conditional variational auto-encoder (CVAE).","Besides, we propose a new baseline network, which takes advantage of both visual and audio cues of 360° video frames by using a new conditional variational auto-encoder (CVAE).","Besides, we propose a new baseline network, which takes advantage of both visual and audio cues of 360° video frames by using a new conditional variational auto-encoder (CVAE).","Besides, we propose a new baseline network, which takes advantage of both visual and audio cues of 360° video frames by using a new conditional variational auto-encoder (CVAE).",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\957758ed.txt,1.0
8,"Our CVAE-based audiovisual network, namely CAV-Net, consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network and audiovisual distribution estimation modules.","Our CVAE-based audiovisual network, namely CAV-Net, consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network and audiovisual distribution estimation modules.","Our CVAE-based audiovisual network, namely CAV-Net, consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network and audiovisual distribution estimation modules.","Our <SOFTWARE>CVAE</SOFTWARE>-based audiovisual network, namely <SOFTWARE>CAV-Net</SOFTWARE>, consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network and audiovisual distribution estimation modules.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\9d156242.txt,1.0
9,"As a result, our CAV-Net outperforms all competing models and is able to estimate the aleatoric uncertainties within PAVS10K.","As a result, our CAV-Net outperforms all competing models and is able to estimate the aleatoric uncertainties within <DATASET>PAVS10K</DATASET>.","As a result, our CAV-Net outperforms all competing models and is able to estimate the aleatoric uncertainties within PAVS10K.","As a result, our <SOFTWARE>CAV-Net</SOFTWARE> outperforms all competing models and is able to estimate the aleatoric uncertainties within <DATASET>PAVS10K</DATASET>.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\bac8b45a.txt,1.0
10,"With extensive experimental results, we gain several findings about PAV-SOD challenges and insights towards PAV-SOD model interpretability.","With extensive experimental results, we gain several findings about <PROJECT>PAV-SOD</PROJECT> challenges and insights towards <SOFTWARE>PAV-SOD </SOFTWARE>model interpretability.","With extensive experimental results, we gain several findings about PAV-SOD challenges and insights towards PAV-SOD model interpretability.","With extensive experimental results, we gain several findings about <PUBLICATION>PAV-SOD</PUBLICATION> challenges and insights towards <PUBLICATION>PAV-SOD</PUBLICATION> model interpretability.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\dde56107.txt,1.0
11,"We hope that our work could serve as a starting point for advancing SOD towards immersive media.  ------  # PAVS10K  <p align=""center"">     <img src="".","We hope that our work could serve as a starting point for advancing SOD towards immersive media.  ------  # <DATASET>PAVS10K</DATASET>  <p align=""center"">     <img src="".","We hope that our work could serve as a starting point for advancing SOD towards immersive media.  ------  # PAVS10K  <p align=""center"">     <img src="".","We hope that our work could serve as a starting point for advancing SOD towards immersive media.  ------  # <DATASET>PAVS10K</DATASET>  <p align=""center"">     <img src="".",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\8970a8cd.txt,1.0
12,"/figures/fig_teaser.jpg""/> <br />     <em>      Figure 1: An example of our PAVS10K where coarse-to-fine annotations are provided, based on a guidance of fixations acquired from subjective experiments conducted by multiple (N) subjects wearing Head-Mounted Displays (HMDs) and headphones.","/figures/fig_teaser.jpg""/> <br />     <em>      Figure 1: An example of our <DATASET>PAVS10K</DATASET> where coarse-to-fine annotations are provided, based on a guidance of fixations acquired from subjective experiments conducted by multiple (N) subjects wearing Head-Mounted Displays (HMDs) and headphones.","/figures/fig_teaser.jpg""/> <br />     <em>      Figure 1: An example of our PAVS10K where coarse-to-fine annotations are provided, based on a guidance of fixations acquired from subjective experiments conducted by multiple (N) subjects wearing Head-Mounted Displays (HMDs) and headphones.","/figures/fig_teaser.jpg""/> <br />     <em>      Figure 1: An example of our <DATASET>PAVS10K</DATASET> where coarse-to-fine annotations are provided, based on a guidance of fixations acquired from subjective experiments conducted by multiple (N) subjects wearing Head-Mounted Displays (HMDs) and headphones.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\41c02e99.txt,1.0
13,"Each (e.g., fk, fl and fn, where random integral values {k, l, n} ∈ [1, T ]) of the total equirectangular (ER) video frames T of the sequence “Speaking”(Super-class)-“Brothers”(sub-class) are manually labeled with both object-level and instance-level pixel-wise masks.","Each (e.g., fk, fl and fn, where random integral values {k, l, n} ∈ [1, T ]) of the total equirectangular (ER) video frames T of the sequence “Speaking”(Super-class)-“Brothers”(sub-class) are manually labeled with both object-level and instance-level pixel-wise masks.","Each (e.g., fk, fl and fn, where random integral values {k, l, n} ∈ [1, T ]) of the total equirectangular (ER) video frames T of the sequence “Speaking”(Super-class)-“Brothers”(sub-class) are manually labeled with both object-level and instance-level pixel-wise masks.","Each (e.g., fk, fl and fn, where random integral values {k, l, n} ∈ [1, T ]) of the total equirectangular (ER) video frames T of the sequence “Speaking”(Super-class)-“Brothers”(sub-class) are manually labeled with both object-level and instance-level pixel-wise masks.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\cc8b55d9.txt,1.0
14,"According to the features of defined salient objects within each of the sequences, multiple attributes, e.g., “multiple objects” (MO), “competing sounds” (CS), “geometrical distortion” (GD), “motion blur” (MB), “occlusions” (OC) and “low resolution” (LR) are further annotated to enable detailed analysis for PAV-SOD modeling.","According to the features of defined salient objects within each of the sequences, multiple attributes, e.g., “multiple objects” (MO), “competing sounds” (CS), “geometrical distortion” (GD), “motion blur” (MB), “occlusions” (OC) and “low resolution” (LR) are further annotated to enable detailed analysis for <SOFTWARE>PAV-SOD</SOFTWARE> modeling.","According to the features of defined salient objects within each of the sequences, multiple attributes, e.g., “multiple objects” (MO), “competing sounds” (CS), “geometrical distortion” (GD), “motion blur” (MB), “occlusions” (OC) and “low resolution” (LR) are further annotated to enable detailed analysis for PAV-SOD modeling.","According to the features of defined salient objects within each of the sequences, multiple attributes, e.g., “multiple objects” (MO), “competing sounds” (CS), “geometrical distortion” (GD), “motion blur” (MB), “occlusions” (OC) and “low resolution” (LR) are further annotated to enable detailed analysis for <PROJECT>PAV-SOD</PROJECT> modeling.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\d687b569.txt,1.0
15,"</em> </p>  <p align=""center"">     <img src="".","</em> </p>  <p align=""center"">     <img src="".","</em> </p>  <p align=""center"">     <img src="".
","</em> </p>  <p align=""center"">     <img src="".
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\61a78319.txt,0.989247311827957
16,"/figures/fig_related_datasets.jpg""/> <br />     <em>      Figure 2: Summary of widely used salient object detection (SOD)/video object segmentation (VOS) datasets and PAVS10K.","/figures/fig_related_datasets.jpg""/> <br />     <em>      Figure 2: Summary of widely used salient object detection (SOD)/video object segmentation (VOS) datasets and <DATASET>PAVS10K</DATASET>.","/figures/fig_related_datasets.jpg""/> <br />     <em>      Figure 2: Summary of widely used salient object detection (SOD)/video object segmentation (VOS) datasets and PAVS10K.
","/figures/fig_related_datasets.jpg""/> <br />     <em>      Figure 2: Summary of widely used salient object detection (SOD)/video object segmentation (VOS) datasets and <DATASET>PAVS10K</DATASET>.
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\5e561992.txt,0.9971509971509972
17,#Img: The number of images/video frames.,#Img: The number of images/video frames.,#Img: The number of images/video frames.,#<DATASET>Img</DATASET>: The number of images/video frames.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\1b5f2807.txt,1.0
18,#GT: The number of object-level pixel-wise masks (ground truth for SOD).,#GT: The number of object-level pixel-wise masks (ground truth for SOD).,#GT: The number of object-level pixel-wise masks (ground truth for SOD).,#<DATASET>GT</DATASET>: The number of object-level pixel-wise masks (ground truth for SOD).,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\2806fbd5.txt,1.0
19,Pub. = Publication.,Pub. = Publication.,Pub. = Publication.,<PUBLICATION>Pub.</PUBLICATION> = <PUBLICATION>Publication</PUBLICATION>.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\3dacb85e.txt,1.0
20,Obj.,Obj.,"Obj.
","Obj.
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\f9846b79.txt,0.8888888888888888
21,-Level = Object-Level Labels.,-Level = Object-Level Labels.,-Level = Object-Level Labels.,-Level = <DATASET>Object-Level Labels</DATASET>.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\1f5fdf47.txt,1.0
22,Ins.,Ins.,Ins.,Ins.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\a61c336a.txt,1.0
23,-Level = Instance-Level Labels.,-Level = Instance-Level Labels.,-Level = Instance-Level Labels.,-Level = <DATASET>Instance-Level Labels</DATASET>.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\2d70a6a9.txt,1.0
24,Fix.,Fix.,Fix.,Fix.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\3ef643d0.txt,1.0
25,GT = Fixation Maps. † denotes equirectangular images.,GT = Fixation Maps. † denotes equirectangular images.,GT = Fixation Maps. † denotes equirectangular images.,GT = Fixation Maps. † denotes equirectangular images.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\3926fa06.txt,1.0
26,"</em> </p>  <p align=""center"">     <img src="".","</em> </p>  <p align=""center"">     <img src="".","</em> </p>  <p align=""center"">     <img src="".
","</em> </p>  <p align=""center"">     <img src="".
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\61a78319.txt,0.989247311827957
27,"/figures/fig_dataset_examples.jpg""/> <br />     <em>      Figure 3: Examples of challenging attributes on equirectangular images from our PAVS10K, with instance-level ground truth and fixations as annotation guidance.","/figures/fig_dataset_examples.jpg""/> <br />     <em>      Figure 3: Examples of challenging attributes on equirectangular images from our <DATASET>PAVS10K</DATASET>, with instance-level ground truth and fixations as annotation guidance.","/figures/fig_dataset_examples.jpg""/> <br />     <em>      Figure 3: Examples of challenging attributes on equirectangular images from our PAVS10K, with instance-level ground truth and fixations as annotation guidance.
","/figures/fig_dataset_examples.jpg""/> <br />     <em>      Figure 3: Examples of challenging attributes on equirectangular images from our <DATASET>PAVS10K</DATASET>, with instance-level ground truth and fixations as annotation guidance.
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\61218527.txt,0.9977011494252873
28,"{𝑓𝑘  , 𝑓𝑙  , 𝑓𝑛  } denote random frames of a given video.","{𝑓𝑘  , 𝑓𝑙  , 𝑓𝑛  } denote random frames of a given video.","{𝑓𝑘  , 𝑓𝑙  , 𝑓𝑛  } denote random frames of a given video.","{𝑓𝑘  , 𝑓𝑙  , 𝑓𝑛  } denote random frames of a given video.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\b8ee3cc3.txt,1.0
29,"</em> </p>  <p align=""center"">     <img src="".","</em> </p>  <p align=""center"">     <img src="".","</em> </p>  <p align=""center"">     <img src="".
","</em> </p>  <p align=""center"">     <img src="".
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\61a78319.txt,0.989247311827957
30,"/figures/fig_dataset_statistics.jpg""/> <br />     <em>      Figure 4: Statistics of the proposed PAVS10K.","/figures/fig_dataset_statistics.jpg""/> <br />     <em>      Figure 4: Statistics of the proposed <DATASET>PAVS10K</DATASET>.","/figures/fig_dataset_statistics.jpg""/> <br />     <em>      Figure 4: Statistics of the proposed PAVS10K.
","/figures/fig_dataset_statistics.jpg""/> <br />     <em>      Figure 4: Statistics of the proposed <DATASET>PAVS10K</DATASET>.
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\692c995d.txt,0.995260663507109
31,(a) Super-/sub-category information.,(a) Super-/sub-category information.,(a) Super-/sub-category information.,(a) Super-/sub-category information.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\b8f5c774.txt,1.0
32,(b) Instance density (labeled frames per sequence) of each sub-class.,(b) Instance density (labeled frames per sequence) of each sub-class.,(b) Instance density (labeled frames per sequence) of each sub-class.,(b) <EVALMETRIC>Instance density</EVALMETRIC> (labeled frames per sequence) of each sub-class.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\ffb401b4.txt,1.0
33,"(c) Sound sources of PAVS10K scenes, such as musical instruments, human instances and animals.","(c) Sound sources of <DATASET>PAVS10K</DATASET> scenes, such as musical instruments, human instances and animals.","(c) Sound sources of PAVS10K scenes, such as musical instruments, human instances and animals.","(c) Sound sources of <DATASET>PAVS10K</DATASET> scenes, such as musical instruments, human instances and animals.",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\5d5cf047.txt,1.0
34,"</em> </p>  ------  # Benchmark Models  **No.** | **Year** | **Pub.** | **Title** | **Links**  :-: | :-:| :-: | :-  | :-:  01 | **2019**| **CVPR** | Cascaded Partial Decoder for Fast and Accurate Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Cascaded_Partial_Decoder_for_Fast_and_Accurate_Salient_Object_Detection_CVPR_2019_paper.pdf)/[Code](https://github.com/wuzhe71/CPD)  02 | **2019**| **CVPR** | See More, Know More: Unsupervised Video Object Segmentation with Co-Attention Siamese Networks | [Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Lu_See_More_Know_More_Unsupervised_Video_Object_Segmentation_With_Co-Attention_CVPR_2019_paper.pdf)/[Code](https://github.com/carrierlxk/COSNet)  03 | **2019**| **ICCV** | Stacked Cross Refinement Network for Edge-Aware Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Stacked_Cross_Refinement_Network_for_Edge-Aware_Salient_Object_Detection_ICCV_2019_paper.pdf)/[Code](https://github.com/wuzhe71/SCRN) 04 | **2019**| **ICCV** | Semi-Supervised Video Salient Object Detection Using Pseudo-Labels | [Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Semi-Supervised_Video_Salient_Object_Detection_Using_Pseudo-Labels_ICCV_2019_paper.pdf)/[Code](https://github.com/Kinpzz/RCRNet-Pytorch) 05 | **2020**| **AAAI** | F³Net: Fusion, Feedback and Focus for Salient Object Detection | [Paper](https://ojs.aaai.org/index.php/AAAI/article/download/6916/6770)/[Code](https://github.com/weijun88/F3Net) 06 | **2020**| **AAAI** | Pyramid Constrained Self-Attention Network for Fast Video Salient Object Detection | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/6718/6572)/[Code](https://github.com/guyuchao/PyramidCSA) 07 | **2020**| **CVPR** | Multi-scale Interactive Network for Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Pang_Multi-Scale_Interactive_Network_for_Salient_Object_Detection_CVPR_2020_paper.pdf)/[Code](https://github.com/lartpang/MINet) 08 | **2020**| **CVPR** | Label Decoupling Framework for Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Label_Decoupling_Framework_for_Salient_Object_Detection_CVPR_2020_paper.pdf)/[Code](https://github.com/weijun88/LDF) 09 | **2020**| **ECCV** | Highly Efficient Salient Object Detection with 100K Parameters | [Paper](http://mftp.mmcheng.net/Papers/20EccvSal100k.pdf)/[Code](https://github.com/ShangHua-Gao/SOD100K) 10 | **2020**| **ECCV** | Suppress and Balance: A Simple Gated Network for Salient Object Detection | [Paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470035.pdf)/[Code](https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency) 11 | **2020**| **BMVC** | Making a Case for 3D Convolutions for Object Segmentation in Videos | [Paper](https://www.bmvc2020-conference.com/assets/papers/0233.pdf)/[Code](https://github.com/sabarim/3DC-Seg) 12 | **2020**| **SPL** | FANet: Features Adaptation Network for 360° Omnidirectional Salient Object Detection | [Paper](https://ieeexplore.ieee.org/document/9211754)/[Code](https://github.com/DreaMKHuang/FANet) 13 | **2021**| **CVPR** | Reciprocal Transformations for Unsupervised Video Object Segmentation | [Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Reciprocal_Transformations_for_Unsupervised_Video_Object_Segmentation_CVPR_2021_paper.pdf)/[Code](https://github.com/OliverRensu/RTNet)  ------  # CAV-Net  The codes are available at [src](https://github.com/PanoAsh/PAV-SOD/tree/main/src).","</em> </p>  ------  # Benchmark Models  **No.** | **Year** | **Pub.** | **Title** | **Links**  :-: | :-:| :-: | :-  | :-:  01 | **2019**| **<CONFERENCE>CVPR</CONFERENCE>** | <PUBLICATION>Cascaded Partial Decoder for Fast and Accurate Salient Object Detection</PUBLICATION> | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>CVPR_2019</CONFERENCE>/papers/Wu_Cascaded_Partial_Decoder_for_Fast_and_Accurate_Salient_Object_Detection_<CONFERENCE>CVPR_2019</CONFERENCE>_paper.pdf)/[Code](https://github.com/wuzhe71/CPD)  02 | **2019**| **<CONFERENCE>CVPR</CONFERENCE>** | <PUBLICATION>See More, Know More: Unsupervised Video Object Segmentation with Co-Attention Siamese Networks</PUBLICATION> | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>CVPR_2019</CONFERENCE>/papers/Lu_See_More_Know_More_Unsupervised_Video_Object_Segmentation_With_Co-Attention_<CONFERENCE>CVPR_2019</CONFERENCE>_paper.pdf)/[Code](https://github.com/carrierlxk/COSNet)  03 | **2019**| **<CONFERENCE>ICCV</CONFERENCE>** | <PUBLICATION>Stacked Cross Refinement Network for Edge-Aware Salient Object Detection</PUBLICATION> | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>ICCV_2019</CONFERENCE>/papers/Wu_Stacked_Cross_Refinement_Network_for_Edge-Aware_Salient_Object_Detection_<CONFERENCE>ICCV_2019</CONFERENCE>_paper.pdf)/[Code](https://github.com/wuzhe71/SCRN) 04 | **2019**| **<CONFERENCE>ICCV</CONFERENCE>** | <PUBLICATION>Semi-Supervised Video Salient Object Detection Using Pseudo-Labels</PUBLICATION> | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>ICCV_2019</CONFERENCE>/papers/Yan_Semi-Supervised_Video_Salient_Object_Detection_Using_Pseudo-Labels_<CONFERENCE>ICCV_2019</CONFERENCE>_paper.pdf)/[Code](https://github.com/Kinpzz/RCRNet-Pytorch) 05 | **2020**| **<CONFERENCE>AAAI</CONFERENCE>** | <PUBLICATION>F³Net: Fusion, Feedback and Focus for Salient Object Detection</PUBLICATION> | [Paper](https://ojs.aaai.org/index.php/<CONFERENCE>AAAI</CONFERENCE>/article/download/6916/6770)/[Code](https://github.com/weijun88/F3Net) 06 | **2020**| **<CONFERENCE>AAAI</CONFERENCE>** | <PUBLICATION>Pyramid Constrained Self-Attention Network for Fast Video Salient Object Detection</PUBLICATION> | [Paper](https://ojs.aaai.org/index.php/<CONFERENCE>AAAI</CONFERENCE>/article/view/6718/6572)/[Code](https://github.com/guyuchao/PyramidCSA) 07 | **2020**| **<CONFERENCE>CVPR</CONFERENCE>** | <PUBLICATION>Multi-scale Interactive Network for Salient Object Detection</PUBLICATION> | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>CVPR_2020</CONFERENCE>/papers/Pang_Multi-Scale_Interactive_Network_for_Salient_Object_Detection_<CONFERENCE>CVPR_2020</CONFERENCE>_paper.pdf)/[Code](https://github.com/lartpang/MINet) 08 | **2020**| **<CONFERENCE>CVPR</CONFERENCE>** | <PUBLICATION>Label Decoupling Framework for Salient Object Detection</PUBLICATION> | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>CVPR_2020</CONFERENCE>/papers/Wei_Label_Decoupling_Framework_for_Salient_Object_Detection_<CONFERENCE>CVPR_2020</CONFERENCE>_paper.pdf)/[Code](https://github.com/weijun88/LDF) 09 | **2020**| **<CONFERENCE>ECCV</CONFERENCE>** | <PUBLICATION>Highly Efficient Salient Object Detection with 100K Parameters</PUBLICATION> | [Paper](http://mftp.mmcheng.net/Papers/20EccvSal100k.pdf)/[Code](https://github.com/ShangHua-Gao/SOD100K) 10 | **2020**| **<CONFERENCE>ECCV</CONFERENCE>** | <PUBLICATION>Suppress and Balance: A Simple Gated Network for Salient Object Detection</PUBLICATION> | [Paper](https://www.ecva.net/papers/<CONFERENCE>eccv_2020</CONFERENCE>/papers_<CONFERENCE>ECCV</CONFERENCE>/papers/123470035.pdf)/[Code](https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency) 11 | **2020**| **<CONFERENCE>BMVC</CONFERENCE>** | <PUBLICATION>Making a Case for 3D Convolutions for Object Segmentation in Videos</PUBLICATION> | [Paper](https://www.bmvc2020-conference.com/assets/papers/0233.pdf)/[Code](https://github.com/sabarim/3DC-Seg) 12 | **2020**| **<CONFERENCE>SPL</CONFERENCE>** | <PUBLICATION>FANet: Features Adaptation Network for 360° Omnidirectional Salient Object Detection</PUBLICATION> | [Paper](https://ieeexplore.ieee.org/document/9211754)/[Code](https://github.com/DreaMKHuang/FANet) 13 | **2021**| **<CONFERENCE>CVPR</CONFERENCE>** | <PUBLICATION>Reciprocal Transformations for Unsupervised Video Object Segmentation</PUBLICATION> | [Paper](https://openaccess.thecvf.com/content/<CONFERENCE>CVPR2021</CONFERENCE>/papers/Ren_Reciprocal_Transformations_for_Unsupervised_Video_Object_Segmentation_<CONFERENCE>CVPR_2021</CONFERENCE>_paper.pdf)/[Code](https://github.com/OliverRensu/RTNet)  ------  # CAV-Net  The codes are available at [src](https://github.com/PanoAsh/<PROJECT>PAV-SOD</PROJECT>/tree/main/src).","</em> </p>  ------  # Benchmark Models  **No.** | **Year** | **Pub.** | **Title** | **Links**  :-: | :-:| :-: | :-  | :-:  01 | **2019**| **CVPR** | Cascaded Partial Decoder for Fast and Accurate Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Cascaded_Partial_Decoder_for_Fast_and_Accurate_Salient_Object_Detection_CVPR_2019_paper.pdf)/[Code](https://github.com/wuzhe71/CPD)  02 | **2019**| **CVPR** | See More, Know More: Unsupervised Video Object Segmentation with Co-Attention Siamese Networks | [Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Lu_See_More_Know_More_Unsupervised_Video_Object_Segmentation_With_Co-Attention_CVPR_2019_paper.pdf)/[Code](https://github.com/carrierlxk/COSNet)  03 | **2019**| **ICCV** | Stacked Cross Refinement Network for Edge-Aware Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Stacked_Cross_Refinement_Network_for_Edge-Aware_Salient_Object_Detection_ICCV_2019_paper.pdf)/[Code](https://github.com/wuzhe71/SCRN) 04 | **2019**| **ICCV** | Semi-Supervised Video Salient Object Detection Using Pseudo-Labels | [Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Semi-Supervised_Video_Salient_Object_Detection_Using_Pseudo-Labels_ICCV_2019_paper.pdf)/[Code](https://github.com/Kinpzz/RCRNet-Pytorch) 05 | **2020**| **AAAI** | F³Net: Fusion, Feedback and Focus for Salient Object Detection | [Paper](https://ojs.aaai.org/index.php/AAAI/article/download/6916/6770)/[Code](https://github.com/weijun88/F3Net) 06 | **2020**| **AAAI** | Pyramid Constrained Self-Attention Network for Fast Video Salient Object Detection | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/6718/6572)/[Code](https://github.com/guyuchao/PyramidCSA) 07 | **2020**| **CVPR** | Multi-scale Interactive Network for Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Pang_Multi-Scale_Interactive_Network_for_Salient_Object_Detection_CVPR_2020_paper.pdf)/[Code](https://github.com/lartpang/MINet) 08 | **2020**| **CVPR** | Label Decoupling Framework for Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Label_Decoupling_Framework_for_Salient_Object_Detection_CVPR_2020_paper.pdf)/[Code](https://github.com/weijun88/LDF) 09 | **2020**| **ECCV** | Highly Efficient Salient Object Detection with 100K Parameters | [Paper](http://mftp.mmcheng.net/Papers/20EccvSal100k.pdf)/[Code](https://github.com/ShangHua-Gao/SOD100K) 10 | **2020**| **ECCV** | Suppress and Balance: A Simple Gated Network for Salient Object Detection | [Paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470035.pdf)/[Code](https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency) 11 | **2020**| **BMVC** | Making a Case for 3D Convolutions for Object Segmentation in Videos | [Paper](https://www.bmvc2020-conference.com/assets/papers/0233.pdf)/[Code](https://github.com/sabarim/3DC-Seg) 12 | **2020**| **SPL** | FANet: Features Adaptation Network for 360° Omnidirectional Salient Object Detection | [Paper](https://ieeexplore.ieee.org/document/9211754)/[Code](https://github.com/DreaMKHuang/FANet) 13 | **2021**| **CVPR** | Reciprocal Transformations for Unsupervised Video Object Segmentation | [Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Reciprocal_Transformations_for_Unsupervised_Video_Object_Segmentation_CVPR_2021_paper.pdf)/[Code](https://github.com/OliverRensu/RTNet)  ------  # CAV-Net  The codes are available at [src](https://github.com/PanoAsh/PAV-SOD/tree/main/src).
","</em> </p>  ------  # Benchmark Models  **No.** | **Year** | **Pub.** | **Title** | **Links**  :-: | :-:| :-: | :-  | :-:  01 | **2019**| **<CONFERENCE>CVPR</CONFERENCE>** | Cascaded Partial Decoder for Fast and Accurate Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>CVPR</CONFERENCE>_2019/papers/Wu_Cascaded_Partial_Decoder_for_Fast_and_Accurate_Salient_Object_Detection_<CONFERENCE>CVPR</CONFERENCE>_2019_paper.pdf)/[Code](https://github.com/wuzhe71/CPD)  02 | **2019**| **<CONFERENCE>CVPR</CONFERENCE>** | See More, Know More: Unsupervised Video Object Segmentation with Co-Attention Siamese Networks | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>CVPR</CONFERENCE>_2019/papers/Lu_See_More_Know_More_Unsupervised_Video_Object_Segmentation_With_Co-Attention_<CONFERENCE>CVPR</CONFERENCE>_2019_paper.pdf)/[Code](https://github.com/carrierlxk/COSNet)  03 | **2019**| **<CONFERENCE>ICCV</CONFERENCE>** | Stacked Cross Refinement Network for Edge-Aware Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>ICCV</CONFERENCE>_2019/papers/Wu_Stacked_Cross_Refinement_Network_for_Edge-Aware_Salient_Object_Detection_<CONFERENCE>ICCV</CONFERENCE>_2019_paper.pdf)/[Code](https://github.com/wuzhe71/SCRN) 04 | **2019**| **<CONFERENCE>ICCV</CONFERENCE>** | Semi-Supervised Video Salient Object Detection Using Pseudo-Labels | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>ICCV</CONFERENCE>_2019/papers/Yan_Semi-Supervised_Video_Salient_Object_Detection_Using_Pseudo-Labels_<CONFERENCE>ICCV</CONFERENCE>_2019_paper.pdf)/[Code](https://github.com/Kinpzz/RCRNet-Pytorch) 05 | **2020**| **<CONFERENCE>AAAI</CONFERENCE>** | F³Net: Fusion, Feedback and Focus for Salient Object Detection | [Paper](https://ojs.aaai.org/index.php/<CONFERENCE>AAAI</CONFERENCE>/article/download/6916/6770)/[Code](https://github.com/weijun88/F3Net) 06 | **2020**| **<CONFERENCE>AAAI</CONFERENCE>** | Pyramid Constrained Self-Attention Network for Fast Video Salient Object Detection | [Paper](https://ojs.aaai.org/index.php/<CONFERENCE>AAAI</CONFERENCE>/article/view/6718/6572)/[Code](https://github.com/guyuchao/PyramidCSA) 07 | **2020**| **<CONFERENCE>CVPR</CONFERENCE>** | Multi-scale Interactive Network for Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>CVPR</CONFERENCE>_2020/papers/Pang_Multi-Scale_Interactive_Network_for_Salient_Object_Detection_<CONFERENCE>CVPR</CONFERENCE>_2020_paper.pdf)/[Code](https://github.com/lartpang/MINet) 08 | **2020**| **<CONFERENCE>CVPR</CONFERENCE>** | Label Decoupling Framework for Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_<CONFERENCE>CVPR</CONFERENCE>_2020/papers/Wei_Label_Decoupling_Framework_for_Salient_Object_Detection_<CONFERENCE>CVPR</CONFERENCE>_2020_paper.pdf)/[Code](https://github.com/weijun88/LDF) 09 | **2020**| **<CONFERENCE>ECCV</CONFERENCE>** | Highly Efficient Salient Object Detection with 100K Parameters | [Paper](http://mftp.mmcheng.net/Papers/20EccvSal100k.pdf)/[Code](https://github.com/ShangHua-Gao/SOD100K) 10 | **2020**| **<CONFERENCE>ECCV</CONFERENCE>** | Suppress and Balance: A Simple Gated Network for Salient Object Detection | [Paper](https://www.ecva.net/papers/<CONFERENCE>eccv</CONFERENCE>_2020/papers_<CONFERENCE>ECCV</CONFERENCE>/papers/123470035.pdf)/[Code](https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency) 11 | **2020**| **<CONFERENCE>BMVC</CONFERENCE>** | Making a Case for 3D Convolutions for Object Segmentation in Videos | [Paper](https://www.<CONFERENCE>bmvc</CONFERENCE>2020-conference.com/assets/papers/0233.pdf)/[Code](https://github.com/sabarim/3DC-Seg) 12 | **2020**| **<CONFERENCE>SPL</CONFERENCE>** | FANet: Features Adaptation Network for 360° Omnidirectional Salient Object Detection | [Paper](https://ieeexplore.ieee.org/document/9211754)/[Code](https://github.com/DreaMKHuang/FANet) 13 | **2021**| **<CONFERENCE>CVPR</CONFERENCE>** | Reciprocal Transformations for Unsupervised Video Object Segmentation | [Paper](https://openaccess.thecvf.com/content/<CONFERENCE>CVPR</CONFERENCE>2021/papers/Ren_Reciprocal_Transformations_for_Unsupervised_Video_Object_Segmentation_<CONFERENCE>CVPR</CONFERENCE>_2021_paper.pdf)/[Code](https://github.com/OliverRensu/RTNet)  ------  # CAV-Net  The codes are available at [src](https://github.com/PanoAsh/<SOFTWARE>PAV-SOD</SOFTWARE>/tree/main/src).
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\decd6da1.txt,0.9998610918183081
35,The pre-trained models can be downloaded at [Google Drive](https://drive.google.com/file/d/1gNWmgmlBfJqCYE5phDuHTMFIou1TAmXs/view?,The pre-trained models can be downloaded at [<SOFTWARE>Google Drive</SOFTWARE>](https://drive.google.com/file/d/1gNWmgmlBfJqCYE5phDuHTMFIou1TAmXs/view?,The pre-trained models can be downloaded at [Google Drive](https://drive.google.com/file/d/1gNWmgmlBfJqCYE5phDuHTMFIou1TAmXs/view?,The pre-trained models can be downloaded at [<SOFTWARE>Google Drive</SOFTWARE>](https://drive.google.com/file/d/1gNWmgmlBfJqCYE5phDuHTMFIou1TAmXs/view?,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\e9617c22.txt,1.0
36,usp=sharing).  ------  # Dataset Downloads  The whole object-/instance-level ground truth with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/1Whp_ftuXza8-vkjNtICdxdRebcmzcrFi/view?,usp=sharing).  ------  # Dataset Downloads  The whole object-/instance-level ground truth with default split can be downloaded from [<SOFTWARE>Google Drive</SOFTWARE>](https://drive.google.com/file/d/1Whp_ftuXza8-vkjNtICdxdRebcmzcrFi/view?,"Here is the annotated text in Markdown format:

usp=sharing).  ------  # Dataset Downloads  The whole object-/instance-level ground truth with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/1Whp_ftuXza8-vkjNtICdxdRebcmzcrFi/view?","Here is the annotated text in Markdown format:

usp=sharing).  ------  # <DATASET>Dataset Downloads</DATASET>  The whole object-/instance-level ground truth with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/1Whp_ftuXza8-vkjNtICdxdRebcmzcrFi/view?",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\600e553b.txt,0.9008264462809917
37,usp=sharing).,usp=sharing).,"usp=sharing).
","usp=sharing).
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\e4c0f51a.txt,0.9629629629629629
38,The videos (with ambisonics) with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/13FEv1yAyMmK4GkiZ2Mce6gJxQuME7vG3/view?,The videos (with ambisonics) with default split can be downloaded from [<SOFTWARE>Google Drive</SOFTWARE>](https://drive.google.com/file/d/13FEv1yAyMmK4GkiZ2Mce6gJxQuME7vG3/view?,The videos (with ambisonics) with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/13FEv1yAyMmK4GkiZ2Mce6gJxQuME7vG3/view?,The videos (with ambisonics) with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/13FEv1yAyMmK4GkiZ2Mce6gJxQuME7vG3/view?,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\16679b30.txt,1.0
39,usp=sharing).,usp=sharing).,"usp=sharing).
","usp=sharing).
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\e4c0f51a.txt,0.9629629629629629
40,The videos (with mono sound) can be downloaded from [Google Drive](https://drive.google.com/file/d/1klJnHSiUM7Ow2LkdaLe-O6CEQ9qmdo2F/view?,The videos (with mono sound) can be downloaded from [<SOFTWARE>Google Drive</SOFTWARE>](https://drive.google.com/file/d/1klJnHSiUM7Ow2LkdaLe-O6CEQ9qmdo2F/view?,The videos (with mono sound) can be downloaded from [Google Drive](https://drive.google.com/file/d/1klJnHSiUM7Ow2LkdaLe-O6CEQ9qmdo2F/view?,The videos (with mono sound) can be downloaded from [<SOFTWARE>Google Drive</SOFTWARE>](https://drive.google.com/file/d/1klJnHSiUM7Ow2LkdaLe-O6CEQ9qmdo2F/view?,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\3dda8446.txt,1.0
41,usp=sharing)  The audio files (.wav) can be downloaded from [Google Drive](https://drive.google.com/file/d/1-jqDArcm8vBhyku3Xb8HLopG1XmpAS13/view?,usp=sharing)  The audio files (.wav) can be downloaded from [<SOFTWARE>Google Drive</SOFTWARE>](https://drive.google.com/file/d/1-jqDArcm8vBhyku3Xb8HLopG1XmpAS13/view?,"Here is the annotated text in Markdown format:

usp=sharing)  The audio files (.wav) can be downloaded from [Google Drive](https://drive.google.com/file/d/1-jqDArcm8vBhyku3Xb8HLopG1XmpAS13/view?","Here is the annotated text in Markdown format:

usp=sharing)  The audio files (.wav) can be downloaded from [Google Drive](https://drive.google.com/file/d/1-jqDArcm8vBhyku3Xb8HLopG1XmpAS13/view?",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\04420409.txt,0.8588235294117647
42,usp=sharing).,usp=sharing).,"usp=sharing).
","usp=sharing).
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\e4c0f51a.txt,0.9629629629629629
43,The head movement and eye fixation data can be downloaded from [Google Drive](https://drive.google.com/drive/folders/1EpWc7GVcGFAn5VigV3c2-ZtIZElfXPX1?,The head movement and eye fixation data can be downloaded from [<SOFTWARE>Google Drive</SOFTWARE>](https://drive.google.com/drive/folders/1EpWc7GVcGFAn5VigV3c2-ZtIZElfXPX1?,The head movement and eye fixation data can be downloaded from [Google Drive](https://drive.google.com/drive/folders/1EpWc7GVcGFAn5VigV3c2-ZtIZElfXPX1?,The head movement and eye fixation data can be downloaded from [Google Drive](https://drive.google.com/drive/folders/1EpWc7GVcGFAn5VigV3c2-ZtIZElfXPX1?,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\d4c5c7bf.txt,1.0
44,usp=sharing).,usp=sharing).,"usp=sharing).
","usp=sharing).
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\e4c0f51a.txt,0.9629629629629629
45,"To generate video frames, please refer to [video_to_frames.py](https://github.com/PanoAsh/ASOD60K/blob/main/video_to_frames.py).","To generate video frames, please refer to [video_to_frames.py](https://github.com/PanoAsh/ASOD60K/blob/main/video_to_frames.py).","To generate video frames, please refer to [video_to_frames.py](https://github.com/PanoAsh/ASOD60K/blob/main/video_to_frames.py).
","To generate video frames, please refer to [<SOFTWARE>video_to_frames.py</SOFTWARE>](https://github.com/<PROJECT>PanoAsh</PROJECT>/<PROJECT>ASOD60K</PROJECT>/blob/main/<SOFTWARE>video_to_frames.py</SOFTWARE>).
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\dfea2186.txt,0.9961089494163424
46,"To get access to raw videos on YouTube, please refer to [video_seq_link](https://github.com/PanoAsh/ASOD60K/blob/main/video_seq_link)","To get access to raw videos on YouTube, please refer to [video_seq_link](https://github.com/PanoAsh/ASOD60K/blob/main/video_seq_link)","To get access to raw videos on YouTube, please refer to [video_seq_link](https://github.com/PanoAsh/ASOD60K/blob/main/video_seq_link)","To get access to raw videos on YouTube, please refer to [video_seq_link](https://github.com/<PROJECT>PanoAsh</PROJECT>/<PROJECT>ASOD60K</PROJECT>/blob/main/video_seq_link)",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\4a7cdab1.txt,1.0
47,.,.,Fix.,Fix.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\3ef643d0.txt,0.4
48,> Note: The PAVS10K dataset does not own the copyright of videos.,> Note: The <DATASET>PAVS10K</DATASET> dataset does not own the copyright of videos.,> Note: The PAVS10K dataset does not own the copyright of videos.,> Note: The <DATASET>PAVS10K</DATASET> dataset does not own the copyright of videos.,../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\9d62c4d2.txt,1.0
49,"Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to PAVS10K.  ------  # Citation          @article{zhang2023pav,       title={PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={ACM Transactions on Multimedia Computing, Communications and Applications},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).","Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to <DATASET>PAVS10K</DATASET>.  ------  # Citation          @article{zhang2023pav,       title={<PUBLICATION>PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection</PUBLICATION><PROJECT>PAV-SOD</PROJECT>: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={<PUBLICATION>ACM Transactions on Multimedia Computing, Communications and Applications</PUBLICATION>},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).","Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to PAVS10K.  ------  # Citation          @article{zhang2023pav},       title={PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={ACM Transactions on Multimedia Computing, Communications and Applications},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).
","Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to <DATASET>PAVS10K</DATASET>.  ------  # Citation          @article{zhang2023pav},       title={<PUBLICATION>PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection</PUBLICATION>},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={<PUBLICATION>ACM Transactions on Multimedia Computing, Communications and Applications</PUBLICATION>},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).
",../results/deepseek-chat/prompt-0/zzz_PanoAsh_ASOD60K_main_README.md.tsv\ec120a26.txt,0.9985754985754985
