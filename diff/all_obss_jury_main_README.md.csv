sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?","<h1 align=""center""><SOFTWARE>Jury</SOFTWARE></h1>  <p align=""center""> <a href=""https://<SOFTWARE>pypi</SOFTWARE>.org/project/<SOFTWARE>jury</SOFTWARE>""><img src=""https://img.shields.io/<SOFTWARE>pypi</SOFTWARE>/pyversions/<SOFTWARE>jury</SOFTWARE>"" alt=""<SOFTWARE>Python</SOFTWARE> versions""></a> <a href=""https://pepy.tech/project/<SOFTWARE>jury</SOFTWARE>""><img src=""https://pepy.tech/badge/<SOFTWARE>jury</SOFTWARE>"" alt=""downloads""></a> <a href=""https://<SOFTWARE>pypi</SOFTWARE>.org/project/<SOFTWARE>jury</SOFTWARE>""><img src=""https://img.shields.io/<SOFTWARE>pypi</SOFTWARE>/v/<SOFTWARE>jury</SOFTWARE>?","<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?","<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/<PROGLANG>jury</PROGLANG>"" alt=""<PROGLANG>Python</PROGLANG> versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/<SOFTWARE>jury</SOFTWARE>"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/<SOFTWARE>jury</SOFTWARE>?",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\80fe2304.txt,1.0
2,"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.","color=blue"" alt=""<SOFTWARE>PyPI</SOFTWARE> version""></a> <a href=""https://github.com/obss/<SOFTWARE>jury</SOFTWARE>/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/<SOFTWARE>jury</SOFTWARE>""></a> <a href=""https://colab.research.google.com/github/obss/<SOFTWARE>jury</SOFTWARE>/blob/main/examples/<SOFTWARE>jury</SOFTWARE>_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/<SOFTWARE>jury</SOFTWARE>/actions""><img alt=""Build status"" src=""https://github.com/obss/<SOFTWARE>jury</SOFTWARE>/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/<SOFTWARE>pypi</SOFTWARE>/<SOFTWARE>jury</SOFTWARE>""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/<SOFTWARE>jury</SOFTWARE>""></a> <a href=""https://github.com/psf/<SOFTWARE>black</SOFTWARE>""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/<SOFTWARE>jury</SOFTWARE>/blob/main/LICENSE""><img alt=""License: <LICENSE>MIT</LICENSE>"" src=""https://img.shields.io/<SOFTWARE>pypi</SOFTWARE>/l/<SOFTWARE>jury</SOFTWARE>""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.","color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.","color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\03a707fc.txt,1.0
3,Jury offers a smooth and easy-to-use interface.,<SOFTWARE>Jury</SOFTWARE> offers a smooth and easy-to-use interface.,`Jury offers a smooth and easy-to-use interface.`,`Jury offers a smooth and easy-to-use interface.`,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\3ad0a21a.txt,0.9791666666666666
4,"It uses a more advanced version of [evaluate](https://github.com/huggingface/evaluate/) design for underlying metric computation, so that adding custom metric is easy as extending proper class.","It uses a more advanced version of [<SOFTWARE>evaluate</SOFTWARE>](https://github.com/huggingface/<SOFTWARE>evaluate</SOFTWARE>/) design for underlying metric computation, so that adding custom metric is easy as extending proper class.","It uses a more advanced version of `evaluate`(https://github.com/huggingface/evaluate/) design for underlying metric computation, so that adding custom metric is easy as extending proper class.","It uses a more advanced version of `<SOFTWARE>evaluate</SOFTWARE>`(https://github.com/huggingface/<SOFTWARE>evaluate</SOFTWARE>/) design for underlying metric computation, so that adding custom metric is easy as extending proper class.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\583e6f9e.txt,0.9896373056994818
5,Main advantages that Jury offers are:  - Easy to use for any NLP project. - Unified structure for computation input across all metrics. - Calculate many metrics at once. - Metrics calculations can be handled concurrently to save processing time. - It seamlessly supports evaluation for multiple predictions/multiple references.,Main advantages that <SOFTWARE>Jury</SOFTWARE> offers are:  - Easy to use for any NLP project. - Unified structure for computation input across all metrics. - Calculate many metrics at once. - Metrics calculations can be handled concurrently to save processing time. - It seamlessly supports evaluation for multiple predictions/multiple references.,Main advantages that Jury offers are:  - Easy to use for any NLP project. - Unified structure for computation input across all metrics. - Calculate many metrics at once. - Metrics calculations can be handled concurrently to save processing time. - It seamlessly supports evaluation for multiple predictions/multiple references.,Main advantages that Jury offers are:  - Easy to use for any <PROGLANG>NLP</PROGLANG> project. - Unified structure for computation input across all <EVALMETRIC>metrics</EVALMETRIC>. - Calculate many <EVALMETRIC>metrics</EVALMETRIC> at once. - <EVALMETRIC>Metrics</EVALMETRIC> calculations can be handled concurrently to save processing time. - It seamlessly supports evaluation for multiple predictions/multiple references.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\a79dabc0.txt,1.0
6,"To see more, check the [official Jury blog post](https://medium.com/codable/jury-evaluating-performance-of-nlg-models-730eb9c9999f).  ## 🔥  News  * (2024.05.29) [Retraction Watch Post](https://retractionwatch.com/2024/05/29/caught-by-a-reviewer-a-plagiarizing-deep-learning-paper-lingers/) regarding retraction of a paper has been posted.","To see more, check the [official Jury blog post](https://medium.com/codable/jury-evaluating-performance-of-nlg-models-730eb9c9999f).  ## 🔥  News  * (2024.05.29) [Retraction Watch Post](https://retractionwatch.com/2024/05/29/caught-by-a-reviewer-a-plagiarizing-deep-learning-paper-lingers/) regarding retraction of a paper has been posted.","To see more, check the [official Jury blog post](https://medium.com/codable/jury-evaluating-performance-of-nlg-models-730eb9c9999f).  ## 🔥  News  * (2024.05.29) [Retraction Watch Post](https://retractionwatch.com/2024/05/29/caught-by-a-reviewer-a-plagiarizing-deep-learning-paper-lingers/) regarding retraction of a paper has been posted.","To see more, check the [official <PROJECT>Jury</PROJECT> blog post](https://medium.com/codable/<PROJECT>jury</PROJECT>-evaluating-performance-of-nlg-models-730eb9c9999f).  ## 🔥  News  * (2024.05.29) [<PUBLICATION>Retraction Watch Post</PUBLICATION>](https://retractionwatch.com/2024/05/29/caught-by-a-reviewer-a-plagiarizing-deep-learning-paper-lingers/) regarding retraction of a paper has been posted.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\7748bde8.txt,1.0
7,[The plagiarised paper](https://aclanthology.org/2022.coling-1.306.pdf) has been retracted. * (2023.10.03) Jury paper is out currently is on [arxiv](https://arxiv.org/abs/2310.02040).,[The plagiarised paper](https://aclanthology.org/2022.coling-1.306.pdf) has been retracted. * (2023.10.03) <SOFTWARE>Jury</SOFTWARE> paper is out currently is on [arxiv](https://arxiv.org/abs/2310.02040).,[The plagiarised paper](https://aclanthology.org/2022.coling-1.306.pdf) has been retracted. * (2023.10.03) Jury paper is out currently is on [arxiv](https://arxiv.org/abs/2310.02040).,[The plagiarised paper](https://aclanthology.org/2022.coling-1.306.pdf) has been retracted. * (2023.10.03) <PUBLICATION>Jury paper</PUBLICATION> is out currently is on [arxiv](https://arxiv.org/abs/2310.02040).,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\7646eddb.txt,1.0
8,"Please cite this paper if your work use Jury, and if your publication material will be submitted to the venues after this date","Please cite this paper if your work use <SOFTWARE>Jury</SOFTWARE>, and if your publication material will be submitted to the venues after this date","`Please cite this paper if your work use Jury, and if your publication material will be submitted to the venues after this date`","`Please cite this paper if your work use <SOFTWARE>Jury</SOFTWARE>, and if your <PUBLICATION>publication material</PUBLICATION> will be submitted to the venues after this date`",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\6870ec74.txt,0.9921259842519685
9,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\cdb4ee2a.txt,1.0
10,* (2023.07.30) **Public notice:** You can reach our official [Public Notice](https://docs.google.com/document/d/1mFFT0cR8BUHKJki8mAg6b36QhmsRxvKR3pwOlcxbnss/edit?,* (2023.07.30) **Public notice:** You can reach our official [Public Notice](https://docs.google.com/document/d/1mFFT0cR8BUHKJki8mAg6b36QhmsRxvKR3pwOlcxbnss/edit?,* (2023.07.30) **Public notice:** You can reach our official [Public Notice](https://docs.google.com/document/d/1mFFT0cR8BUHKJki8mAg6b36QhmsRxvKR3pwOlcxbnss/edit?,* (2023.07.30) **Public notice:** You can reach our official [Public Notice](https://docs.google.com/document/d/1mFFT0cR8BUHKJki8mAg6b36QhmsRxvKR3pwOlcxbnss/edit?,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\a4a1440c.txt,1.0
11,"usp=sharing) document that poses a claim about plagiarism of the work, *jury*, presented in this codebase.  ## Available Metrics  The table below shows the current support status for available metrics","usp=sharing) document that poses a claim about plagiarism of the work, *<SOFTWARE>jury</SOFTWARE>*, presented in this codebase.  ## Available Metrics  The table below shows the current support status for available metrics","usp=sharing) document that poses a claim about plagiarism of the work, *jury*, presented in this codebase.  ## Available Metrics  The table below shows the current support status for available metrics","usp=sharing) document that poses a claim about plagiarism of the work, *jury*, presented in this codebase.  ## Available Metrics  The table below shows the current support status for available <EVALMETRIC>metrics</EVALMETRIC>",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\c48690a8.txt,1.0
12,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\cdb4ee2a.txt,1.0
13,| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | <EVALMETRIC>Accuracy-Numeric</EVALMETRIC>                                                              | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Accuracy-Text</EVALMETRIC>                                                                 | :heavy_check_mark: | :x:                 | | <EVALMETRIC>Bartscore</EVALMETRIC>                                                                     | :heavy_check_mark: | :x:                 | | <EVALMETRIC>Bertscore</EVALMETRIC>                                                                     | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Bleu</EVALMETRIC>                                                                          | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Bleurt</EVALMETRIC>                                                                        | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>CER</EVALMETRIC>                                                                           | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>CHRF</EVALMETRIC>                                                                          | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>COMET</EVALMETRIC>                                                                         | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>F1-Numeric</EVALMETRIC>                                                                    | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>F1-Text</EVALMETRIC>                                                                       | :heavy_check_mark: | :x:                 | | <EVALMETRIC>METEOR</EVALMETRIC>                                                                        | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Precision-Numeric</EVALMETRIC>                                                             | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Precision-Text</EVALMETRIC>                                                                | :heavy_check_mark: | :x:                 | | <EVALMETRIC>Prism</EVALMETRIC>                                                                         | :heavy_check_mark: | :x:                 | | <EVALMETRIC>Recall-Numeric</EVALMETRIC>                                                                | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Recall-Text</EVALMETRIC>                                                                   | :heavy_check_mark: | :x:                 | | <EVALMETRIC>ROUGE</EVALMETRIC>                                                                         | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>SacreBleu</EVALMETRIC>                                                                     | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Seqeval</EVALMETRIC>                                                                       | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Squad</EVALMETRIC>                                                                         | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>TER</EVALMETRIC>                                                                           | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>WER</EVALMETRIC>                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/<SOFTWARE>evaluate</SOFTWARE>/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `<SOFTWARE>evaluate</SOFTWARE>` package apart from those which are present in the  table.,| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | <EVALMETRIC>Accuracy-Numeric</EVALMETRIC>                                                              | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Accuracy-Text</EVALMETRIC>                                                                 | :heavy_check_mark: | :x:                 | | <EVALMETRIC>Bartscore</EVALMETRIC>                                                                     | :heavy_check_mark: | :x:                 | | <EVALMETRIC>Bertscore</EVALMETRIC>                                                                     | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Bleu</EVALMETRIC>                                                                          | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Bleurt</EVALMETRIC>                                                                        | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>CER</EVALMETRIC>                                                                           | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>CHRF</EVALMETRIC>                                                                          | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>COMET</EVALMETRIC>                                                                         | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>F1-Numeric</EVALMETRIC>                                                                    | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>F1-Text</EVALMETRIC>                                                                       | :heavy_check_mark: | :x:                 | | <EVALMETRIC>METEOR</EVALMETRIC>                                                                        | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Precision-Numeric</EVALMETRIC>                                                             | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Precision-Text</EVALMETRIC>                                                                | :heavy_check_mark: | :x:                 | | <EVALMETRIC>Prism</EVALMETRIC>                                                                         | :heavy_check_mark: | :x:                 | | <EVALMETRIC>Recall-Numeric</EVALMETRIC>                                                                | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Recall-Text</EVALMETRIC>                                                                   | :heavy_check_mark: | :x:                 | | <EVALMETRIC>ROUGE</EVALMETRIC>                                                                         | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>SacreBleu</EVALMETRIC>                                                                     | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Seqeval</EVALMETRIC>                                                                       | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>Squad</EVALMETRIC>                                                                         | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>TER</EVALMETRIC>                                                                           | :heavy_check_mark: | :white_check_mark:  | | <EVALMETRIC>WER</EVALMETRIC>                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/<SOFTWARE>evaluate</SOFTWARE>/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `<SOFTWARE>evaluate</SOFTWARE>` package apart from those which are present in the  table.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\d33add8f.txt,1.0
14,"**Notes**  * The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it  can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although  unfortunately full Jury support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?","**Notes**  * The entry :heavy_check_mark: represents that full <SOFTWARE>Jury</SOFTWARE> support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for <SOFTWARE>Jury</SOFTWARE> through the `<SOFTWARE>evaluate</SOFTWARE>`), so that it  can (and should) be used just like the `<SOFTWARE>evaluate</SOFTWARE>` metric as instructed in `<SOFTWARE>evaluate</SOFTWARE>` implementation although  unfortunately full <SOFTWARE>Jur</SOFTWARE>y support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/<SOFTWARE>jury</SOFTWARE>/issues/new?","**Notes**  
* The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple references) are supported  
* The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although unfortunately full Jury support for those metrics are not yet available.  
## Request for a New Metric  
For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?","**Notes**  
* The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple references) are supported  
* The entry :white_check_mark: means that this metric is supported (for Jury through the `<SOFTWARE>evaluate</SOFTWARE>`), so that it can (and should) be used just like the `<SOFTWARE>evaluate</SOFTWARE>` metric as instructed in `<SOFTWARE>evaluate</SOFTWARE>` implementation although unfortunately full Jury support for those metrics are not yet available.  
## Request for a New Metric  
For the request of a new metric please [open an issue](https://github.com/obss/<PROJECT>jury</PROJECT>/issues/new?",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\09c54d15.txt,0.994261119081779
15,assignees=&labels=&template=new-metric.md&title=) providing the minimum information.,assignees=&labels=&template=new-metric.md&title=) providing the minimum information.,assignees=&labels=&template=new-metric.md&title=) providing the minimum information.,assignees=&labels=&template=<PUBLICATION>new-metric.md</PUBLICATION>&title=) providing the minimum information.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\5a61181a.txt,1.0
16,"Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.","Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through <SOFTWARE>pip</SOFTWARE>,      <SOFTWARE>pip</SOFTWARE> install <SOFTWARE>jury</SOFTWARE>  or build from source,      git clone https://github.com/obss/<SOFTWARE>jury</SOFTWARE>.git     cd jury     <SOFTWARE>python</SOFTWARE> setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `<SOFTWARE>sacrebleu</SOFTWARE>` package on Windows machines which is  mainly due to the package `<SOFTWARE>pywin32</SOFTWARE>`.","Also, PRs addressing new metric supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.","Also, PRs addressing new <EVALMETRIC>metric</EVALMETRIC> supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install <SOFTWARE>jury</SOFTWARE>  or build from source,      git clone https://github.com/obss/<SOFTWARE>jury</SOFTWARE>.git     cd <SOFTWARE>jury</SOFTWARE>     <PROGLANG>python</PROGLANG> setup.py install  **NOTE:** There may be malfunctions of some <EVALMETRIC>metrics</EVALMETRIC> depending on `<SOFTWARE>sacrebleu</SOFTWARE>` package on Windows machines which is  mainly due to the package `<SOFTWARE>pywin32</SOFTWARE>`.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\67e8aa2b.txt,0.9987577639751553
17,"For this, we fixed pywin32 version on our setup config for Windows platforms.","For this, we fixed <SOFTWARE>pywin32</SOFTWARE> version on our setup config for Windows platforms.","For this, we fixed pywin32 version on our setup config for Windows platforms.","For this, we fixed <PROGLANG>pywin32</PROGLANG> version on our setup config for Windows platforms.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\e2279dfa.txt,1.0
18,"However, if pywin32 causes trouble in your environment we strongly recommend using `conda` manager install the package  as `conda install pywin32`.  ## <div align=""center""> Usage </div>  ### API Usage  It is only two lines of code to evaluate generated outputs.","However, if <SOFTWARE>pywin32</SOFTWARE> causes trouble in your environment we strongly recommend using `conda` manager install the package  as `<SOFTWARE>conda</SOFTWARE> install <SOFTWARE>pywin32</SOFTWARE>`.  ## <div align=""center""> Usage </div>  ### API Usage  It is only two lines of code to evaluate generated outputs.","However, if pywin32 causes trouble in your environment we strongly recommend using `conda` manager install the package  as `conda install pywin32`.  ## <div align=""center""> Usage </div>  ### API Usage  It is only two lines of code to evaluate generated outputs.","However, if <SOFTWARE>pywin32</SOFTWARE> causes trouble in your environment we strongly recommend using `<PROGLANG>conda</PROGLANG>` manager install the package  as `<PROGLANG>conda</PROGLANG> install <SOFTWARE>pywin32</SOFTWARE>`.  ## <div align=""center""> Usage </div>  ### API Usage  It is only two lines of code to evaluate generated outputs.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\9c9af48c.txt,1.0
19,"```python from jury import Jury  scorer = Jury() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!","```<PROGLANG>python</PROGLANG> from <SOFTWARE>jury</SOFTWARE> import <SOFTWARE>Jury</SOFTWARE>  scorer = <SOFTWARE>Jury</SOFTWARE>() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!","```python from jury import Jury  scorer = Jury() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!
```","```python from <PROGLANG>jury</PROGLANG> import <SOFTWARE>Jury</SOFTWARE>  scorer = <SOFTWARE>Jury</SOFTWARE>() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\09286f03.txt,0.9862068965517241
20,"a wonderful day.""] ] references = [     [""the cat is playing on the mat."", ""The cat plays on the mat.""],      [""Today is a wonderful day"", ""The weather outside is wonderful.""] ] scores = scorer(predictions=predictions, references=references) ```  Specify metrics you want to use on instantiation.","a wonderful day.""] ] references = [     [""the cat is playing on the mat."", ""The cat plays on the mat.""],      [""Today is a wonderful day"", ""The weather outside is wonderful.""] ] scores = scorer(predictions=predictions, references=references) ```  Specify metrics you want to use on instantiation.","a wonderful day.""] ] references = [     [""the cat is playing on the mat."", ""The cat plays on the mat.""],      [""Today is a wonderful day"", ""The weather outside is wonderful.""] ] scores = scorer(predictions=predictions, references=references) ```  Specify metrics you want to use on instantiation.","a wonderful day.""] ] references = [     [""the cat is playing on the mat."", ""The cat plays on the mat.""],      [""Today is a wonderful day"", ""The weather outside is wonderful.""] ] scores = scorer(predictions=predictions, references=references) ```  Specify metrics you want to use on instantiation.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\4d200313.txt,1.0
21,"```python scorer = Jury(metrics=[""bleu"", ""meteor""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.","```<PROGLANG>python</PROGLANG> scorer = <SOFTWARE>Jury</SOFTWARE>(metrics=[""<EVALMETRIC>bleu</EVALMETRIC>"", ""<EVALMETRIC>meteor</EVALMETRIC>""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `<SOFTWARE>jury</SOFTWARE>.metrics` as classes, and then instantiate and use as desired.","```python scorer = Jury(metrics=[""bleu"", ""meteor""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.
```","```python scorer = Jury(metrics=[""<EVALMETRIC>bleu</EVALMETRIC>"", ""<EVALMETRIC>meteor</EVALMETRIC>""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\4f6382f1.txt,0.9915254237288136
22,"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.","```<PROGLANG>python</PROGLANG> from <SOFTWARE>jury</SOFTWARE>.metrics import <EVALMETRIC>Bleu</EVALMETRIC>  <EVALMETRIC>bleu</EVALMETRIC> = <EVALMETRIC>Bleu</EVALMETRIC>.construct() score = <EVALMETRIC>bleu</EVALMETRIC>.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```<PROGLANG>python</PROGLANG> from <SOFTWARE>jury</SOFTWARE>.metrics import <EVALMETRIC>Bleu</EVALMETRIC>  <EVALMETRIC>bleu</EVALMETRIC> = <EVALMETRIC>Bleu</EVALMETRIC>.construct() score = <EVALMETRIC>bleu</EVALMETRIC>.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```<PROGLANG>python</PROGLANG> from <SOFTWARE>jury</SOFTWARE>.metrics import <EVALMETRIC>Bleu</EVALMETRIC> <EVALMETRIC>bleu</EVALMETRIC> = <EVALMETRIC>Bleu</EVALMETRIC>.construct(compute_kwargs={""max_order"": 1}) score = <EVALMETRIC>bleu</EVALMETRIC>.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `<SOFTWARE>jury</SOFTWARE>` and `<SOFTWARE>evaluate</SOFTWARE>` metrics through `<SOFTWARE>jury</SOFTWARE>.load_metric`.","```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.
```","```python from <SOFTWARE>jury.metrics</SOFTWARE> import <EVALMETRIC>Bleu</EVALMETRIC>  bleu = <EVALMETRIC>Bleu</EVALMETRIC>.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from <SOFTWARE>jury.metrics</SOFTWARE> import <EVALMETRIC>Bleu</EVALMETRIC>  bleu = <EVALMETRIC>Bleu</EVALMETRIC>.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from <SOFTWARE>jury.metrics</SOFTWARE> import <EVALMETRIC>Bleu</EVALMETRIC> bleu = <EVALMETRIC>Bleu</EVALMETRIC>.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `<SOFTWARE>jury</SOFTWARE>` and `<SOFTWARE>evaluate</SOFTWARE>` metrics through `<SOFTWARE>jury.load_metric</SOFTWARE>`.
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\ee0b5156.txt,0.9969834087481146
23,"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.","```<PROGLANG>python</PROGLANG> import <SOFTWARE>jury</SOFTWARE>  <EVALMETRIC>bleu</EVALMETRIC> = <SOFTWARE>jury</SOFTWARE>.load_metric(""<EVALMETRIC>bleu</EVALMETRIC>"") <EVALMETRIC>bleu</EVALMETRIC>_1 = <SOFTWARE>jury</SOFTWARE>.load_metric(""<EVALMETRIC>bleu</EVALMETRIC>"", resulting_name=""<EVALMETRIC>bleu</EVALMETRIC>_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `<SOFTWARE>jury</SOFTWARE>` but in `<SOFTWARE>evaluate</SOFTWARE>` wer = <SOFTWARE>jury</SOFTWARE>.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.","```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.
```","```python import <PROGLANG>jury</PROGLANG>  bleu = <PROGLANG>jury</PROGLANG>.load_metric(""<EVALMETRIC>bleu</EVALMETRIC>"") bleu_1 = <PROGLANG>jury</PROGLANG>.load_metric(""<EVALMETRIC>bleu</EVALMETRIC>"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `<PROGLANG>jury</PROGLANG>` but in `<PROGLANG>evaluate</PROGLANG>` wer = <PROGLANG>jury</PROGLANG>.load_metric(""<EVALMETRIC>competition_math</EVALMETRIC>"") # It falls back to `<PROGLANG>evaluate</PROGLANG>` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\38fe95ce.txt,0.9950617283950617
24,Each line should be paired in both files.,Each line should be paired in both files.,Each line should be paired in both files.,Each line should be paired in both files.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\af7b6836.txt,1.0
25,You can optionally provide reduce function and an export path for results to be written.,You can optionally provide reduce function and an export path for results to be written.,You can optionally provide reduce function and an export path for results to be written.,You can optionally provide reduce function and an export path for results to be written.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\8ab597e0.txt,1.0
26,jury eval --predictions /path/to/predictions.txt --references /path/to/references.txt --reduce_fn max --export /path/to/export.txt  You can also provide prediction folders and reference folders to evaluate multiple experiments.,<SOFTWARE>jury</SOFTWARE> eval --predictions /path/to/predictions.txt --references /path/to/references.txt --reduce_fn max --export /path/to/export.txt  You can also provide prediction folders and reference folders to evaluate multiple experiments.,jury eval --predictions /path/to/predictions.txt --references /path/to/references.txt --reduce_fn max --export /path/to/export.txt  You can also provide prediction folders and reference folders to evaluate multiple experiments.,jury eval --predictions /path/to/predictions.txt --references /path/to/references.txt --reduce_fn max --export /path/to/export.txt  You can also provide prediction folders and reference folders to evaluate multiple experiments.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\0dcc90ab.txt,1.0
27,"In this set up, however, it is required that the prediction and references files you need to evaluate as a pair have the same file name.","In this set up, however, it is required that the prediction and references files you need to evaluate as a pair have the same file name.","In this set up, however, it is required that the prediction and references files you need to evaluate as a pair have the same file name.","In this set up, however, it is required that the prediction and references files you need to evaluate as a pair have the same file name.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\0f6f3f53.txt,1.0
28,These common names are paired together for prediction and reference.,These common names are paired together for prediction and reference.,These common names are paired together for prediction and reference.,These common names are paired together for prediction and reference.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\0808e133.txt,1.0
29,"jury eval --predictions /path/to/predictions_folder --references /path/to/references_folder --reduce_fn max --export /path/to/export.txt  If you want to specify metrics, and do not want to use default, specify it in config file (json) in `metrics` key.","<SOFTWARE>jury</SOFTWARE> eval --predictions /path/to/predictions_folder --references /path/to/references_folder --reduce_fn max --export /path/to/export.txt  If you want to specify metrics, and do not want to use default, specify it in config file (json) in `metrics` key.","jury eval --predictions /path/to/predictions_folder --references /path/to/references_folder --reduce_fn max --export /path/to/export.txt  If you want to specify metrics, and do not want to use default, specify it in config file (json) in `metrics` key.","jury eval --predictions /path/to/predictions_folder --references /path/to/references_folder --reduce_fn max --export /path/to/export.txt  If you want to specify <EVALMETRIC>metrics</EVALMETRIC>, and do not want to use default, specify it in config file (json) in `<EVALMETRIC>metrics</EVALMETRIC>` key.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\ba4ab0ea.txt,1.0
30,"```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""bleu"",     ""meteor""   ] } ```  Then, you can call jury eval with `config` argument.","```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""<EVALMETRIC>bleu</EVALMETRIC>"",     ""<EVALMETRIC>meteor</EVALMETRIC>""   ] } ```  Then, you can call <SOFTWARE>jury</SOFTWARE> eval with `config` argument.","```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""bleu"",     ""meteor""   ] } ```  Then, you can call jury eval with `config` argument.
```","```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""<EVALMETRIC>bleu</EVALMETRIC>"",     ""<EVALMETRIC>meteor</EVALMETRIC>""   ] } ```  Then, you can call jury eval with `config` argument.
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\4a63e748.txt,0.9911111111111112
31,"jury eval --config path/to/config.json  ### Custom Metrics  You can use custom metrics with inheriting `jury.metrics.Metric`, you can see current metrics implemented on Jury from [jury/metrics](https://github.com/obss/jury/tree/master/jury/metrics).","<SOFTWARE>jury</SOFTWARE> eval --config path/to/config.json  ### Custom Metrics  You can use custom metrics with inheriting `<SOFTWARE>jury</SOFTWARE>.metrics.Metric`, you can see current metrics implemented on <SOFTWARE>Jury</SOFTWARE> from [<SOFTWARE>jury</SOFTWARE>/metrics](https://github.com/obss/<SOFTWARE>jury</SOFTWARE>/tree/master/<SOFTWARE>jury</SOFTWARE>/metrics).","jury eval --config path/to/config.json  ### Custom Metrics  You can use custom metrics with inheriting `jury.metrics.Metric`, you can see current metrics implemented on Jury from [jury/metrics](https://github.com/obss/jury/tree/master/jury/metrics).","jury eval --config path/to/config.json  ### Custom Metrics  You can use custom metrics with inheriting `<PROGLANG>jury.metrics.Metric</PROGLANG>`, you can see current metrics implemented on Jury from [jury/metrics](https://github.com/obss/jury/tree/master/jury/metrics).",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\d35a169a.txt,1.0
32,"Jury falls back to `evaluate` implementation of metrics for the ones that are currently not supported by Jury, you can see the metrics available for `evaluate` on [evaluate/metrics](https://github.com/huggingface/evaluate/tree/master/metrics).","<SOFTWARE>Jury</SOFTWARE> falls back to `evaluate` implementation of metrics for the ones that are currently not supported by <SOFTWARE>Jury</SOFTWARE>, you can see the metrics available for `evaluate` on [<SOFTWARE>evaluate</SOFTWARE>/metrics](https://github.com/huggingface/<SOFTWARE>evaluate</SOFTWARE>/tree/master/metrics).","Jury falls back to `evaluate` implementation of metrics for the ones that are currently not supported by Jury, you can see the metrics available for `evaluate` on [evaluate/metrics](https://github.com/huggingface/evaluate/tree/master/metrics).","Jury falls back to `evaluate` implementation of metrics for the ones that are currently not supported by Jury, you can see the metrics available for `evaluate` on [evaluate/metrics](https://github.com/huggingface/<EVALMETRIC>evaluate</EVALMETRIC>/tree/master/<EVALMETRIC>metrics</EVALMETRIC>).",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\7d1f3707.txt,1.0
33,Jury itself uses `evaluate.Metric` as a base class to drive its own base class as `jury.metrics.Metric`.,<SOFTWARE>Jury</SOFTWARE> itself uses `evaluate.Metric` as a base class to drive its own base class as `<SOFTWARE>jury</SOFTWARE>.metrics.Metric`.,Jury itself uses `evaluate.Metric` as a base class to drive its own base class as `jury.metrics.Metric`.,Jury itself uses `<EVALMETRIC>evaluate.Metric</EVALMETRIC>` as a base class to drive its own base class as `<EVALMETRIC>jury.metrics.Metric</EVALMETRIC>`.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\75b0268e.txt,1.0
34,"The interface is similar; however, Jury makes the metrics to take a unified input type by handling the inputs for each metrics, and allows supporting several input types as;  - single prediction & single reference - single prediction & multiple reference - multiple prediction & multiple reference  As a custom metric both base classes can be used; however, we strongly recommend using `jury.metrics.Metric` as it has several advantages such as supporting computations for the input types above or unifying the type of the input.","The interface is similar; however, <SOFTWARE>Jury</SOFTWARE> makes the metrics to take a unified input type by handling the inputs for each metrics, and allows supporting several input types as;  - single prediction & single reference - single prediction & multiple reference - multiple prediction & multiple reference  As a custom metric both base classes can be used; however, we strongly recommend using `<SOFTWARE>jury</SOFTWARE>.metrics.Metric` as it has several advantages such as supporting computations for the input types above or unifying the type of the input.","The interface is similar; however, Jury makes the metrics to take a unified input type by handling the inputs for each metrics, and allows supporting several input types as;  - single prediction & single reference - single prediction & multiple reference - multiple prediction & multiple reference  As a custom metric both base classes can be used; however, we strongly recommend using `jury.metrics.Metric` as it has several advantages such as supporting computations for the input types above or unifying the type of the input.","The interface is similar; however, <SOFTWARE>Jury</SOFTWARE> makes the metrics to take a unified input type by handling the inputs for each metrics, and allows supporting several input types as;  - single prediction & single reference - single prediction & multiple reference - multiple prediction & multiple reference  As a custom metric both base classes can be used; however, we strongly recommend using `<SOFTWARE>jury.metrics.Metric</SOFTWARE>` as it has several advantages such as supporting computations for the input types above or unifying the type of the input.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\5b249457.txt,1.0
35,"```python from jury.metrics import MetricForTask  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [jury.metrics.Metric](.","```<PROGLANG>python</PROGLANG> from <SOFTWARE>jury</SOFTWARE>.metrics import <SOFTWARE>MetricForTask</SOFTWARE>  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [<SOFTWARE>jury</SOFTWARE>.metrics.Metric](.","```python from jury.metrics import MetricForTask  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [jury.metrics.Metric](.
```","```python from <PROGLANG>jury.metrics</PROGLANG> import MetricForTask  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [<SOFTWARE>jury.metrics.Metric</SOFTWARE>](.
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\79a8b0fa.txt,0.9967845659163987
36,"/jury/metrics/_base.py)  ## <div align=""center""> Contributing </div>  PRs are welcomed as always :)  ### Installation      git clone https://github.com/obss/jury.git     cd jury     pip install -e "".","/jury/metrics/_base.py)  ## <div align=""center""> Contributing </div>  PRs are welcomed as always :)  ### Installation      <SOFTWARE>git</SOFTWARE> clone https://github.com/obss/<SOFTWARE>jury</SOFTWARE>.git     cd <SOFTWARE>jury</SOFTWARE>     <SOFTWARE>pip</SOFTWARE> install -e "".","/jury/metrics/_base.py)  ## <div align=""center""> Contributing </div>  PRs are welcomed as always :)  ### Installation      git clone https://github.com/obss/jury.git     cd jury     pip install -e "".","/jury/metrics/_base.py)  ## <div align=""center""> Contributing </div>  PRs are welcomed as always :)  ### Installation      git clone https://github.com/obss/<PROJECT>jury</PROJECT>.git     cd <PROJECT>jury</PROJECT>     pip install -e "".",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\cfcf2e76.txt,1.0
37,"[dev]""  Also, you need to install the packages which are available through a git source separately with the following command.","[dev]""  Also, you need to install the packages which are available through a git source separately with the following command.","[dev]""  Also, you need to install the packages which are available through a git source separately with the following command.","[dev]""  Also, you need to install the packages which are available through a git source separately with the following command.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\f3f393ff.txt,1.0
38,"For the folks who are curious about ""why?""","For the folks who are curious about ""why?""","For the folks who are curious about ""why?""","For the folks who are curious about ""why?""",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\f33edfef.txt,1.0
39,; a short explaination is that PYPI does not allow indexing a package which  are directly dependent on non-pypi packages due to security reasons.,; a short explaination is that PYPI does not allow indexing a package which  are directly dependent on non-pypi packages due to security reasons.,; a short explaination is that PYPI does not allow indexing a package which  are directly dependent on non-pypi packages due to security reasons.,; a short explaination is that PYPI does not allow indexing a package which  are directly dependent on non-pypi packages due to security reasons.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\51006c10.txt,1.0
40,"The file `requirements-dev.txt` includes packages  which are currently only available through a git source, or they are PYPI packages with no recent release or  incompatible with Jury, so that they are added as git sources or pointing to specific commits.","The file `requirements-dev.txt` includes packages  which are currently only available through a git source, or they are PYPI packages with no recent release or  incompatible with <SOFTWARE>Jury</SOFTWARE>, so that they are added as git sources or pointing to specific commits.","The file `requirements-dev.txt` includes packages  which are currently only available through a git source, or they are PYPI packages with no recent release or  incompatible with `Jury`, so that they are added as git sources or pointing to specific commits.","The file `<SOFTWARE>requirements-dev.txt</SOFTWARE>` includes packages  which are currently only available through a git source, or they are PYPI packages with no recent release or  incompatible with `<SOFTWARE>Jury</SOFTWARE>`, so that they are added as git sources or pointing to specific commits.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\079b35d1.txt,0.99609375
41,pip install -r requirements-dev.txt  ### Tests  To tests simply run.,<SOFTWARE>pip</SOFTWARE> install -r requirements-dev.txt  ### Tests  To tests simply run.,pip install -r requirements-dev.txt  ### Tests  To tests simply run.,pip install -r <SOFTWARE>requirements-dev.txt</SOFTWARE>  ### Tests  To tests simply run.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\5b79eb28.txt,1.0
42,"python tests/run_tests.py  ### Code Style  To check code style,      python tests/run_code_style.py check  To format codebase,      python tests/run_code_style.py format   ## <div align=""center""> Citation </div>  If you use this package in your work, please cite it as:      @misc{cavusoglu2023jury,       title={Jury: A Comprehensive Evaluation Toolkit},        author={Devrim Cavusoglu and Ulas Sert and Secil Sen and Sinan Altinuc},       year={2023},       eprint={2310.02040},       archivePrefix={arXiv},       primaryClass={cs.CL},       doi={10.48550/arXiv.2310.02040}     }  ## Community Interaction  We use the GitHub Issue Tracker to track issues in general.","<SOFTWARE>python</SOFTWARE> tests/run_tests.py  ### Code Style  To check code style,      <SOFTWARE>python</SOFTWARE> tests/run_code_style.py check  To format codebase,      <SOFTWARE>python</SOFTWARE> tests/run_code_style.py format   ## <div align=""center""> Citation </div>  If you use this package in your work, please cite it as:      @misc{cavusoglu2023<SOFTWARE>jury</SOFTWARE>,       title={<PUBLICATION>Jury: A Comprehensive Evaluation Toolkit</PUBLICATION><SOFTWARE>Jury</SOFTWARE>: A Comprehensive Evaluation Toolkit},        author={Devrim Cavusoglu and Ulas Sert and Secil Sen and Sinan Altinuc},       year={2023},       eprint={2310.02040},       archivePrefix={arXiv},       primaryClass={cs.CL},       doi={10.48550/arXiv.2310.02040}     }  ## Community Interaction  We use the GitHub Issue Tracker to track issues in general.","python tests/run_tests.py  ### Code Style  To check code style,      python tests/run_code_style.py check  To format codebase,      python tests/run_code_style.py format   ## <div align=""center""> Citation </div>  If you use this package in your work, please cite it as:      @misc{cavusoglu2023jury,       title={Jury: A Comprehensive Evaluation Toolkit},        author={Devrim Cavusoglu and Ulas Sert and Secil Sen and Sinan Altinuc},       year={2023},       eprint={2310.02040},       archivePrefix={arXiv},       primaryClass={cs.CL},       doi={10.48550/arXiv.2310.02040}     }  ## Community Interaction  We use the GitHub Issue Tracker to track issues in general.","<PROGLANG>python</PROGLANG> tests/run_tests.py  ### Code Style  To check code style,      <PROGLANG>python</PROGLANG> tests/run_code_style.py check  To format codebase,      <PROGLANG>python</PROGLANG> tests/run_code_style.py format   ## <div align=""center""> Citation </div>  If you use this package in your work, please cite it as:      @misc{cavusoglu2023jury,       title={<PUBLICATION>Jury: A Comprehensive Evaluation Toolkit</PUBLICATION>},        author={Devrim Cavusoglu and Ulas Sert and Secil Sen and Sinan Altinuc},       year={2023},       eprint={2310.02040},       archivePrefix={arXiv},       primaryClass={cs.CL},       doi={10.48550/arXiv.2310.02040}     }  ## Community Interaction  We use the GitHub Issue Tracker to track issues in general.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\23c866d8.txt,1.0
43,"Issues can be bug reports, feature requests or implementation of a new metric type.","Issues can be bug reports, feature requests or implementation of a new metric type.","Issues can be bug reports, feature requests or implementation of a new `metric` type.","Issues can be bug reports, feature requests or implementation of a new `<EVALMETRIC>metric</EVALMETRIC>` type.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\621a0411.txt,0.9880952380952381
44,Please refer to the related issue template for opening new issues,Please refer to the related issue template for opening new issues,Please refer to the related issue template for opening new issues,Please refer to the related issue template for opening new issues,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\bb364b64.txt,1.0
45,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\cdb4ee2a.txt,1.0
46,|                                | Location                                                                                           | |--------------------------------|----------------------------------------------------------------------------------------------------| | Bug Report                     | [Bug Report Template](https://github.com/obss/jury/issues/new?,|                                | Location                                                                                           | |--------------------------------|----------------------------------------------------------------------------------------------------| | Bug Report                     | [Bug Report Template](https://github.com/obss/jury/issues/new?,|                                | Location                                                                                           | |--------------------------------|----------------------------------------------------------------------------------------------------| | Bug Report                     | [Bug Report Template](https://github.com/obss/jury/issues/new?,|                                | Location                                                                                           | |--------------------------------|----------------------------------------------------------------------------------------------------| | Bug Report                     | [Bug Report Template](https://github.com/<PROJECT>obss</PROJECT>/<PROJECT>jury</PROJECT>/issues/new?,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\c5a3c203.txt,1.0
47,assignees=&labels=&projects=&template=bug_report.md&title=) | | New Metric Request             | [Request Metric Implementation](https://github.com/obss/jury/issues/new?,assignees=&labels=&projects=&template=bug_report.md&title=) | | New Metric Request             | [Request Metric Implementation](https://github.com/obss/<SOFTWARE>jury</SOFTWARE>/issues/new?,assignees=&labels=&projects=&template=bug_report.md&title=) | | New Metric Request             | [Request Metric Implementation](https://github.com/obss/jury/issues/new?,assignees=&labels=&projects=&template=bug_report.md&title=) | | New <EVALMETRIC>Metric</EVALMETRIC> Request             | [Request <EVALMETRIC>Metric</EVALMETRIC> Implementation](https://github.com/obss/jury/issues/new?,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\d9142b65.txt,1.0
48,"assignees=&labels=&projects=&template=new-metric.md&title=) | | All other issues and questions | [General Issues](https://github.com/obss/jury/issues/new)                                                            |  ## <div align=""center""> License </div>  Licensed under the [MIT](LICENSE) License.","assignees=&labels=&projects=&template=new-metric.md&title=) | | All other issues and questions | [General Issues](https://github.com/obss/<SOFTWARE>jury</SOFTWARE>/issues/new)                                                            |  ## <div align=""center""> License </div>  Licensed under the [<LICENSE>MIT</LICENSE>](LICENSE) License.","assignees=&labels=&projects=&template=new-metric.md&title=) | | All other issues and questions | [General Issues](https://github.com/obss/jury/issues/new)                                                            |  ## <div align=""center""> License </div>  Licensed under the [MIT](LICENSE) License.","assignees=&labels=&projects=&template=new-metric.md&title=) | | All other issues and questions | [General Issues](https://github.com/obss/jury/issues/new)                                                            |  ## <div align=""center""> License </div>  Licensed under the [<LICENSE>MIT</LICENSE>](LICENSE) License.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\53942213.txt,1.0
