sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
2,The LCF-BERT and LCF-associated codes is based on ABSA-Pytorch [Github codebase](https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/lcf_bert.py)  !,The LCF-BERT and LCF-associated codes is based on ABSA-Pytorch [Github codebase](https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/lcf_bert.py)  !,The `LCF-BERT` and `LCF-associated codes` is based on `ABSA-Pytorch` [Github codebase](https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/lcf_bert.py)  !,The `<SOFTWARE>LCF-BERT</SOFTWARE>` and `<SOFTWARE>LCF-associated codes</SOFTWARE>` is based on `<SOFTWARE>ABSA-Pytorch</SOFTWARE>` [Github codebase](https://github.com/<SOFTWARE>songyouwei</SOFTWARE>/<SOFTWARE>ABSA-PyTorch</SOFTWARE>/blob/master/models/<SOFTWARE>lcf_bert</SOFTWARE>.py)  !,../results/deepseek-chat/prompt-0/zzz_roysoumya_knowledge-aware-med-classification_master_README.md.tsv\d70ea764.txt,0.9813664596273292
4,/examples/ichi/run_ichi.py --model_type bert --model_name_or_path bert-base-uncased --do_lower_case --do_train --do_eval --data_dir .,/examples/ichi/run_ichi.py --model_type bert --model_name_or_path bert-base-uncased --do_lower_case --do_train --do_eval --data_dir .,`/examples/ichi/run_ichi.py --model_type bert --model_name_or_path bert-base-uncased --do_lower_case --do_train --do_eval --data_dir .`,`/examples/ichi/run_ichi.py --model_type bert --model_name_or_path <PROGLANG>bert-base-uncased</PROGLANG> --do_lower_case --do_train --do_eval --data_dir .`,../results/deepseek-chat/prompt-0/zzz_roysoumya_knowledge-aware-med-classification_master_README.md.tsv\6c83c130.txt,0.9925373134328358
6,"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification"".","/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification"".","/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: `CADEC` (multi-label) and `ICHI` (multi-class) (Datasets provided in the data directory).   ### Annotated `CADEC`, a multi-label medical forum question classificaton dataset We annotate `CADEC` as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification"".","/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: `<DATASET>CADEC</DATASET>` (multi-label) and `<DATASET>ICHI</DATASET>` (multi-class) (Datasets provided in the data directory).   ### Annotated `<DATASET>CADEC</DATASET>`, a multi-label medical forum question classificaton dataset We annotate `<DATASET>CADEC</DATASET>` as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification"".",../results/deepseek-chat/prompt-0/zzz_roysoumya_knowledge-aware-med-classification_master_README.md.tsv\b3037516.txt,0.9908256880733946
11,"Here, we develop a novel medical knowledge-aware BERT-based model (MedBERT) that explicitly gives more weightage to medical concept-bearing words, and utilize domain-specific side information obtained from a popular medical knowledge base.","Here, we develop a novel medical knowledge-aware BERT-based model (MedBERT) that explicitly gives more weightage to medical concept-bearing words, and utilize domain-specific side information obtained from a popular medical knowledge base.","Here, we develop a novel medical knowledge-aware BERT-based model (`MedBERT`) that explicitly gives more weightage to medical concept-bearing words, and utilize domain-specific side information obtained from a popular medical knowledge base.","Here, we develop a novel medical knowledge-aware BERT-based model (`<SOFTWARE>MedBERT</SOFTWARE>`) that explicitly gives more weightage to medical concept-bearing words, and utilize domain-specific side information obtained from a popular medical knowledge base.",../results/deepseek-chat/prompt-0/zzz_roysoumya_knowledge-aware-med-classification_master_README.md.tsv\c635f3c7.txt,0.9958333333333333
12,We also contribute a multi-label dataset for the Medical Forum Question Classification (MFQC) task.,We also contribute a multi-label dataset for the Medical Forum Question Classification (MFQC) task.,We also contribute a multi-label `Medical Forum Question Classification (MFQC)` dataset for the `MFQC` task.,We also contribute a multi-label `<DATASET>Medical Forum Question Classification (MFQC)</DATASET>` dataset for the `<DATASET>MFQC</DATASET>` task.,../results/deepseek-chat/prompt-0/zzz_roysoumya_knowledge-aware-med-classification_master_README.md.tsv\bdb43582.txt,0.8019323671497585
13,"MedBERT achieves state-of-the-art performance on two benchmark datasets and performs very well in low resource settings.}, booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management}, pages = {3398–3402}, numpages = {5}, keywords = {online health communities, clinical text classification}, location = {Virtual Event, Queensland, Australia}, series = {CIKM '21} } ```","MedBERT achieves state-of-the-art performance on two benchmark datasets and performs very well in low resource settings.}, booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management}, pages = {3398–3402}, numpages = {5}, keywords = {online health communities, clinical text classification}, location = {Virtual Event, Queensland, Australia}, series = {CIKM '21} } ```","MedBERT achieves state-of-the-art performance on two benchmark datasets and performs very well in low resource settings.}, booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management}, pages = {3398–3402}, numpages = {5}, keywords = {online health communities, clinical text classification}, location = {Virtual Event, Queensland, Australia}, series = {CIKM '21} }","MedBERT achieves state-of-the-art performance on two benchmark <DATASET>datasets</DATASET> and performs very well in low resource settings.}, booktitle = {<PUBLICATION>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</PUBLICATION>}, pages = {3398–3402}, numpages = {5}, keywords = {online health communities, clinical text classification}, location = {Virtual Event, Queensland, Australia}, series = {<CONFERENCE>CIKM '21</CONFERENCE>} }",../results/deepseek-chat/prompt-0/zzz_roysoumya_knowledge-aware-med-classification_master_README.md.tsv\bafe2c24.txt,0.9951690821256038
