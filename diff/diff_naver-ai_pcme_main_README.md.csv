sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
7,Dataset splits can be found in [datasets/annotations](datasets/annotations).,Dataset splits can be found in [datasets/annotations](datasets/annotations).,`Dataset splits can be found in [datasets/annotations](datasets/annotations).`,`<DATASET>Dataset splits</DATASET> can be found in [datasets/annotations](datasets/annotations).`,../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\5b225a2c.txt,0.987012987012987
8,"Note that we also need `instances_<train | val>2014.json` for computing PMRP score.  ### CUB Caption  Download images (CUB-200-2011) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016).","Note that we also need `instances_<train | val>2014.json` for computing PMRP score.  ### CUB Caption  Download images (CUB-200-2011) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016).","Note that we also need `instances_<train | val>2014.json` for computing `PMRP` score.  ### `CUB Caption`  Download images (`CUB-200-2011`) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016).","Note that we also need `<DATASET>instances_<train | val>2014.json</DATASET>` for computing `<EVALMETRIC>PMRP</EVALMETRIC>` score.  ### `<DATASET>CUB Caption</DATASET>`  Download images (`<DATASET>CUB-200-2011</DATASET>`) from [this link](http://www.vision.caltech.edu/visipedia/<DATASET>CUB-200-2011</DATASET>.html), and download caption from [reedscot/<CONFERENCE>cvpr2016</CONFERENCE>](https://github.com/reedscot/<CONFERENCE>cvpr2016</CONFERENCE>).",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\58968a87.txt,0.98989898989899
14,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     --model__eval_method <distance_method> \     # --model__cache_dir /vector_cache # if you use my docker image ```  You can choose `distance_method` in `['elk', 'l2', 'min', 'max', 'wasserstein', 'kl', 'reverse_kl', 'js', 'bhattacharyya', 'matmul', 'matching_prob']`   ## How to train  NOTE: we train each model with mixed-precision training (O2) on a single V100.","/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     --model__eval_method <distance_method> \     # --model__cache_dir /vector_cache # if you use my docker image ```  You can choose `distance_method` in `['elk', 'l2', 'min', 'max', 'wasserstein', 'kl', 'reverse_kl', 'js', 'bhattacharyya', 'matmul', 'matching_prob']`   ## How to train  NOTE: we train each model with mixed-precision training (O2) on a single V100.","/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     --model__eval_method distance_method \     # --model__cache_dir /vector_cache # if you use my docker image ```  You can choose `distance_method` in `['elk', 'l2', 'min', 'max', 'wasserstein', 'kl', 'reverse_kl', 'js', 'bhattacharyya', 'matmul', 'matching_prob']`   ## How to train  NOTE: we train each model with mixed-precision training (O2) on a single V100.","/config/cub/<DATASET>pcme_cub</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     --model__eval_method <EVALMETRIC>distance_method</EVALMETRIC> \     # --model__cache_dir /vector_cache # if you use my docker image ```  You can choose `<EVALMETRIC>distance_method</EVALMETRIC>` in `['<EVALMETRIC>elk</EVALMETRIC>', '<EVALMETRIC>l2</EVALMETRIC>', '<EVALMETRIC>min</EVALMETRIC>', '<EVALMETRIC>max</EVALMETRIC>', '<EVALMETRIC>wasserstein</EVALMETRIC>', '<EVALMETRIC>kl</EVALMETRIC>', '<EVALMETRIC>reverse_kl</EVALMETRIC>', '<EVALMETRIC>js</EVALMETRIC>', '<EVALMETRIC>bhattacharyya</EVALMETRIC>', '<EVALMETRIC>matmul</EVALMETRIC>', '<EVALMETRIC>matching_prob</EVALMETRIC>']`   ## How to train  NOTE: we train each model with mixed-precision training (O2) on a single V100.",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\924b29d7.txt,0.9685658153241651
19,"Since our split is based on the zero-shot learning benchmark [(Xian, et al. 2017)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Xian_Zero-Shot_Learning_-_CVPR_2017_paper.pdf), we leave out 50 classes from 200 bird classes for the evaluation","Since our split is based on the zero-shot learning benchmark [(Xian, et al. 2017)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Xian_Zero-Shot_Learning_-_CVPR_2017_paper.pdf), we leave out 50 classes from 200 bird classes for the evaluation","Since our split is based on the zero-shot learning benchmark `(Xian, et al. 2017)` [(Xian, et al. 2017)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Xian_Zero-Shot_Learning_-_CVPR_2017_paper.pdf), we leave out 50 classes from 200 bird classes for the evaluation","Since our split is based on the zero-shot learning benchmark `<PUBLICATION>(Xian, et al. 2017)</PUBLICATION>` [(Xian, et al. 2017)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Xian_Zero-Shot_Learning_-_CVPR_2017_paper.pdf), we leave out 50 classes from 200 bird classes for the evaluation",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\17c6211f.txt,0.9578544061302682
23,"Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. - Xian, Yongqin, Bernt Schiele, and Zeynep Akata.","Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. - Xian, Yongqin, Bernt Schiele, and Zeynep Akata.","`Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. - Xian, Yongqin, Bernt Schiele, and Zeynep Akata.`","`<PUBLICATION>Proceedings of the IEEE conference on computer vision and pattern recognition</PUBLICATION>. 2016. - Xian, Yongqin, Bernt Schiele, and Zeynep Akata.`",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\d83b73c2.txt,0.9925925925925926
24,"""Zero-shot learning-the good, the bad and the ugly.""","""Zero-shot learning-the good, the bad and the ugly.""","`""Zero-shot learning-the good, the bad and the ugly.""`","`""Zero-shot learning-the good, the bad and the ugly.""`",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\83357a45.txt,0.9811320754716981
30,"IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```","IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```","IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE <SOFTWARE>SOFTWARE</SOFTWARE> OR THE USE OR OTHER DEALINGS IN THE <SOFTWARE>SOFTWARE</SOFTWARE>.",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\a6d2ebf9.txt,0.9923076923076923
