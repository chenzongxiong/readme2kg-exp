sentence_idx,original_sentence,original_annotated,generated_annotated,source_file,similarity_score
1,"# Probabilistic Cross-Modal Embedding (PCME) CVPR 2021  Official Pytorch implementation of PCME | [Paper](https://arxiv.org/abs/2101.05068)  [Sanghyuk Chun](https://sanghyukchun.github.io/home/)<sup>1</sup> [Seong Joon Oh](https://seongjoonoh.com/)<sup>1</sup> Rafael Sampaio de Rezende<sup>2</sup> [Yannis Kalantidis](https://www.skamalas.com/)<sup>2</sup> Diane Larlus<sup>2</sup>  <sup>1</sup><sub>[NAVER AI LAB](https://naver-career.gitbook.io/en/teams/clova-cic)</sub><br> <sup>2</sup><sub>[NAVER LABS Europe](https://europe.naverlabs.com/)</sub>   <a href=""https://www.youtube.com/watch?","# <PUBLICATION>Probabilistic Cross-Modal Embedding (PCME)</PUBLICATION> <CONFERENCE>CVPR 2021</CONFERENCE>  Official <SOFTWARE>Pytorch</SOFTWARE> implementation of PCME | [Paper](https://arxiv.org/abs/2101.05068)  [Sanghyuk Chun](https://sanghyukchun.github.io/home/)<sup>1</sup> [Seong Joon Oh](https://seongjoonoh.com/)<sup>1</sup> Rafael Sampaio de Rezende<sup>2</sup> [Yannis Kalantidis](https://www.skamalas.com/)<sup>2</sup> Diane Larlus<sup>2</sup>  <sup>1</sup><sub>[NAVER AI LAB](https://naver-career.gitbook.io/en/teams/clova-cic)</sub><br> <sup>2</sup><sub>[NAVER LABS Europe](https://europe.naverlabs.com/)</sub>   <a href=""https://www.youtube.com/watch?","# <PROJECT>Probabilistic Cross-Modal Embedding (PCME)</PROJECT> <CONFERENCE>CVPR 2021</CONFERENCE>  Official <PROGLANG>Pytorch</PROGLANG> implementation of <PROJECT>PCME</PROJECT> | [<PUBLICATION>Paper</PUBLICATION>](https://arxiv.org/abs/2101.05068)  [Sanghyuk Chun](https://sanghyukchun.github.io/home/)<sup>1</sup> [Seong Joon Oh](https://seongjoonoh.com/)<sup>1</sup> Rafael Sampaio de Rezende<sup>2</sup> [Yannis Kalantidis](https://www.skamalas.com/)<sup>2</sup> Diane Larlus<sup>2</sup>  <sup>1</sup><sub>[<PROJECT>NAVER AI LAB</PROJECT>](https://naver-career.gitbook.io/en/teams/clova-cic)</sub><br> <sup>2</sup><sub>[<PROJECT>NAVER LABS Europe</PROJECT>](https://europe.naverlabs.com/)</sub>   <a href=""https://www.youtube.com/watch?
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\8647834a.txt,0.9991575400168492
2,"v=J_DaqSLEcVk""><img src=""http://img.youtube.com/vi/J_DaqSLEcVk/0.jpg""  alt=""VIDEO"" width=""700"" border=""10"" /></a>   ## Updates  - Jan 2024: [PCME++](https://openreview.net/forum?","v=J_DaqSLEcVk""><img src=""http://img.youtube.com/vi/J_DaqSLEcVk/0.jpg""  alt=""VIDEO"" width=""700"" border=""10"" /></a>   ## Updates  - Jan 2024: [PCME++](https://openreview.net/forum?","v=J_DaqSLEcVk""><img src=""http://img.youtube.com/vi/J_DaqSLEcVk/0.jpg""  alt=""VIDEO"" width=""700"" border=""10"" /></a>   ## Updates  - Jan 2024: [<PUBLICATION>PCME++</PUBLICATION>](https://openreview.net/forum?
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\7b982729.txt,0.9971988795518207
3,"id=ft1mr3WlGM), the improved version of PCME, is accepted at ICLR 2024.","id=ft1mr3WlGM), the improved version of PCME, is accepted at <CONFERENCE>ICLR 2024</CONFERENCE>.","id=ft1mr3WlGM), the improved version of PCME, is accepted at <CONFERENCE>ICLR 2024</CONFERENCE>.
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\de20239e.txt,0.993006993006993
4,Please use [naver-ai/pcmepp](https://github.com/naver-ai/pcmepp) for the improved version!,Please use [naver-ai/pcmepp](https://github.com/naver-ai/pcmepp) for the improved version!,Please use [<SOFTWARE>naver-ai/pcmepp</SOFTWARE>](https://github.com/<SOFTWARE>naver-ai/pcmepp</SOFTWARE>) for the improved version!,../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\878118e2.txt,1.0
5,"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).","- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` <SOFTWARE>pip</SOFTWARE> install <SOFTWARE>cython</SOFTWARE> && <SOFTWARE>pip</SOFTWARE> install -r requirements.txt <SOFTWARE>python</SOFTWARE> -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && <SOFTWARE>pip</SOFTWARE> install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my <SOFTWARE>docker</SOFTWARE> image as well ``` <SOFTWARE>docker</SOFTWARE> pull sanghyukchun/pcme:<SOFTWARE>torch1.2</SOFTWARE>-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/<DATASET>coco</DATASET>](config/<DATASET>coco</DATASET>) and [config/cub](config/cub)).","- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [<PUBLICATION>ECCV Caption</PUBLICATION>](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.   pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./   ### Dockerfile  You can use my docker image as well  docker pull sanghyukchun/pcme:torch1.2-apex-dali   Please Add --model__cache_dir /vector_cache when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\921f4ba0.txt,0.9908480780964003
6,"If you want to change only a few options, instead of re-writing a new configuration file, you can override the configuration as the follows:  ``` python <train | eval>.py --dataloader__batch_size 32 --dataloader__eval_batch_size 8 --model__eval_method matching_prob ```  See [config/parser.py](config/parser.py) for details  ## Dataset preparation  ### COCO Caption  We followed the same split provided by [VSE++](http://www.cs.toronto.edu/~faghri/vsepp/data.tar).","If you want to change only a few options, instead of re-writing a new configuration file, you can override the configuration as the follows:  ``` <SOFTWARE>python</SOFTWARE> <train | eval>.py --dataloader__batch_size 32 --dataloader__eval_batch_size 8 --model__eval_method matching_prob ```  See [config/parser.py](config/parser.py) for details  ## Dataset preparation  ### <DATASET>COCO Caption</DATASET>  We followed the same split provided by [VSE++](http://www.cs.toronto.edu/~faghri/vsepp/data.tar).","If you want to change only a few options, instead of re-writing a new configuration file, you can override the configuration as the follows:   <PROGLANG>python</PROGLANG> <train | eval>.py --dataloader__batch_size 32 --dataloader__eval_batch_size 8 --model__eval_method matching_prob   See [config/parser.py](config/parser.py) for details  ## Dataset preparation  ### <DATASET>COCO Caption</DATASET>  We followed the same split provided by [<SOFTWARE>VSE++</SOFTWARE>](http://www.cs.toronto.edu/~faghri/vsepp/data.tar).
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\92f6f671.txt,0.9924160346695557
7,Dataset splits can be found in [datasets/annotations](datasets/annotations).,Dataset splits can be found in [datasets/annotations](datasets/annotations).,<DATASET>Dataset splits</DATASET> can be found in [datasets/annotations](datasets/annotations).,../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\5b225a2c.txt,1.0
8,"Note that we also need `instances_<train | val>2014.json` for computing PMRP score.  ### CUB Caption  Download images (CUB-200-2011) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016).","Note that we also need `instances_<train | val>2014.json` for computing <EVALMETRIC>PMRP</EVALMETRIC> score.  ### <DATASET>CUB Caption</DATASET>  Download images (<DATASET>CUB-200-2011</DATASET>) from [this link](http://www.vision.caltech.edu/visipedia/<DATASET>CUB-200-2011</DATASET>.html), and download caption from [reedscot/<CONFERENCE>cvpr2016</CONFERENCE>](https://github.com/reedscot/<CONFERENCE>cvpr2016</CONFERENCE>).","Note that we also need <DATASET>instances_<train | val>2014.json</DATASET> for computing <EVALMETRIC>PMRP</EVALMETRIC> score.  ### <DATASET>CUB Caption</DATASET>  Download images (<DATASET>CUB-200-2011</DATASET>) from [this link](http://www.vision.caltech.edu/visipedia/<DATASET>CUB-200-2011</DATASET>.html), and download caption from [reedscot/<CONFERENCE>cvpr2016</CONFERENCE>](https://github.com/reedscot/<CONFERENCE>cvpr2016</CONFERENCE>).
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\58968a87.txt,0.9948892674616695
9,"You can use the image path and the caption path separately in the code.  ## Evaluate pretrained models  NOTE: the current implementation of plausible match R-Precision (PMRP) is not efficient: <br> It first dumps all ranked items for each item to a local file, and compute R-precision.","You can use the image path and the caption path separately in the code.  ## Evaluate pretrained models  NOTE: the current implementation of <EVALMETRIC>plausible match R-Precision</EVALMETRIC> (<EVALMETRIC>PMRP</EVALMETRIC>) is not efficient: <br> It first dumps all ranked items for each item to a local file, and compute <EVALMETRIC>R-precision</EVALMETRIC>.","You can use the image path and the caption path separately in the code.  ## Evaluate pretrained models  NOTE: the current implementation of plausible match <EVALMETRIC>R-Precision</EVALMETRIC> (<EVALMETRIC>PMRP</EVALMETRIC>) is not efficient: <br> It first dumps all ranked items for each item to a local file, and compute <EVALMETRIC>R-precision</EVALMETRIC>.
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\e4f3f55f.txt,0.9982486865148862
10,<br> We are planning to re-implement efficient PMRP as soon as possible.  ### COCO Caption  ``` # Compute recall metrics python evaluate_recall_coco.py .,<br> We are planning to re-implement efficient <EVALMETRIC>PMRP</EVALMETRIC> as soon as possible.  ### <DATASET>COCO Caption</DATASET>  ``` # Compute <EVALMETRIC>recall</EVALMETRIC> metrics <SOFTWARE>python</SOFTWARE> evaluate_<EVALMETRIC>recall</EVALMETRIC>_<DATASET>coco</DATASET>.py .,"<br> We are planning to re-implement efficient PMRP as soon as possible.  ### <DATASET>COCO Caption</DATASET>   # Compute recall metrics python evaluate_recall_coco.py .
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\97b89453.txt,0.9868421052631579
11,/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  ``` # Compute plausible match R-Precision (PMRP) metric python extract_rankings_coco.py .,/config/<DATASET>coco</DATASET>/pcme_<DATASET>coco</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  ``` # Compute <EVALMETRIC>plausible match R-Precision</EVALMETRIC> (<EVALMETRIC>PMRP</EVALMETRIC>) metric <SOFTWARE>python</SOFTWARE> extract_rankings_coco.py .,"/config/coco/<DATASET>pcme_coco</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image    # Compute plausible match R-Precision (PMRP) metric python extract_rankings_coco.py .
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\aea74ef1.txt,0.9792060491493384
12,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .","/config/<DATASET>coco</DATASET>/<SOFTWARE>pcme</SOFTWARE>_<DATASET>coco</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  <SOFTWARE>python</SOFTWARE> evaluate_<EVALMETRIC>pmrp</EVALMETRIC>_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K <EVALMETRIC>PMRP</EVALMETRIC> | I2T 1K <EVALMETRIC>R@1</EVALMETRIC> | I2T ECCV <EVALMETRIC>mAP@R</EVALMETRIC> | T2I 1K <EVALMETRIC>PMRP</EVALMETRIC> | T2I 1K <EVALMETRIC>R@1</EVALMETRIC> | T2I ECCV <EVALMETRIC>mAP@R</EVALMETRIC> | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_<DATASET>coco</DATASET>.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_<DATASET>coco</DATASET>.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [<DATASET>ECCV Caption</DATASET> dataset](https://github.com/naver-ai/<DATASET>eccv-caption</DATASET>) for more details of ""ECCV <EVALMETRIC>mAP@R</EVALMETRIC>"". - Paper: [<PUBLICATION>ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO</PUBLICATION><DATASET>ECCV Caption</DATASET>: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/<DATASET>eccv-caption</DATASET>](https://github.com/naver-ai/<DATASET>eccv-caption</DATASET>)  ### <DATASET>CUB Caption</DATASET>  ``` <SOFTWARE>python</SOFTWARE> evaluate_cub.py .","/config/coco/<DATASET>pcme_coco</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  <PROGLANG>python</PROGLANG> evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file>   | Method   | I2T 1K <EVALMETRIC>PMRP</EVALMETRIC> | I2T 1K <EVALMETRIC>R@1</EVALMETRIC> | I2T <CONFERENCE>ECCV</CONFERENCE> <EVALMETRIC>mAP@R</EVALMETRIC> | T2I 1K <EVALMETRIC>PMRP</EVALMETRIC> | T2I 1K <EVALMETRIC>R@1</EVALMETRIC> | T2I <CONFERENCE>ECCV</CONFERENCE> <EVALMETRIC>mAP@R</EVALMETRIC> | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | <SOFTWARE>PCME</SOFTWARE>     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | <SOFTWARE>PCME</SOFTWARE> (<DATASET>CutMix</DATASET>-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | <SOFTWARE>PVSE</SOFTWARE> K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | <SOFTWARE>PVSE</SOFTWARE> K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | <SOFTWARE>VSRN</SOFTWARE>     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | <SOFTWARE>VSRN</SOFTWARE> + <SOFTWARE>AOQ</SOFTWARE> | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [<CONFERENCE>ECCV</CONFERENCE> Caption <DATASET>dataset</DATASET>](https://github.com/naver-ai/eccv-caption) for more details of ""<CONFERENCE>ECCV</CONFERENCE> <EVALMETRIC>mAP@R</EVALMETRIC>"". - <PUBLICATION>Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359)</PUBLICATION> - <SOFTWARE>GitHub</SOFTWARE>: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### <DATASET>CUB Caption</DATASET>   <PROGLANG>python</PROGLANG> evaluate_cub.py .
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\ea24a684.txt,0.9977528089887641
13,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  NOTE: If you just download file from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016), then `caption_root` will be `cvpr2016_cub/text_c10`  If you want to test other probabilistic distances, such as Wasserstein distance or KL-divergence, try the following command:  ``` python evaluate_cub.py .","/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  NOTE: If you just download file from [reedscot/<CONFERENCE>cvpr2016</CONFERENCE>](https://github.com/reedscot/<CONFERENCE>cvpr2016</CONFERENCE>), then `caption_root` will be `<CONFERENCE>cvpr2016</CONFERENCE>_cub/text_c10`  If you want to test other probabilistic distances, such as <EVALMETRIC>Wasserstein distance</EVALMETRIC> or <EVALMETRIC>KL-divergence</EVALMETRIC>, try the following command:  ``` <SOFTWARE>python</SOFTWARE> evaluate_cub.py .","/config/cub/<DATASET>pcme_cub</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image   NOTE: If you just download file from [reedscot/<CONFERENCE>cvpr2016</CONFERENCE>](https://github.com/reedscot/<CONFERENCE>cvpr2016</CONFERENCE>), then caption_root will be <CONFERENCE>cvpr2016</CONFERENCE>_cub/text_c10  If you want to test other probabilistic distances, such as Wasserstein distance or KL-divergence, try the following command:   python evaluate_cub.py .
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\6bec0baa.txt,0.9660523763336566
14,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     --model__eval_method <distance_method> \     # --model__cache_dir /vector_cache # if you use my docker image ```  You can choose `distance_method` in `['elk', 'l2', 'min', 'max', 'wasserstein', 'kl', 'reverse_kl', 'js', 'bhattacharyya', 'matmul', 'matching_prob']`   ## How to train  NOTE: we train each model with mixed-precision training (O2) on a single V100.","/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     --model__eval_method <distance_method> \     # --model__cache_dir /vector_cache # if you use my docker image ```  You can choose `distance_method` in `['elk', 'l2', 'min', 'max', 'wasserstein', 'kl', 'reverse_kl', 'js', 'bhattacharyya', 'matmul', 'matching_prob']`   ## How to train  NOTE: we train each model with mixed-precision training (O2) on a single V100.","/config/cub/<DATASET>pcme_cub</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     --model__eval_method <EVALMETRIC>distance_method</EVALMETRIC> \     # --model__cache_dir /vector_cache # if you use my docker image   You can choose <EVALMETRIC>distance_method</EVALMETRIC> in ['<EVALMETRIC>elk</EVALMETRIC>', '<EVALMETRIC>l2</EVALMETRIC>', '<EVALMETRIC>min</EVALMETRIC>', '<EVALMETRIC>max</EVALMETRIC>', '<EVALMETRIC>wasserstein</EVALMETRIC>', '<EVALMETRIC>kl</EVALMETRIC>', '<EVALMETRIC>reverse_kl</EVALMETRIC>', '<EVALMETRIC>js</EVALMETRIC>', '<EVALMETRIC>bhattacharyya</EVALMETRIC>', '<EVALMETRIC>matmul</EVALMETRIC>', '<EVALMETRIC>matching_prob</EVALMETRIC>']   ## How to train  NOTE: we train each model with mixed-precision training (O2) on a single V100.
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\924b29d7.txt,0.9229249011857708
15,"<br> Since, the current code does not support multi-gpu training, if you use different hardware, the batchsize should be reduced.","<br> Since, the current code does not support multi-gpu training, if you use different hardware, the batchsize should be reduced.","<br> Since, the current code does not support multi-gpu training, if you use different hardware, the batchsize should be reduced.
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\41d6ba1e.txt,0.9961389961389961
16,"<br> Please note that, hence, the results couldn't be reproduced if you use smaller hardware than V100.  ### COCO Caption  ``` python train_coco.py .","<br> Please note that, hence, the results couldn't be reproduced if you use smaller hardware than V100.  ### <DATASET>COCO Caption</DATASET>  ``` <SOFTWARE>python</SOFTWARE> train_<DATASET>coco</DATASET>.py .","<br> Please note that, hence, the results couldn't be reproduced if you use smaller hardware than V100.  ### <DATASET>COCO Caption</DATASET>   <PROGLANG>python</PROGLANG> train_coco.py .
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\2dc8ccbf.txt,0.9864864864864865
17,"/config/coco/pcme_coco.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 46 hours in a single V100 with mixed precision training.  ### CUB Caption  We use CUB Caption dataset [(Reed, et al. 2016)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf) as a new cross-modal retrieval benchmark.","/config/<DATASET>coco</DATASET>/pcme_<DATASET>coco</DATASET>.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 46 hours in a single V100 with mixed precision training.  ### <DATASET>CUB Caption</DATASET>  We use <DATASET>CUB Caption</DATASET> dataset [(Reed, et al. 2016)](https://openaccess.thecvf.com/content_<CONFERENCE>cvpr_2016</CONFERENCE>/papers/Reed_Learning_Deep_Representations_<CONFERENCE>CVPR_2016</CONFERENCE>_paper.pdf) as a new cross-modal retrieval benchmark.","/config/coco/pcme_coco.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image   It takes about 46 hours in a single V100 with mixed precision training.  ### CUB Caption  We use <DATASET>CUB Caption</DATASET> dataset [(<PUBLICATION>Reed, et al. 2016</PUBLICATION>)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf) as a new cross-modal retrieval benchmark.
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\6aeb6e03.txt,0.9953271028037384
19,"Since our split is based on the zero-shot learning benchmark [(Xian, et al. 2017)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Xian_Zero-Shot_Learning_-_CVPR_2017_paper.pdf), we leave out 50 classes from 200 bird classes for the evaluation","Since our split is based on the zero-shot learning benchmark [(Xian, et al. 2017)](https://openaccess.thecvf.com/content_<CONFERENCE>cvpr_2017</CONFERENCE>/papers/Xian_Zero-Shot_Learning_-_<CONFERENCE>CVPR_2017</CONFERENCE>_paper.pdf), we leave out 50 classes from 200 bird classes for the evaluation","Since our split is based on the zero-shot learning benchmark <PUBLICATION>(Xian, et al. 2017)</PUBLICATION> [(Xian, et al. 2017)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Xian_Zero-Shot_Learning_-_CVPR_2017_paper.pdf), we leave out 50 classes from 200 bird classes for the evaluation",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\17c6211f.txt,0.9615384615384616
21,"- Reed, Scott, et al.","- Reed, Scott, et al.","- <PUBLICATION>Reed, Scott, et al.</PUBLICATION>
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\28bdafcd.txt,0.9767441860465116
22,"""Learning deep representations of fine-grained visual descriptions.""","""<PUBLICATION>Learning deep representations of fine-grained visual descriptions</PUBLICATION>.""","""Learning deep representations of fine-grained visual descriptions.""",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\1e089603.txt,1.0
23,"Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. - Xian, Yongqin, Bernt Schiele, and Zeynep Akata.","<PUBLICATION>Proceedings of the IEEE conference on computer vision and pattern recognition. 2016</PUBLICATION><CONFERENCE>IEEE conference on computer vision and pattern recognition. 2016</CONFERENCE>. - Xian, Yongqin, Bernt Schiele, and Zeynep Akata.","<PUBLICATION>Proceedings of the IEEE conference on computer vision and pattern recognition</PUBLICATION>. 2016. - Xian, Yongqin, Bernt Schiele, and Zeynep Akata.",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\d83b73c2.txt,1.0
24,"""Zero-shot learning-the good, the bad and the ugly.""","""<PUBLICATION>Zero-shot learning-the good, the bad and the ugly</PUBLICATION>.""","""Zero-shot learning-the good, the bad and the ugly.""",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\83357a45.txt,1.0
25,"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.  #### hyperparameter search  We additionally use cross-validation splits by (Xian, et el. 2017), namely using 100 classes for training and 50 classes for validation.   ``` python train_cub.py .","<PUBLICATION>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017</PUBLICATION><CONFERENCE>IEEE Conference on Computer Vision and Pattern Recognition. 2017</CONFERENCE>.  #### hyperparameter search  We additionally use cross-validation splits by (Xian, et el. 2017), namely using 100 classes for training and 50 classes for validation.   ``` <SOFTWARE>python</SOFTWARE> train_<DATASET>cub</DATASET>.py .","<PUBLICATION>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</PUBLICATION>. 2017.  #### hyperparameter search  We additionally use cross-validation splits by (Xian, et el. 2017), namely using 100 classes for training and 50 classes for validation.    <PROGLANG>python</PROGLANG> train_cub.py .
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\1a05454a.txt,0.9927797833935018
26,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py .","/config/<DATASET>cub</DATASET>/pcme_<DATASET>cub</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name <DATASET>cub</DATASET>_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `<DATASET>cub</DATASET>_trainval2` and `<DATASET>cub</DATASET>_trainval3` as well.  #### training with full training classes  ``` <SOFTWARE>python</SOFTWARE> train_<DATASET>cub</DATASET>.py .","/config/cub/<DATASET>pcme_cub</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name <DATASET>cub_trainval1</DATASET> \     # --model__cache_dir /vector_cache # if you use my docker image   Similarly, you can use <DATASET>cub_trainval2</DATASET> and <DATASET>cub_trainval3</DATASET> as well.  #### training with full training classes   <PROGLANG>python</PROGLANG> train_cub.py .
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\b28dfc0b.txt,0.9699570815450643
27,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp.","/config/cub/pcme_<DATASET>cub</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={<PUBLICATION>Probabilistic Embeddings for Cross-Modal Retrieval</PUBLICATION>},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={<CONFERENCE>Conference on Computer Vision and Pattern Recognition</CONFERENCE> (<CONFERENCE>CVPR</CONFERENCE>)}, } ```  I would like to suggest citing [<DATASET>ECCV Caption</DATASET>](https://github.com/naver-ai/<DATASET>eccv-caption</DATASET>) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022<DATASET>eccv_caption</DATASET>,     title={<PUBLICATION>ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO</PUBLICATION><DATASET>ECCV Caption</DATASET>: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={<CONFERENCE>European Conference on Computer Vision</CONFERENCE> (<CONFERENCE>ECCV</CONFERENCE>)}, }  @inproceedings{chun2024pcmepp,     title={<PUBLICATION>Improved Probabilistic Image-Text Representations</PUBLICATION>},     author={Chun, Sanghyuk},     year={2024},     booktitle={<CONFERENCE>International Conference on Learning Representations</CONFERENCE> (<CONFERENCE>ICLR</CONFERENCE>)}, } ```  ## License  ``` <LICENSE>MIT License</LICENSE>  Copyright (c) 2021-present NAVER Corp.","/config/cub/<DATASET>pcme_cub</DATASET>.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image   It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite   @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={<CONFERENCE>Conference on Computer Vision and Pattern Recognition (CVPR)</CONFERENCE>}, }   I would like to suggest citing [<PROJECT>ECCV Caption</PROJECT>](https://github.com/naver-ai/eccv-caption) and [<PROJECT>PCME++</PROJECT>](https://github.com/naver-ai/pcmepp), too.  @inproceedings{chun2022eccv_caption,     title={<PUBLICATION>ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO</PUBLICATION>},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={<CONFERENCE>European Conference on Computer Vision (ECCV)</CONFERENCE>}, }  @inproceedings{chun2024pcmepp,     title={<PUBLICATION>Improved Probabilistic Image-Text Representations</PUBLICATION>},     author={Chun, Sanghyuk},     year={2024},     booktitle={<CONFERENCE>International Conference on Learning Representations (ICLR)</CONFERENCE>}, }   ## License   <LICENSE>MIT License</LICENSE>  Copyright (c) 2021-present NAVER Corp.
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\b3be4a7d.txt,0.993103448275862
28,"Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.","Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.","Permission is hereby granted, free of charge, to any person obtaining a copy of this <SOFTWARE>software</SOFTWARE> and associated documentation files (the ""<SOFTWARE>Software</SOFTWARE>""), to deal in the <SOFTWARE>Software</SOFTWARE> without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the <SOFTWARE>Software</SOFTWARE>, and to permit persons to whom the <SOFTWARE>Software</SOFTWARE> is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the <SOFTWARE>Software</SOFTWARE>.
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\af9039d2.txt,0.9991079393398751
29,"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.","THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.","THE <SOFTWARE>SOFTWARE</SOFTWARE> IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\6958afd1.txt,0.9975186104218362
30,"IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```","IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```","IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE <SOFTWARE>SOFTWARE</SOFTWARE> OR THE USE OR OTHER DEALINGS IN THE <SOFTWARE>SOFTWARE</SOFTWARE>. 
",../results/deepseek-chat/prompt-0/zzz_naver-ai_pcme_main_README.md.tsv\a6d2ebf9.txt,0.9923371647509579
