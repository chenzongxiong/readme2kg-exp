sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
3,Jury offers a smooth and easy-to-use interface.,<SOFTWARE>Jury</SOFTWARE> offers a smooth and easy-to-use interface.,`Jury offers a smooth and easy-to-use interface.`,`Jury offers a smooth and easy-to-use interface.`,../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\3ad0a21a.txt,0.9791666666666666
4,"It uses a more advanced version of [evaluate](https://github.com/huggingface/evaluate/) design for underlying metric computation, so that adding custom metric is easy as extending proper class.","It uses a more advanced version of [<SOFTWARE>evaluate</SOFTWARE>](https://github.com/huggingface/<SOFTWARE>evaluate</SOFTWARE>/) design for underlying metric computation, so that adding custom metric is easy as extending proper class.","It uses a more advanced version of `evaluate`(https://github.com/huggingface/evaluate/) design for underlying metric computation, so that adding custom metric is easy as extending proper class.","It uses a more advanced version of `<SOFTWARE>evaluate</SOFTWARE>`(https://github.com/huggingface/<SOFTWARE>evaluate</SOFTWARE>/) design for underlying metric computation, so that adding custom metric is easy as extending proper class.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\583e6f9e.txt,0.9896373056994818
8,"Please cite this paper if your work use Jury, and if your publication material will be submitted to the venues after this date","Please cite this paper if your work use <SOFTWARE>Jury</SOFTWARE>, and if your publication material will be submitted to the venues after this date","`Please cite this paper if your work use Jury, and if your publication material will be submitted to the venues after this date`","`Please cite this paper if your work use <SOFTWARE>Jury</SOFTWARE>, and if your <PUBLICATION>publication material</PUBLICATION> will be submitted to the venues after this date`",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\6870ec74.txt,0.9921259842519685
14,"**Notes**  * The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it  can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although  unfortunately full Jury support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?","**Notes**  * The entry :heavy_check_mark: represents that full <SOFTWARE>Jury</SOFTWARE> support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for <SOFTWARE>Jury</SOFTWARE> through the `<SOFTWARE>evaluate</SOFTWARE>`), so that it  can (and should) be used just like the `<SOFTWARE>evaluate</SOFTWARE>` metric as instructed in `<SOFTWARE>evaluate</SOFTWARE>` implementation although  unfortunately full <SOFTWARE>Jur</SOFTWARE>y support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/<SOFTWARE>jury</SOFTWARE>/issues/new?","**Notes**  
* The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple references) are supported  
* The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although unfortunately full Jury support for those metrics are not yet available.  
## Request for a New Metric  
For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?","**Notes**  
* The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple references) are supported  
* The entry :white_check_mark: means that this metric is supported (for Jury through the `<SOFTWARE>evaluate</SOFTWARE>`), so that it can (and should) be used just like the `<SOFTWARE>evaluate</SOFTWARE>` metric as instructed in `<SOFTWARE>evaluate</SOFTWARE>` implementation although unfortunately full Jury support for those metrics are not yet available.  
## Request for a New Metric  
For the request of a new metric please [open an issue](https://github.com/obss/<PROJECT>jury</PROJECT>/issues/new?",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\09c54d15.txt,0.994261119081779
16,"Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.","Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through <SOFTWARE>pip</SOFTWARE>,      <SOFTWARE>pip</SOFTWARE> install <SOFTWARE>jury</SOFTWARE>  or build from source,      git clone https://github.com/obss/<SOFTWARE>jury</SOFTWARE>.git     cd jury     <SOFTWARE>python</SOFTWARE> setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `<SOFTWARE>sacrebleu</SOFTWARE>` package on Windows machines which is  mainly due to the package `<SOFTWARE>pywin32</SOFTWARE>`.","Also, PRs addressing new metric supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.","Also, PRs addressing new <EVALMETRIC>metric</EVALMETRIC> supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install <SOFTWARE>jury</SOFTWARE>  or build from source,      git clone https://github.com/obss/<SOFTWARE>jury</SOFTWARE>.git     cd <SOFTWARE>jury</SOFTWARE>     <PROGLANG>python</PROGLANG> setup.py install  **NOTE:** There may be malfunctions of some <EVALMETRIC>metrics</EVALMETRIC> depending on `<SOFTWARE>sacrebleu</SOFTWARE>` package on Windows machines which is  mainly due to the package `<SOFTWARE>pywin32</SOFTWARE>`.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\67e8aa2b.txt,0.9987577639751553
19,"```python from jury import Jury  scorer = Jury() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!","```<PROGLANG>python</PROGLANG> from <SOFTWARE>jury</SOFTWARE> import <SOFTWARE>Jury</SOFTWARE>  scorer = <SOFTWARE>Jury</SOFTWARE>() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!","```python from jury import Jury  scorer = Jury() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!
```","```python from <PROGLANG>jury</PROGLANG> import <SOFTWARE>Jury</SOFTWARE>  scorer = <SOFTWARE>Jury</SOFTWARE>() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\09286f03.txt,0.9862068965517241
21,"```python scorer = Jury(metrics=[""bleu"", ""meteor""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.","```<PROGLANG>python</PROGLANG> scorer = <SOFTWARE>Jury</SOFTWARE>(metrics=[""<EVALMETRIC>bleu</EVALMETRIC>"", ""<EVALMETRIC>meteor</EVALMETRIC>""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `<SOFTWARE>jury</SOFTWARE>.metrics` as classes, and then instantiate and use as desired.","```python scorer = Jury(metrics=[""bleu"", ""meteor""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.
```","```python scorer = Jury(metrics=[""<EVALMETRIC>bleu</EVALMETRIC>"", ""<EVALMETRIC>meteor</EVALMETRIC>""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\4f6382f1.txt,0.9915254237288136
22,"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.","```<PROGLANG>python</PROGLANG> from <SOFTWARE>jury</SOFTWARE>.metrics import <EVALMETRIC>Bleu</EVALMETRIC>  <EVALMETRIC>bleu</EVALMETRIC> = <EVALMETRIC>Bleu</EVALMETRIC>.construct() score = <EVALMETRIC>bleu</EVALMETRIC>.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```<PROGLANG>python</PROGLANG> from <SOFTWARE>jury</SOFTWARE>.metrics import <EVALMETRIC>Bleu</EVALMETRIC>  <EVALMETRIC>bleu</EVALMETRIC> = <EVALMETRIC>Bleu</EVALMETRIC>.construct() score = <EVALMETRIC>bleu</EVALMETRIC>.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```<PROGLANG>python</PROGLANG> from <SOFTWARE>jury</SOFTWARE>.metrics import <EVALMETRIC>Bleu</EVALMETRIC> <EVALMETRIC>bleu</EVALMETRIC> = <EVALMETRIC>Bleu</EVALMETRIC>.construct(compute_kwargs={""max_order"": 1}) score = <EVALMETRIC>bleu</EVALMETRIC>.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `<SOFTWARE>jury</SOFTWARE>` and `<SOFTWARE>evaluate</SOFTWARE>` metrics through `<SOFTWARE>jury</SOFTWARE>.load_metric`.","```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.
```","```python from <SOFTWARE>jury.metrics</SOFTWARE> import <EVALMETRIC>Bleu</EVALMETRIC>  bleu = <EVALMETRIC>Bleu</EVALMETRIC>.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from <SOFTWARE>jury.metrics</SOFTWARE> import <EVALMETRIC>Bleu</EVALMETRIC>  bleu = <EVALMETRIC>Bleu</EVALMETRIC>.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from <SOFTWARE>jury.metrics</SOFTWARE> import <EVALMETRIC>Bleu</EVALMETRIC> bleu = <EVALMETRIC>Bleu</EVALMETRIC>.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `<SOFTWARE>jury</SOFTWARE>` and `<SOFTWARE>evaluate</SOFTWARE>` metrics through `<SOFTWARE>jury.load_metric</SOFTWARE>`.
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\ee0b5156.txt,0.9969834087481146
23,"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.","```<PROGLANG>python</PROGLANG> import <SOFTWARE>jury</SOFTWARE>  <EVALMETRIC>bleu</EVALMETRIC> = <SOFTWARE>jury</SOFTWARE>.load_metric(""<EVALMETRIC>bleu</EVALMETRIC>"") <EVALMETRIC>bleu</EVALMETRIC>_1 = <SOFTWARE>jury</SOFTWARE>.load_metric(""<EVALMETRIC>bleu</EVALMETRIC>"", resulting_name=""<EVALMETRIC>bleu</EVALMETRIC>_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `<SOFTWARE>jury</SOFTWARE>` but in `<SOFTWARE>evaluate</SOFTWARE>` wer = <SOFTWARE>jury</SOFTWARE>.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.","```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.
```","```python import <PROGLANG>jury</PROGLANG>  bleu = <PROGLANG>jury</PROGLANG>.load_metric(""<EVALMETRIC>bleu</EVALMETRIC>"") bleu_1 = <PROGLANG>jury</PROGLANG>.load_metric(""<EVALMETRIC>bleu</EVALMETRIC>"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `<PROGLANG>jury</PROGLANG>` but in `<PROGLANG>evaluate</PROGLANG>` wer = <PROGLANG>jury</PROGLANG>.load_metric(""<EVALMETRIC>competition_math</EVALMETRIC>"") # It falls back to `<PROGLANG>evaluate</PROGLANG>` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\38fe95ce.txt,0.9950617283950617
30,"```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""bleu"",     ""meteor""   ] } ```  Then, you can call jury eval with `config` argument.","```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""<EVALMETRIC>bleu</EVALMETRIC>"",     ""<EVALMETRIC>meteor</EVALMETRIC>""   ] } ```  Then, you can call <SOFTWARE>jury</SOFTWARE> eval with `config` argument.","```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""bleu"",     ""meteor""   ] } ```  Then, you can call jury eval with `config` argument.
```","```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""<EVALMETRIC>bleu</EVALMETRIC>"",     ""<EVALMETRIC>meteor</EVALMETRIC>""   ] } ```  Then, you can call jury eval with `config` argument.
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\4a63e748.txt,0.9911111111111112
35,"```python from jury.metrics import MetricForTask  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [jury.metrics.Metric](.","```<PROGLANG>python</PROGLANG> from <SOFTWARE>jury</SOFTWARE>.metrics import <SOFTWARE>MetricForTask</SOFTWARE>  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [<SOFTWARE>jury</SOFTWARE>.metrics.Metric](.","```python from jury.metrics import MetricForTask  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [jury.metrics.Metric](.
```","```python from <PROGLANG>jury.metrics</PROGLANG> import MetricForTask  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [<SOFTWARE>jury.metrics.Metric</SOFTWARE>](.
```",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\79a8b0fa.txt,0.9967845659163987
40,"The file `requirements-dev.txt` includes packages  which are currently only available through a git source, or they are PYPI packages with no recent release or  incompatible with Jury, so that they are added as git sources or pointing to specific commits.","The file `requirements-dev.txt` includes packages  which are currently only available through a git source, or they are PYPI packages with no recent release or  incompatible with <SOFTWARE>Jury</SOFTWARE>, so that they are added as git sources or pointing to specific commits.","The file `requirements-dev.txt` includes packages  which are currently only available through a git source, or they are PYPI packages with no recent release or  incompatible with `Jury`, so that they are added as git sources or pointing to specific commits.","The file `<SOFTWARE>requirements-dev.txt</SOFTWARE>` includes packages  which are currently only available through a git source, or they are PYPI packages with no recent release or  incompatible with `<SOFTWARE>Jury</SOFTWARE>`, so that they are added as git sources or pointing to specific commits.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\079b35d1.txt,0.99609375
43,"Issues can be bug reports, feature requests or implementation of a new metric type.","Issues can be bug reports, feature requests or implementation of a new metric type.","Issues can be bug reports, feature requests or implementation of a new `metric` type.","Issues can be bug reports, feature requests or implementation of a new `<EVALMETRIC>metric</EVALMETRIC>` type.",../results/deepseek-chat/prompt-0/zzz_obss_jury_main_README.md.tsv\621a0411.txt,0.9880952380952381
