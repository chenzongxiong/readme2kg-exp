sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,"## Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set ##  <p align=""center"">  <img src=""/images/example.gif""> </p>  ### **_\*\*\*07/20/2021: A [PyTorch implementation](https://github.com/sicxu/Deep3DFaceRecon_pytorch) which has much better performance and is much easier to use is available now.","## <PUBLICATION>Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</PUBLICATION> ##  <p align=""center"">  <img src=""/images/example.gif""> </p>  ### **_\*\*\*07/20/2021: A [<SOFTWARE>PyTorch</SOFTWARE> implementation](https://github.com/sicxu/<PROJECT>Deep3DFaceRecon</PROJECT>_<SOFTWARE>pytorch</SOFTWARE>) which has much better performance and is much easier to use is available now.","## Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set ##  <p align=""center"">  <img src=""/images/example.gif""> </p>  ### **_\*\*\*07/20/2021: A [PyTorch implementation](https://github.com/sicxu/Deep3DFaceRecon_pytorch) which has much better performance and is much easier to use is available now.","## <PUBLICATION>Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</PUBLICATION> ##  <p align=""center"">  <img src=""/images/example.gif""> </p>  ### **_\*\*\*07/20/2021: A [<SOFTWARE>PyTorch implementation</SOFTWARE>](https://github.com/sicxu/<SOFTWARE>Deep3DFaceRecon_pytorch</SOFTWARE>) which has much better performance and is much easier to use is available now.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\25fe8ee3.txt,1.0
2,This repo will not be maintained in future. \*\*\*_**   This is a tensorflow implementation of the following paper:  Y.,This repo will not be maintained in future. \*\*\*_**   This is a <SOFTWARE>tensorflow</SOFTWARE> implementation of the following paper:  Y.,This repo will not be maintained in future. \*\*\*_**   This is a tensorflow implementation of the following paper:  Y.,This repo will not be maintained in future. \*\*\*_**   This is a tensorflow implementation of the following <PUBLICATION>paper</PUBLICATION>:  Y.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\28dc9a9a.txt,1.0
3,"Deng, J.","Deng, J.","Deng, J.","<PUBLICATION>Deng, J.</PUBLICATION>",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\c731efa6.txt,1.0
4,"Yang, S.","Yang, S.","Yang, S.","Yang, S.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\b45a8111.txt,1.0
5,"Xu, D.","Xu, D.","Xu, D.","<PUBLICATION>Xu, D.</PUBLICATION>",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\2a10b24c.txt,1.0
6,"Chen, Y.","Chen, Y.","`Chen, Y.`","`<PUBLICATION>Chen, Y.</PUBLICATION>`",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\a996c251.txt,0.8888888888888888
7,"Jia, and X.","Jia, and X.","Jia, and X.","Jia, and X.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\3ece18c9.txt,1.0
8,"Tong, [Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set](https://arxiv.org/abs/1903.08527), IEEE Computer Vision and Pattern Recognition Workshop (CVPRW) on Analysis and Modeling of Faces and Gestures (AMFG), 2019.","Tong, [<PUBLICATION>Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</PUBLICATION>](https://arxiv.org/abs/1903.08527), <PUBLICATION>IEEE Computer Vision and Pattern Recognition Workshop (CVPRW) on Analysis and Modeling of Faces and Gestures (AMFG)</PUBLICATION><WORKSHOP>Computer Vision and Pattern Recognition Workshop</WORKSHOP> (<WORKSHOP>CVPRW</WORKSHOP>) on Analysis and Modeling of Faces and Gestures (AMFG), 2019.","Tong, [Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set](https://arxiv.org/abs/1903.08527), IEEE Computer Vision and Pattern Recognition Workshop (CVPRW) on Analysis and Modeling of Faces and Gestures (AMFG), 2019.","Tong, <PUBLICATION>[Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set]</PUBLICATION>(https://arxiv.org/abs/1903.08527), <WORKSHOP>IEEE Computer Vision and Pattern Recognition Workshop (CVPRW) on Analysis and Modeling of Faces and Gestures (AMFG)</WORKSHOP>, 2019.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cda5a65f.txt,1.0
9,(**_Best Paper Award!,(**_Best Paper Award!,(**_Best Paper Award!,(**_Best Paper Award!,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\8f0af1ff.txt,1.0
10,_**)  The method enforces a hybrid-level weakly-supervised training for CNN-based 3D face reconstruction.,_**)  The method enforces a hybrid-level weakly-supervised training for CNN-based 3D face reconstruction.,_**)  The method enforces a hybrid-level weakly-supervised training for CNN-based 3D face reconstruction.,_**)  The method enforces a hybrid-level weakly-supervised training for CNN-based 3D face reconstruction.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\eac55935.txt,1.0
11,"It is fast, accurate, and robust to pose and occlussions.","It is fast, accurate, and robust to pose and occlussions.","It is fast, accurate, and robust to pose and occlussions.","It is fast, accurate, and robust to pose and occlussions.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\298a8521.txt,1.0
12,"It achieves state-of-the-art performance on multiple datasets such as FaceWarehouse, MICC Florence and BU-3DFE.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy.","It achieves state-of-the-art performance on multiple datasets such as <DATASET>FaceWarehouse</DATASET>, <DATASET>MICC Florence</DATASET> and <DATASET>BU-3DFE</DATASET>.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy.","It achieves state-of-the-art performance on multiple datasets such as `FaceWarehouse`, `MICC Florence` and `BU-3DFE`.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy.","It achieves state-of-the-art performance on multiple datasets such as `<DATASET>FaceWarehouse</DATASET>`, `<DATASET>MICC Florence</DATASET>` and `<DATASET>BU-3DFE</DATASET>`.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\6e1a5247.txt,0.9852216748768473
13,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|<DATASET>FaceWareHouse</DATASET>|<DATASET>Florence</DATASET>|<DATASET>BU3DFE</DATASET>| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|<DATASET>FaceWareHouse</DATASET>|<DATASET>Florence</DATASET>|<DATASET>BU3DFE</DATASET>| |:---:|:---:|:---:|:---:| |[<PUBLICATION>Tewari et al. 17</PUBLICATION>](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[<PUBLICATION>Tewari et al. 18</PUBLICATION>](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[<PUBLICATION>Genova et al. 18</PUBLICATION>](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[<PUBLICATION>Sela et al. 17</PUBLICATION>](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[<PUBLICATION>PRN 18</PUBLICATION>](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\5fe63ee3.txt,1.0
14,Scene illumination is also disentangled to generate a pure albedo.,Scene illumination is also disentangled to generate a pure albedo.,Scene illumination is also disentangled to generate a pure albedo.,Scene illumination is also disentangled to generate a pure albedo.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\baf083a1.txt,1.0
15,"<p align=""center"">  <img src=""/images/albedo.png""> </p>  ### ● Robust The method can provide reasonable results under extreme conditions such as large pose and occlusions.","<p align=""center"">  <img src=""/images/albedo.png""> </p>  ### ● Robust The method can provide reasonable results under extreme conditions such as large pose and occlusions.","<p align=""center"">  <img src=""/images/albedo.png""> </p>  ### ● Robust The method can provide reasonable results under extreme conditions such as large pose and occlusions.","<p align=""center"">  <img src=""/images/albedo.png""> </p>  ### ● Robust The method can provide reasonable results under extreme conditions such as large pose and occlusions.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\d5e123fc.txt,1.0
16,"<p align=""center"">  <img src=""/images/extreme.png""> </p>  ### ● Aligned with images Our method aligns reconstruction faces with input images.","<p align=""center"">  <img src=""/images/extreme.png""> </p>  ### ● Aligned with images Our method aligns reconstruction faces with input images.","<p align=""center"">  <img src=""/images/extreme.png""> </p>  ### ● Aligned with images Our method aligns reconstruction faces with input images.","<p align=""center"">  <img src=""/images/extreme.png""> </p>  ### ● Aligned with images Our method aligns reconstruction faces with input images.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\275f9b92.txt,1.0
17,It provides face pose estimation and 68 facial landmarks which are useful for other tasks.,It provides face pose estimation and 68 facial landmarks which are useful for other tasks.,It provides face pose estimation and 68 facial landmarks which are useful for other tasks.,It provides face pose estimation and 68 facial landmarks which are useful for other tasks.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\82380346.txt,1.0
18,"We conduct an experiment on AFLW_2000 dataset (NME) to evaluate the performance, as shown in the table below: <p align=""center"">  <img src=""/images/alignment.png""> </p>  |Method|[0°,30°]|[30°,60°]|[60°,90°]|Overall| |:---:|:---:|:---:|:---:|:---:| |[3DDFA 16](https://arxiv.org/abs/1511.07212)</center>|3.78|4.54|7.93|5.42| |[3DDFA+SDM 16](https://arxiv.org/abs/1511.07212)|3.43|4.24|7.17|4.94| |[Bulat et al. 17](https://arxiv.org/abs/1703.00862)|**2.47**|**3.01**|**4.31**|**3.26**| |[PRN 18](https://arxiv.org/abs/1803.07835)|2.75|3.51|4.61|3.62| |Ours|2.56|3.11|4.45|3.37|   ### ● Easy and Fast Faces are represented with Basel Face Model 2009, which is easy for further manipulations (e.g expression transfer).","We conduct an experiment on <DATASET>AFLW_2000</DATASET> dataset (NME) to evaluate the performance, as shown in the table below: <p align=""center"">  <img src=""/images/alignment.png""> </p>  |Method|[0°,30°]|[30°,60°]|[60°,90°]|Overall| |:---:|:---:|:---:|:---:|:---:| |[3DDFA 16](https://arxiv.org/abs/1511.07212)</center>|3.78|4.54|7.93|5.42| |[3DDFA+SDM 16](https://arxiv.org/abs/1511.07212)|3.43|4.24|7.17|4.94| |[Bulat et al. 17](https://arxiv.org/abs/1703.00862)|**2.47**|**3.01**|**4.31**|**3.26**| |[PRN 18](https://arxiv.org/abs/1803.07835)|2.75|3.51|4.61|3.62| |Ours|2.56|3.11|4.45|3.37|   ### ● Easy and Fast Faces are represented with Basel Face Model 2009, which is easy for further manipulations (e.g expression transfer).","We conduct an experiment on AFLW_2000 dataset (NME) to evaluate the performance, as shown in the table below: <p align=""center"">  <img src=""/images/alignment.png""> </p>  |Method|[0°,30°]|[30°,60°]|[60°,90°]|Overall| |:---:|:---:|:---:|:---:|:---:| |[3DDFA 16](https://arxiv.org/abs/1511.07212)</center>|3.78|4.54|7.93|5.42| |[3DDFA+SDM 16](https://arxiv.org/abs/1511.07212)|3.43|4.24|7.17|4.94| |[Bulat et al. 17](https://arxiv.org/abs/1703.00862)|**2.47**|**3.01**|**4.31**|**3.26**| |[PRN 18](https://arxiv.org/abs/1803.07835)|2.75|3.51|4.61|3.62| |Ours|2.56|3.11|4.45|3.37|   ### ● Easy and Fast Faces are represented with Basel Face Model 2009, which is easy for further manipulations (e.g expression transfer).","We conduct an experiment on <DATASET>AFLW_2000</DATASET> dataset (<EVALMETRIC>NME</EVALMETRIC>) to evaluate the performance, as shown in the table below: <p align=""center"">  <img src=""/images/alignment.png""> </p>  |Method|[0°,30°]|[30°,60°]|[60°,90°]|Overall| |:---:|:---:|:---:|:---:|:---:| |[3DDFA 16](https://arxiv.org/abs/1511.07212)</center>|3.78|4.54|7.93|5.42| |[3DDFA+SDM 16](https://arxiv.org/abs/1511.07212)|3.43|4.24|7.17|4.94| |[Bulat et al. 17](https://arxiv.org/abs/1703.00862)|**2.47**|**3.01**|**4.31**|**3.26**| |[PRN 18](https://arxiv.org/abs/1803.07835)|2.75|3.51|4.61|3.62| |Ours|2.56|3.11|4.45|3.37|   ### ● Easy and Fast Faces are represented with <DATASET>Basel Face Model 2009</DATASET>, which is easy for further manipulations (e.g expression transfer).",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\0de0f7e5.txt,1.0
19,ResNet-50 is used as backbone network to achieve over 50 fps (on GTX 1080) for reconstructions.   ## Getting Started ### Testing Requirements ###  - Reconstructions can be done on both Windows and Linux.,ResNet-50 is used as backbone network to achieve over 50 fps (on GTX 1080) for reconstructions.   ## Getting Started ### Testing Requirements ###  - Reconstructions can be done on both Windows and Linux.,ResNet-50 is used as backbone network to achieve over 50 fps (on GTX 1080) for reconstructions.   ## Getting Started ### Testing Requirements ###  - Reconstructions can be done on both Windows and Linux.,<SOFTWARE>ResNet-50</SOFTWARE> is used as backbone network to achieve over 50 fps (on <SOFTWARE>GTX 1080</SOFTWARE>) for reconstructions.   ## Getting Started ### Testing Requirements ###  - Reconstructions can be done on both <PROGLANG>Windows</PROGLANG> and <PROGLANG>Linux</PROGLANG>.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\839eb2e0.txt,1.0
20,"However, we suggest running on Linux because the rendering process is only supported on Linux. - Python 3.6 (numpy, scipy, pillow, argparse). - Tensorflow 1.12. - [Basel Face Model 2009 (BFM09)](https://faces.dmi.unibas.ch/bfm/main.php?","However, we suggest running on Linux because the rendering process is only supported on Linux. - <SOFTWARE>Python 3.6</SOFTWARE> (<SOFTWARE>numpy</SOFTWARE>, <SOFTWARE>scipy</SOFTWARE>, <SOFTWARE>pillow</SOFTWARE>, <SOFTWARE>argparse</SOFTWARE>). - <SOFTWARE>Tensorflow 1.12</SOFTWARE>. - [Basel Face Model 2009 (BFM09)](https://faces.dmi.unibas.ch/bfm/main.php?","However, we suggest running on Linux because the rendering process is only supported on Linux. - Python 3.6 (numpy, scipy, pillow, argparse). - Tensorflow 1.12. - [Basel Face Model 2009 (BFM09)](https://faces.dmi.unibas.ch/bfm/main.php?","However, we suggest running on <PROGLANG>Linux</PROGLANG> because the rendering process is only supported on <PROGLANG>Linux</PROGLANG>. - <PROGLANG>Python 3.6</PROGLANG> (numpy, scipy, pillow, argparse). - <SOFTWARE>Tensorflow 1.12</SOFTWARE>. - [<DATASET>Basel Face Model 2009 (BFM09)</DATASET>](https://faces.dmi.unibas.ch/bfm/main.php?",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\3dd84cab.txt,1.0
21,nav=1-0&id=basel_face_model),nav=1-0&id=basel_face_model),nav=1-0&id=basel_face_model),nav=1-0&id=<DATASET>basel_face_model</DATASET>),../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\c75695c2.txt,1.0
22,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cdb4ee2a.txt,1.0
23,- [Expression Basis (transferred from Facewarehouse by Guo et al.)],- [Expression Basis (transferred from Facewarehouse by Guo et al.)],- [Expression Basis (transferred from Facewarehouse by Guo et al.)],- [<DATASET>Expression Basis (transferred from Facewarehouse by Guo et al.)</DATASET>],../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\2abda417.txt,1.0
24,(https://github.com/Juyong/3DFace).,(https://github.com/Juyong/<PROJECT>3DFace</PROJECT>).,(https://github.com/Juyong/3DFace).,(https://github.com/<PROJECT>Juyong</PROJECT>/<SOFTWARE>3DFace</SOFTWARE>).,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\14a72e6d.txt,1.0
25,The original BFM09 model does not handle expression variations so extra expression basis are needed,The original BFM09 model does not handle expression variations so extra expression basis are needed,The original `BFM09` model does not handle expression variations so extra expression basis are needed,The original `<SOFTWARE>BFM09</SOFTWARE>` model does not handle expression variations so extra expression basis are needed,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\81a13de5.txt,0.99
26,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cdb4ee2a.txt,1.0
27,- [tf mesh renderer](https://github.com/google/tf_mesh_renderer/tree/ba27ea1798f6ee8d03ddbc52f42ab4241f9328bb).,- [<SOFTWARE>tf mesh renderer</SOFTWARE>](https://github.com/google/tf_mesh_renderer/tree/ba27ea1798f6ee8d03ddbc52f42ab4241f9328bb).,- [tf mesh renderer](https://github.com/google/tf_mesh_renderer/tree/ba27ea1798f6ee8d03ddbc52f42ab4241f9328bb).,- [<SOFTWARE>tf mesh renderer</SOFTWARE>](https://github.com/google/<SOFTWARE>tf_mesh_renderer</SOFTWARE>/tree/ba27ea1798f6ee8d03ddbc52f42ab4241f9328bb).,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\8bd7ad01.txt,1.0
28,We use the library to render reconstruction images.,We use the library to render reconstruction images.,We use the library to render reconstruction images.,We use the <SOFTWARE>library</SOFTWARE> to render reconstruction images.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\c13a050c.txt,1.0
29,**Note that the rendering tool can only be used on Linux.**  ### Installation ### #### 1.,**Note that the rendering tool can only be used on Linux.**  ### Installation ### #### 1.,**Note that the rendering tool can only be used on Linux.**  ### Installation ### #### 1.,**Note that the rendering tool can only be used on <PROGLANG>Linux</PROGLANG>.**  ### Installation ### #### 1.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\03ffb90a.txt,1.0
30,Clone the repository ``` git clone https://github.com/Microsoft/Deep3DFaceReconstruction --recursive cd Deep3DFaceReconstruction ```  #### 2.,Clone the repository ``` <SOFTWARE>git</SOFTWARE> clone https://github.com/Microsoft/<PROJECT>Deep3DFaceReconstruction</PROJECT> --recursive cd <PROJECT>Deep3DFaceReconstruction</PROJECT> ```  #### 2.,Clone the repository ``` git clone https://github.com/Microsoft/Deep3DFaceReconstruction --recursive cd Deep3DFaceReconstruction ```  #### 2.,Clone the repository ``` git clone https://github.com/<SOFTWARE>Microsoft/Deep3DFaceReconstruction</SOFTWARE> --recursive cd <SOFTWARE>Deep3DFaceReconstruction</SOFTWARE> ```  #### 2.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\273caa7b.txt,1.0
31,"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.","Set up the <PROGLANG>python</PROGLANG> environment If you use <SOFTWARE>anaconda</SOFTWARE>, run the following: ``` <SOFTWARE>conda</SOFTWARE> create -n deep3d <PROGLANG>python=3.6</PROGLANG> source activate deep3d <SOFTWARE>conda</SOFTWARE> install <SOFTWARE>tensorflow-gpu==1.12.0</SOFTWARE> <PROGLANG>scipy</PROGLANG> <SOFTWARE>pip</SOFTWARE> install <SOFTWARE>pillow</SOFTWARE> <SOFTWARE>argparse</SOFTWARE> ```  Alternatively, you can install <SOFTWARE>tensorflow</SOFTWARE> via <SOFTWARE>pip</SOFTWARE> install (In this way, you need to link /usr/local/<SOFTWARE>cuda</SOFTWARE> to <SOFTWARE>cuda-9.0)</SOFTWARE>: ``` <SOFTWARE>pip</SOFTWARE> install <SOFTWARE>tensorflow-gpu==1.12.0</SOFTWARE> ```  #### 3.","Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.","Set up the <PROGLANG>python</PROGLANG> environment If you use anaconda, run the following: ``` conda create -n deep3d <PROGLANG>python</PROGLANG>=3.6 source activate deep3d conda install <SOFTWARE>tensorflow-gpu</SOFTWARE>==1.12.0 scipy pip install <SOFTWARE>pillow</SOFTWARE> <SOFTWARE>argparse</SOFTWARE> ```  Alternatively, you can install <SOFTWARE>tensorflow</SOFTWARE> via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install <SOFTWARE>tensorflow-gpu</SOFTWARE>==1.12.0 ```  #### 3.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\db9d351d.txt,1.0
32,"Compile tf_mesh_renderer  If you install tensorflow using pip,  we provide a [pre-compiled binary file (rasterize_triangles_kernel.so)](https://drive.google.com/file/d/1VUtJPdg0UiJkKWxkACs8ZTf5L7Y4P9Wj/view?","Compile <SOFTWARE>tf_mesh_renderer</SOFTWARE>  If you install <SOFTWARE>tensorflow</SOFTWARE> using <SOFTWARE>pip</SOFTWARE>,  we provide a [pre-compiled binary file (rasterize_triangles_kernel.so)](https://drive.google.com/file/d/1VUtJPdg0UiJkKWxkACs8ZTf5L7Y4P9Wj/view?","Compile tf_mesh_renderer  If you install tensorflow using pip,  we provide a [pre-compiled binary file (rasterize_triangles_kernel.so)](https://drive.google.com/file/d/1VUtJPdg0UiJkKWxkACs8ZTf5L7Y4P9Wj/view?","Compile <SOFTWARE>tf_mesh_renderer</SOFTWARE>  If you install <SOFTWARE>tensorflow</SOFTWARE> using pip,  we provide a [pre-compiled binary file (<SOFTWARE>rasterize_triangles_kernel.so</SOFTWARE>)](https://drive.google.com/file/d/1VUtJPdg0UiJkKWxkACs8ZTf5L7Y4P9Wj/view?",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\e5324425.txt,1.0
33,usp=sharing) of the library.,usp=sharing) of the library.,usp=sharing) of the library.,usp=sharing) of the library.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\d7e28b3d.txt,1.0
34,"**Note that the pre-compiled file can only be run with tensorflow 1.12.**  If you install tensorflow using conda, you have to compile tf_mesh_renderer from sources.","**Note that the pre-compiled file can only be run with <SOFTWARE>tensorflow 1.12</SOFTWARE>.**  If you install <SOFTWARE>tensorflow</SOFTWARE> using <SOFTWARE>conda</SOFTWARE>, you have to compile <SOFTWARE>tf_mesh_renderer</SOFTWARE> from sources.","**Note that the pre-compiled file can only be run with tensorflow 1.12.**  If you install tensorflow using conda, you have to compile tf_mesh_renderer from sources.","**Note that the pre-compiled file can only be run with <SOFTWARE>tensorflow 1.12</SOFTWARE>.**  If you install <SOFTWARE>tensorflow</SOFTWARE> using <PROGLANG>conda</PROGLANG>, you have to compile <SOFTWARE>tf_mesh_renderer</SOFTWARE> from sources.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\01d840c3.txt,1.0
35,Compile tf_mesh_renderer with Bazel.,Compile <SOFTWARE>tf_mesh_renderer</SOFTWARE> with <SOFTWARE>Bazel</SOFTWARE>.,Compile tf_mesh_renderer with Bazel.,Compile <SOFTWARE>tf_mesh_renderer</SOFTWARE> with <PROGLANG>Bazel</PROGLANG>.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\16c2ddb2.txt,1.0
36,**Set -D_GLIBCXX_USE_CXX11_ABI=1 in .,**Set -D_GLIBCXX_USE_CXX11_ABI=1 in .,**Set -D_GLIBCXX_USE_CXX11_ABI=1 in .**,**Set -D_GLIBCXX_USE_CXX11_ABI=1 in .**,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\1dd29ec7.txt,0.9736842105263158
37,"/mesh_renderer/kernels/BUILD before the compilation**: ``` cd tf_mesh_renderer git checkout ba27ea1798 git checkout master WORKSPACE bazel test ... cd .. ``` If the library is compiled correctly, there should be a file named ""rasterize_triangles_kernel.so"" in .","/mesh_renderer/kernels/BUILD before the compilation**: ``` cd <SOFTWARE>tf_mesh_renderer</SOFTWARE> <SOFTWARE>git</SOFTWARE> checkout ba27ea1798 <SOFTWARE>git</SOFTWARE> checkout master WORKSPACE <SOFTWARE>bazel</SOFTWARE> test ... cd .. ``` If the library is compiled correctly, there should be a file named ""rasterize_triangles_kernel.so"" in .","/mesh_renderer/kernels/BUILD before the compilation**: ``` cd tf_mesh_renderer git checkout ba27ea1798 git checkout master WORKSPACE bazel test ... cd .. ``` If the library is compiled correctly, there should be a file named ""rasterize_triangles_kernel.so"" in .","/mesh_renderer/kernels/<SOFTWARE>BUILD</SOFTWARE> before the compilation**: ``` cd <SOFTWARE>tf_mesh_renderer</SOFTWARE> git checkout ba27ea1798 git checkout master <SOFTWARE>WORKSPACE</SOFTWARE> bazel test ... cd .. ``` If the library is compiled correctly, there should be a file named ""<SOFTWARE>rasterize_triangles_kernel.so</SOFTWARE>"" in .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\f1cade92.txt,1.0
38,/tf_mesh_renderer/bazel-bin/mesh_renderer/kernels.,/<SOFTWARE>tf_mesh_renderer</SOFTWARE>/bazel-bin/mesh_renderer/kernels.,/tf_mesh_renderer/bazel-bin/mesh_renderer/kernels.,/tf_<SOFTWARE>mesh_renderer</SOFTWARE>/bazel-bin/<SOFTWARE>mesh_renderer</SOFTWARE>/kernels.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\0ae36490.txt,1.0
39,"After compilation, copy corresponding files to .","After compilation, copy corresponding files to .","After compilation, copy corresponding files to .","After compilation, copy corresponding files to .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\22043fc0.txt,1.0
40,/renderer subfolder: ``` cd renderer cp .,/renderer subfolder: ``` cd renderer cp .,/renderer subfolder: ``` cd renderer cp .,/renderer subfolder: ``` cd renderer cp .,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\559ebe98.txt,1.0
41,"/tf_mesh_renderer/mesh_renderer/{camera_utils.py,mesh_renderer.py,rasterize_triangles.py} .","/tf_mesh_renderer/mesh_renderer/{camera_utils.py,mesh_renderer.py,rasterize_triangles.py} .","/tf_mesh_renderer/mesh_renderer/{camera_utils.py,mesh_renderer.py,rasterize_triangles.py} .","/tf_mesh_renderer/mesh_renderer/{<SOFTWARE>camera_utils.py</SOFTWARE>,<SOFTWARE>mesh_renderer.py</SOFTWARE>,<SOFTWARE>rasterize_triangles.py</SOFTWARE>} .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\f8902500.txt,1.0
42,/renderer/ cp .,/renderer/ cp .,/renderer/ cp .,/renderer/ cp .,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\fb27dbff.txt,1.0
43,/tf_mesh_renderer/bazel-bin/mesh_renderer/kernels/rasterize_triangles_kernel.so .,/tf_mesh_renderer/bazel-bin/mesh_renderer/kernels/rasterize_triangles_kernel.so .,/tf_mesh_renderer/bazel-bin/mesh_renderer/kernels/rasterize_triangles_kernel.so .,/tf_mesh_renderer/bazel-bin/mesh_renderer/kernels/<SOFTWARE>rasterize_triangles_kernel.so</SOFTWARE> .,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\99952f7d.txt,1.0
44,"/renderer/ ``` If you download our pre-compiled binary file, put it into .","/renderer/ ``` If you download our pre-compiled binary file, put it into .","/renderer/ ``` If you download our pre-compiled binary file, put it into .","/renderer/ ``` If you download our pre-compiled binary file, put it into .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\c4620f18.txt,1.0
45,/renderer subfolder as well.,/renderer subfolder as well.,/renderer subfolder as well.,/renderer subfolder as well.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\fbc7f571.txt,1.0
46,Replace the library path in Line 26 in .,Replace the library path in Line 26 in .,Replace the library path in Line 26 in .,Replace the library path in Line 26 in .,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\214cbb84.txt,1.0
47,"/renderer/rasterize_triangles.py with "".","/renderer/rasterize_triangles.py with "".","`/renderer/rasterize_triangles.py with "".`","`/renderer/<SOFTWARE>rasterize_triangles.py</SOFTWARE> with "".`",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\0c244e13.txt,0.975609756097561
48,"/renderer/rasterize_triangles_kernel.so"".","/renderer/rasterize_triangles_kernel.so"".","/renderer/rasterize_triangles_kernel.so"".","/renderer/rasterize_triangles_kernel.so"".",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\35b1970e.txt,1.0
49,"Replace ""xrange"" function in Line 109 in .","Replace ""xrange"" function in Line 109 in .","Replace ""xrange"" function in Line 109 in .","Replace ""<PROGLANG>xrange</PROGLANG>"" function in Line 109 in .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\1906db07.txt,1.0
50,"/renderer/rasterize_triangles.py with ""range"" function for compatibility with python3.   ### Testing with pre-trained network ###  1.","/renderer/rasterize_triangles.py with ""range"" function for compatibility with python3.   ### Testing with pre-trained network ###  1.","/renderer/rasterize_triangles.py with ""range"" function for compatibility with python3.   ### Testing with pre-trained network ###  1.","/renderer/<SOFTWARE>rasterize_triangles.py</SOFTWARE> with ""range"" function for compatibility with <PROGLANG>python3</PROGLANG>.   ### Testing with pre-trained network ###  1.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\8d91dda3.txt,1.0
51,Download the Basel Face Model.,Download the Basel Face Model.,Download the Basel Face Model.,Download the <DATASET>Basel Face Model</DATASET>.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\f1a9e0ea.txt,1.0
52,"Due to the license agreement of Basel Face Model, you have to download the BFM09 model after submitting an application on its [home page](https://faces.dmi.unibas.ch/bfm/main.php?","Due to the license agreement of Basel Face Model, you have to download the BFM09 model after submitting an application on its [home page](https://faces.dmi.unibas.ch/bfm/main.php?","Due to the license agreement of Basel Face Model, you have to download the BFM09 model after submitting an application on its [home page](https://faces.dmi.unibas.ch/bfm/main.php?","Due to the <LICENSE>license agreement</LICENSE> of <DATASET>Basel Face Model</DATASET>, you have to download the <DATASET>BFM09</DATASET> model after submitting an application on its [home page](https://faces.dmi.unibas.ch/bfm/main.php?",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\640c1280.txt,1.0
53,nav=1-2&id=downloads).,nav=1-2&id=downloads).,nav=1-2&id=downloads).,nav=1-2&id=downloads).,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\145bee47.txt,1.0
54,"After getting the access to BFM data, download ""01_MorphableModel.mat"" and put it into .","After getting the access to BFM data, download ""01_MorphableModel.mat"" and put it into .","After getting the access to `BFM` data, download `01_MorphableModel.mat` and put it into .","After getting the access to `<DATASET>BFM</DATASET>` data, download `<DATASET>01_MorphableModel.mat</DATASET>` and put it into .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\72b136e9.txt,0.9662921348314607
55,/BFM subfolder.  2.,/BFM subfolder.  2.,/BFM subfolder.  2.,/BFM subfolder.  2.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\eee6da3a.txt,1.0
56,Download the Expression Basis provided by [Guo et al.],Download the Expression Basis provided by [Guo et al.],Download the `Expression Basis` provided by [Guo et al.],Download the `<DATASET>Expression Basis</DATASET>` provided by [Guo et al.],../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cfcac155.txt,0.9818181818181818
57,"(https://github.com/Juyong/3DFace) You can find a link named ""CoarseData"" in the first row of Introduction part in their repository.","(https://github.com/Juyong/<PROJECT>3DFace</PROJECT>) You can find a link named ""<DATASET>CoarseData</DATASET>"" in the first row of Introduction part in their repository.","(https://github.com/Juyong/3DFace) You can find a link named ""CoarseData"" in the first row of Introduction part in their repository.","(https://github.com/<PROJECT>Juyong</PROJECT>/<PROJECT>3DFace</PROJECT>) You can find a link named ""<DATASET>CoarseData</DATASET>"" in the first row of Introduction part in their repository.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\a5115f22.txt,1.0
58,Download and unzip the Coarse_Dataset.zip.,Download and unzip the <DATASET>Coarse</DATASET>_Dataset.zip.,Download and unzip the Coarse_Dataset.zip.,Download and unzip the <DATASET>Coarse_Dataset</DATASET>.zip.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\9039f950.txt,1.0
59,"Put ""Exp_Pca.bin"" into .","Put ""Exp_Pca.bin"" into .","Put ""Exp_Pca.bin"" into .","Put ""<SOFTWARE>Exp_Pca.bin</SOFTWARE>"" into .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\61b46095.txt,1.0
60,/BFM subfolder.,/BFM subfolder.,/BFM subfolder.,/BFM subfolder.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\872d3b0d.txt,1.0
61,The expression basis are constructed using [Facewarehouse](http://kunzhou.net/zjugaps/facewarehouse/) data and transferred to BFM topology.  3.,The expression basis are constructed using [<DATASET>Facewarehouse</DATASET>](http://kunzhou.net/zjugaps/facewarehouse/) data and transferred to BFM topology.  3.,The expression basis are constructed using `Facewarehouse` data and transferred to BFM topology.  3.,The expression basis are constructed using `<DATASET>Facewarehouse</DATASET>` data and transferred to BFM topology.  3.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\765b6e7b.txt,0.8065843621399177
62,Download the pre-trained [reconstruction network](https://drive.google.com/file/d/176LCdUDxAj7T2awQ5knPMPawq5Q2RUWM/view?,Download the pre-trained [reconstruction network](https://drive.google.com/file/d/176LCdUDxAj7T2awQ5knPMPawq5Q2RUWM/view?,Download the pre-trained [reconstruction network](https://drive.google.com/file/d/176LCdUDxAj7T2awQ5knPMPawq5Q2RUWM/view?,Download the pre-trained [reconstruction network](https://drive.google.com/file/d/176LCdUDxAj7T2awQ5knPMPawq5Q2RUWM/view?,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\ddf651f1.txt,1.0
63,"usp=sharing), unzip it and put ""FaceReconModel.pb"" into .","usp=sharing), unzip it and put ""FaceReconModel.pb"" into .","usp=sharing), unzip it and put ""FaceReconModel.pb"" into .","usp=sharing), unzip it and put ""<DATASET>FaceReconModel.pb</DATASET>"" into .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\9b3d619b.txt,1.0
64,/network subfolder.  4.,/network subfolder.  4.,/network subfolder.  4.,/network subfolder.  4.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\62e47d8f.txt,1.0
65,Run the demo code.  ``` python demo.py ```  5. .,Run the demo code.  ``` python demo.py ```  5. .,Run the demo code.  ``` python demo.py ```  5. .,Run the demo code.  ``` <PROGLANG>python</PROGLANG> demo.py ```  5. .,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\af132685.txt,1.0
66,/input subfolder contains several test images and .,/input subfolder contains several test images and .,/input subfolder contains several test images and .,/input subfolder contains several test images and .,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\9b8bd8e9.txt,1.0
67,/output subfolder stores their reconstruction results.,/output subfolder stores their reconstruction results.,/output subfolder stores their reconstruction results.,/output subfolder stores their reconstruction results.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\16e7049d.txt,1.0
68,"For each input test image, two output files can be obtained after running the demo code:  - ""xxx.mat"" :    - cropped_img: an RGB image after alignment, which is the input to the R-Net   - recon_img: an RGBA reconstruction image aligned with the input image (only on Linux)","For each input test image, two output files can be obtained after running the demo code:  - ""xxx.mat"" :    - cropped_img: an RGB image after alignment, which is the input to the R-Net   - recon_img: an RGBA reconstruction image aligned with the input image (only on Linux)","For each input test image, two output files can be obtained after running the demo code:  - ""xxx.mat"" :    - cropped_img: an RGB image after alignment, which is the input to the R-Net   - recon_img: an RGBA reconstruction image aligned with the input image (only on Linux)","For each input test image, two output files can be obtained after running the demo code:  - ""xxx.mat"" :    - cropped_img: an RGB image after alignment, which is the input to the <SOFTWARE>R-Net</SOFTWARE>   - recon_img: an RGBA reconstruction image aligned with the input image (only on Linux)",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\2de79d56.txt,1.0
69,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cdb4ee2a.txt,1.0
70,- coeff: output coefficients of R-Net,- coeff: output coefficients of R-Net,- `coeff: output coefficients of R-Net`,- `<EVALMETRIC>coeff</EVALMETRIC>: output coefficients of R-Net`,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\a1e55a3d.txt,0.9736842105263158
71,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cdb4ee2a.txt,1.0
72,- face_shape: vertex positions of 3D face in the world coordinate,- face_shape: vertex positions of 3D face in the world coordinate,- face_shape: vertex positions of 3D face in the world coordinate,- face_shape: vertex positions of 3D face in the world coordinate,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\45d0ba4a.txt,1.0
73,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cdb4ee2a.txt,1.0
74,"- face_texture: vertex texture of 3D face, which excludes lighting effect","- face_texture: vertex texture of 3D face, which excludes lighting effect","- face_texture: vertex texture of 3D face, which excludes lighting effect","- face_texture: vertex texture of 3D face, which excludes lighting effect",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\a831b211.txt,1.0
75,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cdb4ee2a.txt,1.0
76,"- face_color: vertex color of 3D face, which takes lighting into consideration","- face_color: vertex color of 3D face, which takes lighting into consideration","- face_color: vertex color of 3D face, which takes lighting into consideration","- face_color: vertex color of 3D face, which takes lighting into consideration",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\9f2adb5f.txt,1.0
77,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cdb4ee2a.txt,1.0
78,- lm\_68p: 68 2D facial landmarks derived from the reconstructed 3D face.,- lm\_68p: 68 2D facial landmarks derived from the reconstructed 3D face.,- `lm_68p: 68 2D facial landmarks derived from the reconstructed 3D face.`,- `<DATASET>lm_68p</DATASET>: 68 2D facial landmarks derived from the reconstructed 3D face.`,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\61e50acb.txt,0.9795918367346939
79,The landmarks are aligned with cropped_img,The landmarks are aligned with cropped_img,The landmarks are aligned with cropped_img,The landmarks are aligned with cropped_img,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\5d0ad554.txt,1.0
80,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cdb4ee2a.txt,1.0
81,- lm\_5p: 5 detected landmarks aligned with cropped_img,- lm\_5p: 5 detected landmarks aligned with cropped_img,- `lm_5p`: 5 detected landmarks aligned with cropped_img,- `<SOFTWARE>lm_5p</SOFTWARE>`: 5 detected landmarks aligned with cropped_img,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\7bd0b2b8.txt,0.972972972972973
82,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cdb4ee2a.txt,1.0
83,"- ""xxx_mesh.obj"" : 3D face mesh in the world coordinate (best viewed in MeshLab).  ### Training requirements ###  - Training is only supported on Linux.","- ""xxx_mesh.obj"" : 3D face mesh in the world coordinate (best viewed in MeshLab).  ### Training requirements ###  - Training is only supported on Linux.","- ""xxx_mesh.obj"" : 3D face mesh in the world coordinate (best viewed in MeshLab).  ### Training requirements ###  - Training is only supported on Linux.","- ""<DATASET>xxx_mesh.obj</DATASET>"" : 3D face mesh in the world coordinate (best viewed in <SOFTWARE>MeshLab</SOFTWARE>).  ### Training requirements ###  - Training is only supported on <PROGLANG>Linux</PROGLANG>.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\9ee99937.txt,1.0
84,"To train new model from scratch, more requirements are needed on top of the requirements listed in the testing stage. - [Facenet](https://github.com/davidsandberg/facenet) provided by  Sandberg et al.","To train new model from scratch, more requirements are needed on top of the requirements listed in the testing stage. - [Facenet](https://github.com/davidsandberg/facenet) provided by  Sandberg et al.","To train new model from scratch, more requirements are needed on top of the requirements listed in the testing stage. - [Facenet](https://github.com/davidsandberg/facenet) provided by  Sandberg et al.","To train new model from scratch, more requirements are needed on top of the requirements listed in the testing stage. - [<SOFTWARE>Facenet</SOFTWARE>](https://github.com/davidsandberg/facenet) provided by  Sandberg et al.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\9457c268.txt,1.0
85,"In our paper, we use a network to exrtact perceptual face features.","In our paper, we use a network to exrtact perceptual face features.","In our paper, we use a network to exrtact perceptual face features.","In our <PUBLICATION>paper</PUBLICATION>, we use a network to exrtact perceptual face features.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\80ad48df.txt,1.0
86,This network model cannot be publicly released.,This network model cannot be publicly released.,This network model cannot be publicly released.,This network model cannot be publicly released.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\4568a51c.txt,1.0
87,"As an alternative, we recommend using the Facenet from Sandberg et al.","As an alternative, we recommend using the Facenet from Sandberg et al.","As an alternative, we recommend using the `Facenet` from Sandberg et al.","As an alternative, we recommend using the `<SOFTWARE>Facenet</SOFTWARE>` from Sandberg et al.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\1410b244.txt,0.9859154929577465
88,This repo uses the version [20170512-110547](https://github.com/davidsandberg/facenet/blob/529c3b0b5fc8da4e0f48d2818906120f2e5687e6/README.md) trained on MS-Celeb-1M.,This repo uses the version [20170512-110547](https://github.com/davidsandberg/facenet/blob/529c3b0b5fc8da4e0f48d2818906120f2e5687e6/README.md) trained on MS-Celeb-1M.,This repo uses the version [20170512-110547](https://github.com/davidsandberg/facenet/blob/529c3b0b5fc8da4e0f48d2818906120f2e5687e6/README.md) trained on MS-Celeb-1M.,This repo uses the version [20170512-110547](https://github.com/davidsandberg/<SOFTWARE>facenet</SOFTWARE>/blob/529c3b0b5fc8da4e0f48d2818906120f2e5687e6/README.md) trained on <DATASET>MS-Celeb-1M</DATASET>.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\8a64280b.txt,1.0
89,Training process has been tested with this model to ensure similar results. - [Resnet50-v1](https://github.com/tensorflow/models/blob/master/research/slim/README.md) pre-trained on ImageNet from Tensorflow Slim.,Training process has been tested with this model to ensure similar results. - [Resnet50-v1](https://github.com/<SOFTWARE>tensorflow/</SOFTWARE>models/blob/master/research/slim/README.md) pre-trained on <DATASET>ImageNet</DATASET> from <SOFTWARE>Tensorflow</SOFTWARE> Slim.,Training process has been tested with this model to ensure similar results. - [Resnet50-v1](https://github.com/tensorflow/models/blob/master/research/slim/README.md) pre-trained on ImageNet from Tensorflow Slim.,Training process has been tested with this model to ensure similar results. - [<SOFTWARE>Resnet50-v1</SOFTWARE>](https://github.com/tensorflow/models/blob/master/research/slim/README.md) pre-trained on <DATASET>ImageNet</DATASET> from <SOFTWARE>Tensorflow Slim</SOFTWARE>.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\a3cbb11f.txt,1.0
90,We use the version resnet_v1_50_2016_08_28.tar.gz as an initialization of the face reconstruction network. - [68-facial-landmark detector](https://drive.google.com/file/d/1KYFeTb963jg0F47sTiwqDdhBIvRlUkPa/view?,We use the version resnet_v1_50_2016_08_28.tar.gz as an initialization of the face reconstruction network. - [68-facial-landmark detector](https://drive.google.com/file/d/1KYFeTb963jg0F47sTiwqDdhBIvRlUkPa/view?,We use the version resnet_v1_50_2016_08_28.tar.gz as an initialization of the face reconstruction network. - [68-facial-landmark detector](https://drive.google.com/file/d/1KYFeTb963jg0F47sTiwqDdhBIvRlUkPa/view?,We use the version <SOFTWARE>resnet_v1_50_2016_08_28.tar.gz</SOFTWARE> as an initialization of the face reconstruction network. - [68-facial-landmark detector](https://drive.google.com/file/d/1KYFeTb963jg0F47sTiwqDdhBIvRlUkPa/view?,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\f4fc22ef.txt,1.0
91,usp=sharing).,usp=sharing).,usp=sharing).,usp=sharing).,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\e4c0f51a.txt,1.0
92,We use 68 facial landmarks for loss calculation during training.,We use 68 facial landmarks for loss calculation during training.,We use 68 facial landmarks for loss calculation during training.,We use 68 facial landmarks for loss calculation during training.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\1123897a.txt,1.0
93,"To make the training process reproducible, we provide a lightweight detector that produce comparable results to [the method of Bulat et al.]","To make the training process reproducible, we provide a lightweight detector that produce comparable results to [the method of Bulat et al.]","To make the training process reproducible, we provide a lightweight detector that produce comparable results to [the method of Bulat et al.]","To make the training process reproducible, we provide a lightweight detector that produce comparable results to [the method of Bulat et al.]",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\23e7ef14.txt,1.0
94,(https://github.com/1adrianb/2D-and-3D-face-alignment).,(https://github.com/1adrianb/2D-and-3D-face-alignment).,`(https://github.com/1adrianb/2D-and-3D-face-alignment).`,`(https://github.com/1adrianb/<SOFTWARE>2D-and-3D-face-alignment</SOFTWARE>).`,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\f882ec17.txt,0.9821428571428571
95,"The detector is trained on [300WLP](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm), [LFW](http://vis-www.cs.umass.edu/lfw/), and [LS3D-W](https://www.adrianbulat.com/face-alignment).  ### Training preparation ###  1.","The detector is trained on [300WLP](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm), [LFW](http://vis-www.cs.umass.edu/lfw/), and [LS3D-W](https://www.adrianbulat.com/face-alignment).  ### Training preparation ###  1.","The detector is trained on `[300WLP](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm)`, `[LFW](http://vis-www.cs.umass.edu/lfw/)`, and `[LS3D-W](https://www.adrianbulat.com/face-alignment)`.  ### Training preparation ###  1.","The detector is trained on `<DATASET>[300WLP](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm)</DATASET>`, `<DATASET>[LFW](http://vis-www.cs.umass.edu/lfw/)</DATASET>`, and `<DATASET>[LS3D-W](https://www.adrianbulat.com/face-alignment)</DATASET>`.  ### Training preparation ###  1.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\bdb25464.txt,0.9874476987447699
96,"Download the [pre-trained weights](https://drive.google.com/file/d/0B5MzpY9kBtDVZ2RpVDYwWmxoSUk/edit) of Facenet provided by Sandberg et al., unzip it and put all files in .","Download the [pre-trained weights](https://drive.google.com/file/d/0B5MzpY9kBtDVZ2RpVDYwWmxoSUk/edit) of Facenet provided by Sandberg et al., unzip it and put all files in .","Download the [pre-trained weights](https://drive.google.com/file/d/0B5MzpY9kBtDVZ2RpVDYwWmxoSUk/edit) of Facenet provided by Sandberg et al., unzip it and put all files in .","Download the [pre-trained weights](https://drive.google.com/file/d/0B5MzpY9kBtDVZ2RpVDYwWmxoSUk/edit) of <SOFTWARE>Facenet</SOFTWARE> provided by Sandberg et al., unzip it and put all files in .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\d3430ffd.txt,1.0
97,/weights/id_net. 2.,/weights/id_net. 2.,/weights/id_net. 2.,/weights/<SOFTWARE>id_net</SOFTWARE>. 2.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\4ec14931.txt,1.0
98,"Download the [pre-trained weights](http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz) of Resnet_v1_50 provided by Tensorflow Slim, unzip it and put resnet_v1_50.ckpt in .","Download the [pre-trained weights](http://download.<SOFTWARE>tensorflow</SOFTWARE>.org/models/resnet_v1_50_2016_08_28.tar.gz) of Resnet_v1_50 provided by <SOFTWARE>Tensorflow</SOFTWARE> Slim, <SOFTWARE>unzip</SOFTWARE> it and put resnet_v1_50.ckpt in .","Download the [pre-trained weights](http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz) of Resnet_v1_50 provided by Tensorflow Slim, unzip it and put resnet_v1_50.ckpt in .","Download the [pre-trained weights](http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz) of <SOFTWARE>Resnet_v1_50</SOFTWARE> provided by <SOFTWARE>Tensorflow Slim</SOFTWARE>, unzip it and put resnet_v1_50.ckpt in .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\641e1556.txt,1.0
99,/weights/resnet. 3.,/weights/resnet. 3.,/weights/resnet. 3.,/weights/<SOFTWARE>resnet</SOFTWARE>. 3.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\cbe36099.txt,1.0
100,Download the [68 landmark detector](https://drive.google.com/file/d/1KYFeTb963jg0F47sTiwqDdhBIvRlUkPa/view?,Download the [68 landmark detector](https://drive.google.com/file/d/1KYFeTb963jg0F47sTiwqDdhBIvRlUkPa/view?,Download the [68 landmark detector](https://drive.google.com/file/d/1KYFeTb963jg0F47sTiwqDdhBIvRlUkPa/view?,Download the [68 landmark detector](https://drive.google.com/file/d/1KYFeTb963jg0F47sTiwqDdhBIvRlUkPa/view?,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\ef3c41c0.txt,1.0
101,"usp=sharing), put the file in .","usp=sharing), put the file in .","usp=sharing), put the file in .","usp=sharing), put the file in .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\5c236898.txt,1.0
102,/network.  ### Data pre-processing ### 1.,/network.  ### Data pre-processing ### 1.,/network.  ### Data pre-processing ### 1.,/network.  ### Data pre-processing ### 1.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\0c24ea8d.txt,1.0
103,To train our model with custom images，5 facial landmarks of each image are needed in advance for an image pre-alignment process.,To train our model with custom images，5 facial landmarks of each image are needed in advance for an image pre-alignment process.,To train our model with custom images，5 facial landmarks of each image are needed in advance for an image pre-alignment process.,To train our model with custom images，5 facial landmarks of each image are needed in advance for an image pre-alignment process.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\e82891c1.txt,1.0
104,We recommend using [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).,We recommend using [<SOFTWARE>dlib</SOFTWARE>](http://<SOFTWARE>dlib</SOFTWARE>.net/) or [<SOFTWARE>MTCNN</SOFTWARE>](https://github.com/ipazc/<SOFTWARE>mtcnn</SOFTWARE>).,We recommend using `dlib`(http://dlib.net/) or `MTCNN`(https://github.com/ipazc/mtcnn).,We recommend using `<SOFTWARE>dlib</SOFTWARE>`(http://dlib.net/) or `<SOFTWARE>MTCNN</SOFTWARE>`(https://github.com/ipazc/mtcnn).,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\ccd738d9.txt,0.9540229885057471
105,"Use these public face detectors to get 5 landmarks, and save all images and corresponding landmarks in <raw_img_path>.","Use these public face detectors to get 5 landmarks, and save all images and corresponding landmarks in <raw_img_path>.","Use these public face detectors to get 5 landmarks, and save all images and corresponding landmarks in <raw_img_path>.","Use these public face detectors to get 5 landmarks, and save all images and corresponding landmarks in <raw_img_path>.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\80fdbf4c.txt,1.0
106,Note that an image and its detected landmark file should have same name. 2.,Note that an image and its detected landmark file should have same name. 2.,Note that an image and its detected landmark file should have same name. 2.,Note that an image and its detected landmark file should have same name. 2.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\8dee18a4.txt,1.0
107,Align images and generate 68 landmarks as well as skin masks for training:   ``` # Run following command for data pre-processing.,Align images and generate 68 landmarks as well as skin masks for training:   ``` # Run following command for data pre-processing.,Align images and generate 68 landmarks as well as skin masks for training:   ``` # Run following command for data pre-processing.,Align images and generate 68 landmarks as well as skin masks for training:   ``` # Run following command for data pre-processing.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\d72c3cb0.txt,1.0
108,"By default, the code uses example images in .","By default, the code uses example images in .","By default, the code uses example images in .","By default, the code uses example images in .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\1df4415d.txt,1.0
109,/input and saves the processed data in .,/input and saves the processed data in .,/input and saves the processed data in .,/input and saves the processed data in .,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\4adeefae.txt,1.0
110,"/processed_data python preprocess_img.py  # Alternatively, you can set your custom image path and save path python preprocess_img.py --img_path <raw_img_path> --save_path <save_path_for_processed_data>  ```  ### Training networks ### 1.","/processed_data python preprocess_img.py  # Alternatively, you can set your custom image path and save path python preprocess_img.py --img_path <raw_img_path> --save_path <save_path_for_processed_data>  ```  ### Training networks ### 1.","/processed_data python preprocess_img.py  # Alternatively, you can set your custom image path and save path python preprocess_img.py --img_path <raw_img_path> --save_path <save_path_for_processed_data>  ```  ### Training networks ### 1.","/processed_data <PROGLANG>python</PROGLANG> preprocess_img.py  # Alternatively, you can set your custom image path and save path <PROGLANG>python</PROGLANG> preprocess_img.py --img_path <raw_img_path> --save_path <save_path_for_processed_data>  ```  ### Training networks ### 1.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\775d2322.txt,1.0
111,"Train the reconstruction network with the following command: ``` # By default, the code uses the data in .","Train the reconstruction network with the following command: ``` # By default, the code uses the data in .","Train the reconstruction network with the following command: ``` # By default, the code uses the data in .","Train the reconstruction network with the following command: ``` # By default, the code uses the data in .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\5dc6b895.txt,1.0
112,"/processed_data as training data as well as validation data python train.py  # Alternatively, you can set your custom data path python train.py --data_path <custom_data_path> --val_data_path <custom_val_data_path> --model_name <custom_model_name>  ``` 2.","/processed_data as training data as well as validation data python train.py  # Alternatively, you can set your custom data path python train.py --data_path <custom_data_path> --val_data_path <custom_val_data_path> --model_name <custom_model_name>  ``` 2.","/processed_data as training data as well as validation data python train.py  # Alternatively, you can set your custom data path python train.py --data_path <custom_data_path> --val_data_path <custom_val_data_path> --model_name <custom_model_name>  ``` 2.","/processed_data as training data as well as validation data <PROGLANG>python</PROGLANG> train.py  # Alternatively, you can set your custom data path <PROGLANG>python</PROGLANG> train.py --data_path <custom_data_path> --val_data_path <custom_val_data_path> --model_name <custom_model_name>  ``` 2.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\7fb0857e.txt,1.0
113,Monitoring the training process via tensorboard: ``` tensorboard --logdir=result/<custom_model_name> --port=10001 ``` 3.,Monitoring the training process via <SOFTWARE>tensorboard</SOFTWARE>: ``` <SOFTWARE>tensorboard</SOFTWARE> --logdir=result/<custom_model_name> --port=10001 ``` 3.,Monitoring the training process via tensorboard: ``` tensorboard --logdir=result/<custom_model_name> --port=10001 ``` 3.,Monitoring the training process via <SOFTWARE>tensorboard</SOFTWARE>: ``` <PROGLANG>tensorboard</PROGLANG> --logdir=result/<custom_model_name> --port=10001 ``` 3.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\8acd66de.txt,1.0
114,Evaluating trained model: ``` python demo.py --use_pb 0 --pretrain_weights <custom_weights>.ckpt ``` Training a model with a batchsize of 16 and 200K iterations takes 20 hours on a single Tesla M40 GPU.  ## Latest Update  ### 2020.4 ### The face reconstruction process is totally transferred to tensorflow version while the old version uses numpy.,Evaluating trained model: ``` python demo.py --use_pb 0 --pretrain_weights <custom_weights>.ckpt ``` Training a model with a batchsize of 16 and 200K iterations takes 20 hours on a single Tesla M40 GPU.  ## Latest Update  ### 2020.4 ### The face reconstruction process is totally transferred to tensorflow version while the old version uses numpy.,Evaluating trained model: ``` python demo.py --use_pb 0 --pretrain_weights <custom_weights>.ckpt ``` Training a model with a batchsize of 16 and 200K iterations takes 20 hours on a single Tesla M40 GPU.  ## Latest Update  ### 2020.4 ### The face reconstruction process is totally transferred to tensorflow version while the old version uses numpy.,Evaluating trained model: ``` <PROGLANG>python</PROGLANG> demo.py --use_pb 0 --pretrain_weights <custom_weights>.ckpt ``` Training a model with a batchsize of 16 and 200K iterations takes 20 hours on a single Tesla M40 GPU.  ## Latest Update  ### 2020.4 ### The face reconstruction process is totally transferred to <SOFTWARE>tensorflow</SOFTWARE> version while the old version uses <SOFTWARE>numpy</SOFTWARE>.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\2c4c7730.txt,1.0
115,We have also integrated the rendering process into the framework.,We have also integrated the rendering process into the framework.,We have also integrated the rendering process into the framework.,We have also integrated the rendering process into the framework.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\9c2579e3.txt,1.0
116,"As a result, reconstruction images aligned with the input can be easily obtained without extra efforts.","As a result, reconstruction images aligned with the input can be easily obtained without extra efforts.","As a result, reconstruction images aligned with the input can be easily obtained without extra efforts.","As a result, reconstruction images aligned with the input can be easily obtained without extra efforts.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\5f2ae66e.txt,1.0
117,The whole process is tensorflow-based which allows gradient back-propagation for other tasks. ### 2020.6 ### Upload a [pre-trained model](https://drive.google.com/file/d/1fPsvLKghlCK8rknb9GPiKwIq9HIqWWwV/view?,The whole process is <SOFTWARE>tensorflow</SOFTWARE>-based which allows gradient back-propagation for other tasks. ### 2020.6 ### Upload a [pre-trained model](https://drive.google.com/file/d/1fPsvLKghlCK8rknb9GPiKwIq9HIqWWwV/view?,The whole process is tensorflow-based which allows gradient back-propagation for other tasks. ### 2020.6 ### Upload a [pre-trained model](https://drive.google.com/file/d/1fPsvLKghlCK8rknb9GPiKwIq9HIqWWwV/view?,The whole process is <PROGLANG>tensorflow</PROGLANG>-based which allows gradient back-propagation for other tasks. ### 2020.6 ### Upload a [pre-trained model](https://drive.google.com/file/d/1fPsvLKghlCK8rknb9GPiKwIq9HIqWWwV/view?,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\b5bb5172.txt,1.0
118,usp=sharing) with white light assumption as described in the paper.  ### 2020.12 ### Upload the training code for single image face reconstruction.  ## Note  1.,usp=sharing) with white light assumption as described in the paper.  ### 2020.12 ### Upload the training code for single image face reconstruction.  ## Note  1.,usp=sharing) with white light assumption as described in the paper.  ### 2020.12 ### Upload the training code for single image face reconstruction.  ## Note  1.,usp=sharing) with white light assumption as described in the paper.  ### 2020.12 ### Upload the training code for single image face reconstruction.  ## Note  1.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\8e5bf669.txt,1.0
119,An image pre-alignment with 5 facial landmarks is necessary before reconstruction.,An image pre-alignment with 5 facial landmarks is necessary before reconstruction.,An image pre-alignment with 5 facial landmarks is necessary before reconstruction.,An image pre-alignment with 5 facial landmarks is necessary before reconstruction.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\81b141f1.txt,1.0
120,"In our image pre-processing stage, we solve a least square problem between 5 facial landmarks on the image and 5 facial landmarks of the BFM09 average 3D face to cancel out face scales and misalignment.","In our image pre-processing stage, we solve a least square problem between 5 facial landmarks on the image and 5 facial landmarks of the BFM09 average 3D face to cancel out face scales and misalignment.","In our image pre-processing stage, we solve a least square problem between 5 facial landmarks on the image and 5 facial landmarks of the BFM09 average 3D face to cancel out face scales and misalignment.","In our image pre-processing stage, we solve a least square problem between 5 facial landmarks on the image and 5 facial landmarks of the <DATASET>BFM09</DATASET> average 3D face to cancel out face scales and misalignment.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\b62e6db9.txt,1.0
121,"To get 5 facial landmarks, you can choose any open source face detector that returns them, such as [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).","To get 5 facial landmarks, you can choose any open source face detector that returns them, such as [<SOFTWARE>dlib</SOFTWARE>](http://<SOFTWARE>dlib</SOFTWARE>.net/) or [<SOFTWARE>MTCNN</SOFTWARE>](https://github.com/ipazc/<SOFTWARE>mtcnn</SOFTWARE>).","To get 5 facial landmarks, you can choose any open source face detector that returns them, such as [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).","To get 5 facial landmarks, you can choose any open source face detector that returns them, such as [<SOFTWARE>dlib</SOFTWARE>](http://dlib.net/) or [<SOFTWARE>MTCNN</SOFTWARE>](https://github.com/ipazc/mtcnn).",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\4bbdd1aa.txt,1.0
122,"However, these traditional 2D detectors may return wrong landmarks under large poses which could influence the alignment result.","However, these traditional 2D detectors may return wrong landmarks under large poses which could influence the alignment result.","However, these traditional 2D detectors may return wrong landmarks under large poses which could influence the alignment result.","However, these traditional 2D detectors may return wrong landmarks under large poses which could influence the alignment result.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\d56f5c59.txt,1.0
123,"Therefore, we recommend using [the method of Bulat et al.]","Therefore, we recommend using [the method of Bulat et al.]","Therefore, we recommend using [the method of Bulat et al.]","Therefore, we recommend using [the method of <PUBLICATION>Bulat et al.</PUBLICATION>]",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\80664460.txt,1.0
124,(https://github.com/1adrianb/2D-and-3D-face-alignment) to get facial landmarks (3D definition) with semantic consistency for large pose images.,(https://github.com/1adrianb/2D-and-3D-face-alignment) to get facial landmarks (3D definition) with semantic consistency for large pose images.,(https://github.com/1adrianb/2D-and-3D-face-alignment) to get facial landmarks (3D definition) with semantic consistency for large pose images.,(https://github.com/1adrianb/2D-and-3D-face-alignment) to get facial landmarks (3D definition) with semantic consistency for large pose images.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\e8221f15.txt,1.0
125,Note that our model is trained without position augmentation so that a bad alignment may lead to inaccurate reconstruction results.,Note that our model is trained without position augmentation so that a bad alignment may lead to inaccurate reconstruction results.,Note that our model is trained without position augmentation so that a bad alignment may lead to inaccurate reconstruction results.,Note that our model is trained without position augmentation so that a bad alignment may lead to inaccurate reconstruction results.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\37252b65.txt,1.0
126,We put some examples in the .,We put some examples in the .,We put some examples in the .,We put some examples in the .,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\7418e638.txt,1.0
127,/input subfolder for reference.   2.,/input subfolder for reference.   2.,/BFM subfolder.  2.,/BFM subfolder.  2.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\eee6da3a.txt,0.5818181818181818
128,We assume a [pinhole camera model](https://en.wikipedia.org/wiki/Pinhole_camera_model) for face projection.,We assume a [pinhole camera model](https://en.wikipedia.org/wiki/Pinhole_camera_model) for face projection.,We assume a `pinhole camera model` for face projection.,We assume a `<SOFTWARE>pinhole camera model</SOFTWARE>` for face projection.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\d528b2f7.txt,0.654320987654321
129,"The camera is positioned at (0,0,10) (dm) in the world coordinate and points to the negative z axis.","The camera is positioned at (0,0,10) (dm) in the world coordinate and points to the negative z axis.","The camera is positioned at (0,0,10) (dm) in the world coordinate and points to the negative z axis.","The camera is positioned at (0,0,10) (dm) in the world coordinate and points to the negative z axis.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\e9b413e6.txt,1.0
130,We set the camera fov to 12.6 empirically and fix it during training and inference time.,We set the camera fov to 12.6 empirically and fix it during training and inference time.,We set the camera fov to 12.6 empirically and fix it during training and inference time.,We set the camera fov to 12.6 empirically and fix it during training and inference time.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\4c019ac0.txt,1.0
131,Faces in canonical views are at the origin of the world coordinate and facing the positive z axis.,Faces in canonical views are at the origin of the world coordinate and facing the positive z axis.,Faces in canonical views are at the origin of the world coordinate and facing the positive z axis.,Faces in canonical views are at the origin of the world coordinate and facing the positive z axis.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\97d1b390.txt,1.0
132,Rotations and translations predicted by the R-Net are all with respect to the world coordinate.,Rotations and translations predicted by the R-Net are all with respect to the world coordinate.,Rotations and translations predicted by the R-Net are all with respect to the world coordinate.,Rotations and translations predicted by the <SOFTWARE>R-Net</SOFTWARE> are all with respect to the world coordinate.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\ed61a5d0.txt,1.0
133,"<p align=""center"">  <img src=""/images/camera.png"" width=""300""> </p>  3.","<p align=""center"">  <img src=""/images/camera.png"" width=""300""> </p>  3.","<p align=""center"">  <img src=""/images/camera.png"" width=""300""> </p>  3.","<p align=""center"">  <img src=""/images/camera.png"" width=""300""> </p>  3.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\c3eb8c6a.txt,1.0
134,"The current model is trained using 3-channel (r,g,b) scene illumination instead of white light described in the paper.","The current model is trained using 3-channel (r,g,b) scene illumination instead of white light described in the paper.","The current model is trained using 3-channel (r,g,b) scene illumination instead of white light described in the paper.","The current model is trained using 3-channel (r,g,b) scene illumination instead of white light described in the paper.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\13e2720c.txt,1.0
135,"As a result, the gamma coefficient that controls lighting has a dimension of 27 instead of 9.   4.","As a result, the gamma coefficient that controls lighting has a dimension of 27 instead of 9.   4.","As a result, the gamma coefficient that controls lighting has a dimension of 27 instead of 9.   4.","As a result, the gamma coefficient that controls lighting has a dimension of 27 instead of 9.   4.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\64c3e1ab.txt,1.0
136,We excluded ear and neck region of original BFM09 to allow the network concentrate on the face region.,We excluded ear and neck region of original BFM09 to allow the network concentrate on the face region.,We excluded ear and neck region of original `BFM09` to allow the network concentrate on the face region.,We excluded ear and neck region of original `<DATASET>BFM09</DATASET>` to allow the network concentrate on the face region.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\18c2eaf7.txt,0.9902912621359223
137,"To see which vertices in the original model are preserved, check select_vertex_id.mat in the .","To see which vertices in the original model are preserved, check select_vertex_id.mat in the .","To see which vertices in the original model are preserved, check `select_vertex_id.mat` in the .","To see which vertices in the original model are preserved, check `<SOFTWARE>select_vertex_id.mat</SOFTWARE>` in the .",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\6f8ce1e1.txt,0.9894736842105263
138,/BFM subfolder.,/BFM subfolder.,/BFM subfolder.,/BFM subfolder.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\872d3b0d.txt,1.0
139,Note that index starts from 1.  5.,Note that index starts from 1.  5.,Note that index starts from 1.  5.,Note that index starts from 1.  5.,../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\8f11a2e9.txt,1.0
140,"Our model may give inferior results for images with severe perspetive distortions (e.g., some selfies).","Our model may give inferior results for images with severe perspetive distortions (e.g., some selfies).","Our model may give inferior results for images with severe perspetive distortions (e.g., some selfies).","Our model may give inferior results for images with severe perspetive distortions (e.g., some selfies).",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\0ddf881d.txt,1.0
141,"In addition, we cannot well handle faces with eyes closed due to the lack of these kind of images in the training data.    5.","In addition, we cannot well handle faces with eyes closed due to the lack of these kind of images in the training data.    5.","In addition, we cannot well handle faces with eyes closed due to the lack of these kind of images in the training data.    5.","In addition, we cannot well handle faces with eyes closed due to the lack of these kind of images in the training data.    5.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\20da21c3.txt,1.0
142,"If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={IEEE Computer Vision and Pattern Recognition Workshops},      year={2019}  } ## The face images on this page are from the public [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset released by MMLab, CUHK.","If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={<PUBLICATION>Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</PUBLICATION>},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={<PUBLICATION>IEEE Computer Vision and Pattern Recognition Workshops</PUBLICATION><WORKSHOP>Computer Vision and Pattern Recognition Workshops</WORKSHOP>},      year={2019}  } ## The face images on this page are from the public [<DATASET>CelebA</DATASET>](http://mmlab.ie.cuhk.edu.hk/projects/<DATASET>CelebA</DATASET>.html) dataset released by MMLab, CUHK.","If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={IEEE Computer Vision and Pattern Recognition Workshops},      year={2019}  } ## The face images on this page are from the public CelebA dataset released by MMLab, CUHK.","If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={<PUBLICATION>Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</PUBLICATION>},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={<CONFERENCE>IEEE Computer Vision and Pattern Recognition Workshops</CONFERENCE>},      year={2019}  } ## The face images on this page are from the public <DATASET>CelebA</DATASET> dataset released by <PROJECT>MMLab</PROJECT>, <PROJECT>CUHK</PROJECT>.",../results/deepseek-chat/prompt-0/zzz_Microsoft_Deep3DFaceReconstruction_master_readme.md.tsv\7f96a982.txt,0.9606060606060606
