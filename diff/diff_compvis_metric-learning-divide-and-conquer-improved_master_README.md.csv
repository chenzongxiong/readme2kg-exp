sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,"# Improving Deep Metric Learning by Divide and Conquer ## About  PyTorch implementation for the paper _Improving Deep Metric  Learning by Divide and Conquer_ accepted to **TPAMI** (Sep. 2021), which is our follow-up paper of [_Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019)_](https://github.com/CompVis/metric-learning-divide-and-conquer)  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * TPAMI early access: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * PyTorch 1.1.0 * Faiss-GPU >= 1.5.0, [Link](https://github.com/facebookresearch/faiss) * albumentations >= 0.4.5, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling `python train.py` and setting the respective params, all of which are listed and explained  in `/experiment/margin_loss_resnet50.py` (the default setup for all our experiments).","# <PUBLICATION>Improving Deep Metric Learning by Divide and Conquer</PUBLICATION> ## About  <SOFTWARE>PyTorch</SOFTWARE> implementation for the paper _<PUBLICATION>Improving Deep Metric  Learning by Divide and Conquer</PUBLICATION>_ accepted to **<PUBLICATION>TPAMI</PUBLICATION>** (Sep. 2021), which is our follow-up paper of [_<PUBLICATION>Divide and Conquer the Embedding Space for Metric Learning</PUBLICATION> (<CONFERENCE>CVPR 2019</CONFERENCE>)_](https://github.com/CompVis/metric-learning-divide-and-conquer)  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * <PUBLICATION>TPAMI</PUBLICATION> early access: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * <SOFTWARE>PyTorch 1.1.0</SOFTWARE> * <SOFTWARE>Faiss-GPU >= 1.5.0</SOFTWARE>, [Link](https://github.com/facebookresearch/<SOFTWARE>faiss</SOFTWARE>) * <SOFTWARE>albumentations >= 0.4.5</SOFTWARE>, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling `<SOFTWARE>python</SOFTWARE> train.py` and setting the respective params, all of which are listed and explained  in `/experiment/margin_loss_resnet50.py` (the default setup for all our experiments).","# Improving Deep Metric Learning by Divide and Conquer ## About  PyTorch implementation for the paper _Improving Deep Metric  Learning by Divide and Conquer_ accepted to **TPAMI** (Sep. 2021), which is our follow-up paper of [_Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019)_](https://github.com/CompVis/metric-learning-divide-and-conquer)  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * TPAMI early access: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * PyTorch 1.1.0 * Faiss-GPU >= 1.5.0, [Link](https://github.com/facebookresearch/faiss) * albumentations >= 0.4.5, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling python train.py and setting the respective params, all of which are listed and explained  in /experiment/margin_loss_resnet50.py (the default setup for all our experiments).
","# Improving Deep Metric Learning by Divide and Conquer ## About  <PROGLANG>PyTorch</PROGLANG> implementation for the <PUBLICATION>paper _Improving Deep Metric  Learning by Divide and Conquer_</PUBLICATION> accepted to <PUBLICATION>**TPAMI** (Sep. 2021)</PUBLICATION>, which is our follow-up paper of <PUBLICATION>[_Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019)_](https://github.com/CompVis/metric-learning-divide-and-conquer)</PUBLICATION>  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * <PUBLICATION>TPAMI early access</PUBLICATION>: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * <PROGLANG>PyTorch 1.1.0</PROGLANG> * <SOFTWARE>Faiss-GPU >= 1.5.0</SOFTWARE>, [Link](https://github.com/facebookresearch/faiss) * <SOFTWARE>albumentations >= 0.4.5</SOFTWARE>, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling python train.py and setting the respective params, all of which are listed and explained  in /experiment/margin_loss_resnet50.py (the default setup for all our experiments).
",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\ec705d3a.txt,0.9972602739726028
3,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.","**A basic sample run using default parameters would like this**:  ``` <SOFTWARE>python</SOFTWARE> train.py --experiment margin_loss_resnet50 \                 --dataset=<DATASET>sop</DATASET> -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=<SOFTWARE>faiss-gpu</SOFTWARE> \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=<DATASET>cub</DATASET>` (<DATASET>CUB200-2011</DATASET>), `<DATASET>cars</DATASET>` (<DATASET>CARS196</DATASET>), `<DATASET>sop</DATASET>` (<DATASET>Standford Online Porducts</DATASET>), `<DATASET>inshop</DATASET>` (<DATASET>In-Shop cloths retireval</DATASET>) or `<DATASET>vid</DATASET>` (<DATASET>PKU Vehicle id</DATASET>). - **--nb_clusters**: could be maximumly possible set to 16.","**A basic sample run using default parameters would like this**:   python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir   - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of --dataset=cub (CUB200-2011), cars (CARS196), sop (Standford Online Porducts), inshop (In-Shop cloths retireval) or vid (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.
","**A basic sample run using default parameters would like this**:   <PROGLANG>python</PROGLANG> train.py --experiment margin_loss_resnet50 \                 --dataset=<DATASET>sop</DATASET> -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir   - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of --dataset=<DATASET>cub</DATASET> (<DATASET>CUB200-2011</DATASET>), <DATASET>cars</DATASET> (<DATASET>CARS196</DATASET>), <DATASET>sop</DATASET> (<DATASET>Standford Online Porducts</DATASET>), <DATASET>inshop</DATASET> (<DATASET>In-Shop cloths retireval</DATASET>) or <DATASET>vid</DATASET> (<DATASET>PKU Vehicle id</DATASET>). - **--nb_clusters**: could be maximumly possible set to 16.
",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\82356afa.txt,0.9666505558240696
5,"For example, if you are a ubuntu user, `ulimit -n <twice the number of the default>` usually will do the trick.","For example, if you are a ubuntu user, `ulimit -n <twice the number of the default>` usually will do the trick.","For example, if you are a ubuntu user, ulimit -n <twice the number of the default> usually will do the trick.","For example, if you are a <PROGLANG>ubuntu</PROGLANG> user, ulimit -n <twice the number of the default> usually will do the trick.",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\eefc73fc.txt,0.990909090909091
6,"Besides, please take the total number of different classes in the dataset and the sampling strategy used into account when you setting this value. - **--dataset-dir**: the path to the datasets, check the *Datasets* section below. - **--sampler**: batchminer used to sample pairs or triplets (in embedding space) to create learning signal, check `/metriclearning/sampler` for details. - **--batch-sampler**: data sampler used to generate training batches, check `/dataset/sampler.py` for details. - **--nb-epochs**: the maximum training epochs. - **--mod-epoch**: division frequency in the paper - the number of training epochs between consecutive divisions. - **--wandb-enabled**: by setting this flag, you will enable the [Weights&Biases](https://wandb.ai/site) logging.","Besides, please take the total number of different classes in the dataset and the sampling strategy used into account when you setting this value. - **--dataset-dir**: the path to the datasets, check the *Datasets* section below. - **--sampler**: batchminer used to sample pairs or triplets (in embedding space) to create learning signal, check `/metriclearning/sampler` for details. - **--batch-sampler**: data sampler used to generate training batches, check `/dataset/sampler.py` for details. - **--nb-epochs**: the maximum training epochs. - **--mod-epoch**: division frequency in the paper - the number of training epochs between consecutive divisions. - **--<SOFTWARE>wandb</SOFTWARE>-enabled**: by setting this flag, you will enable the [Weights&Biases](https://<SOFTWARE>wandb</SOFTWARE>.ai/site) logging.","Besides, please take the total number of different classes in the dataset and the sampling strategy used into account when you setting this value. - **--dataset-dir**: the path to the datasets, check the *Datasets* section below. - **--sampler**: batchminer used to sample pairs or triplets (in embedding space) to create learning signal, check /metriclearning/sampler for details. - **--batch-sampler**: data sampler used to generate training batches, check /dataset/sampler.py for details. - **--nb-epochs**: the maximum training epochs. - **--mod-epoch**: division frequency in the paper - the number of training epochs between consecutive divisions. - **--wandb-enabled**: by setting this flag, you will enable the [Weights&Biases](https://wandb.ai/site) logging.
","Besides, please take the total number of different classes in the <DATASET>dataset</DATASET> and the sampling strategy used into account when you setting this value. - **--dataset-dir**: the path to the <DATASET>datasets</DATASET>, check the *<DATASET>Datasets</DATASET>* section below. - **--sampler**: batchminer used to sample pairs or triplets (in embedding space) to create learning signal, check /metriclearning/sampler for details. - **--batch-sampler**: data sampler used to generate training batches, check /dataset/sampler.py for details. - **--nb-epochs**: the maximum training epochs. - **--mod-epoch**: division frequency in the <PUBLICATION>paper</PUBLICATION> - the number of training epochs between consecutive divisions. - **--wandb-enabled**: by setting this flag, you will enable the [<SOFTWARE>Weights&Biases</SOFTWARE>](https://wandb.ai/site) logging.
",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\1cb11b48.txt,0.9967511371020142
7,Please change the w&b initial setting at the last part of `/experiment/margin_loss_resnet50.py` accordingly.,Please change the w&b initial setting at the last part of `/experiment/margin_loss_resnet50.py` accordingly.,Please change the w&b initial setting at the last part of /experiment/margin_loss_resnet50.py accordingly.,Please change the w&b initial setting at the last part of /experiment/<SOFTWARE>margin_loss_resnet50.py</SOFTWARE> accordingly.,../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\212ccf03.txt,0.9906542056074766
8,"**_Note:_** For exact settings for different datasets, please check the original paper.","**_Note:_** For exact settings for different datasets, please check the original paper.","**_Note:_** For exact settings for different datasets, please check the original paper.","**_Note:_** For exact settings for different <DATASET>datasets</DATASET>, please check the original <PUBLICATION>paper</PUBLICATION>.",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\108f841d.txt,1.0
9,"For arguments not mentioned above,  please check `/experiment/margin_loss_resnet50.py` for explanation.   ### Evaluate or check results during:  * **evaluate a trained model**: `eval_model.py <log and model checkpoint path>`.","For arguments not mentioned above,  please check `/experiment/margin_loss_resnet50.py` for explanation.   ### Evaluate or check results during:  * **evaluate a trained model**: `eval_model.py <log and model checkpoint path>`.","For arguments not mentioned above,  please check /experiment/margin_loss_resnet50.py for explanation.   ### Evaluate or check results during:  * **evaluate a trained model**: eval_model.py <log and model checkpoint path>.
","For arguments not mentioned above,  please check /experiment/<SOFTWARE>margin_loss_resnet50.py</SOFTWARE> for explanation.   ### Evaluate or check results during:  * **evaluate a trained model**: <SOFTWARE>eval_model.py</SOFTWARE> <log and model checkpoint path>.
",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\e01e706c.txt,0.9843400447427293
10,It is suggested to put only those models (checkpoints and logs) you want to eval into one folder otherwise it will evaluate all the models ind the folder  * **check intermediate results**: the model checkpoints and log files are saved in the selected log-directory (by default: `/log`).,It is suggested to put only those models (checkpoints and logs) you want to eval into one folder otherwise it will evaluate all the models ind the folder  * **check intermediate results**: the model checkpoints and log files are saved in the selected log-directory (by default: `/log`).,It is suggested to put only those models (checkpoints and logs) you want to eval into one folder otherwise it will evaluate all the models ind the folder  * **check intermediate results**: the model checkpoints and log files are saved in the selected log-directory (by default: /log).,It is suggested to put only those models (checkpoints and logs) you want to eval into one folder otherwise it will evaluate all the models ind the folder  * **check intermediate results**: the model checkpoints and log files are saved in the selected log-directory (by default: /log).,../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\d34d11a6.txt,0.9964912280701754
11,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * <DATASET>CUB200-2011</DATASET> (http://www.vision.caltech.edu/visipedia/<DATASET>CUB-200</DATASET>.html) * <DATASET>CARS196</DATASET> (https://ai.stanford.edu/~jkrause/<DATASET>cars</DATASET>/car_dataset.html) * <DATASET>Stanford Online Products</DATASET> (http://cvgl.stanford.edu/projects/lifted_struct/) * <DATASET>In-shop Clothes Retrieval Benchmark</DATASET> (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * <DATASET>PKU VehicleID</DATASET> (https://www.pkuml.org/resources/<DATASET>pku-vehicleid</DATASET>.html)  Assuming your folder is placed in e.g.,"You can print a summary of the results with python browse_results <log path>.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.
","You can print a summary of the results with <PROGLANG>python</PROGLANG> browse_results <log path>.     ### Datasets:  The method is tested on the following datasets:  * <DATASET>CUB200-2011</DATASET> (http://www.vision.caltech.edu/visipedia/CUB-200.html) * <DATASET>CARS196</DATASET> (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * <DATASET>Stanford Online Products</DATASET> (http://cvgl.stanford.edu/projects/lifted_struct/) * <DATASET>In-shop Clothes Retrieval Benchmark</DATASET> (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * <DATASET>PKU VehicleID</DATASET> (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.
",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\c00e67f8.txt,0.997338065661047
12,"`<$datadir/cars196>`, pass `$datadir` as input to `--dataset-dir`, by default.","`<$datadir/<DATASET>cars196</DATASET>>`, pass `$datadir` as input to `--dataset-dir`, by default.","<$datadir/cars196>, pass $datadir as input to --dataset-dir, by default.","<$datadir/<DATASET>cars196</DATASET>>, pass $datadir as input to --dataset-dir, by default.",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\d757f060.txt,0.96
13,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...","To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For <DATASET>CUB200-2011</DATASET>, ``` <DATASET>cub-200-2011</DATASET> └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For <DATASET>Cars196</DATASET>, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/<DATASET>cars</DATASET>/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` <DATASET>cars196</DATASET> └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For <DATASET>Stanford Online Products</DATASET>: ``` <DATASET>sop</DATASET> └───bicycle_final |   │   111085122871_0.jpg |           ... |...","To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011,  cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ...  * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader.  cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat    * For Stanford Online Products:  sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...
","To avoid conflicts between the folder structure and our pipeline, please make sure that the <DATASET>datasets</DATASET>  have the following internal structure:  * For <DATASET>CUB200-2011</DATASET>,  cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ...  * For <DATASET>Cars196</DATASET>, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader.  cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat    * For <DATASET>Stanford Online Products</DATASET>:  sop └───bicycle_final |   │   111085122871_0.jpg |           ... |...
",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\85d0860b.txt,0.9898862199747156
14,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).","└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For <DATASET>In-shop Clothes</DATASET> and <DATASET>PKU Vehicle id</DATASET> datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __<DATASET>CUB200</DATASET>__  Variants | Loss/Sampling |   <EVALMETRIC>NMI</EVALMETRIC>  |  <EVALMETRIC>mARP</EVALMETRIC>  | <EVALMETRIC>Recall</EVALMETRIC> @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __<DATASET>Cars196</DATASET>__  Variants | Loss/Sampling |   <EVALMETRIC>NMI</EVALMETRIC>  |  <EVALMETRIC>mARP</EVALMETRIC>  | <EVALMETRIC>Recall</EVALMETRIC> @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __<DATASET>In-Shop Clothes</DATASET>__  Variants | Loss/Sampling    |   <EVALMETRIC>NMI</EVALMETRIC>  |  <EVALMETRIC>mARP</EVALMETRIC>  | <EVALMETRIC>Recall</EVALMETRIC> @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __<DATASET>Online Products</DATASET>__  Variants | Loss/Sampling |   <EVALMETRIC>NMI</EVALMETRIC>  |  <EVALMETRIC>mARP</EVALMETRIC>  | <EVALMETRIC>Recall</EVALMETRIC> @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __<DATASET>VID</DATASET> (Large eval set)__   Variants | Loss/Sampling |   <EVALMETRIC>NMI</EVALMETRIC>  |  <EVALMETRIC>mARP</EVALMETRIC>  | <EVALMETRIC>Recall</EVALMETRIC> @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with <SOFTWARE>PyTorch (0.4.1)</SOFTWARE> and <SOFTWARE>Faiss (1.4.0)</SOFTWARE>.","└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt   * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).
","└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt   * For <DATASET>In-shop Clothes</DATASET> and <DATASET>PKU Vehicle id</DATASET> datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __<DATASET>CUB200</DATASET>__  Variants | Loss/Sampling |   <EVALMETRIC>NMI</EVALMETRIC>  |  <EVALMETRIC>mARP</EVALMETRIC>  | <EVALMETRIC>Recall @ 1 -- 2 -- 4 -- 8</EVALMETRIC> ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __<DATASET>Cars196</DATASET>__  Variants | Loss/Sampling |   <EVALMETRIC>NMI</EVALMETRIC>  |  <EVALMETRIC>mARP</EVALMETRIC>  | <EVALMETRIC>Recall @ 1 -- 2 -- 4 -- 8</EVALMETRIC> ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __<DATASET>In-Shop Clothes</DATASET>__  Variants | Loss/Sampling    |   <EVALMETRIC>NMI</EVALMETRIC>  |  <EVALMETRIC>mARP</EVALMETRIC>  | <EVALMETRIC>Recall @ 1 -- 10 -- 20 -- 30</EVALMETRIC> ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __<DATASET>Online Products</DATASET>__  Variants | Loss/Sampling |   <EVALMETRIC>NMI</EVALMETRIC>  |  <EVALMETRIC>mARP</EVALMETRIC>  | <EVALMETRIC>Recall @ 1 -- 10 -- 100</EVALMETRIC> ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __<DATASET>VID (Large eval set)</DATASET>__   Variants | Loss/Sampling |   <EVALMETRIC>NMI</EVALMETRIC>  |  <EVALMETRIC>mARP</EVALMETRIC>  | <EVALMETRIC>Recall @ 1 -- 5</EVALMETRIC> ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with <SOFTWARE>PyTorch (0.4.1)</SOFTWARE> and <SOFTWARE>Faiss (1.4.0)</SOFTWARE>.
",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\34e03e80.txt,0.9989764585465711
15,"Some deviations in results based on different PyTorch/Cuda/Faiss versions and hardware (e.g. between P100 and RTX GPUs) are to be expected._   ## Related Repos  * Divide and Conquer the Embedding Space for Metric Learning (our previous paper on CVPR 2019):  https://github.com/CompVis/metric-learning-divide-and-conquer * An easy-to-use repo to start your DML research, containing collections of models, losses, and samplers implemented in PyTorch: https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch  ## BibTex If you use this code in your research, please cite the following papers:  ``` @InProceedings{dcesml,   title={Divide and Conquer the Embedding Space for Metric Learning},   author={Sanakoyeu, Artsiom and Tschernezki, Vadim and B\""uchler, Uta and Ommer, Bj\""orn},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   year={2019} }  @article{sanakoyeu2021improving,   title={Improving Deep Metric Learning by Divide and Conquer},    author={Artsiom Sanakoyeu and Pingchuan Ma and Vadim Tschernezki and Björn Ommer},   journal={IEEE Transactions on pattern analysis and machine intelligence},   year={2021},   publisher={IEEE} } ```","Some deviations in results based on different <SOFTWARE>PyTorch</SOFTWARE>/<SOFTWARE>Cuda</SOFTWARE>/<SOFTWARE>Faiss</SOFTWARE> versions and hardware (e.g. between P100 and RTX GPUs) are to be expected._   ## Related Repos  * <PUBLICATION>Divide and Conquer the Embedding Space for Metric Learning</PUBLICATION> (our previous paper on <CONFERENCE>CVPR 2019</CONFERENCE>):  https://github.com/CompVis/metric-learning-divide-and-conquer * An easy-to-use repo to start your DML research, containing collections of models, losses, and samplers implemented in <SOFTWARE>PyTorch</SOFTWARE>: https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_<SOFTWARE>PyTorch</SOFTWARE>  ## BibTex If you use this code in your research, please cite the following papers:  ``` @InProceedings{dcesml,   title={<PUBLICATION>Divide and Conquer the Embedding Space for Metric Learning</PUBLICATION>},   author={Sanakoyeu, Artsiom and Tschernezki, Vadim and B\""uchler, Uta and Ommer, Bj\""orn},   booktitle={<PUBLICATION>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</PUBLICATION><CONFERENCE>IEEE Conference on Computer Vision and Pattern Recognition</CONFERENCE>},   year={2019} }  @article{sanakoyeu2021improving,   title={<PUBLICATION>Improving Deep Metric Learning by Divide and Conquer</PUBLICATION>},    author={Artsiom Sanakoyeu and Pingchuan Ma and Vadim Tschernezki and Björn Ommer},   journal={<PUBLICATION>IEEE Transactions on pattern analysis and machine intelligence</PUBLICATION>},   year={2021},   publisher={IEEE} } ```","Some deviations in results based on different PyTorch/Cuda/Faiss versions and hardware (e.g. between P100 and RTX GPUs) are to be expected._   ## Related Repos  * Divide and Conquer the Embedding Space for Metric Learning (our previous paper on CVPR 2019):  https://github.com/CompVis/metric-learning-divide-and-conquer * An easy-to-use repo to start your DML research, containing collections of models, losses, and samplers implemented in PyTorch: https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch  ## BibTex If you use this code in your research, please cite the following papers:   @InProceedings{dcesml,   title={Divide and Conquer the Embedding Space for Metric Learning},   author={Sanakoyeu, Artsiom and Tschernezki, Vadim and B\""uchler, Uta and Ommer, Bj\""orn},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   year={2019} }  @article{sanakoyeu2021improving,   title={Improving Deep Metric Learning by Divide and Conquer},    author={Artsiom Sanakoyeu and Pingchuan Ma and Vadim Tschernezki and Björn Ommer},   journal={IEEE Transactions on pattern analysis and machine intelligence},   year={2021},   publisher={IEEE} } 
","Some deviations in results based on different <SOFTWARE>PyTorch</SOFTWARE>/<SOFTWARE>Cuda</SOFTWARE>/<SOFTWARE>Faiss</SOFTWARE> versions and hardware (e.g. between <SOFTWARE>P100</SOFTWARE> and <SOFTWARE>RTX GPUs</SOFTWARE>) are to be expected._   ## Related Repos  * <PROJECT>Divide and Conquer the Embedding Space for Metric Learning</PROJECT> (our previous paper on <CONFERENCE>CVPR 2019</CONFERENCE>):  https://github.com/CompVis/metric-learning-divide-and-conquer * An easy-to-use repo to start your <PROJECT>DML</PROJECT> research, containing collections of models, losses, and samplers implemented in <SOFTWARE>PyTorch</SOFTWARE>: https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch  ## BibTex If you use this code in your research, please cite the following papers:   @InProceedings{dcesml,   title={<PUBLICATION>Divide and Conquer the Embedding Space for Metric Learning</PUBLICATION>},   author={Sanakoyeu, Artsiom and Tschernezki, Vadim and B\""uchler, Uta and Ommer, Bj\""orn},   booktitle={<PUBLICATION>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</PUBLICATION>},   year={2019} }  @article{sanakoyeu2021improving,   title={<PUBLICATION>Improving Deep Metric Learning by Divide and Conquer</PUBLICATION>},    author={Artsiom Sanakoyeu and Pingchuan Ma and Vadim Tschernezki and Björn Ommer},   journal={<PUBLICATION>IEEE Transactions on pattern analysis and machine intelligence</PUBLICATION>},   year={2021},   publisher={<PUBLICATION>IEEE</PUBLICATION>} } 
",../results/deepseek-chat/prompt-0/zzz_compvis_metric-learning-divide-and-conquer-improved_master_README.md.tsv\cdfcc4f1.txt,0.9970821175489787
