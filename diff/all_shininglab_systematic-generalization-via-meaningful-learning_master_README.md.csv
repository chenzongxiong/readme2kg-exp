sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,# Systematic-Generalization-via-Meaningful-Learning This repository is for the paper [Revisit Systematic Generalization via Meaningful Learning](https://aclanthology.org/2022.blackboxnlp-1.6).,# Systematic-Generalization-via-Meaningful-Learning This repository is for the paper [<PUBLICATION>Revisit Systematic Generalization via Meaningful Learning</PUBLICATION>](https://aclanthology.org/2022.blackboxnlp-1.6).,# Systematic-Generalization-via-Meaningful-Learning This repository is for the paper [Revisit Systematic Generalization via Meaningful Learning](https://aclanthology.org/2022.blackboxnlp-1.6).,# Systematic-Generalization-via-Meaningful-Learning This repository is for the <PUBLICATION>paper [Revisit Systematic Generalization via Meaningful Learning](https://aclanthology.org/2022.blackboxnlp-1.6)</PUBLICATION>.,../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\aaad7db5.txt,1.0
2,"*In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP*, pages 62–79, Abu Dhabi, United Arab Emirates (Hybrid).","*In <PUBLICATION>Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</PUBLICATION><WORKSHOP>Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</WORKSHOP>*, pages 62–79, Abu Dhabi, United Arab Emirates (Hybrid).","*In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP*, pages 62–79, Abu Dhabi, United Arab Emirates (Hybrid).","*In <PUBLICATION>Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</PUBLICATION>*, pages 62–79, Abu Dhabi, United Arab Emirates (Hybrid).",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\00f8ae29.txt,1.0
3,Association for Computational Linguistics.,Association for Computational Linguistics.,`Association for Computational Linguistics.`,`<PUBLICATION>Association for Computational Linguistics</PUBLICATION>.`,../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\c224ebd2.txt,0.9767441860465116
4,"[[arXiv](https://arxiv.org/abs/2003.06658)] [[Poster](https://www.shininglab.ai/assets/posters/Revisit%20Systematic%20Generalization%20via%20Meaningful%20Learning.pdf)]  ## Directory + **main/config.py** - Configurations + **main/res** - Resources including model check points, datasets, experiment records, and results + **main/src** - Source code including model structures and utility functions ``` Systematic-Generalization-via-Meaningful-Learning ├── README.md ├── main │   ├── config.py │   ├── res │   │   ├── check_points │   │   ├── data │   │   │   ├── scan │   │   │   ├── geography │   │   │   ├── advising │   │   │   ├── geo_vars.txt │   │   │   ├── adv_vars.txt │   │   │   ├── iwslt14 │   │   │   ├── iwslt15 │   │   │   ├── prepare-iwslt14.sh │   │   │   └── prepare-iwslt15.sh │   │   ├── log │   │   └── result │   ├── src │   │   ├── models │   │   └── utils │   └── train.py └── requirements.txt ```  ## Dependencies + python >= 3.10.6 + tqdm >= 4.64.1 + numpy >= 1.23.4 + torch >= 1.13.0  ## Data All datasets can be downloaded [here](https://drive.google.com/drive/folders/19vFBn5C-nTdjxMeuMgw-BvsPNsLF6DpV?","[[arXiv](https://arxiv.org/abs/2003.06658)] [[Poster](https://www.shininglab.ai/assets/posters/Revisit%20Systematic%20Generalization%20via%20Meaningful%20Learning.pdf)]  ## Directory + **main/config.py** - Configurations + **main/res** - Resources including model check points, datasets, experiment records, and results + **main/src** - Source code including model structures and utility functions ``` Systematic-Generalization-via-Meaningful-Learning ├── README.md ├── main │   ├── config.py │   ├── res │   │   ├── check_points │   │   ├── data │   │   │   ├── scan │   │   │   ├── geography │   │   │   ├── advising │   │   │   ├── geo_vars.txt │   │   │   ├── adv_vars.txt │   │   │   ├── iwslt14 │   │   │   ├── iwslt15 │   │   │   ├── prepare-iwslt14.sh │   │   │   └── prepare-iwslt15.sh │   │   ├── log │   │   └── result │   ├── src │   │   ├── models │   │   └── utils │   └── train.py └── requirements.txt ```  ## Dependencies + <SOFTWARE>python >= 3.10.6</SOFTWARE> + <SOFTWARE>tqdm >= 4.64.1</SOFTWARE> + <SOFTWARE>numpy >= 1.23.4</SOFTWARE> + <SOFTWARE>torch >= 1.13.0</SOFTWARE>  ## Data All datasets can be downloaded [here](https://drive.google.com/drive/folders/19vFBn5C-nTdjxMeuMgw-BvsPNsLF6DpV?","[[arXiv](https://arxiv.org/abs/2003.06658)] [[Poster](https://www.shininglab.ai/assets/posters/Revisit%20Systematic%20Generalization%20via%20Meaningful%20Learning.pdf)]  ## Directory + **main/config.py** - Configurations + **main/res** - Resources including model check points, datasets, experiment records, and results + **main/src** - Source code including model structures and utility functions ``` Systematic-Generalization-via-Meaningful-Learning ├── README.md ├── main │   ├── config.py │   ├── res │   │   ├── check_points │   │   ├── data │   │   │   ├── scan │   │   │   ├── geography │   │   │   ├── advising │   │   │   ├── geo_vars.txt │   │   │   ├── adv_vars.txt │   │   │   ├── iwslt14 │   │   │   ├── iwslt15 │   │   │   ├── prepare-iwslt14.sh │   │   │   └── prepare-iwslt15.sh │   │   ├── log │   │   └── result │   ├── src │   │   ├── models │   │   └── utils │   └── train.py └── requirements.txt ```  ## Dependencies + python >= 3.10.6 + tqdm >= 4.64.1 + numpy >= 1.23.4 + torch >= 1.13.0  ## Data All datasets can be downloaded [here](https://drive.google.com/drive/folders/19vFBn5C-nTdjxMeuMgw-BvsPNsLF6DpV?","[[arXiv](https://arxiv.org/abs/2003.06658)] [[Poster](https://www.shininglab.ai/assets/posters/Revisit%20Systematic%20Generalization%20via%20Meaningful%20Learning.pdf)]  ## Directory + **main/config.py** - Configurations + **main/res** - Resources including model check points, <DATASET>datasets</DATASET>, experiment records, and results + **main/src** - Source code including model structures and utility functions ``` Systematic-Generalization-via-Meaningful-Learning ├── README.md ├── main │   ├── config.py │   ├── res │   │   ├── check_points │   │   ├── data │   │   │   ├── <DATASET>scan</DATASET> │   │   │   ├── <DATASET>geography</DATASET> │   │   │   ├── <DATASET>advising</DATASET> │   │   │   ├── geo_vars.txt │   │   │   ├── adv_vars.txt │   │   │   ├── <DATASET>iwslt14</DATASET> │   │   │   ├── <DATASET>iwslt15</DATASET> │   │   │   ├── prepare-iwslt14.sh │   │   │   └── prepare-iwslt15.sh │   │   ├── log │   │   └── result │   ├── src │   │   ├── models │   │   └── utils │   └── train.py └── requirements.txt ```  ## Dependencies + <PROGLANG>python</PROGLANG> >= 3.10.6 + <SOFTWARE>tqdm</SOFTWARE> >= 4.64.1 + <SOFTWARE>numpy</SOFTWARE> >= 1.23.4 + <SOFTWARE>torch</SOFTWARE> >= 1.13.0  ## Data All <DATASET>datasets</DATASET> can be downloaded [here](https://drive.google.com/drive/folders/19vFBn5C-nTdjxMeuMgw-BvsPNsLF6DpV?",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\c3f02636.txt,1.0
5,usp=sharing) and should be placed under **main/res/data** according to specific tasks.,usp=sharing) and should be placed under **main/res/data** according to specific tasks.,usp=sharing) and should be placed under **main/res/data** according to specific tasks.,usp=sharing) and should be placed under **main/res/data** according to specific tasks.,../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\57bf05c8.txt,1.0
6,Please refer to [text2sql-data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data) for details. * main/res/data/scan * main/res/data/geography * main/res/data/advising  ### Notes + main/res/data/iwslt14 - both vocabulary augmentation set and the entire dataset for IWSLT14 + main/res/data/iwslt15 - both vocabulary augmentation set and the entire dataset for IWSLT15 + main/res/data/prepare-iwslt14.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT14 + main/res/data/prepare-iwslt15.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT15 + main/res/data/geo_vars.txt - the entity augmentation set for Grography + main/res/data/adv_vars.txt - the entity augmentation set for Advising  ## Setup Please ensure required packages are already installed.,Please refer to [text2sql-data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data) for details. * main/res/data/scan * main/res/data/geography * main/res/data/advising  ### Notes + main/res/data/iwslt14 - both vocabulary augmentation set and the entire dataset for <WORKSHOP>IWSLT14</WORKSHOP> + main/res/data/iwslt15 - both vocabulary augmentation set and the entire dataset for <WORKSHOP>IWSLT15</WORKSHOP> + main/res/data/prepare-iwslt14.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for <WORKSHOP>IWSLT14</WORKSHOP> + main/res/data/prepare-iwslt15.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for <WORKSHOP>IWSLT15</WORKSHOP> + main/res/data/geo_vars.txt - the entity augmentation set for Grography + main/res/data/adv_vars.txt - the entity augmentation set for Advising  ## Setup Please ensure required packages are already installed.,Please refer to [text2sql-data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data) for details. * main/res/data/scan * main/res/data/geography * main/res/data/advising  ### Notes + main/res/data/iwslt14 - both vocabulary augmentation set and the entire dataset for IWSLT14 + main/res/data/iwslt15 - both vocabulary augmentation set and the entire dataset for IWSLT15 + main/res/data/prepare-iwslt14.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT14 + main/res/data/prepare-iwslt15.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT15 + main/res/data/geo_vars.txt - the entity augmentation set for Grography + main/res/data/adv_vars.txt - the entity augmentation set for Advising  ## Setup Please ensure required packages are already installed.,Please refer to [<DATASET>text2sql-data</DATASET>](https://github.com/jkkummerfeld/<DATASET>text2sql-data</DATASET>/tree/master/data) for details. * main/res/data/scan * main/res/data/geography * main/res/data/advising  ### Notes + main/res/data/iwslt14 - both vocabulary augmentation set and the entire dataset for <DATASET>IWSLT14</DATASET> + main/res/data/iwslt15 - both vocabulary augmentation set and the entire dataset for <DATASET>IWSLT15</DATASET> + main/res/data/prepare-iwslt14.sh - [<SOFTWARE>fairseq</SOFTWARE>](https://github.com/facebookresearch/<SOFTWARE>fairseq</SOFTWARE>) preprocess script for <DATASET>IWSLT14</DATASET> + main/res/data/prepare-iwslt15.sh - [<SOFTWARE>fairseq</SOFTWARE>](https://github.com/facebookresearch/<SOFTWARE>fairseq</SOFTWARE>) preprocess script for <DATASET>IWSLT15</DATASET> + main/res/data/geo_vars.txt - the entity augmentation set for Grography + main/res/data/adv_vars.txt - the entity augmentation set for Advising  ## Setup Please ensure required packages are already installed.,../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\b8034475.txt,1.0
7,"A virtual environment is recommended. ``` $ cd Systematic-Generalization-via-Meaningful-Learning $ cd main $ pip install pip --upgrade $ pip install -r requirements.txt ```  ## Run Before training, please double check **config.py** to ensure training configurations. ``` $ vim config.py $ python train.py ```  ## Outputs If everything goes well, there should be a similar progressing shown as below. ``` Initialize...","A virtual environment is recommended. ``` $ cd Systematic-Generalization-via-Meaningful-Learning $ cd main $ <SOFTWARE>pip</SOFTWARE> install <SOFTWARE>pip</SOFTWARE> --upgrade $ <SOFTWARE>pip</SOFTWARE> install -r requirements.txt ```  ## Run Before training, please double check **config.py** to ensure training configurations. ``` $ <SOFTWARE>vim</SOFTWARE> config.py $ <SOFTWARE>python</SOFTWARE> train.py ```  ## Outputs If everything goes well, there should be a similar progressing shown as below. ``` Initialize...","A virtual environment is recommended. ``` $ cd Systematic-Generalization-via-Meaningful-Learning $ cd main $ pip install pip --upgrade $ pip install -r requirements.txt ```  ## Run Before training, please double check **config.py** to ensure training configurations. ``` $ vim config.py $ python train.py ```  ## Outputs If everything goes well, there should be a similar progressing shown as below. ``` Initialize...","A virtual environment is recommended. ``` $ cd <PROJECT>Systematic-Generalization-via-Meaningful-Learning</PROJECT> $ cd main $ pip install pip --upgrade $ pip install -r requirements.txt ```  ## Run Before training, please double check **<SOFTWARE>config.py</SOFTWARE>** to ensure training configurations. ``` $ vim <SOFTWARE>config.py</SOFTWARE> $ python train.py ```  ## Outputs If everything goes well, there should be a similar progressing shown as below. ``` Initialize...",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\2368c921.txt,1.0
8,"*Configuration* model name: bi_lstm_rnn_att trainable parameters:5,027,337 ...","*Configuration* model name: bi_lstm_rnn_att trainable parameters:5,027,337 ...","*Configuration* model name: bi_lstm_rnn_att trainable parameters:5,027,337 ...","*Configuration* model name: bi_lstm_rnn_att trainable parameters:5,027,337 ...",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\f99cff3a.txt,1.0
9,Training...,Training...,Training...,Training...,../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\56a592a9.txt,1.0
10,"Loss:1.7061: 100%|██████████| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use [fairseq](https://github.com/facebookresearch/fairseq) for NMT tasks in Section 4.1.","Loss:1.7061: 100%|██████████| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use [<SOFTWARE>fairseq</SOFTWARE>](https://github.com/facebookresearch/<SOFTWARE>fairseq</SOFTWARE>) for NMT tasks in Section 4.1.","Loss:1.7061: 100%|██████████| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use fairseq(https://github.com/facebookresearch/fairseq) for NMT tasks in Section 4.1.","Loss:1.7061: 100%|██████████| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use <SOFTWARE>fairseq</SOFTWARE>(https://github.com/facebookresearch/<SOFTWARE>fairseq</SOFTWARE>) for NMT tasks in Section 4.1.",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\4ce4b52c.txt,0.9952380952380953
11,"Please find the example pipeline shown below.  ### Models + LSTM - lstm_luong_wmt_en_de + Transformer - transformer_iwslt_de_en + Dynamic Conv. - lightconv_iwslt_de_en  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch transformer_iwslt_de_en --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lightconv_iwslt_de_en \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""Revisit Systematic Generalization via Meaningful Learning"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.","Please find the example pipeline shown below.  ### Models + LSTM - lstm_luong_wmt_en_de + Transformer - transformer_iwslt_de_en + Dynamic Conv. - lightconv_iwslt_de_en  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch transformer_iwslt_de_en --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lightconv_iwslt_de_en \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation <EVALMETRIC>BLEU</EVALMETRIC> ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` <EVALMETRIC>ScareBLEU</EVALMETRIC> ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""<PUBLICATION>Revisit Systematic Generalization via Meaningful Learning</PUBLICATION>"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""<PUBLICATION>Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</PUBLICATION><WORKSHOP>Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</WORKSHOP>"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.","Please find the example pipeline shown below.  ### Models + LSTM - `lstm_luong_wmt_en_de` + Transformer - `transformer_iwslt_de_en` + Dynamic Conv. - `lightconv_iwslt_de_en`  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch transformer_iwslt_de_en --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lightconv_iwslt_de_en \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""Revisit Systematic Generalization via Meaningful Learning"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.","Please find the example pipeline shown below.  ### Models + LSTM - `<SOFTWARE>lstm_luong_wmt_en_de</SOFTWARE>` + Transformer - `<SOFTWARE>transformer_iwslt_de_en</SOFTWARE>` + Dynamic Conv. - `<SOFTWARE>lightconv_iwslt_de_en</SOFTWARE>`  ### BPE ``` <SOFTWARE>examples/translation/subword-nmt/apply_bpe.py</SOFTWARE> -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=<DATASET>examples/translation/iwslt14.tokenized.de-en</DATASET> <SOFTWARE>fairseq-preprocess</SOFTWARE> --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` <SOFTWARE>fairseq-train</SOFTWARE> \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch <SOFTWARE>lstm_luong_wmt_en_de</SOFTWARE> --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` <SOFTWARE>fairseq-train</SOFTWARE> \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch <SOFTWARE>transformer_iwslt_de_en</SOFTWARE> --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` <SOFTWARE>fairseq-train</SOFTWARE> \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch <SOFTWARE>lightconv_iwslt_de_en</SOFTWARE> \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` <SOFTWARE>fairseq-generate</SOFTWARE> data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring <EVALMETRIC>bleu</EVALMETRIC> --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` <SOFTWARE>fairseq-generate</SOFTWARE> data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring <EVALMETRIC>sacrebleu</EVALMETRIC> --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""Revisit Systematic Generalization via Meaningful Learning"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""<PUBLICATION>Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</PUBLICATION>"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\5f6d92ad.txt,0.9991251093613298
12,"Recent studies argue that neural networks appear inherently ineffective in such cognitive capacity, leading to a pessimistic view and a lack of attention to optimistic results.","Recent studies argue that neural networks appear inherently ineffective in such cognitive capacity, leading to a pessimistic view and a lack of attention to optimistic results.","Recent studies argue that neural networks appear inherently ineffective in such cognitive capacity, leading to a pessimistic view and a lack of attention to optimistic results.","Recent studies argue that neural networks appear inherently ineffective in such cognitive capacity, leading to a pessimistic view and a lack of attention to optimistic results.",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\cd4c024d.txt,1.0
13,"We revisit this controversial topic from the perspective of meaningful learning, an exceptional capability of humans to learn novel concepts by connecting them with known ones.","We revisit this controversial topic from the perspective of meaningful learning, an exceptional capability of humans to learn novel concepts by connecting them with known ones.","We revisit this controversial topic from the perspective of meaningful learning, an exceptional capability of humans to learn novel concepts by connecting them with known ones.","We revisit this controversial topic from the perspective of meaningful learning, an exceptional capability of humans to learn novel concepts by connecting them with known ones.",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\8e052d93.txt,1.0
14,We reassess the compositional skills of sequence-to-sequence models conditioned on the semantic links between new and old concepts.,We reassess the compositional skills of sequence-to-sequence models conditioned on the semantic links between new and old concepts.,We reassess the compositional skills of sequence-to-sequence models conditioned on the semantic links between new and old concepts.,We reassess the compositional skills of sequence-to-sequence models conditioned on the semantic links between new and old concepts.,../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\988fe8f6.txt,1.0
15,"Our observations suggest that models can successfully one-shot generalize to novel concepts and compositions through semantic linking, either inductively or deductively.","Our observations suggest that models can successfully one-shot generalize to novel concepts and compositions through semantic linking, either inductively or deductively.","Our observations suggest that models can successfully one-shot generalize to novel concepts and compositions through semantic linking, either inductively or deductively.","Our observations suggest that models can successfully one-shot generalize to novel concepts and compositions through semantic linking, either inductively or deductively.",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\5b326865.txt,1.0
16,We demonstrate that prior knowledge plays a key role as well.,We demonstrate that prior knowledge plays a key role as well.,We demonstrate that prior knowledge plays a key role as well.,We demonstrate that prior knowledge plays a key role as well.,../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\1d1ae034.txt,1.0
17,"In addition to synthetic tests, we further conduct proof-of-concept experiments in machine translation and semantic parsing, showing the benefits of meaningful learning in applications.","In addition to synthetic tests, we further conduct proof-of-concept experiments in machine translation and semantic parsing, showing the benefits of meaningful learning in applications.","In addition to synthetic tests, we further conduct proof-of-concept experiments in machine translation and semantic parsing, showing the benefits of meaningful learning in applications.","In addition to synthetic tests, we further conduct proof-of-concept experiments in machine translation and semantic parsing, showing the benefits of meaningful learning in applications.",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\29bfeabb.txt,1.0
18,"We hope our positive findings will encourage excavating modern neural networks{'} potential in systematic generalization through more advanced learning schemes."", } ```","We hope our positive findings will encourage excavating modern neural networks{'} potential in systematic generalization through more advanced learning schemes."", } ```","We hope our positive findings will encourage excavating modern neural networks{'} potential in systematic generalization through more advanced learning schemes."", } ```","We hope our positive findings will encourage excavating modern neural networks{'} potential in systematic generalization through more advanced learning schemes."", } ```",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\2c21034d.txt,1.0
