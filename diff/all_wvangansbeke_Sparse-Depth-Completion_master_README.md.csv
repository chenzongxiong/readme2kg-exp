sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,"# Sparse-Depth-Completion  This repo contains the implementation of our paper [Sparse and Noisy LiDAR Completion with RGB Guidance and Uncertainty](https://arxiv.org/abs/1902.05356) by [Wouter Van Gansbeke](https://github.com/wvangansbeke), Davy Neven, Bert De Brabandere and Luc Van Gool.","# Sparse-Depth-Completion  This repo contains the implementation of our paper [<PUBLICATION>Sparse and Noisy LiDAR Completion with RGB Guidance and Uncertainty</PUBLICATION>](https://arxiv.org/abs/1902.05356) by [Wouter Van Gansbeke](https://github.com/wvangansbeke), Davy Neven, Bert De Brabandere and Luc Van Gool.","# Sparse-Depth-Completion  This repo contains the implementation of our paper [Sparse and Noisy LiDAR Completion with RGB Guidance and Uncertainty](https://arxiv.org/abs/1902.05356) by [Wouter Van Gansbeke](https://github.com/wvangansbeke), Davy Neven, Bert De Brabandere and Luc Van Gool.","# <PROJECT>Sparse-Depth-Completion</PROJECT>  This repo contains the implementation of our <PUBLICATION>paper [Sparse and Noisy LiDAR Completion with RGB Guidance and Uncertainty](https://arxiv.org/abs/1902.05356)</PUBLICATION> by [<SOFTWARE>Wouter Van Gansbeke</SOFTWARE>](https://github.com/wvangansbeke), Davy Neven, Bert De Brabandere and Luc Van Gool.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\c78160ad.txt,1.0
2,"If you find this interesting or relevant to your work, consider citing:  ``` @inproceedings{wvangansbeke_depth_2019,     author={Van Gansbeke, Wouter and Neven, Davy and De Brabandere, Bert and Van Gool, Luc},     booktitle={2019 16th International Conference on Machine Vision Applications (MVA)},     title={Sparse and Noisy LiDAR Completion with RGB Guidance and Uncertainty},     year={2019},     pages={1-6},     organization={IEEE} } ```  ## License  This software is released under a creative commons license which allows for personal and research use only.","If you find this interesting or relevant to your work, consider citing:  ``` @inproceedings{wvangansbeke_depth_2019,     author={Van Gansbeke, Wouter and Neven, Davy and De Brabandere, Bert and Van Gool, Luc},     booktitle={<CONFERENCE>2019 16th International Conference on Machine Vision Applications</CONFERENCE> (<CONFERENCE>MVA</CONFERENCE>)},     title={<PUBLICATION>Sparse and Noisy LiDAR Completion with RGB Guidance and Uncertainty</PUBLICATION>},     year={2019},     pages={1-6},     organization={<CONFERENCE>IEEE</CONFERENCE>} } ```  ## License  This software is released under a creative commons license which allows for personal and research use only.","If you find this interesting or relevant to your work, consider citing:  ``` @inproceedings{wvangansbeke_depth_2019,     author={Van Gansbeke, Wouter and Neven, Davy and De Brabandere, Bert and Van Gool, Luc},     booktitle={2019 16th International Conference on Machine Vision Applications (MVA)},     title={Sparse and Noisy LiDAR Completion with RGB Guidance and Uncertainty},     year={2019},     pages={1-6},     organization={IEEE} } ```  ## License  This software is released under a creative commons license which allows for personal and research use only.","If you find this interesting or relevant to your work, consider citing:  ``` @inproceedings{wvangansbeke_depth_2019,     author={Van Gansbeke, Wouter and Neven, Davy and De Brabandere, Bert and Van Gool, Luc},     booktitle={<CONFERENCE>2019 16th International Conference on Machine Vision Applications (MVA)</CONFERENCE>},     title={<PUBLICATION>Sparse and Noisy LiDAR Completion with RGB Guidance and Uncertainty</PUBLICATION>},     year={2019},     pages={1-6},     organization={<PROJECT>IEEE</PROJECT>} } ```  ## License  This software is released under a <LICENSE>creative commons license</LICENSE> which allows for personal and research use only.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\dfaa66fc.txt,1.0
3,For a commercial license please contact the authors.,For a commercial license please contact the authors.,For a commercial license please contact the authors.,For a <LICENSE>commercial license</LICENSE> please contact the authors.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\dcdbd976.txt,1.0
4,You can view a license summary [here](http://creativecommons.org/licenses/by-nc/4.0/)  ## Introduction Monocular depth prediction methods fail to generate absolute and precise depth maps and stereoscopic approaches are still significantly outperformed by LiDAR based approaches.,You can view a license summary [here](http://creativecommons.org/licenses/by-nc/4.0/)  ## Introduction Monocular depth prediction methods fail to generate absolute and precise depth maps and stereoscopic approaches are still significantly outperformed by LiDAR based approaches.,You can view a license summary [here](http://creativecommons.org/licenses/by-nc/4.0/)  ## Introduction Monocular depth prediction methods fail to generate absolute and precise depth maps and stereoscopic approaches are still significantly outperformed by LiDAR based approaches.,You can view a license summary [here](http://creativecommons.org/licenses/by-nc/4.0/)  ## Introduction Monocular depth prediction methods fail to generate absolute and precise depth maps and stereoscopic approaches are still significantly outperformed by LiDAR based approaches.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\cdc8a418.txt,1.0
5,The goal of the depth completion task is to generate dense depth predictions from sparse and irregular point clouds.,The goal of the depth completion task is to generate dense depth predictions from sparse and irregular point clouds.,The goal of the depth completion task is to generate dense depth predictions from sparse and irregular point clouds.,The goal of the depth completion task is to generate dense depth predictions from sparse and irregular point clouds.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\0b740858.txt,1.0
6,This project makes use of uncertainty to combine multiple sensor data in order to generate accurate depth predictions.,This project makes use of uncertainty to combine multiple sensor data in order to generate accurate depth predictions.,`This project makes use of uncertainty to combine multiple sensor data in order to generate accurate depth predictions.`,`This <PROJECT>project</PROJECT> makes use of uncertainty to combine multiple sensor data in order to generate accurate depth predictions.`,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\c5b892af.txt,0.9915966386554622
7,Mapped lidar points together with RGB images (monocular) are used in this framework.,Mapped lidar points together with RGB images (monocular) are used in this framework.,Mapped lidar points together with RGB images (monocular) are used in this framework.,Mapped lidar points together with RGB images (monocular) are used in this framework.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\f93e77bd.txt,1.0
8,This method holds the **1st place** entry on the [KITTI depth completion benchmark](http://www.cvlibs.net/datasets/kitti/eval_depth.php?,This method holds the **1st place** entry on the [<DATASET>KITTI</DATASET> depth completion benchmark](http://www.cvlibs.net/datasets/<DATASET>kitti</DATASET>/eval_depth.php?,This method holds the **1st place** entry on the [KITTI depth completion benchmark](http://www.cvlibs.net/datasets/kitti/eval_depth.php?,This method holds the **1st place** entry on the [<DATASET>KITTI depth completion benchmark</DATASET>](http://www.cvlibs.net/datasets/kitti/eval_depth.php?,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\e53d5038.txt,1.0
9,benchmark=depth_completion) at the time of submission of the paper.,benchmark=depth_completion) at the time of submission of the paper.,benchmark=depth_completion) at the time of submission of the paper.,benchmark=<DATASET>depth_completion</DATASET>) at the time of submission of the paper.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\238d3edc.txt,1.0
10,The contribution of this paper is threefold: * Global and local information are combined in order to accurately complete and correct the sparse and noisy LiDAR input.,The contribution of this paper is threefold: * Global and local information are combined in order to accurately complete and correct the sparse and noisy LiDAR input.,The contribution of this paper is threefold: * Global and local information are combined in order to accurately complete and correct the sparse and noisy LiDAR input.,The contribution of this paper is threefold: * Global and local information are combined in order to accurately complete and correct the sparse and noisy LiDAR input.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\381e57e1.txt,1.0
11,Monocular RGB images are used for the guidance of this depth completion task. * Confidence maps are learned for the global branch and the local branch in an unsupervised manner.,Monocular RGB images are used for the guidance of this depth completion task. * Confidence maps are learned for the global branch and the local branch in an unsupervised manner.,Monocular RGB images are used for the guidance of this depth completion task. * Confidence maps are learned for the global branch and the local branch in an unsupervised manner.,Monocular RGB images are used for the guidance of this depth completion task. * Confidence maps are learned for the global branch and the local branch in an unsupervised manner.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\c69e5d42.txt,1.0
12,The predicted depth maps are weighted by their respective confidence map.,The predicted depth maps are weighted by their respective confidence map.,The predicted depth maps are weighted by their respective confidence map.,The predicted depth maps are weighted by their respective confidence map.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\c0dce7e3.txt,1.0
13,This is the late fusion technique used in our framework. * This method ranks first on the KITTI depth completion benchmark without using additional data or postprocessing.,This is the late fusion technique used in our framework. * This method ranks first on the <DATASET>KITTI</DATASET> depth completion benchmark without using additional data or postprocessing.,This is the late fusion technique used in our framework. * This method ranks first on the KITTI depth completion benchmark without using additional data or postprocessing.,This is the late fusion technique used in our framework. * This method ranks first on the <DATASET>KITTI</DATASET> depth completion benchmark without using additional data or postprocessing.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\0a31f574.txt,1.0
14,See full demo on [YouTube](https://www.youtube.com/watch?,See full demo on [<SOFTWARE>YouTube</SOFTWARE>](https://www.youtube.com/watch?,See full demo on [YouTube](https://www.youtube.com/watch?,See full demo on [YouTube](https://www.youtube.com/watch?,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\916be034.txt,1.0
15,v=Kr0W7io5rHw&feature=youtu.be).,v=Kr0W7io5rHw&feature=youtu.be).,v=Kr0W7io5rHw&feature=youtu.be).,v=Kr0W7io5rHw&feature=youtu.be).,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\7395bcf7.txt,1.0
16,The predictions of our model for the KITTI test set can be downloaded [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x).  !,The predictions of our model for the <DATASET>KITTI</DATASET> test set can be downloaded [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x).  !,The predictions of our model for the `KITTI` test set can be downloaded [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x).  !,The predictions of our model for the `<DATASET>KITTI</DATASET>` test set can be downloaded [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x).  !,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\cb6f5062.txt,0.9935483870967742
17,"[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements Python 3.7 The most important packages are pytorch, torchvision, numpy, pillow and matplotlib.","[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements <SOFTWARE>Python 3.7</SOFTWARE> The most important packages are <SOFTWARE>pytorch</SOFTWARE>, <SOFTWARE>torchvision</SOFTWARE>, <SOFTWARE>numpy</SOFTWARE>, <SOFTWARE>pillow</SOFTWARE> and <SOFTWARE>matplotlib</SOFTWARE>.","[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements Python 3.7 The most important packages are pytorch, torchvision, numpy, pillow and matplotlib.","[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements <PROGLANG>Python 3.7</PROGLANG> The most important packages are <SOFTWARE>pytorch</SOFTWARE>, <SOFTWARE>torchvision</SOFTWARE>, <SOFTWARE>numpy</SOFTWARE>, <SOFTWARE>pillow</SOFTWARE> and <SOFTWARE>matplotlib</SOFTWARE>.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\ada3d2fa.txt,1.0
18,(Works with Pytorch 1.1)   ## Dataset The [Kitti dataset](www.cvlibs.net/datasets/kitti/) has been used.,(Works with <SOFTWARE>Pytorch 1.1</SOFTWARE>)   ## Dataset The [<DATASET>Kitti</DATASET> dataset](www.cvlibs.net/datasets/<DATASET>kitti</DATASET>/) has been used.,(Works with Pytorch 1.1)   ## Dataset The [Kitti dataset](www.cvlibs.net/datasets/kitti/) has been used.,(Works with <PROGLANG>Pytorch</PROGLANG> 1.1)   ## <DATASET>Dataset</DATASET> The [<DATASET>Kitti dataset</DATASET>](www.cvlibs.net/datasets/<DATASET>kitti</DATASET>/) has been used.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\4beebee9.txt,1.0
19,First download the dataset of the depth completion.,First download the dataset of the depth completion.,First download the `dataset of the depth completion`.,First download the `<DATASET>dataset of the depth completion</DATASET>`.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\14cfd00a.txt,0.9807692307692307
20,"Secondly, you'll need to unzip and download the camera images from kitti.","Secondly, you'll need to unzip and download the camera images from <DATASET>kitti</DATASET>.","Secondly, you'll need to unzip and download the camera images from kitti.","Secondly, you'll need to unzip and download the camera images from <DATASET>kitti</DATASET>.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\0a4a054f.txt,1.0
21,"I used the file `download_raw_files.sh`, but this is at your own risk.","I used the file `download_raw_files.sh`, but this is at your own risk.","I used the file `download_raw_files.sh`, but this is at your own risk.","I used the file `<SOFTWARE>download_raw_files.sh</SOFTWARE>`, but this is at your own risk.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\a8bffb4d.txt,1.0
22,"Make sure you understand it, otherwise don't use it.","Make sure you understand it, otherwise don't use it.","Make sure you understand it, otherwise don't use it.","Make sure you understand it, otherwise don't use it.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\f8e43400.txt,1.0
23,"If you want to keep it safe, go to kitti's website.","If you want to keep it safe, go to kitti's website.","If you want to keep it safe, go to `kitti`'s website.","If you want to keep it safe, go to `<DATASET>kitti</DATASET>`'s website.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\26fe576c.txt,0.9807692307692307
24,"The complete dataset consists of 85898 training samples, 6852 validation samples, 1000 selected validation samples and 1000 test samples.  ## Preprocessing This step is optional, but allows you to transform the images to jpgs and to downsample the original lidar frames.","The complete dataset consists of 85898 training samples, 6852 validation samples, 1000 selected validation samples and 1000 test samples.  ## Preprocessing This step is optional, but allows you to transform the images to jpgs and to downsample the original lidar frames.","The complete `dataset` consists of 85898 training samples, 6852 validation samples, 1000 selected validation samples and 1000 test samples.  ## Preprocessing This step is optional, but allows you to transform the images to jpgs and to downsample the original lidar frames.","The complete `<DATASET>dataset</DATASET>` consists of 85898 training samples, 6852 validation samples, 1000 selected validation samples and 1000 test samples.  ## Preprocessing This step is optional, but allows you to transform the images to jpgs and to downsample the original lidar frames.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\02b77d32.txt,0.9704797047970479
25,This will create a new dataset in $dest.,This will create a new dataset in $dest.,This will create a new dataset in $dest.,This will create a new <DATASET>dataset</DATASET> in $dest.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\5ff91bfe.txt,1.0
26,"You can find the required preprocessing in: `Datasets/Kitti_loader.py`  Run:  `source Shell/preprocess $datapath $dest $num_samples`  (Firstly, I transformed the png's to jpg - images to save place.","You can find the required preprocessing in: `Datasets/Kitti_loader.py`  Run:  `source Shell/preprocess $datapath $dest $num_samples`  (Firstly, I transformed the png's to jpg - images to save place.","You can find the required preprocessing in: `Datasets/Kitti_loader.py`  Run:  `source Shell/preprocess $datapath $dest $num_samples`  (Firstly, I transformed the png's to jpg - images to save place.","You can find the required preprocessing in: `<DATASET>Datasets/Kitti_loader.py</DATASET>`  Run:  `source <SOFTWARE>Shell/preprocess</SOFTWARE> $datapath $dest $num_samples`  (Firstly, I transformed the png's to jpg - images to save place.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\0c4fc2be.txt,1.0
27,"Secondly, two directories are built i.e. one for training and one for validation.","Secondly, two directories are built i.e. one for training and one for validation.","Secondly, two directories are built i.e. one for training and one for validation.","Secondly, two directories are built i.e. one for training and one for validation.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\cd5de390.txt,1.0
28,See `Datasets/Kitti_loader.py`)  Dataset structure should look like this: ``` |--depth selection |-- Depth      |-- train            |--date                |--sequence1                | ...,See `Datasets/Kitti_loader.py`)  Dataset structure should look like this: ``` |--depth selection |-- Depth      |-- train            |--date                |--sequence1                | ...,See `Datasets/Kitti_loader.py`)  `Dataset` structure should look like this: ``` |--depth selection |-- Depth      |-- train            |--date                |--sequence1                | ...,See `<SOFTWARE>Datasets/Kitti_loader.py</SOFTWARE>`)  `<DATASET>Dataset</DATASET>` structure should look like this: ``` |--depth selection |-- Depth      |-- train            |--date                |--sequence1                | ...,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\fa94dd8e.txt,0.9947368421052631
29,|--validation |--RGB     |--train          |--date              |--sequence1              | ...,|--validation |--RGB     |--train          |--date              |--sequence1              | ...,See `Datasets/Kitti_loader.py`)  `Dataset` structure should look like this: ``` |--depth selection |-- Depth      |-- train            |--date                |--sequence1                | ...,See `<SOFTWARE>Datasets/Kitti_loader.py</SOFTWARE>`)  `<DATASET>Dataset</DATASET>` structure should look like this: ``` |--depth selection |-- Depth      |-- train            |--date                |--sequence1                | ...,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\fa94dd8e.txt,0.6083916083916084
30,"|--validation ```   ## Run Code To run the code:  `python main.py --data_path /path/to/data/ --lr_policy plateau`  Flags: - Set flag ""input_type"" to rgb or depth. - Set flag ""pretrained"" to true or false to use a model pretrained on Cityscapes for the global branch. - See `python main.py --help` for more information.","|--validation ```   ## Run Code To run the code:  `<SOFTWARE>python</SOFTWARE> main.py --data_path /path/to/data/ --lr_policy plateau`  Flags: - Set flag ""input_type"" to rgb or depth. - Set flag ""pretrained"" to true or false to use a model pretrained on <DATASET>Cityscapes</DATASET> for the global branch. - See `<SOFTWARE>python</SOFTWARE> main.py --help` for more information.","|--validation ```   ## Run Code To run the code:  `python main.py --data_path /path/to/data/ --lr_policy plateau`  Flags: - Set flag ""input_type"" to rgb or depth. - Set flag ""pretrained"" to true or false to use a model pretrained on Cityscapes for the global branch. - See `python main.py --help` for more information.","|--validation ```   ## Run Code To run the code:  `<PROGLANG>python</PROGLANG> main.py --data_path /path/to/data/ --lr_policy plateau`  Flags: - Set flag ""input_type"" to rgb or depth. - Set flag ""pretrained"" to true or false to use a model pretrained on Cityscapes for the global branch. - See `<PROGLANG>python</PROGLANG> main.py --help` for more information.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\13c5b836.txt,1.0
31,or   `source Shell/train.sh $datapath`  checkout more details in the bash file.  ## Trained models Our network architecture is based on [ERFNet](https://github.com/Eromera/erfnet_pytorch).,or   `source Shell/train.sh $datapath`  checkout more details in the bash file.  ## Trained models Our network architecture is based on [ERFNet](https://github.com/Eromera/erfnet_<SOFTWARE>pytorch</SOFTWARE>).,or   `source Shell/train.sh $datapath`  checkout more details in the bash file.  ## Trained models Our network architecture is based on [ERFNet](https://github.com/Eromera/erfnet_pytorch).,or   `source Shell/train.sh $datapath`  checkout more details in the bash file.  ## Trained models Our network architecture is based on [<SOFTWARE>ERFNet</SOFTWARE>](https://github.com/Eromera/erfnet_pytorch).,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\b0a0a627.txt,1.0
32,You can find the model pretrained on Cityscapes [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,You can find the model pretrained on <DATASET>Cityscapes</DATASET> [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,You can find the model pretrained on Cityscapes [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,You can find the model pretrained on <DATASET>Cityscapes</DATASET> [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\f41afde0.txt,1.0
33,usp=sharing).,usp=sharing).,usp=sharing).,usp=sharing).,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\e4c0f51a.txt,1.0
34,This model is used for the global network.,This model is used for the global network.,This model is used for the global network.,This model is used for the global network.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\d4df907a.txt,1.0
35,You can find a fully trained model and its corresponding predictions for the KITTI test set [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,You can find a fully trained model and its corresponding predictions for the <DATASET>KITTI</DATASET> test set [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,You can find a fully trained model and its corresponding predictions for the KITTI test set [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,You can find a fully trained model and its corresponding predictions for the <DATASET>KITTI</DATASET> test set [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\3e36b014.txt,1.0
36,usp=sharing).,usp=sharing).,usp=sharing).,usp=sharing).,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\e4c0f51a.txt,1.0
37,The RMSE is around 802 mm on the selected validation set for this model as reported in the paper.,The <EVALMETRIC>RMSE</EVALMETRIC> is around 802 mm on the selected validation set for this model as reported in the paper.,The `RMSE` is around 802 mm on the selected validation set for this model as reported in the paper.,The `<EVALMETRIC>RMSE</EVALMETRIC>` is around 802 mm on the selected validation set for this model as reported in the paper.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\1112162d.txt,0.9897959183673469
38,To test it:  Save the model in a folder in the `Saved` directory.,To test it:  Save the model in a folder in the `Saved` directory.,To test it:  Save the model in a folder in the `Saved` directory.,To test it:  Save the model in a folder in the `Saved` directory.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\19ac4f0e.txt,1.0
39,"and execute the following command:  `source Test/test.sh /path/to/directory_with_saved_model/ $num_samples /path/to/dataset/ /path/to/directory_with_ground_truth_for_selected_validation_files/`  (You might have to recompile the C files for testing, provided by KITTI, if your architecture is different from mine)  ## Results  Comparision with state-of-the-art:  !","and execute the following command:  `source Test/test.sh /path/to/directory_with_saved_model/ $num_samples /path/to/dataset/ /path/to/directory_with_ground_truth_for_selected_validation_files/`  (You might have to recompile the <PROGLANG>C</PROGLANG> files for testing, provided by <DATASET>KITTI</DATASET>, if your architecture is different from mine)  ## Results  Comparision with state-of-the-art:  !","and execute the following command:  `source Test/test.sh /path/to/directory_with_saved_model/ $num_samples /path/to/dataset/ /path/to/directory_with_ground_truth_for_selected_validation_files/`  (You might have to recompile the C files for testing, provided by KITTI, if your architecture is different from mine)  ## Results  Comparision with state-of-the-art:  !","and execute the following command:  `source <SOFTWARE>Test/test.sh</SOFTWARE> /path/to/directory_with_saved_model/ $num_samples /path/to/<DATASET>dataset</DATASET>/ /path/to/directory_with_ground_truth_for_selected_validation_files/`  (You might have to recompile the C files for testing, provided by <SOFTWARE>KITTI</SOFTWARE>, if your architecture is different from mine)  ## Results  Comparision with state-of-the-art:  !",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\307b7875.txt,1.0
40,[results](https://user-images.githubusercontent.com/9694230/59205060-49c32780-8ba2-11e9-8a87-34d8c3f99756.PNG)   ## Discussion  Practical discussion:  - I recently increased the stability of the training process and I also made the convergence faster by adding some skip connections between the global and local networks.,[results](https://user-images.githubusercontent.com/9694230/59205060-49c32780-8ba2-11e9-8a87-34d8c3f99756.PNG)   ## Discussion  Practical discussion:  - I recently increased the stability of the training process and I also made the convergence faster by adding some skip connections between the global and local networks.,[results](https://user-images.githubusercontent.com/9694230/59205060-49c32780-8ba2-11e9-8a87-34d8c3f99756.PNG)   ## Discussion  Practical discussion:  - I recently increased the stability of the training process and I also made the convergence faster by adding some skip connections between the global and local networks.,[results](https://user-images.githubusercontent.com/9694230/59205060-49c32780-8ba2-11e9-8a87-34d8c3f99756.PNG)   ## Discussion  Practical discussion:  - I recently increased the stability of the training process and I also made the convergence faster by adding some skip connections between the global and local networks.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\5514c4ab.txt,1.0
41,"Initially I only used guidance by multiplication with an attention map (=probability), but found out that it is less robust and that differences between a focal MSE and vanilla MSE loss function were now negligible.","Initially I only used guidance by multiplication with an attention map (=probability), but found out that it is less robust and that differences between a focal MSE and vanilla MSE loss function were now negligible.","Initially I only used guidance by multiplication with an attention map (=probability), but found out that it is less robust and that differences between a focal `MSE` and vanilla `MSE` loss function were now negligible.","Initially I only used guidance by multiplication with an attention map (=probability), but found out that it is less robust and that differences between a focal `<EVALMETRIC>MSE</EVALMETRIC>` and vanilla `<EVALMETRIC>MSE</EVALMETRIC>` loss function were now negligible.",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\5ddee327.txt,0.9907834101382489
42,Be aware that this change will alter the appearance of the confidence maps since fusion happens at mutliple stages now,Be aware that this change will alter the appearance of the confidence maps since fusion happens at mutliple stages now,Be aware that this change will alter the appearance of the confidence maps since fusion happens at mutliple stages now,Be aware that this change will alter the appearance of the confidence maps since fusion happens at mutliple stages now,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\04be4550.txt,1.0
43,.,.,.,.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\cdb4ee2a.txt,1.0
44,- Feel free to experiment with different architectures for the global or local network.,- Feel free to experiment with different architectures for the global or local network.,- Feel free to experiment with different architectures for the global or local network.,- Feel free to experiment with different architectures for the global or local network.,../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\acf3a3b9.txt,1.0
45,"It is easy to add new architectures to `Models/__init__.py`  - I used a Tesla V100 GPU for evaluation.  ## Acknowledgement This work was supported by Toyota, and was carried out at the TRACE Lab at KU Leuven (Toyota Research on Automated Cars in Europe - Leuven)","It is easy to add new architectures to `Models/__init__.py`  - I used a Tesla V100 GPU for evaluation.  ## Acknowledgement This work was supported by Toyota, and was carried out at the TRACE Lab at KU Leuven (Toyota Research on Automated Cars in Europe - Leuven)","It is easy to add new architectures to `Models/__init__.py`  - I used a Tesla V100 GPU for evaluation.  ## Acknowledgement This work was supported by Toyota, and was carried out at the `TRACE Lab` at `KU Leuven` (Toyota Research on Automated Cars in Europe - Leuven)","It is easy to add new architectures to `<SOFTWARE>Models/__init__.py</SOFTWARE>`  - I used a Tesla V100 GPU for evaluation.  ## Acknowledgement This work was supported by Toyota, and was carried out at the `<PROJECT>TRACE Lab</PROJECT>` at `<PROJECT>KU Leuven</PROJECT>` (Toyota Research on Automated Cars in Europe - Leuven)",../results/deepseek-chat/prompt-0/zzz_wvangansbeke_Sparse-Depth-Completion_master_README.md.tsv\f0009fe0.txt,0.9772727272727273
