sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
3,Association for Computational Linguistics.,Association for Computational Linguistics.,`Association for Computational Linguistics.`,`<PUBLICATION>Association for Computational Linguistics</PUBLICATION>.`,../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\c224ebd2.txt,0.9767441860465116
10,"Loss:1.7061: 100%|██████████| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use [fairseq](https://github.com/facebookresearch/fairseq) for NMT tasks in Section 4.1.","Loss:1.7061: 100%|██████████| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use [<SOFTWARE>fairseq</SOFTWARE>](https://github.com/facebookresearch/<SOFTWARE>fairseq</SOFTWARE>) for NMT tasks in Section 4.1.","Loss:1.7061: 100%|██████████| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use fairseq(https://github.com/facebookresearch/fairseq) for NMT tasks in Section 4.1.","Loss:1.7061: 100%|██████████| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use <SOFTWARE>fairseq</SOFTWARE>(https://github.com/facebookresearch/<SOFTWARE>fairseq</SOFTWARE>) for NMT tasks in Section 4.1.",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\4ce4b52c.txt,0.9952380952380953
11,"Please find the example pipeline shown below.  ### Models + LSTM - lstm_luong_wmt_en_de + Transformer - transformer_iwslt_de_en + Dynamic Conv. - lightconv_iwslt_de_en  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch transformer_iwslt_de_en --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lightconv_iwslt_de_en \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""Revisit Systematic Generalization via Meaningful Learning"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.","Please find the example pipeline shown below.  ### Models + LSTM - lstm_luong_wmt_en_de + Transformer - transformer_iwslt_de_en + Dynamic Conv. - lightconv_iwslt_de_en  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch transformer_iwslt_de_en --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lightconv_iwslt_de_en \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation <EVALMETRIC>BLEU</EVALMETRIC> ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` <EVALMETRIC>ScareBLEU</EVALMETRIC> ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""<PUBLICATION>Revisit Systematic Generalization via Meaningful Learning</PUBLICATION>"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""<PUBLICATION>Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</PUBLICATION><WORKSHOP>Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</WORKSHOP>"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.","Please find the example pipeline shown below.  ### Models + LSTM - `lstm_luong_wmt_en_de` + Transformer - `transformer_iwslt_de_en` + Dynamic Conv. - `lightconv_iwslt_de_en`  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch transformer_iwslt_de_en --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lightconv_iwslt_de_en \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""Revisit Systematic Generalization via Meaningful Learning"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.","Please find the example pipeline shown below.  ### Models + LSTM - `<SOFTWARE>lstm_luong_wmt_en_de</SOFTWARE>` + Transformer - `<SOFTWARE>transformer_iwslt_de_en</SOFTWARE>` + Dynamic Conv. - `<SOFTWARE>lightconv_iwslt_de_en</SOFTWARE>`  ### BPE ``` <SOFTWARE>examples/translation/subword-nmt/apply_bpe.py</SOFTWARE> -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=<DATASET>examples/translation/iwslt14.tokenized.de-en</DATASET> <SOFTWARE>fairseq-preprocess</SOFTWARE> --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` <SOFTWARE>fairseq-train</SOFTWARE> \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch <SOFTWARE>lstm_luong_wmt_en_de</SOFTWARE> --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` <SOFTWARE>fairseq-train</SOFTWARE> \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch <SOFTWARE>transformer_iwslt_de_en</SOFTWARE> --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` <SOFTWARE>fairseq-train</SOFTWARE> \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch <SOFTWARE>lightconv_iwslt_de_en</SOFTWARE> \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` <SOFTWARE>fairseq-generate</SOFTWARE> data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring <EVALMETRIC>bleu</EVALMETRIC> --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` <SOFTWARE>fairseq-generate</SOFTWARE> data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring <EVALMETRIC>sacrebleu</EVALMETRIC> --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""Revisit Systematic Generalization via Meaningful Learning"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""<PUBLICATION>Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</PUBLICATION>"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.",../results/deepseek-chat/prompt-0/zzz_shininglab_systematic-generalization-via-meaningful-learning_master_README.md.tsv\5f6d92ad.txt,0.9991251093613298
