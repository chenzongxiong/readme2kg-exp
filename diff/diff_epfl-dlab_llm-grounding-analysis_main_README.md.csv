sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
3,"Locating and Detecting Language Model Grounding with Fakepedia},        author={Giovanni Monea and Maxime Peyrard and Martin Josifoski and Vishrav Chaudhary and Jason Eisner and Emre Kıcıman and Hamid Palangi and Barun Patra and Robert West},       year={2023},       eprint={2312.02073},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  > **Abstract:** Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context.","<PUBLICATION></PUBLICATION><DATASET>Fakepedia</DATASET>},        author={Giovanni Monea and Maxime Peyrard and Martin Josifoski and Vishrav Chaudhary and Jason Eisner and Emre Kıcıman and Hamid Palangi and Barun Patra and Robert West},       year={2023},       eprint={2312.02073},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  > **Abstract:** Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context.","Locating and Detecting Language Model Grounding with Fakepedia,        author={Giovanni Monea and Maxime Peyrard and Martin Josifoski and Vishrav Chaudhary and Jason Eisner and Emre Kıcıman and Hamid Palangi and Barun Patra and Robert West},       year={2023},       eprint={2312.02073},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  > **Abstract:** Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context.","<PUBLICATION>Locating and Detecting Language Model Grounding with Fakepedia</PUBLICATION>,        author={Giovanni Monea and Maxime Peyrard and Martin Josifoski and Vishrav Chaudhary and Jason Eisner and Emre Kıcıman and Hamid Palangi and Barun Patra and Robert West},       year={2023},       eprint={2312.02073},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  > **Abstract:** Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context.",../results/deepseek-chat/prompt-0/zzz_epfl-dlab_llm-grounding-analysis_main_README.md.tsv\d3dd61c7.txt,0.9989550679205852
6,"We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge.","We present a novel method to study grounding abilities using <DATASET>Fakepedia</DATASET>, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge.","We present a novel method to study grounding abilities using `Fakepedia`, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge.","We present a novel method to study grounding abilities using `<DATASET>Fakepedia</DATASET>`, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge.",../results/deepseek-chat/prompt-0/zzz_epfl-dlab_llm-grounding-analysis_main_README.md.tsv\b4e7667a.txt,0.9942196531791907
7,"We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries.","We benchmark various LLMs with <DATASET>Fakepedia</DATASET> and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering <DATASET>Fakepedia</DATASET> queries.","We benchmark various LLMs with `Fakepedia` and then we conduct a causal mediation analysis, based on our `Masked Grouped Causal Tracing (MGCT)`, on LLM components when answering `Fakepedia` queries.","We benchmark various LLMs with `<DATASET>Fakepedia</DATASET>` and then we conduct a causal mediation analysis, based on our `<SOFTWARE>Masked Grouped Causal Tracing (MGCT)</SOFTWARE>`, on LLM components when answering `<DATASET>Fakepedia</DATASET>` queries.",../results/deepseek-chat/prompt-0/zzz_epfl-dlab_llm-grounding-analysis_main_README.md.tsv\146d9056.txt,0.9846153846153847
