sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,# EntQA  This repo provides the code for our ICLR 2022 paper [EntQA: Entitly Linking as Question Answering](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup  ``` conda create --name entqa python=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch faiss-gpu cudatoolkit=11.0  ```  ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?,# EntQA  This repo provides the code for our <CONFERENCE>ICLR 2022</CONFERENCE> paper [<PUBLICATION>EntQA: Entitly Linking as Question Answering</PUBLICATION>](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup  ``` <SOFTWARE>conda</SOFTWARE> create --name entqa <PROGLANG>python=3.8</PROGLANG> <SOFTWARE>conda</SOFTWARE> activate entqa <SOFTWARE>pip</SOFTWARE> install -r requirements.txt <SOFTWARE>conda</SOFTWARE> install -c <SOFTWARE>pytorch</SOFTWARE> faiss-gpu <SOFTWARE>cudatoolkit=11.0</SOFTWARE>  ```  ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?,"# EntQA  This repo provides the code for our ICLR 2022 paper [EntQA: Entitly Linking as Question Answering](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup   conda create --name entqa python=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch faiss-gpu cudatoolkit=11.0    ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?
","# <PROJECT>EntQA</PROJECT>  This repo provides the code for our <PUBLICATION>ICLR 2022 paper [EntQA: Entitly Linking as Question Answering]</PUBLICATION>(https://arxiv.org/pdf/2110.02369.pdf)  ## Setup   conda create --name entqa <PROGLANG>python</PROGLANG>=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch <SOFTWARE>faiss-gpu</SOFTWARE> cudatoolkit=11.0    ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\3291ab7b.txt,0.992399565689468
2,"usp=sharing), you can skip following preprocess steps.","usp=sharing), you can skip following preprocess steps.","usp=sharing), you can skip following preprocess steps.
","usp=sharing), you can skip following preprocess steps.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\64328b8d.txt,0.9908256880733946
3,Or preprocess by yourself:  1.,Or preprocess by yourself:  1.,Or preprocess by yourself:  1.,Or preprocess by yourself:  1.,../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\e6ab6487.txt,1.0
4,Download KILT wikipedia knowledge base [here](https://github.com/facebookresearch/KILT) and put it under a kb directory like /raw_kb/  \ 2.,Download <DATASET>KILT</DATASET> wikipedia knowledge base [here](https://github.com/facebookresearch/KILT) and put it under a kb directory like /raw_kb/  \ 2.,Download KILT wikipedia knowledge base [here](https://github.com/facebookresearch/KILT) and put it under a kb directory like /raw_kb/  \ 2.,Download <DATASET>KILT wikipedia knowledge base</DATASET> [here](https://github.com/<PROJECT>facebookresearch</PROJECT>/<PROJECT>KILT</PROJECT>) and put it under a kb directory like /raw_kb/  \ 2.,../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\aff14cc4.txt,1.0
5,Download BLINK pretrained retriever model [here](https://github.com/facebookresearch/BLINK)  \ 3.,Download <SOFTWARE>BLINK</SOFTWARE> pretrained retriever model [here](https://github.com/facebookresearch/<SOFTWARE>BLINK</SOFTWARE>)  \ 3.,"Download BLINK pretrained retriever model [here](https://github.com/facebookresearch/BLINK)  \ 3.
","Download <SOFTWARE>BLINK</SOFTWARE> pretrained retriever model [here](https://github.com/facebookresearch/<SOFTWARE>BLINK</SOFTWARE>)  \ 3.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\df251d84.txt,0.9948717948717949
6,Download AIDA CoNLL datasets [here](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) and place them under a raw aida directory like /raw_aida/ \ 4.,Download <DATASET>AIDA CoNLL</DATASET> datasets [here](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) and place them under a raw aida directory like /raw_aida/ \ 4.,"Download AIDA CoNLL datasets [here](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) and place them under a raw aida directory like /raw_aida/ \ 4.
","Download <DATASET>AIDA CoNLL</DATASET> datasets [here](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) and place them under a raw aida directory like /raw_aida/ \ 4.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\077d7e34.txt,0.997624703087886
7,Download entity title map dictionary [here](https://drive.google.com/file/d/1QE3N8S_tVkGhYz_5fjRahLHfkIwghi-4/view?,Download entity title map dictionary [here](https://drive.google.com/file/d/1QE3N8S_tVkGhYz_5fjRahLHfkIwghi-4/view?,"Download entity title map dictionary [here](https://drive.google.com/file/d/1QE3N8S_tVkGhYz_5fjRahLHfkIwghi-4/view?
","Download entity title map dictionary [here](https://drive.google.com/file/d/1QE3N8S_tVkGhYz_5fjRahLHfkIwghi-4/view?
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\75a69d72.txt,0.9956709956709957
8,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.","usp=sharing) and put it under /raw_aida/ for remapping outdated entities of <DATASET>AIDA</DATASET> datasets to <DATASET>KILT</DATASET> wikipedia entity titles \ 5. preprocess <DATASET>AIDA</DATASET> data and <DATASET>KILT</DATASET> kb by ``` <SOFTWARE>python</SOFTWARE> preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` <SOFTWARE>python</SOFTWARE> run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/<SOFTWARE>BLINK</SOFTWARE>/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.","usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by  python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/    ## Train Retriever   Train retriever by   python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic  It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.
","usp=sharing) and put it under /raw_aida/ for remapping outdated entities of <DATASET>AIDA</DATASET> datasets to <DATASET>KILT</DATASET> wikipedia entity titles \ 5. preprocess <DATASET>AIDA</DATASET> data and <DATASET>KILT</DATASET> kb by  <PROGLANG>python</PROGLANG> preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[<DATASET>kilt wikipedia</DATASET> file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/    ## Train Retriever   Train retriever by   <PROGLANG>python</PROGLANG> run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /<SOFTWARE>blink</SOFTWARE>/<SOFTWARE>BLINK</SOFTWARE>/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --<SOFTWARE>blink</SOFTWARE>  --add_topic  It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\5fc1a9a5.txt,0.9939506747324337
9,It takes up 32G GPU memory for the main GPU and 23.8G GPU memory for other GPUs. ### Retriever Local Evaluation 1.,It takes up 32G GPU memory for the main GPU and 23.8G GPU memory for other GPUs. ### Retriever Local Evaluation 1.,It takes up 32G GPU memory for the main GPU and 23.8G GPU memory for other GPUs. ### Retriever Local Evaluation 1.,It takes up 32G GPU memory for the main GPU and 23.8G GPU memory for other GPUs. ### <EVALMETRIC>Retriever Local Evaluation</EVALMETRIC> 1.,../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\b8f8bd55.txt,1.0
10,You can train the retriever by yourself using the above scripts to get your trained retriever and entity embeddings.,You can train the retriever by yourself using the above scripts to get your trained retriever and entity embeddings.,You can train the retriever by yourself using the above scripts to get your trained retriever and entity embeddings.,You can train the retriever by yourself using the above scripts to get your trained retriever and entity embeddings.,../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\4c43220b.txt,1.0
11,You can also Download our trained retriever [here](https://drive.google.com/file/d/1bHS5rxGbHJ5omQ-t8rjQogw7QJq-qYFO/view?,You can also Download our trained retriever [here](https://drive.google.com/file/d/1bHS5rxGbHJ5omQ-t8rjQogw7QJq-qYFO/view?,"You can also Download our trained retriever [here](https://drive.google.com/file/d/1bHS5rxGbHJ5omQ-t8rjQogw7QJq-qYFO/view?
","You can also Download our trained retriever [here](https://drive.google.com/file/d/1bHS5rxGbHJ5omQ-t8rjQogw7QJq-qYFO/view?
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\8678b5ad.txt,0.9959183673469387
12,"usp=sharing), our cached entity embeddings [here](https://drive.google.com/file/d/1znMYd5HS80XpLpvpp_dFkQMbJiaFsQIn/view?","usp=sharing), our cached entity embeddings [here](https://drive.google.com/file/d/1znMYd5HS80XpLpvpp_dFkQMbJiaFsQIn/view?","usp=sharing), our cached entity embeddings [here](https://drive.google.com/file/d/1znMYd5HS80XpLpvpp_dFkQMbJiaFsQIn/view?
","usp=sharing), our cached entity embeddings [here](https://drive.google.com/file/d/1znMYd5HS80XpLpvpp_dFkQMbJiaFsQIn/view?
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\8951d7d8.txt,0.9958847736625515
13,usp=sharing).  2.,usp=sharing).  2.,"usp=sharing) 3.
","usp=sharing) 3.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\fbba97df.txt,0.7878787878787878
14,"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.","Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val <EVALMETRIC>Recall@100</EVALMETRIC> | test <EVALMETRIC>Recall@100</EVALMETRIC> | val <EVALMETRIC>LRAP</EVALMETRIC> | test <EVALMETRIC>LRAP</EVALMETRIC> | val <EVALMETRIC>passage-level Recall@100</EVALMETRIC> | test <EVALMETRIC>passage-level Recall@100</EVALMETRIC>| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **<EVALMETRIC>Recall@k</EVALMETRIC>** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **<EVALMETRIC>passage-level Recall@k</EVALMETRIC>** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **<EVALMETRIC>LRAP</EVALMETRIC>** is [<EVALMETRIC>Label ranking average precision</EVALMETRIC> ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.<EVALMETRIC>label_ranking_average_precision_score</EVALMETRIC>.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` <SOFTWARE>python</SOFTWARE> run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.","Use the above training scripts and set --epochs to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by   python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/   It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.
","Use the above training scripts and set --epochs to be 0 for evaluation. ### Retrieval Results | val <EVALMETRIC>Recall@100</EVALMETRIC> | test <EVALMETRIC>Recall@100</EVALMETRIC> | val <EVALMETRIC>LRAP</EVALMETRIC> | test <EVALMETRIC>LRAP</EVALMETRIC> | val passage-level <EVALMETRIC>Recall@100</EVALMETRIC> | test passage-level <EVALMETRIC>Recall@100</EVALMETRIC>| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **<EVALMETRIC>Recall@k</EVALMETRIC>** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **<EVALMETRIC>passage-level Recall@k</EVALMETRIC>** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **<EVALMETRIC>LRAP</EVALMETRIC>** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by   <PROGLANG>python</PROGLANG> run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/   It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\1e3eaca0.txt,0.9922958397534669
15,It takes up 36G GPU memory for the main GPU and 32G GPU memory for the other GPU. ### Reader Local Evaluation 1.,It takes up 36G GPU memory for the main GPU and 32G GPU memory for the other GPU. ### Reader Local Evaluation 1.,It takes up 36G GPU memory for the main GPU and 32G GPU memory for the other GPU. ### Reader Local Evaluation 1.,It takes up 36G GPU memory for the main GPU and 32G GPU memory for the other GPU. ### <EVALMETRIC>Reader Local Evaluation</EVALMETRIC> 1.,../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\798f3018.txt,1.0
16,You can follow the above instructions to train your reader or you can download our trained reader [here](https://drive.google.com/file/d/1A4I1fJZKxmROIE1fd0mdXN6b1emP_xt4/view?,You can follow the above instructions to train your reader or you can download our trained reader [here](https://drive.google.com/file/d/1A4I1fJZKxmROIE1fd0mdXN6b1emP_xt4/view?,You can follow the above instructions to train your reader or you can download our trained reader [here](https://drive.google.com/file/d/1A4I1fJZKxmROIE1fd0mdXN6b1emP_xt4/view?,You can follow the above instructions to train your reader or you can download our trained reader [here](https://drive.google.com/file/d/1A4I1fJZKxmROIE1fd0mdXN6b1emP_xt4/view?,../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\2e1cd84e.txt,1.0
17,usp=sharing) 2.,usp=sharing) 2.,"usp=sharing) 3.
","usp=sharing) 3.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\fbba97df.txt,0.9032258064516129
18,You can run retriever local evaluation for your trained retriever or our trained retriever to get reader input data.,You can run retriever local evaluation for your trained retriever or our trained retriever to get reader input data.,You can run retriever local evaluation for your trained retriever or our trained retriever to get reader input data.,You can run retriever local evaluation for your trained retriever or our trained retriever to get reader input data.,../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\fd20e44e.txt,1.0
19,Or you can download our reader input data [here](https://drive.google.com/drive/folders/1xfEgXCREe6pbSmAsnidsMuVYMK_mlOao?,Or you can download our reader input data [here](https://drive.google.com/drive/folders/1xfEgXCREe6pbSmAsnidsMuVYMK_mlOao?,Or you can download our reader input data [here](https://drive.google.com/drive/folders/1xfEgXCREe6pbSmAsnidsMuVYMK_mlOao?,Or you can download our reader input data [here](https://drive.google.com/drive/folders/1xfEgXCREe6pbSmAsnidsMuVYMK_mlOao?,../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\759b033a.txt,1.0
20,usp=sharing) 3.,usp=sharing) 3.,"usp=sharing) 3.
","usp=sharing) 3.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\fbba97df.txt,0.967741935483871
21,"Use the above reader training scripts and set `--epochs` to be 0 for evaluation.  ### Reader Results  |   val F1  |  test F1  |  val Recall |  test Recall |  val Precision  |  test Precision  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## GERBIL evaluation Our GERBIL evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.","Use the above reader training scripts and set `--epochs` to be 0 for evaluation.  ### Reader Results  |   val <EVALMETRIC>F1</EVALMETRIC>  |  test <EVALMETRIC>F1</EVALMETRIC>  |  val <EVALMETRIC>Recall</EVALMETRIC> |  test <EVALMETRIC>Recall</EVALMETRIC> |  val <EVALMETRIC>Precision</EVALMETRIC>  |  test <EVALMETRIC>Precision</EVALMETRIC>  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## GERBIL evaluation Our GERBIL evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.","Use the above reader training scripts and set --epochs to be 0 for evaluation.  ### Reader Results  |   val F1  |  test F1  |  val Recall |  test Recall |  val Precision  |  test Precision  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## GERBIL evaluation Our GERBIL evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.
","Use the above reader training scripts and set --epochs to be 0 for evaluation.  ### Reader Results  |   val <EVALMETRIC>F1</EVALMETRIC>  |  test <EVALMETRIC>F1</EVALMETRIC>  |  val <EVALMETRIC>Recall</EVALMETRIC> |  test <EVALMETRIC>Recall</EVALMETRIC> |  val <EVALMETRIC>Precision</EVALMETRIC>  |  test <EVALMETRIC>Precision</EVALMETRIC>  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## <SOFTWARE>GERBIL</SOFTWARE> evaluation Our <SOFTWARE>GERBIL</SOFTWARE> evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\069c0b70.txt,0.9970674486803519
22,Download our snapshot of GERBIL repo [here](https://drive.google.com/file/d/1Sp-G9631ormzIYfenCDsaWgiBPVBkF6F/view?,Download our snapshot of GERBIL repo [here](https://drive.google.com/file/d/1Sp-G9631ormzIYfenCDsaWgiBPVBkF6F/view?,"Download our snapshot of GERBIL repo [here](https://drive.google.com/file/d/1Sp-G9631ormzIYfenCDsaWgiBPVBkF6F/view?
","Download our snapshot of <PROJECT>GERBIL</PROJECT> repo [here](https://drive.google.com/file/d/1Sp-G9631ormzIYfenCDsaWgiBPVBkF6F/view?
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\07b1a115.txt,0.9956709956709957
23,"usp=sharing), our pretrained retriever [here](https://drive.google.com/file/d/1bHS5rxGbHJ5omQ-t8rjQogw7QJq-qYFO/view?","usp=sharing), our pretrained retriever [here](https://drive.google.com/file/d/1bHS5rxGbHJ5omQ-t8rjQogw7QJq-qYFO/view?","usp=sharing), our pretrained retriever [here](https://drive.google.com/file/d/1bHS5rxGbHJ5omQ-t8rjQogw7QJq-qYFO/view?
","usp=sharing), our pretrained retriever [here](https://drive.google.com/file/d/1bHS5rxGbHJ5omQ-t8rjQogw7QJq-qYFO/view?
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\fb5eb27d.txt,0.9957446808510638
24,"usp=sharing), our cached entities embeddings [here](https://drive.google.com/file/d/1znMYd5HS80XpLpvpp_dFkQMbJiaFsQIn/view?","usp=sharing), our cached entities embeddings [here](https://drive.google.com/file/d/1znMYd5HS80XpLpvpp_dFkQMbJiaFsQIn/view?","usp=sharing), our cached entity embeddings [here](https://drive.google.com/file/d/1znMYd5HS80XpLpvpp_dFkQMbJiaFsQIn/view?
","usp=sharing), our cached entity embeddings [here](https://drive.google.com/file/d/1znMYd5HS80XpLpvpp_dFkQMbJiaFsQIn/view?
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\8951d7d8.txt,0.9795918367346939
25,usp=sharing) and our pretrained reader [here](https://drive.google.com/file/d/1A4I1fJZKxmROIE1fd0mdXN6b1emP_xt4/view?,usp=sharing) and our pretrained reader [here](https://drive.google.com/file/d/1A4I1fJZKxmROIE1fd0mdXN6b1emP_xt4/view?,"usp=sharing) and our pretrained reader [here](https://drive.google.com/file/d/1A4I1fJZKxmROIE1fd0mdXN6b1emP_xt4/view?
","usp=sharing) and our pretrained reader [here](https://drive.google.com/file/d/1A4I1fJZKxmROIE1fd0mdXN6b1emP_xt4/view?
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\f37ab4f2.txt,0.9957446808510638
26,usp=sharing) 2.,usp=sharing) 2.,"usp=sharing) 3.
","usp=sharing) 3.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\fbba97df.txt,0.9032258064516129
27,One one terminal/screen run GERBIL by: ``` cd gerbil .,One one <SOFTWARE>terminal</SOFTWARE>/screen run <SOFTWARE>GERBIL</SOFTWARE> by: ``` cd gerbil .,"One one terminal/screen run GERBIL by:  cd gerbil .
","One one terminal/screen run <SOFTWARE>GERBIL</SOFTWARE> by:  cd gerbil .
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\64887d9b.txt,0.9622641509433962
28,/start.sh  ``` 3.,/start.sh  ``` 3.,"/start.sh   3.
","/start.sh   3.
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\a65ec0fd.txt,0.875
29,On another terminal/screen run: ``` cd gerbil-SpotWrapNifWS4Test/ mvn clean -Dmaven.tomcat.port=1235 tomcat:run  ``` 4.,On another <SOFTWARE>terminal</SOFTWARE>/screen run: ``` cd gerbil-SpotWrapNifWS4Test/ mvn clean -Dmaven.tomcat.port=1235 tomcat:run  ``` 4.,On another terminal/screen run:  cd gerbil-SpotWrapNifWS4Test/ mvn clean -Dmaven.tomcat.port=1235 tomcat:run   4.,On another terminal/screen run:  cd gerbil-SpotWrapNifWS4Test/ <PROGLANG>mvn</PROGLANG> clean -Dmaven.tomcat.port=1235 tomcat:run   4.,../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\ce34869e.txt,0.9741379310344828
30,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|","On a third <SOFTWARE>terminal</SOFTWARE>/screen run: ``` <SOFTWARE>python</SOFTWARE> gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | <DATASET>AIDA testb</DATASET>| <DATASET>MSNBC</DATASET>| <DATASET>Der</DATASET>|<DATASET>K50</DATASET>|<DATASET>R128</DATASET>|<DATASET>R500</DATASET>|<DATASET>OKE15</DATASET>|<DATASET>OKE16</DATASET>|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|","On a third terminal/screen run:  python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192     Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|
","On a third terminal/screen run:  <PROGLANG>python</PROGLANG> gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192     Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the <DATASET>datasets</DATASET> that you want to evaluate the model on - Run experiment  ### GERBIL Results | <DATASET>AIDA testb</DATASET>| <DATASET>MSNBC</DATASET>| <DATASET>Der</DATASET>|<DATASET>K50</DATASET>|<DATASET>R128</DATASET>|<DATASET>R500</DATASET>|<DATASET>OKE15</DATASET>|<DATASET>OKE16</DATASET>|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|
",../results/deepseek-chat/prompt-0/zzz_wenzhengzhang_entqa_main_README.md.tsv\2f7c4ab7.txt,0.9963560645497137
