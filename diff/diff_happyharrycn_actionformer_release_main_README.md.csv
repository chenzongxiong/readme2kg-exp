sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
2,"Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% mAP for the first time.","Without bells and whistles, <SOFTWARE>ActionFormer</SOFTWARE> achieves 71.0% <EVALMETRIC>mAP</EVALMETRIC> at <EVALMETRIC>tIoU</EVALMETRIC>=0.5 on <DATASET>THUMOS14</DATASET>, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% <EVALMETRIC>mAP</EVALMETRIC> for the first time.","Without bells and whistles, `ActionFormer` achieves 71.0% `mAP` at `tIoU=0.5` on `THUMOS14`, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% `mAP` for the first time.","Without bells and whistles, `<SOFTWARE>ActionFormer</SOFTWARE>` achieves 71.0% `<EVALMETRIC>mAP</EVALMETRIC>` at `<EVALMETRIC>tIoU</EVALMETRIC>=0.5` on `<DATASET>THUMOS14</DATASET>`, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% `<EVALMETRIC>mAP</EVALMETRIC>` for the first time.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c0141251.txt,0.9362745098039216
3,"Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.56% average mAP) and the more challenging EPIC-Kitchens 100 (+13.5% average mAP over prior works).","Further, <SOFTWARE>ActionFormer</SOFTWARE> demonstrates strong results on <DATASET>ActivityNet 1.3</DATASET> (36.56% average <EVALMETRIC>mAP</EVALMETRIC>) and the more challenging <DATASET>EPIC-Kitchens 100</DATASET> (+13.5% average <EVALMETRIC>mAP</EVALMETRIC> over prior works).","Further, ActionFormer demonstrates strong results on `ActivityNet 1.3` (36.56% average `mAP`) and the more challenging `EPIC-Kitchens 100` (+13.5% average `mAP` over prior works).","Further, ActionFormer demonstrates strong results on `<DATASET>ActivityNet 1.3</DATASET>` (36.56% average `<EVALMETRIC>mAP</EVALMETRIC>`) and the more challenging `<DATASET>EPIC-Kitchens 100</DATASET>` (+13.5% average `<EVALMETRIC>mAP</EVALMETRIC>` over prior works).",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\3c5cfe6b.txt,0.9771428571428571
5,"In addition, ActionFormer is the backbone for many winning solutions in the Ego4D Moment Queries Challenge 2022.","In addition, <SOFTWARE>ActionFormer</SOFTWARE> is the backbone for many winning solutions in the <DATASET>Ego4D Moment Queries</DATASET> Challenge 2022.","In addition, `ActionFormer` is the backbone for many winning solutions in the `Ego4D Moment Queries Challenge 2022`.","In addition, `<SOFTWARE>ActionFormer</SOFTWARE>` is the backbone for many winning solutions in the `<CONFERENCE>Ego4D Moment Queries Challenge 2022</CONFERENCE>`.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1ca1641f.txt,0.9824561403508771
6,"Our submission in particular is ranked 2nd with a record 21.76% average mAP and 42.54% Recall@1x, tIoU=0.5, nearly three times higher than the official baseline.","Our submission in particular is ranked 2nd with a record 21.76% average <EVALMETRIC>mAP</EVALMETRIC> and 42.54% <EVALMETRIC>Recall@1x</EVALMETRIC>, <EVALMETRIC>tIoU</EVALMETRIC>=0.5, nearly three times higher than the official baseline.","Our submission in particular is ranked 2nd with a record `21.76% average mAP` and `42.54% Recall@1x`, `tIoU=0.5`, nearly three times higher than the official baseline.","Our submission in particular is ranked 2nd with a record `<EVALMETRIC>21.76% average mAP</EVALMETRIC>` and `<EVALMETRIC>42.54% Recall@1x</EVALMETRIC>`, `<EVALMETRIC>tIoU=0.5</EVALMETRIC>`, nearly three times higher than the official baseline.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\8196017b.txt,0.9817073170731707
7,An arXiv version of our tech report can be found at [this link](https://arxiv.org/abs/2211.09074).,An arXiv version of our tech report can be found at [this link](https://arxiv.org/abs/2211.09074).,`An arXiv version of our tech report can be found at [this link](https://arxiv.org/abs/2211.09074).`,`An arXiv version of our tech report can be found at [this link](https://arxiv.org/abs/2211.09074).`,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1ea322f2.txt,0.98989898989899
19,"The code repo now includes config files, pre-trained models and results on the Ego4D MQ benchmark","The code repo now includes config files, pre-trained models and results on the <DATASET>Ego4D MQ</DATASET> benchmark","The code repo now includes config files, pre-trained models and results on the `Ego4D MQ` benchmark","The code repo now includes config files, pre-trained models and results on the `<DATASET>Ego4D MQ</DATASET>` benchmark",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\511c7879.txt,0.9897959183673469
21,* 08/29/2022: Updated arXiv version,* 08/29/2022: Updated arXiv version,`* 08/29/2022: Updated arXiv version`,`* 08/29/2022: Updated <PUBLICATION>arXiv</PUBLICATION> version`,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\cede2201.txt,0.9722222222222222
23,* 08/01/2022: Updated code repo with latest results on ActivityNet,* 08/01/2022: Updated code repo with latest results on <DATASET>ActivityNet</DATASET>,* 08/01/2022: Updated code repo with latest results on `ActivityNet`,* 08/01/2022: Updated code repo with latest results on `<DATASET>ActivityNet</DATASET>`,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\010bfa95.txt,0.9850746268656716
29,"* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average mAP on THUMOS14 (>66.0%) and slightly improved results on ActivityNet and EPIC-Kitchens 100.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2.","* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average <EVALMETRIC>mAP</EVALMETRIC> on <DATASET>THUMOS14</DATASET> (>66.0%) and slightly improved results on <DATASET>ActivityNet</DATASET> and <DATASET>EPIC-Kitchens 100</DATASET>.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2.","* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average `mAP` on `THUMOS14` (>66.0%) and slightly improved results on `ActivityNet` and `EPIC-Kitchens 100`.   ## Code Overview The structure of this code repo is heavily inspired by `Detectron2`.","* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average `<EVALMETRIC>mAP</EVALMETRIC>` on `<DATASET>THUMOS14</DATASET>` (>66.0%) and slightly improved results on `<DATASET>ActivityNet</DATASET>` and `<DATASET>EPIC-Kitchens 100</DATASET>`.   ## Code Overview The structure of this code repo is heavily inspired by `<SOFTWARE>Detectron2</SOFTWARE>`.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\8ae43482.txt,0.95625
31,/libs/core: Parameter configuration module. * .,/libs/core: Parameter configuration module. * .,`/libs/core: Parameter configuration module. * .`,`/libs/core: <SOFTWARE>Parameter configuration module</SOFTWARE>. * .`,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1d295d6b.txt,0.9791666666666666
32,/libs/datasets: Data loader and IO module. * .,/libs/datasets: Data loader and IO module. * .,`/libs/datasets: Data loader and IO module. * .`,`/libs/<DATASET>datasets</DATASET>: Data loader and IO module. * .`,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\1088967f.txt,0.9787234042553191
33,/libs/modeling: Our main model with all its building blocks. * .,/libs/modeling: Our main model with all its building blocks. * .,`/libs/modeling: Our main model with all its building blocks. * .`,`/libs/<SOFTWARE>modeling</SOFTWARE>: Our main model with all its building blocks. * .`,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\ea2310ae.txt,0.9846153846153847
47,"/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/`ckpt`* that stores training config, logs, and checkpoints.","/`<SOFTWARE>ckpt</SOFTWARE>`* that stores training config, logs, and checkpoints.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3231b41.txt,0.9830508474576272
48,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
52,The expected average mAP should be around 62.6(%) as in Table 1 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 62.6(%) as in Table 1 of our main paper.,The expected average `mAP` should be around 62.6(%) as in Table 1 of our main paper.,The expected average `<EVALMETRIC>mAP</EVALMETRIC>` should be around 62.6(%) as in Table 1 of our main paper.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\b193c6d8.txt,0.9879518072289156
53,"**With recent commits, the expected average mAP should be higher than 66.0(%)**.","**With recent commits, the expected average <EVALMETRIC>mAP</EVALMETRIC> should be higher than 66.0(%)**.","**With recent commits, the expected average `mAP` should be higher than 66.0(%).**","**With recent commits, the expected average `<EVALMETRIC>mAP</EVALMETRIC>` should be higher than 66.0(%).**",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\054261f3.txt,0.9753086419753086
54,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
64,* Create a folder *.,* Create a folder *.,"To produce the results, create a folder \*.","To produce the results, create a folder \*.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0a48399e.txt,0.5714285714285714
80,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
86,"**Details**: The features are extracted from the R(2+1)D-34 model pretrained with TSP on ActivityNet using clips of `16 frames` at a frame rate of `15 fps` and a stride of `16 frames` (*i.e.,* **non-overlapping** clips).","**Details**: The features are extracted from the R(2+1)D-34 model pretrained with TSP on <DATASET>ActivityNet</DATASET> using clips of `16 frames` at a frame rate of `15 fps` and a stride of `16 frames` (*i.e.,* **non-overlapping** clips).","**Details**: The features are extracted from the `R(2+1)D-34` model pretrained with `TSP` on `ActivityNet` using clips of `16 frames` at a frame rate of `15 fps` and a stride of `16 frames` (*i.e.,* **non-overlapping** clips).","**Details**: The features are extracted from the `<SOFTWARE>R(2+1)D-34</SOFTWARE>` model pretrained with `<DATASET>TSP</DATASET>` on `<DATASET>ActivityNet</DATASET>` using clips of `16 frames` at a frame rate of `15 fps` and a stride of `16 frames` (*i.e.,* **non-overlapping** clips).",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\6ac13e5d.txt,0.968609865470852
88,The features are converted into numpy files for our code.,The features are converted into numpy files for our code.,The features are converted into numpy files for our `code`.,The features are converted into numpy files for our `<PROGLANG>code</PROGLANG>`.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\c7b35489.txt,0.9827586206896551
97,"/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/`ckpt`* that stores training config, logs, and checkpoints.","/`<SOFTWARE>ckpt</SOFTWARE>`* that stores training config, logs, and checkpoints.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3231b41.txt,0.9830508474576272
98,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
102,The expected average mAP should be around 36.5(%) as in Table 1 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 36.5(%) as in Table 1 of our main paper.,The expected average `mAP` should be around 36.5(%) as in Table 1 of our main paper.,The expected average `<EVALMETRIC>mAP</EVALMETRIC>` should be around 36.5(%) as in Table 1 of our main paper.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\61ae5e13.txt,0.9879518072289156
103,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
108,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for ActivityNet 1.3.,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for <DATASET>ActivityNet 1.3</DATASET>.,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for `ActivityNet 1.3`.,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for `<DATASET>ActivityNet 1.3</DATASET>`.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\9c0c1799.txt,0.9905660377358491
113,* Create a folder *.,* Create a folder *.,"To produce the results, create a folder \*.","To produce the results, create a folder \*.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0a48399e.txt,0.5714285714285714
128,/pretrained/anet_tsp_reproduce/epoch_014.pth.tar*. * Evaluate the pre-trained model.,/pretrained/anet_tsp_reproduce/epoch_014.pth.tar*. * Evaluate the pre-trained model.,`/pretrained/anet_tsp_reproduce/epoch_014.pth.tar*. * Evaluate the pre-trained model.`,`/pretrained/anet_tsp_reproduce/epoch_014.pth.tar*. * Evaluate the pre-trained model.`,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\80804bb1.txt,0.9882352941176471
129,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
136,The features are converted into numpy files for our code,The features are converted into numpy files for our code,The features are converted into numpy files for our `code`,The features are converted into numpy files for our `<PROGLANG>code</PROGLANG>`,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\db2247d9.txt,0.9824561403508771
140,"/data*), similar to TSP features","/data*), similar to TSP features","`/data*), similar to TSP features`","`/data*), similar to TSP features`",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\ca6d2344.txt,0.9696969696969697
144,"/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/`ckpt`* that stores training config, logs, and checkpoints.","/`<SOFTWARE>ckpt</SOFTWARE>`* that stores training config, logs, and checkpoints.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3231b41.txt,0.9830508474576272
145,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
148,The expected average mAP should be around 36.0(%).,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 36.0(%).,The expected average `mAP` should be around 36.0(%).,The expected average `<EVALMETRIC>mAP</EVALMETRIC>` should be around 36.0(%).,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\5ed23461.txt,0.9803921568627451
149,This is slightly improved from our paper.,This is slightly improved from our paper.,`This is slightly improved from our paper.`,`This is slightly improved from our <PUBLICATION>paper</PUBLICATION>.`,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\7c7b7559.txt,0.9761904761904762
151,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
156,"To produce the results, create a folder *.","To produce the results, create a folder *.","To produce the results, create a folder \*.","To produce the results, create a folder \*.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0a48399e.txt,0.9882352941176471
159,"/pretrained*), and run ```shell python .","/pretrained*), and run ```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .",/pretrained* and unpack the file under *.,/pretrained* and unpack the file under *.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\fca77404.txt,0.6666666666666666
178,The expected average mAP should be around 23.4(%) as in Table 2 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 23.4(%) as in Table 2 of our main paper.,The expected average `mAP` should be around 23.4(%) as in Table 2 of our main paper.,The expected average `<EVALMETRIC>mAP</EVALMETRIC>` should be around 23.4(%) as in Table 2 of our main paper.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\9a7a5605.txt,0.9879518072289156
179,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
183,The expected average mAP should be around 21.9(%) as in Table 2 of our main paper.,The expected average <EVALMETRIC>mAP</EVALMETRIC> should be around 21.9(%) as in Table 2 of our main paper.,The expected average `mAP` should be around 21.9(%) as in Table 2 of our main paper.,The expected average `<EVALMETRIC>mAP</EVALMETRIC>` should be around 21.9(%) as in Table 2 of our main paper.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\27da51ce.txt,0.9879518072289156
184,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
192,usp=sharing) (noun).,usp=sharing) (noun).,usp=sharing).,usp=sharing).,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\e4c0f51a.txt,0.7878787878787878
195,* Create a folder *.,* Create a folder *.,"To produce the results, create a folder \*.","To produce the results, create a folder \*.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0a48399e.txt,0.5714285714285714
215,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
219,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
225,/tools/convert_ego4d_trainval.py`.,/tools/convert_ego4d_trainval.py`.,/tools/convert_ego4d_trainval.py`,/tools/<SOFTWARE>convert_ego4d_trainval.py</SOFTWARE>`,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\4411e14a.txt,0.9850746268656716
228,Please refer to Ego4D and EgoVLP's documentation for more details on feature extraction.,Please refer to <PROJECT>Ego4D</PROJECT> and <PROJECT>EgoVLP</PROJECT>'s documentation for more details on feature extraction.,Please refer to `Ego4D` and `EgoVLP`'s documentation for more details on feature extraction.,Please refer to `<PROJECT>Ego4D</PROJECT>` and `<PROJECT>EgoVLP</PROJECT>`'s documentation for more details on feature extraction.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\602a2b54.txt,0.9777777777777777
236,"For example, training on Omnivore and EgoVLP features will create an experiment folder under *.","For example, training on <DATASET>Omnivore</DATASET> and <DATASET>EgoVLP</DATASET> features will create an experiment folder under *.","For example, training on `Omnivore` and `EgoVLP` features will create an experiment folder under *.","For example, training on `<DATASET>Omnivore</DATASET>` and `<DATASET>EgoVLP</DATASET>` features will create an experiment folder under *.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\e65b010e.txt,0.979381443298969
237,"/ckpt* that stores training config, logs, and checkpoints.","/ckpt* that stores training config, logs, and checkpoints.","/`ckpt`* that stores training config, logs, and checkpoints.","/`<SOFTWARE>ckpt</SOFTWARE>`* that stores training config, logs, and checkpoints.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\d3231b41.txt,0.9830508474576272
238,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
240,/configs/ego4d_omnivore_egovlp.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,/configs/ego4d_omnivore_egovlp.yaml --output reproduce ``` * [Optional] Monitor the training using <SOFTWARE>TensorBoard</SOFTWARE> ```<PROGLANG>shell</PROGLANG> <SOFTWARE>tensorboard</SOFTWARE> --logdir=.,/configs/ego4d_omnivore_egovlp.yaml --output reproduce ``` * [Optional] Monitor the training using `TensorBoard` ```shell `tensorboard` --logdir=.,/configs/ego4d_omnivore_egovlp.yaml --output reproduce ``` * [Optional] Monitor the training using `<SOFTWARE>TensorBoard</SOFTWARE>` ```shell `<SOFTWARE>tensorboard</SOFTWARE>` --logdir=.,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\23d9dda6.txt,0.9861111111111112
242,"The expected average mAP and Recall@1x, tIoU=0.5 should be around 22.0(%) and 40.0(%) respectively.","The expected average <EVALMETRIC>mAP</EVALMETRIC> and <EVALMETRIC>Recall@1x</EVALMETRIC>, <EVALMETRIC>tIoU</EVALMETRIC>=0.5 should be around 22.0(%) and 40.0(%) respectively.","The expected average `mAP` and `Recall@1x, tIoU=0.5` should be around 22.0(%) and 40.0(%) respectively.","The expected average `<EVALMETRIC>mAP</EVALMETRIC>` and `<EVALMETRIC>Recall@1x, tIoU=0.5</EVALMETRIC>` should be around 22.0(%) and 40.0(%) respectively.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\a09545db.txt,0.9801980198019802
243,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
253,* Create a folder *.,* Create a folder *.,"To produce the results, create a folder \*.","To produce the results, create a folder \*.",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\0a48399e.txt,0.5714285714285714
269,```shell python .,```<PROGLANG>shell</PROGLANG> <SOFTWARE>python</SOFTWARE> .,```shell python .```,```shell python .```,../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\09ac0c71.txt,0.918918918918919
273,"Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper. ``` @inproceedings{zhang2022actionformer,   title={ActionFormer: Localizing Moments of Actions with Transformers},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={European Conference on Computer Vision},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} } ```  If you cite our results on Ego4D, please consider citing our tech report in addition to the main paper. ``` @article{mu2022actionformerego4d,   title={Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} } ```  If you are using TSP features, please cite ``` @inproceedings{alwassel2021tsp,   title={{TSP}: Temporally-sensitive pretraining of video encoders for localization tasks},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},   pages={3173--3183},   year={2021} } ```","Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper. ``` @inproceedings{zhang2022actionformer,   title={<PUBLICATION>ActionFormer: Localizing Moments of Actions with Transformers</PUBLICATION>},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={<CONFERENCE>European Conference on Computer Vision</CONFERENCE>},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} } ```  If you cite our results on <DATASET>Ego4D</DATASET>, please consider citing our tech report in addition to the main paper. ``` @article{mu2022actionformerego4d,   title={<PUBLICATION>Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge</PUBLICATION>},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} } ```  If you are using TSP features, please cite ``` @inproceedings{alwassel2021tsp,   title={<PUBLICATION>{TSP}: Temporally-sensitive pretraining of video encoders for localization tasks</PUBLICATION>},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={<PUBLICATION>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</PUBLICATION><WORKSHOP>the IEEE/CVF International Conference on Computer Vision Workshops</WORKSHOP>},   pages={3173--3183},   year={2021} } ```","Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper. ``` @inproceedings{zhang2022actionformer,   title={ActionFormer: Localizing Moments of Actions with Transformers},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={European Conference on Computer Vision},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} } ```  If you cite our results on Ego4D, please consider citing our tech report in addition to the main paper. ``` @article{mu2022actionformerego4d,   title={Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} } ```  If you are using TSP features, please cite ``` @inproceedings{alwassel2021tsp,   title={TSP: Temporally-sensitive pretraining of video encoders for localization tasks},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},   pages={3173--3183},   year={2021} }","Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper. ``` @inproceedings{zhang2022actionformer,   title={<PUBLICATION>ActionFormer: Localizing Moments of Actions with Transformers</PUBLICATION>},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={<CONFERENCE>European Conference on Computer Vision</CONFERENCE>},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} } ```  If you cite our results on Ego4D, please consider citing our tech report in addition to the main paper. ``` @article{mu2022actionformerego4d,   title={<PUBLICATION>Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge</PUBLICATION>},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} } ```  If you are using TSP features, please cite ``` @inproceedings{alwassel2021tsp,   title={<PUBLICATION>TSP: Temporally-sensitive pretraining of video encoders for localization tasks</PUBLICATION>},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={<CONFERENCE>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</CONFERENCE>},   pages={3173--3183},   year={2021} }",../results/deepseek-chat/prompt-0/zzz_happyharrycn_actionformer_release_main_README.md.tsv\9688548d.txt,0.9974380871050385
