sentence_idx,original_sentence,original_annotated,de_annotated_content,generated_annotated,source_file,similarity_score
1,# 🪄  Prompt-OIRL: Learning to Prompt LLMs with Expert Knowledge (Known Magic Words 🧙 )  ### 💻  Implementation and 📒  tutorial for ICLR 2024 paper   !,# 🪄  <SOFTWARE>Prompt-OIRL</SOFTWARE>: Learning to Prompt LLMs with Expert Knowledge (Known Magic Words 🧙 )  ### 💻  Implementation and 📒  tutorial for <CONFERENCE>ICLR 2024</CONFERENCE> paper   !,# 🪄  Prompt-OIRL: Learning to Prompt LLMs with Expert Knowledge (Known Magic Words 🧙 )  ### 💻  Implementation and 📒  tutorial for ICLR 2024 paper   !,# 🪄  <PROJECT>Prompt-OIRL</PROJECT>: Learning to Prompt LLMs with Expert Knowledge (Known Magic Words 🧙 )  ### 💻  Implementation and 📒  tutorial for <CONFERENCE>ICLR 2024</CONFERENCE> paper   !,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\47ed2f41.txt,1.0
2,[Image](prompt-oirl-title.png)  - [Paper Link](https://arxiv.org/pdf/2309.06553.pdf) - [Open Review Link](https://openreview.net/forum?,[Image](prompt-oirl-title.png)  - [Paper Link](https://arxiv.org/pdf/2309.06553.pdf) - [Open Review Link](https://openreview.net/forum?,[Image](prompt-oirl-title.png)  - [Paper Link](https://arxiv.org/pdf/2309.06553.pdf) - [Open Review Link](https://openreview.net/forum?,[Image](prompt-oirl-title.png)  - [Paper Link](https://arxiv.org/pdf/2309.06553.pdf) - [Open Review Link](https://<PUBLICATION>openreview</PUBLICATION>.net/forum?,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\e52e4f8a.txt,1.0
3,id=N6o0ZtPzTg)   ## 🔥  News - (2024.2) (Internal Code-Reviewing) Code with GPT3.5 and TigerBot to be released. - (2024.1) Prompt-OIRL has been accepted by ICLR'2024.,id=N6o0ZtPzTg)   ## 🔥  News - (2024.2) (Internal Code-Reviewing) Code with <SOFTWARE>GPT3.5</SOFTWARE> and <SOFTWARE>TigerBot</SOFTWARE> to be released. - (2024.1) <SOFTWARE>Prompt-OIRL</SOFTWARE> has been accepted by <CONFERENCE>ICLR'2024</CONFERENCE>.,id=N6o0ZtPzTg)   ## 🔥  News - (2024.2) (Internal Code-Reviewing) Code with GPT3.5 and TigerBot to be released. - (2024.1) Prompt-OIRL has been accepted by ICLR'2024.,id=N6o0ZtPzTg)   ## 🔥  News - (2024.2) (Internal Code-Reviewing) Code with <PROGLANG>GPT3.5</PROGLANG> and <PROGLANG>TigerBot</PROGLANG> to be released. - (2024.1) <PUBLICATION>Prompt-OIRL</PUBLICATION> has been accepted by <CONFERENCE>ICLR'2024</CONFERENCE>.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\0347690f.txt,1.0
4,We look forward to talking with you in Vienna!,We look forward to talking with you in Vienna!,We look forward to talking with you in <LOCATION>Vienna</LOCATION>!,We look forward to talking with you in <LOCATION>Vienna</LOCATION>!,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\cba0f4e4.txt,0.8141592920353983
5,- (2024.12) Prompt-OIRL has been presented at the NeurIPS conference.,- (2024.12) <SOFTWARE>Prompt-OIRL</SOFTWARE> has been presented at the <CONFERENCE>NeurIPS</CONFERENCE> conference.,- (2024.12) Prompt-OIRL has been presented at the `NeurIPS` conference.,- (2024.12) Prompt-OIRL has been presented at the `<CONFERENCE>NeurIPS</CONFERENCE>` conference.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\d5704bf7.txt,0.9857142857142858
6,Thanks for all the invaluable feedback!,Thanks for all the invaluable feedback!,Thanks for all the invaluable feedback!,Thanks for all the invaluable feedback!,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\d4c31b58.txt,1.0
7,"- (2023.10) Code with LLaMA2 has been released. - (2023.10) Prompt-OIRL has been featured in a positioning [paper](https://arxiv.org/pdf/2310.06147.pdf) as an example of **inverse alignment**. - (2023.9) Prompt-OIRL has been selected as an **oral presentation** at the ENLSP workshop at NeurIPS'2023.  ## 📖  Abstract  > In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization.","- (2023.10) Code with <SOFTWARE>LLaMA2</SOFTWARE> has been released. - (2023.10) <SOFTWARE>Prompt-OIRL</SOFTWARE> has been featured in a positioning [paper](https://arxiv.org/pdf/2310.06147.pdf) as an example of **inverse alignment**. - (2023.9) <SOFTWARE>Prompt-OIRL</SOFTWARE> has been selected as an **oral presentation** at the <WORKSHOP>ENLSP</WORKSHOP> workshop at <CONFERENCE>NeurIPS'2023</CONFERENCE>.  ## 📖  Abstract  > In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization.","- (2023.10) Code with LLaMA2 has been released. - (2023.10) Prompt-OIRL has been featured in a positioning [paper](https://arxiv.org/pdf/2310.06147.pdf) as an example of **inverse alignment**. - (2023.9) Prompt-OIRL has been selected as an **oral presentation** at the ENLSP workshop at NeurIPS'2023.  ## 📖  Abstract  > In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization.","- (2023.10) Code with <SOFTWARE>LLaMA2</SOFTWARE> has been released. - (2023.10) <PROJECT>Prompt-OIRL</PROJECT> has been featured in a positioning [paper](https://arxiv.org/pdf/2310.06147.pdf) as an example of **inverse alignment**. - (2023.9) <PROJECT>Prompt-OIRL</PROJECT> has been selected as an **oral presentation** at the <WORKSHOP>ENLSP</WORKSHOP> workshop at <CONFERENCE>NeurIPS'2023</CONFERENCE>.  ## 📖  Abstract  > In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\95ac4223.txt,1.0
8,We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques.,We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques.,We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques.,We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\e33f7ed9.txt,1.0
9,One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable.,One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable.,One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable.,One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\091f1cc4.txt,1.0
10,"Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive.","Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive.","Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive.","Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\4878a7c9.txt,1.0
11,"To address this, we introduce Prompt-OIRL, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data.","To address this, we introduce <SOFTWARE>Prompt-OIRL</SOFTWARE>, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data.","To address this, we introduce `Prompt-OIRL`, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data.","To address this, we introduce `<SOFTWARE>Prompt-OIRL</SOFTWARE>`, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\b56a24d1.txt,0.9936708860759493
12,Such data exists as by-products when diverse prompts are benchmarked on open-accessible datasets.,Such data exists as by-products when diverse prompts are benchmarked on open-accessible datasets.,Such data exists as by-products when diverse prompts are benchmarked on open-accessible `datasets`.,Such data exists as by-products when diverse prompts are benchmarked on open-accessible `<DATASET>datasets</DATASET>`.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\f0d4161b.txt,0.9897959183673469
13,"With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model.","With <SOFTWARE>Prompt-OIRL</SOFTWARE>, the query-dependent prompt optimization objective is achieved by first learning an offline reward model.","With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model.","With <PROJECT>Prompt-OIRL</PROJECT>, the query-dependent prompt optimization objective is achieved by first learning an offline reward model.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\778eadb1.txt,1.0
14,This model can evaluate any query-prompt pairs without accessing LLMs.,This model can evaluate any query-prompt pairs without accessing LLMs.,This model can evaluate any query-prompt pairs without accessing LLMs.,This model can evaluate any query-prompt pairs without accessing LLMs.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\cf97a4f8.txt,1.0
15,"Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt.","Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt.","Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt.","Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\37a2c33c.txt,1.0
16,Our experimental evaluations across various LLM scales and arithmetic reasoning datasets underscore both the efficacy and economic viability of the proposed approach.  ## 🤔  Motivating Example  !,Our experimental evaluations across various LLM scales and arithmetic reasoning datasets underscore both the efficacy and economic viability of the proposed approach.  ## 🤔  Motivating Example  !,Our experimental evaluations across various LLM scales and arithmetic reasoning `datasets` underscore both the efficacy and economic viability of the proposed approach.  ## 🤔  Motivating Example  !,Our experimental evaluations across various LLM scales and arithmetic reasoning `<DATASET>datasets</DATASET>` underscore both the efficacy and economic viability of the proposed approach.  ## 🤔  Motivating Example  !,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\7672a064.txt,0.9948979591836735
17,[Image](motivatingexample.png) Figure 1.,[Image](motivatingexample.png) Figure 1.,[Image](motivatingexample.png) Figure 1.,[Image](motivatingexample.png) Figure 1.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\c18130f8.txt,1.0
18,**No prompt is perfect that works for all queries**.,**No prompt is perfect that works for all queries**.,**No prompt is perfect that works for all queries**.,**No prompt is perfect that works for all queries**.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\ff633b3c.txt,1.0
19,The optimal prompt is query-dependent.,The optimal prompt is query-dependent.,The optimal prompt is query-dependent.,The optimal prompt is query-dependent.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\6bfec50d.txt,1.0
20,Yet the seeking of such prompts can be costly and inefficient.,Yet the seeking of such prompts can be costly and inefficient.,Yet the seeking of such prompts can be costly and inefficient.,Yet the seeking of such prompts can be costly and inefficient.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\07f91257.txt,1.0
21,Prompt-OIRL optimizes prompt during inference time on a **query-dependent** level effectively and cost-efficiently.,<SOFTWARE>Prompt-OIRL</SOFTWARE> optimizes prompt during inference time on a **query-dependent** level effectively and cost-efficiently.,Prompt-OIRL optimizes prompt during inference time on a **query-dependent** level effectively and cost-efficiently.,Prompt-OIRL optimizes prompt during inference time on a **query-dependent** level effectively and cost-efficiently.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\079c8a96.txt,1.0
22,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.","(original chat logs with <SOFTWARE>GPT4</SOFTWARE> for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using <SOFTWARE>LLaMA2</SOFTWARE>)  1. get the [license to use <SOFTWARE>LLaMA-2</SOFTWARE>](https://ai.meta.com/<SOFTWARE>llama</SOFTWARE>/).  2. get access to the datasets: [<DATASET>SVAMP</DATASET>](https://github.com/arkilpatel/<DATASET>SVAMP</DATASET>), [<DATASET>GSM8K</DATASET>](https://huggingface.co/datasets/<DATASET>gsm8k</DATASET>), [<DATASET>MAWPS</DATASET>](https://github.com/sroy9/<DATASET>mawps</DATASET>)  ### Create a Virtual Env 1.","(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the license to use LLaMA-2(https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.","(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using <SOFTWARE>LLaMA2</SOFTWARE>)  1. get the <LICENSE>license to use LLaMA-2</LICENSE>(https://ai.meta.com/llama/).  2. get access to the datasets: [<DATASET>SVAMP</DATASET>](https://github.com/arkilpatel/SVAMP), [<DATASET>GSM8K</DATASET>](https://huggingface.co/datasets/gsm8k), [<DATASET>MAWPS</DATASET>](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\84065e62.txt,0.9982578397212544
23,Clone the repository ``` git clone git@github.com:holarissun/Prompt-OIRL.git ``` 2.,Clone the repository ``` <SOFTWARE>git</SOFTWARE> clone <SOFTWARE>git</SOFTWARE>@github.com:holarissun/Prompt-OIRL.git ``` 2.,Clone the repository ``` git clone git@github.com:holarissun/Prompt-OIRL.git ``` 2.,Clone the repository ``` git clone git@github.com:holarissun/<PROJECT>Prompt-OIRL</PROJECT>.git ``` 2.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\19742ddc.txt,1.0
24,"Create a new virtual environment with Python 3.10, e.g., ``` conda create --name prompt-oirl python==3.10 conda activate prompt-oirl cd Prompt-OIRL ``` 3.","Create a new virtual environment with <PROGLANG>Python 3.10</PROGLANG>, e.g., ``` <SOFTWARE>conda</SOFTWARE> create --name prompt-oirl <PROGLANG>python==3.10</PROGLANG> <SOFTWARE>conda</SOFTWARE> activate prompt-oirl cd <SOFTWARE>Prompt-OIRL</SOFTWARE> ``` 3.","Create a new virtual environment with Python 3.10, e.g., ``` conda create --name prompt-oirl python==3.10 conda activate prompt-oirl cd Prompt-OIRL ``` 3.","Create a new virtual environment with <PROGLANG>Python 3.10</PROGLANG>, e.g., ``` conda create --name prompt-oirl python==3.10 conda activate prompt-oirl cd <PROJECT>Prompt-OIRL</PROJECT> ``` 3.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\88d16c27.txt,1.0
25,Install the requirements ``` pip install -r requirements.txt ```  ### Reproduce the Main Results  #### Step 1.,Install the requirements ``` <SOFTWARE>pip</SOFTWARE> install -r requirements.txt ```  ### Reproduce the Main Results  #### Step 1.,Install the requirements ``` pip install -r requirements.txt ```  ### Reproduce the Main Results  #### Step 1.,Install the requirements ``` <PROGLANG>pip</PROGLANG> install -r requirements.txt ```  ### Reproduce the Main Results  #### Step 1.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\40bb43d9.txt,1.0
26,"(Optional, as we also released the offline dataset) Generate an offline dataset by interacting with the LLMs.","(Optional, as we also released the offline dataset) Generate an offline dataset by interacting with the LLMs.","(Optional, as we also released the offline dataset) Generate an offline dataset by interacting with the LLMs.","(Optional, as we also released the offline <DATASET>dataset</DATASET>) Generate an offline <DATASET>dataset</DATASET> by interacting with the <SOFTWARE>LLMs</SOFTWARE>.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\fa297852.txt,1.0
27,This step will take a long time --- typically a few days.,This step will take a long time --- typically a few days.,This step will take a long time --- typically a few days.,This step will take a long time --- typically a few days.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\19a40450.txt,1.0
28,"To avoid repeating such a computationally expensive (when running LLMs on local machines) or costly (when calling the commercial APIs like GPT3.5 or TigerBot) process, we have **released all the interactive logs with those LLMs collected in our experiments.**.","To avoid repeating such a computationally expensive (when running LLMs on local machines) or costly (when calling the commercial APIs like <SOFTWARE>GPT3.5</SOFTWARE> or <SOFTWARE>TigerBot</SOFTWARE>) process, we have **released all the interactive logs with those LLMs collected in our experiments.**.","To avoid repeating such a computationally expensive (when running LLMs on local machines) or costly (when calling the commercial APIs like `GPT3.5` or `TigerBot`) process, we have **released all the interactive logs with those LLMs collected in our experiments.**.","To avoid repeating such a computationally expensive (when running LLMs on local machines) or costly (when calling the commercial APIs like `<SOFTWARE>GPT3.5</SOFTWARE>` or `<SOFTWARE>TigerBot</SOFTWARE>`) process, we have **released all the interactive logs with those LLMs collected in our experiments.**.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\1046fb77.txt,0.9770992366412213
29,"Therefore, if you are just looking to explore the method and don't need to re-create everything from scratch, we recommend that you skip this step,   If you would like to reproduce the offline dataset with the llama2 model, you need to follow these steps:   ```  git clone git@github.com:facebookresearch/llama.git  ``` and then move ```Prompt-OIRL/llama_exps/llama_step1_gen_offline.py``` to the ```llama``` folder  then run the following command   ``` torchrun --nproc_per_node 1 llama_step1_gen_offline.py \     --ckpt_dir llama-2-7b-chat/ \     --tokenizer_path tokenizer.model \     --max_seq_len 512 --max_batch_size 8 --prompt_idx 0 --dataset_eval gsm8k  ```  #### Step 2.","Therefore, if you are just looking to explore the method and don't need to re-create everything from scratch, we recommend that you skip this step,   If you would like to reproduce the offline dataset with the <SOFTWARE>llama2</SOFTWARE> model, you need to follow these steps:   ```  <SOFTWARE>git</SOFTWARE> clone git@github.com:facebookresearch/llama.git  ``` and then move ```Prompt-OIRL/llama_exps/llama_step1_gen_offline.py``` to the ```llama``` folder  then run the following command   ``` <SOFTWARE>torchrun</SOFTWARE> --nproc_per_node 1 llama_step1_gen_offline.py \     --ckpt_dir llama-2-7b-chat/ \     --tokenizer_path tokenizer.model \     --max_seq_len 512 --max_batch_size 8 --prompt_idx 0 --dataset_eval <DATASET>gsm8k</DATASET>  ```  #### Step 2.","Therefore, if you are just looking to explore the method and don't need to re-create everything from scratch, we recommend that you skip this step,   If you would like to reproduce the offline dataset with the llama2 model, you need to follow these steps:   ```  git clone git@github.com:facebookresearch/llama.git  ``` and then move ```Prompt-OIRL/llama_exps/llama_step1_gen_offline.py``` to the ```llama``` folder  then run the following command   ``` torchrun --nproc_per_node 1 llama_step1_gen_offline.py \     --ckpt_dir llama-2-7b-chat/ \     --tokenizer_path tokenizer.model \     --max_seq_len 512 --max_batch_size 8 --prompt_idx 0 --dataset_eval gsm8k  ```  #### Step 2.","Therefore, if you are just looking to explore the method and don't need to re-create everything from scratch, we recommend that you skip this step,   If you would like to reproduce the offline <DATASET>dataset</DATASET> with the <SOFTWARE>llama2</SOFTWARE> model, you need to follow these steps:   ```  git clone git@github.com:facebookresearch/<SOFTWARE>llama</SOFTWARE>.git  ``` and then move ```Prompt-OIRL/<SOFTWARE>llama_exps/llama_step1_gen_offline.py</SOFTWARE>``` to the ```<SOFTWARE>llama</SOFTWARE>``` folder  then run the following command   ``` torchrun --nproc_per_node 1 <SOFTWARE>llama_step1_gen_offline.py</SOFTWARE> \     --ckpt_dir <SOFTWARE>llama-2-7b-chat/</SOFTWARE> \     --tokenizer_path <SOFTWARE>tokenizer.model</SOFTWARE> \     --max_seq_len 512 --max_batch_size 8 --prompt_idx 0 --dataset_eval <DATASET>gsm8k</DATASET>  ```  #### Step 2.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\d3ca8284.txt,1.0
30,Reorganize the collected offline data  For the easiest way to run the experiment it is recommended that you start with this step.,Reorganize the collected offline data  For the easiest way to run the experiment it is recommended that you start with this step.,Reorganize the collected offline data  For the easiest way to run the experiment it is recommended that you start with this step.,Reorganize the collected offline data  For the easiest way to run the experiment it is recommended that you start with this step.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\9cf85c4c.txt,1.0
31,"This step will take a few seconds to finish, it will do some file renaming and training-test split and save corresponding files to a new folder ```LMllama2```   ```  python3 llama_step2_reorg_data.py  ```   #### Step 3.","This step will take a few seconds to finish, it will do some file renaming and training-test split and save corresponding files to a new folder ```<SOFTWARE>LMllama2</SOFTWARE>```   ```  <SOFTWARE>python3</SOFTWARE> llama_step2_reorg_data.py  ```   #### Step 3.","This step will take a few seconds to finish, it will do some file renaming and training-test split and save corresponding files to a new folder ```LMllama2```   ```  python3 llama_step2_reorg_data.py  ```   #### Step 3.","This step will take a few seconds to finish, it will do some file renaming and training-test split and save corresponding files to a new folder ```LM<PROJECT>llama2</PROJECT>```   ```  <PROGLANG>python3</PROGLANG> <SOFTWARE>llama_step2_reorg_data.py</SOFTWARE>  ```   #### Step 3.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\9953cb92.txt,1.0
32,"Pre-process the offline data  This step will take a few seconds to finish, it will process the data and store embeddings and labels for different experiment settings (i.e., with different availability of training prompts) with ```.npy``` format files.","Pre-process the offline data  This step will take a few seconds to finish, it will process the data and store embeddings and labels for different experiment settings (i.e., with different availability of training prompts) with ```.npy``` format files.","Pre-process the offline data  This step will take a few seconds to finish, it will process the data and store embeddings and labels for different experiment settings (i.e., with different availability of training prompts) with ```.npy``` format files.","Pre-process the offline data  This step will take a few seconds to finish, it will process the data and store embeddings and labels for different experiment settings (i.e., with different availability of training prompts) with ```.npy``` format files.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\3d0066ee.txt,1.0
33,Note: please make sure that you select the relevant task by updating the code (the line marked with `NOTE`) in this and the following 2 steps.  ```  python3 llama_step3_data_processing.py  ```  #### Step 4.,Note: please make sure that you select the relevant task by updating the code (the line marked with `NOTE`) in this and the following 2 steps.  ```  <SOFTWARE>python3</SOFTWARE> llama_step3_data_processing.py  ```  #### Step 4.,Note: please make sure that you select the relevant task by updating the code (the line marked with `NOTE`) in this and the following 2 steps.  ```  python3 llama_step3_data_processing.py  ```  #### Step 4.,Note: please make sure that you select the relevant task by updating the code (the line marked with `NOTE`) in this and the following 2 steps.  ```  python3 <SOFTWARE>llama_step3_data_processing.py</SOFTWARE>  ```  #### Step 4.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\3dffcd2c.txt,1.0
34,"Proxy Reward Model Learning (i.e., Offline Prompt Evaluation)  This step will take a few minutes to a few hours to finish, depending on the algorithms chosen and the processor.","Proxy Reward Model Learning (i.e., Offline Prompt Evaluation)  This step will take a few minutes to a few hours to finish, depending on the algorithms chosen and the processor.","Proxy Reward Model Learning (i.e., Offline Prompt Evaluation)  This step will take a few minutes to a few hours to finish, depending on the algorithms chosen and the processor.","<PROJECT>Proxy Reward Model Learning</PROJECT> (i.e., <PROJECT>Offline Prompt Evaluation</PROJECT>)  This step will take a few minutes to a few hours to finish, depending on the algorithms chosen and the processor.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\ad1f09c3.txt,1.0
35,"In general, training an XGBoost reward model will take a bit longer time, and using a LightGBM reward model can be faster.  ```  python3 llama_step4_offline_evaluation.py  ``` - Note: you will need to download a missing embedding file from [this link](https://drive.google.com/file/d/1ER50FoLInO1pTr50dDjjZMBGPX-pXVA1/view?","In general, training an XGBoost reward model will take a bit longer time, and using a LightGBM reward model can be faster.  ```  <SOFTWARE>python3</SOFTWARE> llama_step4_offline_evaluation.py  ``` - Note: you will need to download a missing embedding file from [this link](https://drive.google.com/file/d/1ER50FoLInO1pTr50dDjjZMBGPX-pXVA1/view?","In general, training an XGBoost reward model will take a bit longer time, and using a LightGBM reward model can be faster.  ```  python3 llama_step4_offline_evaluation.py  ``` - Note: you will need to download a missing embedding file from [this link](https://drive.google.com/file/d/1ER50FoLInO1pTr50dDjjZMBGPX-pXVA1/view?","In general, training an <SOFTWARE>XGBoost</SOFTWARE> reward model will take a bit longer time, and using a <SOFTWARE>LightGBM</SOFTWARE> reward model can be faster.  ```  <PROGLANG>python3</PROGLANG> <SOFTWARE>llama_step4_offline_evaluation.py</SOFTWARE>  ``` - Note: you will need to download a missing embedding file from [this link](https://drive.google.com/file/d/1ER50FoLInO1pTr50dDjjZMBGPX-pXVA1/view?",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\6858a0f6.txt,1.0
36,usp=sharing) and place it in the `embeddings` directory to run this step.,usp=sharing) and place it in the `embeddings` directory to run this step.,usp=sharing) and place it in the `embeddings` directory to run this step.,usp=sharing) and place it in the `embeddings` directory to run this step.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\e8d09a36.txt,1.0
37,"(oversized for Github, ~ 230Mb)   #### Step 5.","(oversized for <SOFTWARE>Github</SOFTWARE>, ~ 230Mb)   #### Step 5.","(oversized for Github, ~ 230Mb)   #### Step 5.","(oversized for Github, ~ 230Mb)   #### Step 5.",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\617db719.txt,1.0
38,(Offline) Prompt Optimization  This step will take a few minutes to finish.,(Offline) Prompt Optimization  This step will take a few minutes to finish.,(Offline) Prompt Optimization  This step will take a few minutes to finish.,(Offline) <PROJECT>Prompt Optimization</PROJECT>  This step will take a few minutes to finish.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\a2df9414.txt,1.0
39,Evaluating the algorithms by interacting with the LLMs can also be an option but could be slower.,Evaluating the algorithms by interacting with the LLMs can also be an option but could be slower.,Evaluating the algorithms by interacting with the `LLMs` can also be an option but could be slower.,Evaluating the algorithms by interacting with the `<SOFTWARE>LLMs</SOFTWARE>` can also be an option but could be slower.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\0bb05acb.txt,0.9897959183673469
40,Results under different settings will be all saved to ```.csv``` files  ```  python3 llama_step5_offline_optimization.py  ```    ## 🚀  A Related Discussion on RLHF: Prompt-OIRL addresses the prompting problems in LLMs using an RLAIF approach.,Results under different settings will be all saved to ```.csv``` files  ```  <SOFTWARE>python3</SOFTWARE> llama_step5_offline_optimization.py  ```    ## 🚀  A Related Discussion on RLHF: <SOFTWARE>Prompt-OIRL</SOFTWARE> addresses the prompting problems in LLMs using an RLAIF approach.,Results under different settings will be all saved to ```.csv``` files  ```  python3 llama_step5_offline_optimization.py  ```    ## 🚀  A Related Discussion on RLHF: Prompt-OIRL addresses the prompting problems in LLMs using an RLAIF approach.,Results under different settings will be all saved to ```.csv``` files  ```  <PROGLANG>python3</PROGLANG> <SOFTWARE>llama_step5_offline_optimization.py</SOFTWARE>  ```    ## 🚀  A Related Discussion on RLHF: Prompt-OIRL addresses the prompting problems in LLMs using an RLAIF approach.,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\58c49203.txt,1.0
41,"For readers who are also interested in RLHF and RLAIF, and in the intersection between RL and LLM research, we would refer to our related positioning paper discussing RL in LLM research: [RL in the Era of LLMs: What is Essential?","For readers who are also interested in RLHF and RLAIF, and in the intersection between RL and LLM research, we would refer to our related positioning paper discussing RL in LLM research: [<PUBLICATION>RL in the Era of LLMs: What is Essential?</PUBLICATION>","For readers who are also interested in RLHF and RLAIF, and in the intersection between RL and LLM research, we would refer to our related positioning paper discussing RL in LLM research: [RL in the Era of LLMs: What is Essential?","For readers who are also interested in RLHF and RLAIF, and in the intersection between RL and LLM research, we would refer to our related positioning paper discussing RL in LLM research: [<PUBLICATION>RL in the Era of LLMs: What is Essential?</PUBLICATION>",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\074a9b38.txt,1.0
42,What is Needed?,<PUBLICATION>What is Needed?</PUBLICATION>,What is Needed?,What is Needed?,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\8d08a598.txt,1.0
43,"RLHF, Prompting, and Beyond.]","<PUBLICATION>RLHF, Prompting, and Beyond</PUBLICATION>.]","`RLHF, Prompting, and Beyond.]`","`<PROGLANG>RLHF</PROGLANG>, Prompting, and Beyond.]`",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\8278bd25.txt,0.9666666666666667
44,"(https://arxiv.org/pdf/2310.06147.pdf)     ## 📚  BibTex Citation If you would like to cite our code or paper, please use  ``` @inproceedings{sun2023query,   title={Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL},   author={Sun, Hao and H{\""u}y{\""u}k, Alihan and van der Schaar, Mihaela},   booktitle={The Twelfth International Conference on Learning Representations},   year={2024} }   @article{sun2023reinforcement,   title={Reinforcement Learning in the Era of LLMs: What is Essential?","(https://arxiv.org/pdf/2310.06147.pdf)     ## 📚  BibTex Citation If you would like to cite our code or paper, please use  ``` @inproceedings{sun2023query,   title={<PUBLICATION>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</PUBLICATION>},   author={Sun, Hao and H{\""u}y{\""u}k, Alihan and van der Schaar, Mihaela},   booktitle={The <CONFERENCE>Twelfth International Conference on Learning Representations</CONFERENCE>},   year={2024} }   @article{sun2023reinforcement,   title={<PUBLICATION>Reinforcement Learning in the Era of LLMs: What is Essential?</PUBLICATION>","(https://arxiv.org/pdf/2310.06147.pdf)     ## 📚  BibTex Citation If you would like to cite our code or paper, please use  ``` @inproceedings{sun2023query,   title={Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL},   author={Sun, Hao and H{\""u}y{\""u}k, Alihan and van der Schaar, Mihaela},   booktitle={The Twelfth International Conference on Learning Representations},   year={2024} }   @article{sun2023reinforcement,   title={Reinforcement Learning in the Era of LLMs: What is Essential?","(https://arxiv.org/pdf/2310.06147.pdf)     ## 📚  BibTex Citation If you would like to cite our code or paper, please use  ``` @inproceedings{sun2023query,   title={<PUBLICATION>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</PUBLICATION>},   author={Sun, Hao and H{\""u}y{\""u}k, Alihan and van der Schaar, Mihaela},   booktitle={<CONFERENCE>The Twelfth International Conference on Learning Representations</CONFERENCE>},   year={2024} }   @article{sun2023reinforcement,   title={<PUBLICATION>Reinforcement Learning in the Era of LLMs: What is Essential?</PUBLICATION>",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\93e73d22.txt,1.0
45,What is needed?,<PUBLICATION>What is needed?</PUBLICATION>,What is needed?,What is needed?,../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\b41390aa.txt,1.0
46,"An RL Perspective on RLHF, Prompting, and Beyond},   author={Sun, Hao},   journal={arXiv preprint arXiv:2310.06147},   year={2023} }","<PUBLICATION></PUBLICATION>},   author={Sun, Hao},   journal={arXiv preprint arXiv:2310.06147},   year={2023} }","An RL Perspective on RLHF, Prompting, and Beyond,   author={Sun, Hao},   journal={arXiv preprint arXiv:2310.06147},   year={2023} }","<PUBLICATION>An RL Perspective on RLHF, Prompting, and Beyond</PUBLICATION>,   author={Sun, Hao},   journal={<PUBLICATION>arXiv preprint arXiv:2310.06147</PUBLICATION>},   year={2023} }",../results/deepseek-chat/prompt-0/zzz_vanderschaarlab_prompt-oirl_main_README.md.tsv\4efcc858.txt,0.9961977186311787
