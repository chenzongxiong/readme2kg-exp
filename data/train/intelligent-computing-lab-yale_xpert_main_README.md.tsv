#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# XPert: Peripheral Circuit & Neural Architecture Co-search for Area and Energy-efficient Xbar-based Computing (Design & Automation Conference, 2023)
#Text=The hardware-efficiency and accuracy of Deep Neural Networks (DNNs) implemented on In-memory Computing (IMC) architectures primarily depend on the DNN architecture and the peripheral circuit parameters.
1-1	0-1	#	_	_	
1-2	2-7	XPert	*[1]	PUBLICATION[1]	
1-3	7-8	:	*[1]	PUBLICATION[1]	
1-4	9-19	Peripheral	*[1]	PUBLICATION[1]	
1-5	20-27	Circuit	*[1]	PUBLICATION[1]	
1-6	28-29	&	*[1]	PUBLICATION[1]	
1-7	30-36	Neural	*[1]	PUBLICATION[1]	
1-8	37-49	Architecture	*[1]	PUBLICATION[1]	
1-9	50-59	Co-search	*[1]	PUBLICATION[1]	
1-10	60-63	for	*[1]	PUBLICATION[1]	
1-11	64-68	Area	*[1]	PUBLICATION[1]	
1-12	69-72	and	*[1]	PUBLICATION[1]	
1-13	73-89	Energy-efficient	*[1]	PUBLICATION[1]	
1-14	90-100	Xbar-based	*[1]	PUBLICATION[1]	
1-15	101-110	Computing	*[1]	PUBLICATION[1]	
1-16	111-112	(	_	_	
1-17	112-118	Design	*[2]	CONFERENCE[2]	
1-18	119-120	&	*[2]	CONFERENCE[2]	
1-19	121-131	Automation	*[2]	CONFERENCE[2]	
1-20	132-142	Conference	*[2]	CONFERENCE[2]	
1-21	142-143	,	_	_	
1-22	144-148	2023	_	_	
1-23	148-149	)	_	_	
1-24	150-153	The	_	_	
1-25	154-173	hardware-efficiency	_	_	
1-26	174-177	and	_	_	
1-27	178-186	accuracy	_	_	
1-28	187-189	of	_	_	
1-29	190-194	Deep	_	_	
1-30	195-201	Neural	_	_	
1-31	202-210	Networks	_	_	
1-32	211-212	(	_	_	
1-33	212-216	DNNs	_	_	
1-34	216-217	)	_	_	
1-35	218-229	implemented	_	_	
1-36	230-232	on	_	_	
1-37	233-242	In-memory	_	_	
1-38	243-252	Computing	_	_	
1-39	253-254	(	_	_	
1-40	254-257	IMC	_	_	
1-41	257-258	)	_	_	
1-42	259-272	architectures	_	_	
1-43	273-282	primarily	_	_	
1-44	283-289	depend	_	_	
1-45	290-292	on	_	_	
1-46	293-296	the	_	_	
1-47	297-300	DNN	_	_	
1-48	301-313	architecture	_	_	
1-49	314-317	and	_	_	
1-50	318-321	the	_	_	
1-51	322-332	peripheral	_	_	
1-52	333-340	circuit	_	_	
1-53	341-351	parameters	_	_	
1-54	351-352	.	_	_	

#Text=It is therefore essential to holistically co-search the network and peripheral parameters to achieve optimal performance.
2-1	353-355	It	_	_	
2-2	356-358	is	_	_	
2-3	359-368	therefore	_	_	
2-4	369-378	essential	_	_	
2-5	379-381	to	_	_	
2-6	382-394	holistically	_	_	
2-7	395-404	co-search	_	_	
2-8	405-408	the	_	_	
2-9	409-416	network	_	_	
2-10	417-420	and	_	_	
2-11	421-431	peripheral	_	_	
2-12	432-442	parameters	_	_	
2-13	443-445	to	_	_	
2-14	446-453	achieve	_	_	
2-15	454-461	optimal	_	_	
2-16	462-473	performance	_	_	
2-17	473-474	.	_	_	

#Text=To this end, we propose XPert, which co-searches network architecture in tandem with peripheral parameters such as the type and precision of analog-to-digital converters, crossbar column sharing and the layer-specific input precision using an optimization-based design space exploration.
3-1	475-477	To	_	_	
3-2	478-482	this	_	_	
3-3	483-486	end	_	_	
3-4	486-487	,	_	_	
3-5	488-490	we	_	_	
3-6	491-498	propose	_	_	
3-7	499-504	XPert	_	_	
3-8	504-505	,	_	_	
3-9	506-511	which	_	_	
3-10	512-523	co-searches	_	_	
3-11	524-531	network	_	_	
3-12	532-544	architecture	_	_	
3-13	545-547	in	_	_	
3-14	548-554	tandem	_	_	
3-15	555-559	with	_	_	
3-16	560-570	peripheral	_	_	
3-17	571-581	parameters	_	_	
3-18	582-586	such	_	_	
3-19	587-589	as	_	_	
3-20	590-593	the	_	_	
3-21	594-598	type	_	_	
3-22	599-602	and	_	_	
3-23	603-612	precision	_	_	
3-24	613-615	of	_	_	
3-25	616-633	analog-to-digital	_	_	
3-26	634-644	converters	_	_	
3-27	644-645	,	_	_	
3-28	646-654	crossbar	_	_	
3-29	655-661	column	_	_	
3-30	662-669	sharing	_	_	
3-31	670-673	and	_	_	
3-32	674-677	the	_	_	
3-33	678-692	layer-specific	_	_	
3-34	693-698	input	_	_	
3-35	699-708	precision	_	_	
3-36	709-714	using	_	_	
3-37	715-717	an	_	_	
3-38	718-736	optimization-based	_	_	
3-39	737-743	design	_	_	
3-40	744-749	space	_	_	
3-41	750-761	exploration	_	_	
3-42	761-762	.	_	_	

#Text=Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lower EDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%) accuracy for CIFAR10 (TinyImagenet) datasets.
4-1	763-771	Compared	_	_	
4-2	772-774	to	_	_	
4-3	775-780	VGG16	_	_	
4-4	781-790	baselines	_	_	
4-5	790-791	,	_	_	
4-6	792-797	XPert	_	_	
4-7	798-806	achieves	_	_	
4-8	807-813	10.24x	_	_	
4-9	814-815	(	_	_	
4-10	815-819	4.7x	_	_	
4-11	819-820	)	_	_	
4-12	821-826	lower	_	_	
4-13	827-831	EDAP	_	_	
4-14	831-832	,	_	_	
4-15	833-838	1.72x	_	_	
4-16	839-840	(	_	_	
4-17	840-845	1.62x	_	_	
4-18	845-846	)	_	_	
4-19	847-853	higher	_	_	
4-20	854-858	TOPS	_	_	
4-21	858-859	/	_	_	
4-22	859-860	W	_	_	
4-23	860-861	,	_	_	
4-24	861-866	1.93x	_	_	
4-25	867-868	(	_	_	
4-26	868-870	3x	_	_	
4-27	870-871	)	_	_	
4-28	872-878	higher	_	_	
4-29	879-883	TOPS	_	_	
4-30	883-884	/	_	_	
4-31	884-887	mm2	_	_	
4-32	888-890	at	_	_	
4-33	891-897	92.46%	_	_	
4-34	898-899	(	_	_	
4-35	899-904	56.7%	_	_	
4-36	904-905	)	_	_	
4-37	906-914	accuracy	_	_	
4-38	915-918	for	_	_	
4-39	919-926	CIFAR10	*[3]	DATASET[3]	
4-40	927-928	(	*[3]	DATASET[3]	
4-41	928-940	TinyImagenet	*[3]	DATASET[3]	
4-42	940-941	)	*[3]	DATASET[3]	
4-43	942-950	datasets	_	_	
4-44	950-951	.	_	_	

#Text=The paper is available at https://arxiv.org/abs/2303.17646
#Text=
#Text=# Instructions for running Phase1-Cosearch
#Text=
#Text=## Running the Co-search
#Text=Run the Code using 
#Text=```
#Text=python Phase1_VGG16_backbone.py with the variables described below.
#Text=```
#Text=## Variable Description 
#Text=
#Text=```
#Text=--lr: Learning Rate
#Text=--hw_params: Tuple of #PEs per Tile, #Crossbars per PE, #Tiles, Crossbar Size
#Text=--target_latency: Target Latency
#Text=--target_area: Target Area Constraint (mm^2)
#Text=--epochs: Total Number of Search Epochs
#Text=--wt_prec: Weight Precision
#Text=--cellbit: Number of bits per NVM device
#Text=--area_tolerance: The uncerainity in the on-chip area tolerated in the searched model. 
#Text=```
#Text=# Instructions for XPertSim C++ Evaluation
#Text=
#Text=## Step 1: Create the Network_custom.csv file
#Text=
#Text=The Network_custom.csv file contains the DNN information required for evaluation.
5-1	952-955	The	_	_	
5-2	956-961	paper	_	_	
5-3	962-964	is	_	_	
5-4	965-974	available	_	_	
5-5	975-977	at	_	_	
5-6	978-983	https	_	_	
5-7	983-984	:	_	_	
5-8	984-985	/	_	_	
5-9	985-986	/	_	_	
5-10	986-995	arxiv.org	_	_	
5-11	995-996	/	_	_	
5-12	996-999	abs	_	_	
5-13	999-1000	/	_	_	
5-14	1000-1010	2303.17646	_	_	
5-15	1012-1013	#	_	_	
5-16	1014-1026	Instructions	_	_	
5-17	1027-1030	for	_	_	
5-18	1031-1038	running	_	_	
5-19	1039-1045	Phase1	_	_	
5-20	1045-1046	-	_	_	
5-21	1046-1054	Cosearch	_	_	
5-22	1056-1057	#	_	_	
5-23	1057-1058	#	_	_	
5-24	1059-1066	Running	_	_	
5-25	1067-1070	the	_	_	
5-26	1071-1080	Co-search	_	_	
5-27	1081-1084	Run	_	_	
5-28	1085-1088	the	_	_	
5-29	1089-1093	Code	_	_	
5-30	1094-1099	using	_	_	
5-31	1101-1102	`	_	_	
5-32	1102-1103	`	_	_	
5-33	1103-1104	`	_	_	
5-34	1105-1111	python	*	SOFTWARE	
5-35	1112-1118	Phase1	_	_	
5-36	1118-1119	_	_	_	
5-37	1119-1124	VGG16	_	_	
5-38	1124-1125	_	_	_	
5-39	1125-1136	backbone.py	_	_	
5-40	1137-1141	with	_	_	
5-41	1142-1145	the	_	_	
5-42	1146-1155	variables	_	_	
5-43	1156-1165	described	_	_	
5-44	1166-1171	below	_	_	
5-45	1171-1172	.	_	_	
5-46	1173-1174	`	_	_	
5-47	1174-1175	`	_	_	
5-48	1175-1176	`	_	_	
5-49	1177-1178	#	_	_	
5-50	1178-1179	#	_	_	
5-51	1180-1188	Variable	_	_	
5-52	1189-1200	Description	_	_	
5-53	1203-1204	`	_	_	
5-54	1204-1205	`	_	_	
5-55	1205-1206	`	_	_	
5-56	1207-1208	-	_	_	
5-57	1208-1209	-	_	_	
5-58	1209-1211	lr	_	_	
5-59	1211-1212	:	_	_	
5-60	1213-1221	Learning	_	_	
5-61	1222-1226	Rate	_	_	
5-62	1227-1228	-	_	_	
5-63	1228-1229	-	_	_	
5-64	1229-1238	hw_params	_	_	
5-65	1238-1239	:	_	_	
5-66	1240-1245	Tuple	_	_	
5-67	1246-1248	of	_	_	
5-68	1249-1250	#	_	_	
5-69	1250-1253	PEs	_	_	
5-70	1254-1257	per	_	_	
5-71	1258-1262	Tile	_	_	
5-72	1262-1263	,	_	_	
5-73	1264-1265	#	_	_	
5-74	1265-1274	Crossbars	_	_	
5-75	1275-1278	per	_	_	
5-76	1279-1281	PE	_	_	
5-77	1281-1282	,	_	_	
5-78	1283-1284	#	_	_	
5-79	1284-1289	Tiles	_	_	
5-80	1289-1290	,	_	_	
5-81	1291-1299	Crossbar	_	_	
5-82	1300-1304	Size	_	_	
5-83	1305-1306	-	_	_	
5-84	1306-1307	-	_	_	
5-85	1307-1321	target_latency	_	_	
5-86	1321-1322	:	_	_	
5-87	1323-1329	Target	_	_	
5-88	1330-1337	Latency	_	_	
5-89	1338-1339	-	_	_	
5-90	1339-1340	-	_	_	
5-91	1340-1351	target_area	_	_	
5-92	1351-1352	:	_	_	
5-93	1353-1359	Target	_	_	
5-94	1360-1364	Area	_	_	
5-95	1365-1375	Constraint	_	_	
5-96	1376-1377	(	_	_	
5-97	1377-1379	mm	_	_	
5-98	1379-1380	^	_	_	
5-99	1380-1381	2	_	_	
5-100	1381-1382	)	_	_	
5-101	1383-1384	-	_	_	
5-102	1384-1385	-	_	_	
5-103	1385-1391	epochs	_	_	
5-104	1391-1392	:	_	_	
5-105	1393-1398	Total	_	_	
5-106	1399-1405	Number	_	_	
5-107	1406-1408	of	_	_	
5-108	1409-1415	Search	_	_	
5-109	1416-1422	Epochs	_	_	
5-110	1423-1424	-	_	_	
5-111	1424-1425	-	_	_	
5-112	1425-1432	wt_prec	_	_	
5-113	1432-1433	:	_	_	
5-114	1434-1440	Weight	_	_	
5-115	1441-1450	Precision	_	_	
5-116	1451-1452	-	_	_	
5-117	1452-1453	-	_	_	
5-118	1453-1460	cellbit	_	_	
5-119	1460-1461	:	_	_	
5-120	1462-1468	Number	_	_	
5-121	1469-1471	of	_	_	
5-122	1472-1476	bits	_	_	
5-123	1477-1480	per	_	_	
5-124	1481-1484	NVM	_	_	
5-125	1485-1491	device	_	_	
5-126	1492-1493	-	_	_	
5-127	1493-1494	-	_	_	
5-128	1494-1508	area_tolerance	_	_	
5-129	1508-1509	:	_	_	
5-130	1510-1513	The	_	_	
5-131	1514-1525	uncerainity	_	_	
5-132	1526-1528	in	_	_	
5-133	1529-1532	the	_	_	
5-134	1533-1540	on-chip	_	_	
5-135	1541-1545	area	_	_	
5-136	1546-1555	tolerated	_	_	
5-137	1556-1558	in	_	_	
5-138	1559-1562	the	_	_	
5-139	1563-1571	searched	_	_	
5-140	1572-1577	model	_	_	
5-141	1577-1578	.	_	_	
5-142	1580-1581	`	_	_	
5-143	1581-1582	`	_	_	
5-144	1582-1583	`	_	_	
5-145	1584-1585	#	_	_	
5-146	1586-1598	Instructions	_	_	
5-147	1599-1602	for	_	_	
5-148	1603-1611	XPertSim	_	_	
5-149	1612-1613	C	*[4]	PROGLANG[4]	
5-150	1613-1614	+	*[4]	PROGLANG[4]	
5-151	1614-1615	+	*[4]	PROGLANG[4]	
5-152	1616-1626	Evaluation	_	_	
5-153	1628-1629	#	_	_	
5-154	1629-1630	#	_	_	
5-155	1631-1635	Step	_	_	
5-156	1636-1637	1	_	_	
5-157	1637-1638	:	_	_	
5-158	1639-1645	Create	_	_	
5-159	1646-1649	the	_	_	
5-160	1650-1668	Network_custom.csv	_	_	
5-161	1669-1673	file	_	_	
5-162	1675-1678	The	_	_	
5-163	1679-1697	Network_custom.csv	_	_	
5-164	1698-1702	file	_	_	
5-165	1703-1711	contains	_	_	
5-166	1712-1715	the	_	_	
5-167	1716-1719	DNN	_	_	
5-168	1720-1731	information	_	_	
5-169	1732-1740	required	_	_	
5-170	1741-1744	for	_	_	
5-171	1745-1755	evaluation	_	_	
5-172	1755-1756	.	_	_	

#Text=For each layer, we append an additional layer to ensure 64 Crossbar arrays per tile.
6-1	1757-1760	For	_	_	
6-2	1761-1765	each	_	_	
6-3	1766-1771	layer	_	_	
6-4	1771-1772	,	_	_	
6-5	1773-1775	we	_	_	
6-6	1776-1782	append	_	_	
6-7	1783-1785	an	_	_	
6-8	1786-1796	additional	_	_	
6-9	1797-1802	layer	_	_	
6-10	1803-1805	to	_	_	
6-11	1806-1812	ensure	_	_	
6-12	1813-1815	64	_	_	
6-13	1816-1824	Crossbar	_	_	
6-14	1825-1831	arrays	_	_	
6-15	1832-1835	per	_	_	
6-16	1836-1840	tile	_	_	
6-17	1840-1841	.	_	_	

#Text=Each row has the following format: 
#Text=
#Text=feature size, feature size, input channels, kernel size, kernel size, output channels, max-pool flag, padding, column sharing, ADC type (1: Flash ADC, 0: SAR ADC), Input Precision, ADC Precision.
#Text=
#Text=## Step 2: Update the VGG.py with the desired model
#Text=
#Text=Add the desired model configurations to the cfg_list and make sure to add the cfg variable in the "def VGG8()" function.
7-1	1842-1846	Each	_	_	
7-2	1847-1850	row	_	_	
7-3	1851-1854	has	_	_	
7-4	1855-1858	the	_	_	
7-5	1859-1868	following	_	_	
7-6	1869-1875	format	_	_	
7-7	1875-1876	:	_	_	
7-8	1879-1886	feature	_	_	
7-9	1887-1891	size	_	_	
7-10	1891-1892	,	_	_	
7-11	1893-1900	feature	_	_	
7-12	1901-1905	size	_	_	
7-13	1905-1906	,	_	_	
7-14	1907-1912	input	_	_	
7-15	1913-1921	channels	_	_	
7-16	1921-1922	,	_	_	
7-17	1923-1929	kernel	_	_	
7-18	1930-1934	size	_	_	
7-19	1934-1935	,	_	_	
7-20	1936-1942	kernel	_	_	
7-21	1943-1947	size	_	_	
7-22	1947-1948	,	_	_	
7-23	1949-1955	output	_	_	
7-24	1956-1964	channels	_	_	
7-25	1964-1965	,	_	_	
7-26	1966-1974	max-pool	_	_	
7-27	1975-1979	flag	_	_	
7-28	1979-1980	,	_	_	
7-29	1981-1988	padding	_	_	
7-30	1988-1989	,	_	_	
7-31	1990-1996	column	_	_	
7-32	1997-2004	sharing	_	_	
7-33	2004-2005	,	_	_	
7-34	2006-2009	ADC	_	_	
7-35	2010-2014	type	_	_	
7-36	2015-2016	(	_	_	
7-37	2016-2017	1	_	_	
7-38	2017-2018	:	_	_	
7-39	2019-2024	Flash	_	_	
7-40	2025-2028	ADC	_	_	
7-41	2028-2029	,	_	_	
7-42	2030-2031	0	_	_	
7-43	2031-2032	:	_	_	
7-44	2033-2036	SAR	_	_	
7-45	2037-2040	ADC	_	_	
7-46	2040-2041	)	_	_	
7-47	2041-2042	,	_	_	
7-48	2043-2048	Input	_	_	
7-49	2049-2058	Precision	_	_	
7-50	2058-2059	,	_	_	
7-51	2060-2063	ADC	_	_	
7-52	2064-2073	Precision	_	_	
7-53	2073-2074	.	_	_	
7-54	2076-2077	#	_	_	
7-55	2077-2078	#	_	_	
7-56	2079-2083	Step	_	_	
7-57	2084-2085	2	_	_	
7-58	2085-2086	:	_	_	
7-59	2087-2093	Update	_	_	
7-60	2094-2097	the	_	_	
7-61	2098-2104	VGG.py	_	_	
7-62	2105-2109	with	_	_	
7-63	2110-2113	the	_	_	
7-64	2114-2121	desired	_	_	
7-65	2122-2127	model	_	_	
7-66	2129-2132	Add	_	_	
7-67	2133-2136	the	_	_	
7-68	2137-2144	desired	_	_	
7-69	2145-2150	model	_	_	
7-70	2151-2165	configurations	_	_	
7-71	2166-2168	to	_	_	
7-72	2169-2172	the	_	_	
7-73	2173-2181	cfg_list	_	_	
7-74	2182-2185	and	_	_	
7-75	2186-2190	make	_	_	
7-76	2191-2195	sure	_	_	
7-77	2196-2198	to	_	_	
7-78	2199-2202	add	_	_	
7-79	2203-2206	the	_	_	
7-80	2207-2210	cfg	_	_	
7-81	2211-2219	variable	_	_	
7-82	2220-2222	in	_	_	
7-83	2223-2226	the	_	_	
7-84	2227-2228	"	_	_	
7-85	2228-2231	def	_	_	
7-86	2232-2236	VGG8	_	_	
7-87	2236-2237	(	_	_	
7-88	2237-2238	)	_	_	
7-89	2238-2239	"	_	_	
7-90	2240-2248	function	_	_	
7-91	2248-2249	.	_	_	

#Text=The model can be defined inside the cfg_list by using the following format: 
#Text=
#Text=(Layer Type: ('C': Convolutional layer, 'L': Linear Layer), input channels, output channels, kernel size, output size ('same': if output feature size is same as input feature size), scaling factor.
8-1	2251-2254	The	_	_	
8-2	2255-2260	model	_	_	
8-3	2261-2264	can	_	_	
8-4	2265-2267	be	_	_	
8-5	2268-2275	defined	_	_	
8-6	2276-2282	inside	_	_	
8-7	2283-2286	the	_	_	
8-8	2287-2295	cfg_list	_	_	
8-9	2296-2298	by	_	_	
8-10	2299-2304	using	_	_	
8-11	2305-2308	the	_	_	
8-12	2309-2318	following	_	_	
8-13	2319-2325	format	_	_	
8-14	2325-2326	:	_	_	
8-15	2329-2330	(	_	_	
8-16	2330-2335	Layer	_	_	
8-17	2336-2340	Type	_	_	
8-18	2340-2341	:	_	_	
8-19	2342-2343	(	_	_	
8-20	2343-2344	'	_	_	
8-21	2344-2345	C	_	_	
8-22	2345-2346	'	_	_	
8-23	2346-2347	:	_	_	
8-24	2348-2361	Convolutional	_	_	
8-25	2362-2367	layer	_	_	
8-26	2367-2368	,	_	_	
8-27	2369-2370	'	_	_	
8-28	2370-2371	L	_	_	
8-29	2371-2372	'	_	_	
8-30	2372-2373	:	_	_	
8-31	2374-2380	Linear	_	_	
8-32	2381-2386	Layer	_	_	
8-33	2386-2387	)	_	_	
8-34	2387-2388	,	_	_	
8-35	2389-2394	input	_	_	
8-36	2395-2403	channels	_	_	
8-37	2403-2404	,	_	_	
8-38	2405-2411	output	_	_	
8-39	2412-2420	channels	_	_	
8-40	2420-2421	,	_	_	
8-41	2422-2428	kernel	_	_	
8-42	2429-2433	size	_	_	
8-43	2433-2434	,	_	_	
8-44	2435-2441	output	_	_	
8-45	2442-2446	size	_	_	
8-46	2447-2448	(	_	_	
8-47	2448-2449	'	_	_	
8-48	2449-2453	same	_	_	
8-49	2453-2454	'	_	_	
8-50	2454-2455	:	_	_	
8-51	2456-2458	if	_	_	
8-52	2459-2465	output	_	_	
8-53	2466-2473	feature	_	_	
8-54	2474-2478	size	_	_	
8-55	2479-2481	is	_	_	
8-56	2482-2486	same	_	_	
8-57	2487-2489	as	_	_	
8-58	2490-2495	input	_	_	
8-59	2496-2503	feature	_	_	
8-60	2504-2508	size	_	_	
8-61	2508-2509	)	_	_	
8-62	2509-2510	,	_	_	
8-63	2511-2518	scaling	_	_	
8-64	2519-2525	factor	_	_	
8-65	2525-2526	.	_	_	

#Text=Like Network_custom.csv we append each layer with ('C', 64, 512, 3, 'same', 8). 
#Text=
#Text=## Step 3 Run the Evaluation
#Text=
#Text=Run the inference.py using "python inference.py --model custom"
#Text=
#Text=
#Text=# Acknowledgement
#Text=This repository is adapted from the Neurosim platform https://github.com/neurosim/DNN_NeuroSim_V1.3.
9-1	2529-2533	Like	_	_	
9-2	2534-2552	Network_custom.csv	_	_	
9-3	2553-2555	we	_	_	
9-4	2556-2562	append	_	_	
9-5	2563-2567	each	_	_	
9-6	2568-2573	layer	_	_	
9-7	2574-2578	with	_	_	
9-8	2579-2580	(	_	_	
9-9	2580-2581	'	_	_	
9-10	2581-2582	C	_	_	
9-11	2582-2583	'	_	_	
9-12	2583-2584	,	_	_	
9-13	2585-2587	64	_	_	
9-14	2587-2588	,	_	_	
9-15	2589-2592	512	_	_	
9-16	2592-2593	,	_	_	
9-17	2594-2595	3	_	_	
9-18	2595-2596	,	_	_	
9-19	2597-2598	'	_	_	
9-20	2598-2602	same	_	_	
9-21	2602-2603	'	_	_	
9-22	2603-2604	,	_	_	
9-23	2605-2606	8	_	_	
9-24	2606-2607	)	_	_	
9-25	2607-2608	.	_	_	
9-26	2611-2612	#	_	_	
9-27	2612-2613	#	_	_	
9-28	2614-2618	Step	_	_	
9-29	2619-2620	3	_	_	
9-30	2621-2624	Run	_	_	
9-31	2625-2628	the	_	_	
9-32	2629-2639	Evaluation	_	_	
9-33	2641-2644	Run	_	_	
9-34	2645-2648	the	_	_	
9-35	2649-2661	inference.py	_	_	
9-36	2662-2667	using	_	_	
9-37	2668-2669	"	_	_	
9-38	2669-2675	python	_	_	
9-39	2676-2688	inference.py	_	_	
9-40	2689-2690	-	_	_	
9-41	2690-2691	-	_	_	
9-42	2691-2696	model	_	_	
9-43	2697-2703	custom	_	_	
9-44	2703-2704	"	_	_	
9-45	2707-2708	#	_	_	
9-46	2709-2724	Acknowledgement	_	_	
9-47	2725-2729	This	_	_	
9-48	2730-2740	repository	_	_	
9-49	2741-2743	is	_	_	
9-50	2744-2751	adapted	_	_	
9-51	2752-2756	from	_	_	
9-52	2757-2760	the	_	_	
9-53	2761-2769	Neurosim	_	_	
9-54	2770-2778	platform	_	_	
9-55	2779-2784	https	_	_	
9-56	2784-2785	:	_	_	
9-57	2785-2786	/	_	_	
9-58	2786-2787	/	_	_	
9-59	2787-2797	github.com	_	_	
9-60	2797-2798	/	_	_	
9-61	2798-2806	neurosim	_	_	
9-62	2806-2807	/	_	_	
9-63	2807-2824	DNN_NeuroSim_V1.3	_	_	
9-64	2824-2825	.	_	_	
