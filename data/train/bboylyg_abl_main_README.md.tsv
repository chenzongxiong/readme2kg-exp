#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# Anti-Backdoor Learning
#Text=
#Text=PyTorch Code for NeurIPS 2021 paper **[Anti-Backdoor Learning: Training Clean Models on Poisoned Data](https://arxiv.org/pdf/2110.11571.pdf)**.
#Text=
#Text=!
1-1	0-1	#	_	_	
1-2	2-15	Anti-Backdoor	_	_	
1-3	16-24	Learning	_	_	
1-4	26-33	PyTorch	*	SOFTWARE	
1-5	34-38	Code	_	_	
1-6	39-42	for	_	_	
1-7	43-50	NeurIPS	*[1]	CONFERENCE[1]	
1-8	51-55	2021	*[1]	CONFERENCE[1]	
1-9	56-61	paper	_	_	
1-10	62-63	*	_	_	
1-11	63-64	*	_	_	
1-12	64-65	[	_	_	
1-13	65-78	Anti-Backdoor	*[2]	PUBLICATION[2]	
1-14	79-87	Learning	*[2]	PUBLICATION[2]	
1-15	87-88	:	*[2]	PUBLICATION[2]	
1-16	89-97	Training	*[2]	PUBLICATION[2]	
1-17	98-103	Clean	*[2]	PUBLICATION[2]	
1-18	104-110	Models	*[2]	PUBLICATION[2]	
1-19	111-113	on	*[2]	PUBLICATION[2]	
1-20	114-122	Poisoned	*[2]	PUBLICATION[2]	
1-21	123-127	Data	*[2]	PUBLICATION[2]	
1-22	127-128	]	_	_	
1-23	128-129	(	_	_	
1-24	129-134	https	_	_	
1-25	134-135	:	_	_	
1-26	135-136	/	_	_	
1-27	136-137	/	_	_	
1-28	137-146	arxiv.org	_	_	
1-29	146-147	/	_	_	
1-30	147-150	pdf	_	_	
1-31	150-151	/	_	_	
1-32	151-161	2110.11571	_	_	
1-33	161-162	.	_	_	
1-34	162-165	pdf	_	_	
1-35	165-166	)	_	_	
1-36	166-167	*	_	_	
1-37	167-168	*	_	_	
1-38	168-169	.	_	_	
1-39	171-172	!	_	_	

#Text=[Python 3.6](https://img.shields.io/badge/python-3.6-DodgerBlue.svg?
2-1	172-173	[	_	_	
2-2	173-179	Python	_	_	
2-3	180-183	3.6	_	_	
2-4	183-184	]	_	_	
2-5	184-185	(	_	_	
2-6	185-190	https	_	_	
2-7	190-191	:	_	_	
2-8	191-192	/	_	_	
2-9	192-193	/	_	_	
2-10	193-207	img.shields.io	_	_	
2-11	207-208	/	_	_	
2-12	208-213	badge	_	_	
2-13	213-214	/	_	_	
2-14	214-220	python	_	_	
2-15	220-221	-	_	_	
2-16	221-224	3.6	_	_	
2-17	224-225	-	_	_	
2-18	225-239	DodgerBlue.svg	_	_	
2-19	239-240	?	_	_	

#Text=style=plastic)
#Text=!
3-1	240-245	style	_	_	
3-2	245-246	=	_	_	
3-3	246-253	plastic	_	_	
3-4	253-254	)	_	_	
3-5	255-256	!	_	_	

#Text=[Pytorch 1.10](https://img.shields.io/badge/pytorch-1.2.0-DodgerBlue.svg?
4-1	256-257	[	_	_	
4-2	257-264	Pytorch	*[3]	PROGLANG[3]	
4-3	265-269	1.10	*[3]	PROGLANG[3]	
4-4	269-270	]	_	_	
4-5	270-271	(	_	_	
4-6	271-276	https	_	_	
4-7	276-277	:	_	_	
4-8	277-278	/	_	_	
4-9	278-279	/	_	_	
4-10	279-293	img.shields.io	_	_	
4-11	293-294	/	_	_	
4-12	294-299	badge	_	_	
4-13	299-300	/	_	_	
4-14	300-307	pytorch	_	_	
4-15	307-308	-	_	_	
4-16	308-313	1.2.0	_	_	
4-17	313-314	-	_	_	
4-18	314-328	DodgerBlue.svg	_	_	
4-19	328-329	?	_	_	

#Text=style=plastic)
#Text=!
5-1	329-334	style	_	_	
5-2	334-335	=	_	_	
5-3	335-342	plastic	_	_	
5-4	342-343	)	_	_	
5-5	344-345	!	_	_	

#Text=[CUDA 10.0](https://img.shields.io/badge/cuda-10.0-DodgerBlue.svg?
6-1	345-346	[	_	_	
6-2	346-350	CUDA	*[4]	SOFTWARE[4]	
6-3	351-355	10.0	*[4]	SOFTWARE[4]	
6-4	355-356	]	_	_	
6-5	356-357	(	_	_	
6-6	357-362	https	_	_	
6-7	362-363	:	_	_	
6-8	363-364	/	_	_	
6-9	364-365	/	_	_	
6-10	365-379	img.shields.io	_	_	
6-11	379-380	/	_	_	
6-12	380-385	badge	_	_	
6-13	385-386	/	_	_	
6-14	386-390	cuda	*[5]	SOFTWARE[5]	
6-15	390-391	-	*[5]	SOFTWARE[5]	
6-16	391-395	10.0	*[5]	SOFTWARE[5]	
6-17	395-396	-	_	_	
6-18	396-410	DodgerBlue.svg	_	_	
6-19	410-411	?	_	_	

#Text=style=plastic)
#Text=!
7-1	411-416	style	_	_	
7-2	416-417	=	_	_	
7-3	417-424	plastic	_	_	
7-4	424-425	)	_	_	
7-5	426-427	!	_	_	

#Text=[License CC BY-NC](https://img.shields.io/badge/license-CC_BY--NC-DodgerBlue.svg?
8-1	427-428	[	_	_	
8-2	428-435	License	*[6]	LICENSE[6]	
8-3	436-438	CC	*[6]	LICENSE[6]	
8-4	439-444	BY-NC	*[6]	LICENSE[6]	
8-5	444-445	]	_	_	
8-6	445-446	(	_	_	
8-7	446-451	https	_	_	
8-8	451-452	:	_	_	
8-9	452-453	/	_	_	
8-10	453-454	/	_	_	
8-11	454-468	img.shields.io	_	_	
8-12	468-469	/	_	_	
8-13	469-474	badge	_	_	
8-14	474-475	/	_	_	
8-15	475-488	license-CC_BY	_	_	
8-15.1	483-488	CC_BY	*[7]	LICENSE[7]	
8-16	488-489	-	*[7]	LICENSE[7]	
8-17	489-490	-	*[7]	LICENSE[7]	
8-18	490-507	NC-DodgerBlue.svg	_	_	
8-18.1	490-492	NC	*[7]	LICENSE[7]	
8-19	507-508	?	_	_	

#Text=style=plastic)
#Text=
#Text=
#Text=## The Anti-Backdoor Learning Leaderboard
#Text=
#Text=We encourage submissions of anti-backdoor learning methods to our leaderboard.
9-1	508-513	style	_	_	
9-2	513-514	=	_	_	
9-3	514-521	plastic	_	_	
9-4	521-522	)	_	_	
9-5	525-526	#	_	_	
9-6	526-527	#	_	_	
9-7	528-531	The	_	_	
9-8	532-545	Anti-Backdoor	_	_	
9-9	546-554	Learning	_	_	
9-10	555-566	Leaderboard	_	_	
9-11	568-570	We	_	_	
9-12	571-580	encourage	_	_	
9-13	581-592	submissions	_	_	
9-14	593-595	of	_	_	
9-15	596-609	anti-backdoor	_	_	
9-16	610-618	learning	_	_	
9-17	619-626	methods	_	_	
9-18	627-629	to	_	_	
9-19	630-633	our	_	_	
9-20	634-645	leaderboard	_	_	
9-21	645-646	.	_	_	

#Text=**Evaluation**：We will run the submitted learning method on poisoned CIFAR-10 datasets by 10 backdoor attacks used in our paper, then test the Attack Sucess Rate (ASR) and Clean Accuracy (CA) of the final model.
10-1	649-650	*	_	_	
10-2	650-651	*	_	_	
10-3	651-661	Evaluation	_	_	
10-4	661-662	*	_	_	
10-5	662-663	*	_	_	
10-6	663-664	：	_	_	
10-7	664-666	We	_	_	
10-8	667-671	will	_	_	
10-9	672-675	run	_	_	
10-10	676-679	the	_	_	
10-11	680-689	submitted	_	_	
10-12	690-698	learning	_	_	
10-13	699-705	method	_	_	
10-14	706-708	on	_	_	
10-15	709-717	poisoned	_	_	
10-16	718-723	CIFAR	*[8]	DATASET[8]	
10-17	723-724	-	*[8]	DATASET[8]	
10-18	724-726	10	*[8]	DATASET[8]	
10-19	727-735	datasets	_	_	
10-20	736-738	by	_	_	
10-21	739-741	10	_	_	
10-22	742-750	backdoor	_	_	
10-23	751-758	attacks	_	_	
10-24	759-763	used	_	_	
10-25	764-766	in	_	_	
10-26	767-770	our	_	_	
10-27	771-776	paper	_	_	
10-28	776-777	,	_	_	
10-29	778-782	then	_	_	
10-30	783-787	test	_	_	
10-31	788-791	the	_	_	
10-32	792-798	Attack	*[9]	EVALMETRIC[9]	
10-33	799-805	Sucess	*[9]	EVALMETRIC[9]	
10-34	806-810	Rate	*[9]	EVALMETRIC[9]	
10-35	811-812	(	_	_	
10-36	812-815	ASR	*	EVALMETRIC	
10-37	815-816	)	_	_	
10-38	817-820	and	_	_	
10-39	821-826	Clean	*[10]	EVALMETRIC[10]	
10-40	827-835	Accuracy	*[10]	EVALMETRIC[10]	
10-41	836-837	(	_	_	
10-42	837-839	CA	*	EVALMETRIC	
10-43	839-840	)	_	_	
10-44	841-843	of	_	_	
10-45	844-847	the	_	_	
10-46	848-853	final	_	_	
10-47	854-859	model	_	_	
10-48	859-860	.	_	_	

#Text=**Update**: This leaderboard is created on 2021/10/21 and updated on 021/10/21
11-1	862-863	*	_	_	
11-2	863-864	*	_	_	
11-3	864-870	Update	_	_	
11-4	870-871	*	_	_	
11-5	871-872	*	_	_	
11-6	872-873	:	_	_	
11-7	874-878	This	_	_	
11-8	879-890	leaderboard	_	_	
11-9	891-893	is	_	_	
11-10	894-901	created	_	_	
11-11	902-904	on	_	_	
11-12	905-909	2021	_	_	
11-13	909-910	/	_	_	
11-14	910-912	10	_	_	
11-15	912-913	/	_	_	
11-16	913-915	21	_	_	
11-17	916-919	and	_	_	
11-18	920-927	updated	_	_	
11-19	928-930	on	_	_	
11-20	931-934	021	_	_	
11-21	934-935	/	_	_	
11-22	935-937	10	_	_	
11-23	937-938	/	_	_	
11-24	938-940	21	_	_	

#Text=.
12-1	940-941	.	_	_	

#Text=| #     |           Paper            |    Venue     | Poisoned data | Architecture | Attack | ASR (%)| CA (%)|
#Text=| ----- | :------------------------: | :----------: | :------------: | :----------: | :---------: | :-----------: | :----------: |
#Text=| **1** | **[ABL]()** | NeurIPS 2021 |  *available* |    WRN-16-1    |   BadNets   |     3.04     |    86.11      |
#Text=| **2** |                            |              |                |              |             |               |              |
#Text=| **3** |                            |              |                |              |             |               |              |
#Text=| **4** |                            |              |                |              |             |               |              |
#Text=| **5** |                            |              |                |              |             |               |              |
#Text=| **6** |                            |              |                |              |             |               |              |
#Text=| **7** |                            |              |                |              |             |               |              |
#Text=| **8** |                            |              |                |              |             |               |              |
#Text=
#Text=------
#Text=
#Text=## Verifying the unlearning effect of ABL with 1% isolated data: 
#Text=### An example with a pretrained model
#Text=WRN-16-1, CIFAR-10, GridTrigger, target label 0, weights: `.
13-1	943-944	|	_	_	
13-2	945-946	#	_	_	
13-3	951-952	|	_	_	
13-4	963-968	Paper	_	_	
13-5	980-981	|	_	_	
13-6	985-990	Venue	_	_	
13-7	995-996	|	_	_	
13-8	997-1005	Poisoned	_	_	
13-9	1006-1010	data	_	_	
13-10	1011-1012	|	_	_	
13-11	1013-1025	Architecture	_	_	
13-12	1026-1027	|	_	_	
13-13	1028-1034	Attack	_	_	
13-14	1035-1036	|	_	_	
13-15	1037-1040	ASR	*	EVALMETRIC	
13-16	1041-1042	(	_	_	
13-17	1042-1043	%	_	_	
13-18	1043-1044	)	_	_	
13-19	1044-1045	|	_	_	
13-20	1046-1048	CA	*	EVALMETRIC	
13-21	1049-1050	(	_	_	
13-22	1050-1051	%	_	_	
13-23	1051-1052	)	_	_	
13-24	1052-1053	|	_	_	
13-25	1054-1055	|	_	_	
13-26	1056-1057	-	_	_	
13-27	1057-1058	-	_	_	
13-28	1058-1059	-	_	_	
13-29	1059-1060	-	_	_	
13-30	1060-1061	-	_	_	
13-31	1062-1063	|	_	_	
13-32	1064-1065	:	_	_	
13-33	1065-1066	-	_	_	
13-34	1066-1067	-	_	_	
13-35	1067-1068	-	_	_	
13-36	1068-1069	-	_	_	
13-37	1069-1070	-	_	_	
13-38	1070-1071	-	_	_	
13-39	1071-1072	-	_	_	
13-40	1072-1073	-	_	_	
13-41	1073-1074	-	_	_	
13-42	1074-1075	-	_	_	
13-43	1075-1076	-	_	_	
13-44	1076-1077	-	_	_	
13-45	1077-1078	-	_	_	
13-46	1078-1079	-	_	_	
13-47	1079-1080	-	_	_	
13-48	1080-1081	-	_	_	
13-49	1081-1082	-	_	_	
13-50	1082-1083	-	_	_	
13-51	1083-1084	-	_	_	
13-52	1084-1085	-	_	_	
13-53	1085-1086	-	_	_	
13-54	1086-1087	-	_	_	
13-55	1087-1088	-	_	_	
13-56	1088-1089	-	_	_	
13-57	1089-1090	:	_	_	
13-58	1091-1092	|	_	_	
13-59	1093-1094	:	_	_	
13-60	1094-1095	-	_	_	
13-61	1095-1096	-	_	_	
13-62	1096-1097	-	_	_	
13-63	1097-1098	-	_	_	
13-64	1098-1099	-	_	_	
13-65	1099-1100	-	_	_	
13-66	1100-1101	-	_	_	
13-67	1101-1102	-	_	_	
13-68	1102-1103	-	_	_	
13-69	1103-1104	-	_	_	
13-70	1104-1105	:	_	_	
13-71	1106-1107	|	_	_	
13-72	1108-1109	:	_	_	
13-73	1109-1110	-	_	_	
13-74	1110-1111	-	_	_	
13-75	1111-1112	-	_	_	
13-76	1112-1113	-	_	_	
13-77	1113-1114	-	_	_	
13-78	1114-1115	-	_	_	
13-79	1115-1116	-	_	_	
13-80	1116-1117	-	_	_	
13-81	1117-1118	-	_	_	
13-82	1118-1119	-	_	_	
13-83	1119-1120	-	_	_	
13-84	1120-1121	-	_	_	
13-85	1121-1122	:	_	_	
13-86	1123-1124	|	_	_	
13-87	1125-1126	:	_	_	
13-88	1126-1127	-	_	_	
13-89	1127-1128	-	_	_	
13-90	1128-1129	-	_	_	
13-91	1129-1130	-	_	_	
13-92	1130-1131	-	_	_	
13-93	1131-1132	-	_	_	
13-94	1132-1133	-	_	_	
13-95	1133-1134	-	_	_	
13-96	1134-1135	-	_	_	
13-97	1135-1136	-	_	_	
13-98	1136-1137	:	_	_	
13-99	1138-1139	|	_	_	
13-100	1140-1141	:	_	_	
13-101	1141-1142	-	_	_	
13-102	1142-1143	-	_	_	
13-103	1143-1144	-	_	_	
13-104	1144-1145	-	_	_	
13-105	1145-1146	-	_	_	
13-106	1146-1147	-	_	_	
13-107	1147-1148	-	_	_	
13-108	1148-1149	-	_	_	
13-109	1149-1150	-	_	_	
13-110	1150-1151	:	_	_	
13-111	1152-1153	|	_	_	
13-112	1154-1155	:	_	_	
13-113	1155-1156	-	_	_	
13-114	1156-1157	-	_	_	
13-115	1157-1158	-	_	_	
13-116	1158-1159	-	_	_	
13-117	1159-1160	-	_	_	
13-118	1160-1161	-	_	_	
13-119	1161-1162	-	_	_	
13-120	1162-1163	-	_	_	
13-121	1163-1164	-	_	_	
13-122	1164-1165	-	_	_	
13-123	1165-1166	-	_	_	
13-124	1166-1167	:	_	_	
13-125	1168-1169	|	_	_	
13-126	1170-1171	:	_	_	
13-127	1171-1172	-	_	_	
13-128	1172-1173	-	_	_	
13-129	1173-1174	-	_	_	
13-130	1174-1175	-	_	_	
13-131	1175-1176	-	_	_	
13-132	1176-1177	-	_	_	
13-133	1177-1178	-	_	_	
13-134	1178-1179	-	_	_	
13-135	1179-1180	-	_	_	
13-136	1180-1181	-	_	_	
13-137	1181-1182	:	_	_	
13-138	1183-1184	|	_	_	
13-139	1185-1186	|	_	_	
13-140	1187-1188	*	_	_	
13-141	1188-1189	*	_	_	
13-142	1189-1190	1	_	_	
13-143	1190-1191	*	_	_	
13-144	1191-1192	*	_	_	
13-145	1193-1194	|	_	_	
13-146	1195-1196	*	_	_	
13-147	1196-1197	*	_	_	
13-148	1197-1198	[	_	_	
13-149	1198-1201	ABL	_	_	
13-150	1201-1202	]	_	_	
13-151	1202-1203	(	_	_	
13-152	1203-1204	)	_	_	
13-153	1204-1205	*	_	_	
13-154	1205-1206	*	_	_	
13-155	1207-1208	|	_	_	
13-156	1209-1216	NeurIPS	*[11]	CONFERENCE[11]	
13-157	1217-1221	2021	*[11]	CONFERENCE[11]	
13-158	1222-1223	|	_	_	
13-159	1225-1226	*	_	_	
13-160	1226-1235	available	_	_	
13-161	1235-1236	*	_	_	
13-162	1237-1238	|	_	_	
13-163	1242-1245	WRN	_	_	
13-164	1245-1246	-	_	_	
13-165	1246-1248	16	_	_	
13-166	1248-1249	-	_	_	
13-167	1249-1250	1	_	_	
13-168	1254-1255	|	_	_	
13-169	1258-1265	BadNets	_	_	
13-170	1268-1269	|	_	_	
13-171	1274-1278	3.04	_	_	
13-172	1283-1284	|	_	_	
13-173	1288-1293	86.11	_	_	
13-174	1299-1300	|	_	_	
13-175	1301-1302	|	_	_	
13-176	1303-1304	*	_	_	
13-177	1304-1305	*	_	_	
13-178	1305-1306	2	_	_	
13-179	1306-1307	*	_	_	
13-180	1307-1308	*	_	_	
13-181	1309-1310	|	_	_	
13-182	1338-1339	|	_	_	
13-183	1353-1354	|	_	_	
13-184	1370-1371	|	_	_	
13-185	1385-1386	|	_	_	
13-186	1399-1400	|	_	_	
13-187	1415-1416	|	_	_	
13-188	1430-1431	|	_	_	
13-189	1432-1433	|	_	_	
13-190	1434-1435	*	_	_	
13-191	1435-1436	*	_	_	
13-192	1436-1437	3	_	_	
13-193	1437-1438	*	_	_	
13-194	1438-1439	*	_	_	
13-195	1440-1441	|	_	_	
13-196	1469-1470	|	_	_	
13-197	1484-1485	|	_	_	
13-198	1501-1502	|	_	_	
13-199	1516-1517	|	_	_	
13-200	1530-1531	|	_	_	
13-201	1546-1547	|	_	_	
13-202	1561-1562	|	_	_	
13-203	1563-1564	|	_	_	
13-204	1565-1566	*	_	_	
13-205	1566-1567	*	_	_	
13-206	1567-1568	4	_	_	
13-207	1568-1569	*	_	_	
13-208	1569-1570	*	_	_	
13-209	1571-1572	|	_	_	
13-210	1600-1601	|	_	_	
13-211	1615-1616	|	_	_	
13-212	1632-1633	|	_	_	
13-213	1647-1648	|	_	_	
13-214	1661-1662	|	_	_	
13-215	1677-1678	|	_	_	
13-216	1692-1693	|	_	_	
13-217	1694-1695	|	_	_	
13-218	1696-1697	*	_	_	
13-219	1697-1698	*	_	_	
13-220	1698-1699	5	_	_	
13-221	1699-1700	*	_	_	
13-222	1700-1701	*	_	_	
13-223	1702-1703	|	_	_	
13-224	1731-1732	|	_	_	
13-225	1746-1747	|	_	_	
13-226	1763-1764	|	_	_	
13-227	1778-1779	|	_	_	
13-228	1792-1793	|	_	_	
13-229	1808-1809	|	_	_	
13-230	1823-1824	|	_	_	
13-231	1825-1826	|	_	_	
13-232	1827-1828	*	_	_	
13-233	1828-1829	*	_	_	
13-234	1829-1830	6	_	_	
13-235	1830-1831	*	_	_	
13-236	1831-1832	*	_	_	
13-237	1833-1834	|	_	_	
13-238	1862-1863	|	_	_	
13-239	1877-1878	|	_	_	
13-240	1894-1895	|	_	_	
13-241	1909-1910	|	_	_	
13-242	1923-1924	|	_	_	
13-243	1939-1940	|	_	_	
13-244	1954-1955	|	_	_	
13-245	1956-1957	|	_	_	
13-246	1958-1959	*	_	_	
13-247	1959-1960	*	_	_	
13-248	1960-1961	7	_	_	
13-249	1961-1962	*	_	_	
13-250	1962-1963	*	_	_	
13-251	1964-1965	|	_	_	
13-252	1993-1994	|	_	_	
13-253	2008-2009	|	_	_	
13-254	2025-2026	|	_	_	
13-255	2040-2041	|	_	_	
13-256	2054-2055	|	_	_	
13-257	2070-2071	|	_	_	
13-258	2085-2086	|	_	_	
13-259	2087-2088	|	_	_	
13-260	2089-2090	*	_	_	
13-261	2090-2091	*	_	_	
13-262	2091-2092	8	_	_	
13-263	2092-2093	*	_	_	
13-264	2093-2094	*	_	_	
13-265	2095-2096	|	_	_	
13-266	2124-2125	|	_	_	
13-267	2139-2140	|	_	_	
13-268	2156-2157	|	_	_	
13-269	2171-2172	|	_	_	
13-270	2185-2186	|	_	_	
13-271	2201-2202	|	_	_	
13-272	2216-2217	|	_	_	
13-273	2219-2220	-	_	_	
13-274	2220-2221	-	_	_	
13-275	2221-2222	-	_	_	
13-276	2222-2223	-	_	_	
13-277	2223-2224	-	_	_	
13-278	2224-2225	-	_	_	
13-279	2227-2228	#	_	_	
13-280	2228-2229	#	_	_	
13-281	2230-2239	Verifying	_	_	
13-282	2240-2243	the	_	_	
13-283	2244-2254	unlearning	_	_	
13-284	2255-2261	effect	_	_	
13-285	2262-2264	of	_	_	
13-286	2265-2268	ABL	_	_	
13-287	2269-2273	with	_	_	
13-288	2274-2276	1%	_	_	
13-289	2277-2285	isolated	_	_	
13-290	2286-2290	data	_	_	
13-291	2290-2291	:	_	_	
13-292	2293-2294	#	_	_	
13-293	2294-2295	#	_	_	
13-294	2295-2296	#	_	_	
13-295	2297-2299	An	_	_	
13-296	2300-2307	example	_	_	
13-297	2308-2312	with	_	_	
13-298	2313-2314	a	_	_	
13-299	2315-2325	pretrained	_	_	
13-300	2326-2331	model	_	_	
13-301	2332-2335	WRN	_	_	
13-302	2335-2336	-	_	_	
13-303	2336-2338	16	_	_	
13-304	2338-2339	-	_	_	
13-305	2339-2340	1	_	_	
13-306	2340-2341	,	_	_	
13-307	2342-2347	CIFAR	*[12]	DATASET[12]	
13-308	2347-2348	-	*[12]	DATASET[12]	
13-309	2348-2350	10	*[12]	DATASET[12]	
13-310	2350-2351	,	_	_	
13-311	2352-2363	GridTrigger	_	_	
13-312	2363-2364	,	_	_	
13-313	2365-2371	target	_	_	
13-314	2372-2377	label	_	_	
13-315	2378-2379	0	_	_	
13-316	2379-2380	,	_	_	
13-317	2381-2388	weights	_	_	
13-318	2388-2389	:	_	_	
13-319	2390-2391	`	_	_	
13-320	2391-2392	.	_	_	

#Text=/weight/backdoored_model`.
14-1	2392-2393	/	_	_	
14-2	2393-2399	weight	_	_	
14-3	2399-2400	/	_	_	
14-4	2400-2416	backdoored_model	_	_	
14-5	2416-2417	`	_	_	
14-6	2417-2418	.	_	_	

#Text=Run the following command to verify the unlearning effect:
#Text=
#Text=```bash
#Text=$ python quick_unlearning_demo.py 
#Text=```
#Text=The training logs are shown below. 1% isolation = 500 images from poisoned CIFAR-10.
15-1	2420-2423	Run	_	_	
15-2	2424-2427	the	_	_	
15-3	2428-2437	following	_	_	
15-4	2438-2445	command	_	_	
15-5	2446-2448	to	_	_	
15-6	2449-2455	verify	_	_	
15-7	2456-2459	the	_	_	
15-8	2460-2470	unlearning	_	_	
15-9	2471-2477	effect	_	_	
15-10	2477-2478	:	_	_	
15-11	2480-2481	`	_	_	
15-12	2481-2482	`	_	_	
15-13	2482-2483	`	_	_	
15-14	2483-2487	bash	_	_	
15-15	2488-2489	$	_	_	
15-16	2490-2496	python	*	SOFTWARE	
15-17	2497-2521	quick_unlearning_demo.py	_	_	
15-18	2523-2524	`	_	_	
15-19	2524-2525	`	_	_	
15-20	2525-2526	`	_	_	
15-21	2527-2530	The	_	_	
15-22	2531-2539	training	_	_	
15-23	2540-2544	logs	_	_	
15-24	2545-2548	are	_	_	
15-25	2549-2554	shown	_	_	
15-26	2555-2560	below	_	_	
15-27	2560-2561	.	_	_	
15-28	2562-2564	1%	_	_	
15-29	2565-2574	isolation	_	_	
15-30	2575-2576	=	_	_	
15-31	2577-2580	500	_	_	
15-32	2581-2587	images	_	_	
15-33	2588-2592	from	_	_	
15-34	2593-2601	poisoned	_	_	
15-35	2602-2607	CIFAR	*[13]	DATASET[13]	
15-36	2607-2608	-	*[13]	DATASET[13]	
15-37	2608-2610	10	*[13]	DATASET[13]	
15-38	2610-2611	.	_	_	

#Text=It shows the ASR (bad acc) of drops from 99.99% to 0.48% with no obviouse drop of clean acc.
16-1	2612-2614	It	_	_	
16-2	2615-2620	shows	_	_	
16-3	2621-2624	the	_	_	
16-4	2625-2628	ASR	*	EVALMETRIC	
16-5	2629-2630	(	_	_	
16-6	2630-2633	bad	_	_	
16-7	2634-2637	acc	*	EVALMETRIC	
16-8	2637-2638	)	_	_	
16-9	2639-2641	of	_	_	
16-10	2642-2647	drops	_	_	
16-11	2648-2652	from	_	_	
16-12	2653-2659	99.99%	_	_	
16-13	2660-2662	to	_	_	
16-14	2663-2668	0.48%	_	_	
16-15	2669-2673	with	_	_	
16-16	2674-2676	no	_	_	
16-17	2677-2685	obviouse	_	_	
16-18	2686-2690	drop	_	_	
16-19	2691-2693	of	_	_	
16-20	2694-2699	clean	_	_	
16-21	2700-2703	acc	*	EVALMETRIC	
16-22	2703-2704	.	_	_	

#Text=```python
#Text=Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss
#Text=0,82.77777777777777,99.9888888888889,0.9145596397187975,0.0007119161817762587
#Text=Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss
#Text=1,82.97777777777777,47.13333333333333,0.9546798907385932,4.189897534688313
#Text=Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss
#Text=2,82.46666666666667,5.766666666666667,1.034722186088562,15.361101960923937
#Text=Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss
#Text=3,82.15555555555555,1.5222222222222221,1.0855470676422119,22.175255742390952
#Text=Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss
#Text=4,82.0111111111111,0.7111111111111111,1.1183592330084906,26.754894670274524
#Text=Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss
#Text=5,81.86666666666666,0.4777777777777778,1.1441074348025853,30.429284422132703
#Text=```
#Text=
#Text=The unlearned model will be saved to `'weight/ABL_results/<model_name>.tar'`
#Text=
#Text=Please read `quick_unlearning_demo.py` to adjust the default parameters for your experiment.
#Text=
#Text=---
#Text=
#Text=## More defense results on BadNets model trained with Data Augmentation 
#Text=
#Text=```python
#Text=[Logs for our ABL against Badnet Attacks]
#Text=
#Text=----------- Model Fine-tuning --------------
#Text=epoch: 40  lr: 0.0100
#Text=Epoch[41]:[200/774] loss:0.1456(0.1240)  prec@1:98.44(95.84)  prec@5:98.44(99.96)
#Text=Epoch[41]:[400/774] loss:0.0553(0.1080)  prec@1:98.44(96.38)  prec@5:100.00(99.97)
#Text=Epoch[41]:[600/774] loss:0.0693(0.1015)  prec@1:96.88(96.63)  prec@5:100.00(99.97)
#Text=[Clean] Prec@1: 92.23, Loss: 0.2408
#Text=[Bad] Prec@1: 100.00, Loss: 0.0001
#Text=epoch: 41  lr: 0.0100
#Text=Epoch[42]:[200/774] loss:0.0532(0.0653)  prec@1:98.44(97.89)  prec@5:100.00(100.00)
#Text=Epoch[42]:[400/774] loss:0.0534(0.0659)  prec@1:98.44(97.76)  prec@5:100.00(100.00)
#Text=Epoch[42]:[600/774] loss:0.0514(0.0659)  prec@1:96.88(97.76)  prec@5:100.00(99.99)
#Text=[Clean] Prec@1: 92.60, Loss: 0.2390
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 42  lr: 0.0100
#Text=Epoch[43]:[200/774] loss:0.0054(0.0499)  prec@1:100.00(98.33)  prec@5:100.00(99.99)
#Text=Epoch[43]:[400/774] loss:0.0429(0.0525)  prec@1:98.44(98.21)  prec@5:100.00(99.99)
#Text=Epoch[43]:[600/774] loss:0.0448(0.0537)  prec@1:98.44(98.19)  prec@5:100.00(99.99)
#Text=[Clean] Prec@1: 92.52, Loss: 0.2409
#Text=[Bad] Prec@1: 100.00, Loss: 0.0001
#Text=epoch: 43  lr: 0.0100
#Text=Epoch[44]:[200/774] loss:0.0253(0.0472)  prec@1:98.44(98.41)  prec@5:100.00(99.99)
#Text=Epoch[44]:[400/774] loss:0.0104(0.0463)  prec@1:100.00(98.43)  prec@5:100.00(99.99)
#Text=Epoch[44]:[600/774] loss:0.0200(0.0452)  prec@1:100.00(98.46)  prec@5:100.00(99.99)
#Text=[Clean] Prec@1: 92.60, Loss: 0.2459
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 44  lr: 0.0100
#Text=Epoch[45]:[200/774] loss:0.0510(0.0385)  prec@1:98.44(98.79)  prec@5:100.00(99.99)
#Text=Epoch[45]:[400/774] loss:0.0244(0.0381)  prec@1:98.44(98.82)  prec@5:100.00(100.00)
#Text=Epoch[45]:[600/774] loss:0.0203(0.0391)  prec@1:100.00(98.83)  prec@5:100.00(99.99)
#Text=[Clean] Prec@1: 92.81, Loss: 0.2484
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 45  lr: 0.0100
#Text=Epoch[46]:[200/774] loss:0.0110(0.0374)  prec@1:100.00(98.75)  prec@5:100.00(99.99)
#Text=Epoch[46]:[400/774] loss:0.0204(0.0371)  prec@1:98.44(98.79)  prec@5:100.00(99.99)
#Text=Epoch[46]:[600/774] loss:0.0183(0.0369)  prec@1:100.00(98.76)  prec@5:100.00(99.99)
#Text=[Clean] Prec@1: 92.99, Loss: 0.2495
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 46  lr: 0.0100
#Text=Epoch[47]:[200/774] loss:0.0452(0.0315)  prec@1:98.44(98.97)  prec@5:100.00(100.00)
#Text=Epoch[47]:[400/774] loss:0.0315(0.0310)  prec@1:98.44(98.98)  prec@5:100.00(100.00)
#Text=Epoch[47]:[600/774] loss:0.0298(0.0303)  prec@1:100.00(99.01)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 92.82, Loss: 0.2563
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 47  lr: 0.0100
#Text=Epoch[48]:[200/774] loss:0.0397(0.0269)  prec@1:98.44(99.12)  prec@5:100.00(100.00)
#Text=Epoch[48]:[400/774] loss:0.0617(0.0262)  prec@1:98.44(99.16)  prec@5:100.00(100.00)
#Text=Epoch[48]:[600/774] loss:0.0630(0.0270)  prec@1:98.44(99.16)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 92.81, Loss: 0.2678
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 48  lr: 0.0100
#Text=Epoch[49]:[200/774] loss:0.0251(0.0267)  prec@1:100.00(99.15)  prec@5:100.00(100.00)
#Text=Epoch[49]:[400/774] loss:0.0298(0.0262)  prec@1:98.44(99.14)  prec@5:100.00(100.00)
#Text=Epoch[49]:[600/774] loss:0.0384(0.0258)  prec@1:98.44(99.15)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.09, Loss: 0.2586
#Text=[Bad] Prec@1: 100.00, Loss: 0.0002
#Text=epoch: 49  lr: 0.0100
#Text=Epoch[50]:[200/774] loss:0.0359(0.0203)  prec@1:98.44(99.30)  prec@5:100.00(100.00)
#Text=Epoch[50]:[400/774] loss:0.0062(0.0214)  prec@1:100.00(99.27)  prec@5:100.00(100.00)
#Text=Epoch[50]:[600/774] loss:0.0418(0.0222)  prec@1:98.44(99.25)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.03, Loss: 0.2626
#Text=[Bad] Prec@1: 100.00, Loss: 0.0001
#Text=epoch: 50  lr: 0.0100
#Text=Epoch[51]:[200/774] loss:0.0040(0.0222)  prec@1:100.00(99.27)  prec@5:100.00(100.00)
#Text=Epoch[51]:[400/774] loss:0.0269(0.0236)  prec@1:98.44(99.21)  prec@5:100.00(100.00)
#Text=Epoch[51]:[600/774] loss:0.0219(0.0234)  prec@1:100.00(99.23)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.19, Loss: 0.2604
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 51  lr: 0.0100
#Text=Epoch[52]:[200/774] loss:0.0154(0.0201)  prec@1:98.44(99.34)  prec@5:100.00(100.00)
#Text=Epoch[52]:[400/774] loss:0.0328(0.0200)  prec@1:98.44(99.38)  prec@5:100.00(100.00)
#Text=Epoch[52]:[600/774] loss:0.0220(0.0204)  prec@1:98.44(99.36)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.27, Loss: 0.2652
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 52  lr: 0.0100
#Text=Epoch[53]:[200/774] loss:0.0090(0.0194)  prec@1:100.00(99.39)  prec@5:100.00(100.00)
#Text=Epoch[53]:[400/774] loss:0.0019(0.0195)  prec@1:100.00(99.41)  prec@5:100.00(100.00)
#Text=Epoch[53]:[600/774] loss:0.0402(0.0190)  prec@1:98.44(99.45)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.04, Loss: 0.2735
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 53  lr: 0.0100
#Text=Epoch[54]:[200/774] loss:0.0154(0.0186)  prec@1:100.00(99.38)  prec@5:100.00(100.00)
#Text=Epoch[54]:[400/774] loss:0.0124(0.0182)  prec@1:100.00(99.40)  prec@5:100.00(100.00)
#Text=Epoch[54]:[600/774] loss:0.0144(0.0181)  prec@1:100.00(99.45)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.17, Loss: 0.2693
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 54  lr: 0.0100
#Text=Epoch[55]:[200/774] loss:0.0119(0.0168)  prec@1:100.00(99.43)  prec@5:100.00(100.00)
#Text=Epoch[55]:[400/774] loss:0.0228(0.0170)  prec@1:98.44(99.42)  prec@5:100.00(100.00)
#Text=Epoch[55]:[600/774] loss:0.0096(0.0164)  prec@1:100.00(99.47)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 92.84, Loss: 0.2786
#Text=[Bad] Prec@1: 100.00, Loss: 0.0001
#Text=epoch: 55  lr: 0.0100
#Text=Epoch[56]:[200/774] loss:0.0307(0.0146)  prec@1:98.44(99.51)  prec@5:100.00(100.00)
#Text=Epoch[56]:[400/774] loss:0.0065(0.0149)  prec@1:100.00(99.52)  prec@5:100.00(100.00)
#Text=Epoch[56]:[600/774] loss:0.0348(0.0155)  prec@1:98.44(99.50)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.12, Loss: 0.2794
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 56  lr: 0.0100
#Text=Epoch[57]:[200/774] loss:0.0014(0.0134)  prec@1:100.00(99.59)  prec@5:100.00(100.00)
#Text=Epoch[57]:[400/774] loss:0.0060(0.0133)  prec@1:100.00(99.59)  prec@5:100.00(100.00)
#Text=Epoch[57]:[600/774] loss:0.0400(0.0133)  prec@1:95.31(99.61)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.13, Loss: 0.2819
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 57  lr: 0.0100
#Text=Epoch[58]:[200/774] loss:0.0062(0.0122)  prec@1:100.00(99.60)  prec@5:100.00(100.00)
#Text=Epoch[58]:[400/774] loss:0.0065(0.0134)  prec@1:100.00(99.56)  prec@5:100.00(100.00)
#Text=Epoch[58]:[600/774] loss:0.0198(0.0134)  prec@1:100.00(99.59)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.11, Loss: 0.2795
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 58  lr: 0.0100
#Text=Epoch[59]:[200/774] loss:0.0053(0.0094)  prec@1:100.00(99.73)  prec@5:100.00(100.00)
#Text=Epoch[59]:[400/774] loss:0.0064(0.0105)  prec@1:100.00(99.70)  prec@5:100.00(100.00)
#Text=Epoch[59]:[600/774] loss:0.0068(0.0112)  prec@1:100.00(99.67)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.04, Loss: 0.2900
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 59  lr: 0.0100
#Text=Epoch[60]:[200/774] loss:0.0039(0.0147)  prec@1:100.00(99.55)  prec@5:100.00(99.99)
#Text=Epoch[60]:[400/774] loss:0.0399(0.0142)  prec@1:96.88(99.58)  prec@5:100.00(100.00)
#Text=Epoch[60]:[600/774] loss:0.0030(0.0134)  prec@1:100.00(99.59)  prec@5:100.00(100.00)
#Text=[Clean] Prec@1: 93.24, Loss: 0.2905
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=
#Text=----------- Model unlearning --------------
#Text=epoch: 0  lr: 0.0005
#Text=[Clean] Prec@1: 93.24, Loss: 0.2905
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=testing the ascended model......
17-1	2706-2707	`	_	_	
17-2	2707-2708	`	_	_	
17-3	2708-2709	`	_	_	
17-4	2709-2715	python	*	SOFTWARE	
17-5	2716-2721	Epoch	_	_	
17-6	2721-2722	,	_	_	
17-7	2722-2736	Test_clean_acc	_	_	
17-8	2736-2737	,	_	_	
17-9	2737-2749	Test_bad_acc	_	_	
17-10	2749-2750	,	_	_	
17-11	2750-2765	Test_clean_loss	_	_	
17-12	2765-2766	,	_	_	
17-13	2766-2779	Test_bad_loss	_	_	
17-14	2780-2857	0,82.77777777777777,99.9888888888889,0.9145596397187975,0.0007119161817762587	_	_	
17-15	2858-2863	Epoch	_	_	
17-16	2863-2864	,	_	_	
17-17	2864-2878	Test_clean_acc	_	_	
17-18	2878-2879	,	_	_	
17-19	2879-2891	Test_bad_acc	_	_	
17-20	2891-2892	,	_	_	
17-21	2892-2907	Test_clean_loss	_	_	
17-22	2907-2908	,	_	_	
17-23	2908-2921	Test_bad_loss	_	_	
17-24	2922-2996	1,82.97777777777777,47.13333333333333,0.9546798907385932,4.189897534688313	_	_	
17-25	2997-3002	Epoch	_	_	
17-26	3002-3003	,	_	_	
17-27	3003-3017	Test_clean_acc	_	_	
17-28	3017-3018	,	_	_	
17-29	3018-3030	Test_bad_acc	_	_	
17-30	3030-3031	,	_	_	
17-31	3031-3046	Test_clean_loss	_	_	
17-32	3046-3047	,	_	_	
17-33	3047-3060	Test_bad_loss	_	_	
17-34	3061-3135	2,82.46666666666667,5.766666666666667,1.034722186088562,15.361101960923937	_	_	
17-35	3136-3141	Epoch	_	_	
17-36	3141-3142	,	_	_	
17-37	3142-3156	Test_clean_acc	_	_	
17-38	3156-3157	,	_	_	
17-39	3157-3169	Test_bad_acc	_	_	
17-40	3169-3170	,	_	_	
17-41	3170-3185	Test_clean_loss	_	_	
17-42	3185-3186	,	_	_	
17-43	3186-3199	Test_bad_loss	_	_	
17-44	3200-3276	3,82.15555555555555,1.5222222222222221,1.0855470676422119,22.175255742390952	_	_	
17-45	3277-3282	Epoch	_	_	
17-46	3282-3283	,	_	_	
17-47	3283-3297	Test_clean_acc	_	_	
17-48	3297-3298	,	_	_	
17-49	3298-3310	Test_bad_acc	_	_	
17-50	3310-3311	,	_	_	
17-51	3311-3326	Test_clean_loss	_	_	
17-52	3326-3327	,	_	_	
17-53	3327-3340	Test_bad_loss	_	_	
17-54	3341-3416	4,82.0111111111111,0.7111111111111111,1.1183592330084906,26.754894670274524	_	_	
17-55	3417-3422	Epoch	_	_	
17-56	3422-3423	,	_	_	
17-57	3423-3437	Test_clean_acc	_	_	
17-58	3437-3438	,	_	_	
17-59	3438-3450	Test_bad_acc	_	_	
17-60	3450-3451	,	_	_	
17-61	3451-3466	Test_clean_loss	_	_	
17-62	3466-3467	,	_	_	
17-63	3467-3480	Test_bad_loss	_	_	
17-64	3481-3557	5,81.86666666666666,0.4777777777777778,1.1441074348025853,30.429284422132703	_	_	
17-65	3558-3559	`	_	_	
17-66	3559-3560	`	_	_	
17-67	3560-3561	`	_	_	
17-68	3563-3566	The	_	_	
17-69	3567-3576	unlearned	_	_	
17-70	3577-3582	model	_	_	
17-71	3583-3587	will	_	_	
17-72	3588-3590	be	_	_	
17-73	3591-3596	saved	_	_	
17-74	3597-3599	to	_	_	
17-75	3600-3601	`	_	_	
17-76	3601-3602	'	_	_	
17-77	3602-3608	weight	_	_	
17-78	3608-3609	/	_	_	
17-79	3609-3620	ABL_results	_	_	
17-80	3620-3621	/	_	_	
17-81	3621-3622	<	_	_	
17-82	3622-3632	model_name	_	_	
17-83	3632-3633	>	_	_	
17-84	3633-3634	.	_	_	
17-85	3634-3637	tar	_	_	
17-86	3637-3638	'	_	_	
17-87	3638-3639	`	_	_	
17-88	3641-3647	Please	_	_	
17-89	3648-3652	read	_	_	
17-90	3653-3654	`	_	_	
17-91	3654-3678	quick_unlearning_demo.py	_	_	
17-92	3678-3679	`	_	_	
17-93	3680-3682	to	_	_	
17-94	3683-3689	adjust	_	_	
17-95	3690-3693	the	_	_	
17-96	3694-3701	default	_	_	
17-97	3702-3712	parameters	_	_	
17-98	3713-3716	for	_	_	
17-99	3717-3721	your	_	_	
17-100	3722-3732	experiment	_	_	
17-101	3732-3733	.	_	_	
17-102	3735-3736	-	_	_	
17-103	3736-3737	-	_	_	
17-104	3737-3738	-	_	_	
17-105	3740-3741	#	_	_	
17-106	3741-3742	#	_	_	
17-107	3743-3747	More	_	_	
17-108	3748-3755	defense	_	_	
17-109	3756-3763	results	_	_	
17-110	3764-3766	on	_	_	
17-111	3767-3774	BadNets	_	_	
17-112	3775-3780	model	_	_	
17-113	3781-3788	trained	_	_	
17-114	3789-3793	with	_	_	
17-115	3794-3798	Data	_	_	
17-116	3799-3811	Augmentation	_	_	
17-117	3814-3815	`	_	_	
17-118	3815-3816	`	_	_	
17-119	3816-3817	`	_	_	
17-120	3817-3823	python	*	PROGLANG	
17-121	3824-3825	[	_	_	
17-122	3825-3829	Logs	_	_	
17-123	3830-3833	for	_	_	
17-124	3834-3837	our	_	_	
17-125	3838-3841	ABL	_	_	
17-126	3842-3849	against	_	_	
17-127	3850-3856	Badnet	_	_	
17-128	3857-3864	Attacks	_	_	
17-129	3864-3865	]	_	_	
17-130	3867-3868	-	_	_	
17-131	3868-3869	-	_	_	
17-132	3869-3870	-	_	_	
17-133	3870-3871	-	_	_	
17-134	3871-3872	-	_	_	
17-135	3872-3873	-	_	_	
17-136	3873-3874	-	_	_	
17-137	3874-3875	-	_	_	
17-138	3875-3876	-	_	_	
17-139	3876-3877	-	_	_	
17-140	3877-3878	-	_	_	
17-141	3879-3884	Model	_	_	
17-142	3885-3896	Fine-tuning	_	_	
17-143	3897-3898	-	_	_	
17-144	3898-3899	-	_	_	
17-145	3899-3900	-	_	_	
17-146	3900-3901	-	_	_	
17-147	3901-3902	-	_	_	
17-148	3902-3903	-	_	_	
17-149	3903-3904	-	_	_	
17-150	3904-3905	-	_	_	
17-151	3905-3906	-	_	_	
17-152	3906-3907	-	_	_	
17-153	3907-3908	-	_	_	
17-154	3908-3909	-	_	_	
17-155	3909-3910	-	_	_	
17-156	3910-3911	-	_	_	
17-157	3912-3917	epoch	_	_	
17-158	3917-3918	:	_	_	
17-159	3919-3921	40	_	_	
17-160	3923-3925	lr	_	_	
17-161	3925-3926	:	_	_	
17-162	3927-3933	0.0100	_	_	
17-163	3934-3939	Epoch	_	_	
17-164	3939-3940	[	_	_	
17-165	3940-3942	41	_	_	
17-166	3942-3943	]	_	_	
17-167	3943-3944	:	_	_	
17-168	3944-3945	[	_	_	
17-169	3945-3948	200	_	_	
17-170	3948-3949	/	_	_	
17-171	3949-3952	774	_	_	
17-172	3952-3953	]	_	_	
17-173	3954-3958	loss	_	_	
17-174	3958-3959	:	_	_	
17-175	3959-3965	0.1456	_	_	
17-176	3965-3966	(	_	_	
17-177	3966-3972	0.1240	_	_	
17-178	3972-3973	)	_	_	
17-179	3975-3979	prec	_	_	
17-180	3979-3980	@	_	_	
17-181	3980-3981	1	_	_	
17-182	3981-3982	:	_	_	
17-183	3982-3987	98.44	_	_	
17-184	3987-3988	(	_	_	
17-185	3988-3993	95.84	_	_	
17-186	3993-3994	)	_	_	
17-187	3996-4000	prec	_	_	
17-188	4000-4001	@	_	_	
17-189	4001-4002	5	_	_	
17-190	4002-4003	:	_	_	
17-191	4003-4008	98.44	_	_	
17-192	4008-4009	(	_	_	
17-193	4009-4014	99.96	_	_	
17-194	4014-4015	)	_	_	
17-195	4016-4021	Epoch	_	_	
17-196	4021-4022	[	_	_	
17-197	4022-4024	41	_	_	
17-198	4024-4025	]	_	_	
17-199	4025-4026	:	_	_	
17-200	4026-4027	[	_	_	
17-201	4027-4030	400	_	_	
17-202	4030-4031	/	_	_	
17-203	4031-4034	774	_	_	
17-204	4034-4035	]	_	_	
17-205	4036-4040	loss	_	_	
17-206	4040-4041	:	_	_	
17-207	4041-4047	0.0553	_	_	
17-208	4047-4048	(	_	_	
17-209	4048-4054	0.1080	_	_	
17-210	4054-4055	)	_	_	
17-211	4057-4061	prec	_	_	
17-212	4061-4062	@	_	_	
17-213	4062-4063	1	_	_	
17-214	4063-4064	:	_	_	
17-215	4064-4069	98.44	_	_	
17-216	4069-4070	(	_	_	
17-217	4070-4075	96.38	_	_	
17-218	4075-4076	)	_	_	
17-219	4078-4082	prec	_	_	
17-220	4082-4083	@	_	_	
17-221	4083-4084	5	_	_	
17-222	4084-4085	:	_	_	
17-223	4085-4091	100.00	_	_	
17-224	4091-4092	(	_	_	
17-225	4092-4097	99.97	_	_	
17-226	4097-4098	)	_	_	
17-227	4099-4104	Epoch	_	_	
17-228	4104-4105	[	_	_	
17-229	4105-4107	41	_	_	
17-230	4107-4108	]	_	_	
17-231	4108-4109	:	_	_	
17-232	4109-4110	[	_	_	
17-233	4110-4113	600	_	_	
17-234	4113-4114	/	_	_	
17-235	4114-4117	774	_	_	
17-236	4117-4118	]	_	_	
17-237	4119-4123	loss	_	_	
17-238	4123-4124	:	_	_	
17-239	4124-4130	0.0693	_	_	
17-240	4130-4131	(	_	_	
17-241	4131-4137	0.1015	_	_	
17-242	4137-4138	)	_	_	
17-243	4140-4144	prec	_	_	
17-244	4144-4145	@	_	_	
17-245	4145-4146	1	_	_	
17-246	4146-4147	:	_	_	
17-247	4147-4152	96.88	_	_	
17-248	4152-4153	(	_	_	
17-249	4153-4158	96.63	_	_	
17-250	4158-4159	)	_	_	
17-251	4161-4165	prec	_	_	
17-252	4165-4166	@	_	_	
17-253	4166-4167	5	_	_	
17-254	4167-4168	:	_	_	
17-255	4168-4174	100.00	_	_	
17-256	4174-4175	(	_	_	
17-257	4175-4180	99.97	_	_	
17-258	4180-4181	)	_	_	
17-259	4182-4183	[	_	_	
17-260	4183-4188	Clean	_	_	
17-261	4188-4189	]	_	_	
17-262	4190-4194	Prec	_	_	
17-263	4194-4195	@	_	_	
17-264	4195-4196	1	_	_	
17-265	4196-4197	:	_	_	
17-266	4198-4203	92.23	_	_	
17-267	4203-4204	,	_	_	
17-268	4205-4209	Loss	_	_	
17-269	4209-4210	:	_	_	
17-270	4211-4217	0.2408	_	_	
17-271	4218-4219	[	_	_	
17-272	4219-4222	Bad	_	_	
17-273	4222-4223	]	_	_	
17-274	4224-4228	Prec	_	_	
17-275	4228-4229	@	_	_	
17-276	4229-4230	1	_	_	
17-277	4230-4231	:	_	_	
17-278	4232-4238	100.00	_	_	
17-279	4238-4239	,	_	_	
17-280	4240-4244	Loss	_	_	
17-281	4244-4245	:	_	_	
17-282	4246-4252	0.0001	_	_	
17-283	4253-4258	epoch	_	_	
17-284	4258-4259	:	_	_	
17-285	4260-4262	41	_	_	
17-286	4264-4266	lr	_	_	
17-287	4266-4267	:	_	_	
17-288	4268-4274	0.0100	_	_	
17-289	4275-4280	Epoch	_	_	
17-290	4280-4281	[	_	_	
17-291	4281-4283	42	_	_	
17-292	4283-4284	]	_	_	
17-293	4284-4285	:	_	_	
17-294	4285-4286	[	_	_	
17-295	4286-4289	200	_	_	
17-296	4289-4290	/	_	_	
17-297	4290-4293	774	_	_	
17-298	4293-4294	]	_	_	
17-299	4295-4299	loss	_	_	
17-300	4299-4300	:	_	_	
17-301	4300-4306	0.0532	_	_	
17-302	4306-4307	(	_	_	
17-303	4307-4313	0.0653	_	_	
17-304	4313-4314	)	_	_	
17-305	4316-4320	prec	_	_	
17-306	4320-4321	@	_	_	
17-307	4321-4322	1	_	_	
17-308	4322-4323	:	_	_	
17-309	4323-4328	98.44	_	_	
17-310	4328-4329	(	_	_	
17-311	4329-4334	97.89	_	_	
17-312	4334-4335	)	_	_	
17-313	4337-4341	prec	_	_	
17-314	4341-4342	@	_	_	
17-315	4342-4343	5	_	_	
17-316	4343-4344	:	_	_	
17-317	4344-4350	100.00	_	_	
17-318	4350-4351	(	_	_	
17-319	4351-4357	100.00	_	_	
17-320	4357-4358	)	_	_	
17-321	4359-4364	Epoch	_	_	
17-322	4364-4365	[	_	_	
17-323	4365-4367	42	_	_	
17-324	4367-4368	]	_	_	
17-325	4368-4369	:	_	_	
17-326	4369-4370	[	_	_	
17-327	4370-4373	400	_	_	
17-328	4373-4374	/	_	_	
17-329	4374-4377	774	_	_	
17-330	4377-4378	]	_	_	
17-331	4379-4383	loss	_	_	
17-332	4383-4384	:	_	_	
17-333	4384-4390	0.0534	_	_	
17-334	4390-4391	(	_	_	
17-335	4391-4397	0.0659	_	_	
17-336	4397-4398	)	_	_	
17-337	4400-4404	prec	_	_	
17-338	4404-4405	@	_	_	
17-339	4405-4406	1	_	_	
17-340	4406-4407	:	_	_	
17-341	4407-4412	98.44	_	_	
17-342	4412-4413	(	_	_	
17-343	4413-4418	97.76	_	_	
17-344	4418-4419	)	_	_	
17-345	4421-4425	prec	_	_	
17-346	4425-4426	@	_	_	
17-347	4426-4427	5	_	_	
17-348	4427-4428	:	_	_	
17-349	4428-4434	100.00	_	_	
17-350	4434-4435	(	_	_	
17-351	4435-4441	100.00	_	_	
17-352	4441-4442	)	_	_	
17-353	4443-4448	Epoch	_	_	
17-354	4448-4449	[	_	_	
17-355	4449-4451	42	_	_	
17-356	4451-4452	]	_	_	
17-357	4452-4453	:	_	_	
17-358	4453-4454	[	_	_	
17-359	4454-4457	600	_	_	
17-360	4457-4458	/	_	_	
17-361	4458-4461	774	_	_	
17-362	4461-4462	]	_	_	
17-363	4463-4467	loss	_	_	
17-364	4467-4468	:	_	_	
17-365	4468-4474	0.0514	_	_	
17-366	4474-4475	(	_	_	
17-367	4475-4481	0.0659	_	_	
17-368	4481-4482	)	_	_	
17-369	4484-4488	prec	_	_	
17-370	4488-4489	@	_	_	
17-371	4489-4490	1	_	_	
17-372	4490-4491	:	_	_	
17-373	4491-4496	96.88	_	_	
17-374	4496-4497	(	_	_	
17-375	4497-4502	97.76	_	_	
17-376	4502-4503	)	_	_	
17-377	4505-4509	prec	_	_	
17-378	4509-4510	@	_	_	
17-379	4510-4511	5	_	_	
17-380	4511-4512	:	_	_	
17-381	4512-4518	100.00	_	_	
17-382	4518-4519	(	_	_	
17-383	4519-4524	99.99	_	_	
17-384	4524-4525	)	_	_	
17-385	4526-4527	[	_	_	
17-386	4527-4532	Clean	_	_	
17-387	4532-4533	]	_	_	
17-388	4534-4538	Prec	_	_	
17-389	4538-4539	@	_	_	
17-390	4539-4540	1	_	_	
17-391	4540-4541	:	_	_	
17-392	4542-4547	92.60	_	_	
17-393	4547-4548	,	_	_	
17-394	4549-4553	Loss	_	_	
17-395	4553-4554	:	_	_	
17-396	4555-4561	0.2390	_	_	
17-397	4562-4563	[	_	_	
17-398	4563-4566	Bad	_	_	
17-399	4566-4567	]	_	_	
17-400	4568-4572	Prec	_	_	
17-401	4572-4573	@	_	_	
17-402	4573-4574	1	_	_	
17-403	4574-4575	:	_	_	
17-404	4576-4582	100.00	_	_	
17-405	4582-4583	,	_	_	
17-406	4584-4588	Loss	_	_	
17-407	4588-4589	:	_	_	
17-408	4590-4596	0.0000	_	_	
17-409	4597-4602	epoch	_	_	
17-410	4602-4603	:	_	_	
17-411	4604-4606	42	_	_	
17-412	4608-4610	lr	_	_	
17-413	4610-4611	:	_	_	
17-414	4612-4618	0.0100	_	_	
17-415	4619-4624	Epoch	_	_	
17-416	4624-4625	[	_	_	
17-417	4625-4627	43	_	_	
17-418	4627-4628	]	_	_	
17-419	4628-4629	:	_	_	
17-420	4629-4630	[	_	_	
17-421	4630-4633	200	_	_	
17-422	4633-4634	/	_	_	
17-423	4634-4637	774	_	_	
17-424	4637-4638	]	_	_	
17-425	4639-4643	loss	_	_	
17-426	4643-4644	:	_	_	
17-427	4644-4650	0.0054	_	_	
17-428	4650-4651	(	_	_	
17-429	4651-4657	0.0499	_	_	
17-430	4657-4658	)	_	_	
17-431	4660-4664	prec	_	_	
17-432	4664-4665	@	_	_	
17-433	4665-4666	1	_	_	
17-434	4666-4667	:	_	_	
17-435	4667-4673	100.00	_	_	
17-436	4673-4674	(	_	_	
17-437	4674-4679	98.33	_	_	
17-438	4679-4680	)	_	_	
17-439	4682-4686	prec	_	_	
17-440	4686-4687	@	_	_	
17-441	4687-4688	5	_	_	
17-442	4688-4689	:	_	_	
17-443	4689-4695	100.00	_	_	
17-444	4695-4696	(	_	_	
17-445	4696-4701	99.99	_	_	
17-446	4701-4702	)	_	_	
17-447	4703-4708	Epoch	_	_	
17-448	4708-4709	[	_	_	
17-449	4709-4711	43	_	_	
17-450	4711-4712	]	_	_	
17-451	4712-4713	:	_	_	
17-452	4713-4714	[	_	_	
17-453	4714-4717	400	_	_	
17-454	4717-4718	/	_	_	
17-455	4718-4721	774	_	_	
17-456	4721-4722	]	_	_	
17-457	4723-4727	loss	_	_	
17-458	4727-4728	:	_	_	
17-459	4728-4734	0.0429	_	_	
17-460	4734-4735	(	_	_	
17-461	4735-4741	0.0525	_	_	
17-462	4741-4742	)	_	_	
17-463	4744-4748	prec	_	_	
17-464	4748-4749	@	_	_	
17-465	4749-4750	1	_	_	
17-466	4750-4751	:	_	_	
17-467	4751-4756	98.44	_	_	
17-468	4756-4757	(	_	_	
17-469	4757-4762	98.21	_	_	
17-470	4762-4763	)	_	_	
17-471	4765-4769	prec	_	_	
17-472	4769-4770	@	_	_	
17-473	4770-4771	5	_	_	
17-474	4771-4772	:	_	_	
17-475	4772-4778	100.00	_	_	
17-476	4778-4779	(	_	_	
17-477	4779-4784	99.99	_	_	
17-478	4784-4785	)	_	_	
17-479	4786-4791	Epoch	_	_	
17-480	4791-4792	[	_	_	
17-481	4792-4794	43	_	_	
17-482	4794-4795	]	_	_	
17-483	4795-4796	:	_	_	
17-484	4796-4797	[	_	_	
17-485	4797-4800	600	_	_	
17-486	4800-4801	/	_	_	
17-487	4801-4804	774	_	_	
17-488	4804-4805	]	_	_	
17-489	4806-4810	loss	_	_	
17-490	4810-4811	:	_	_	
17-491	4811-4817	0.0448	_	_	
17-492	4817-4818	(	_	_	
17-493	4818-4824	0.0537	_	_	
17-494	4824-4825	)	_	_	
17-495	4827-4831	prec	_	_	
17-496	4831-4832	@	_	_	
17-497	4832-4833	1	_	_	
17-498	4833-4834	:	_	_	
17-499	4834-4839	98.44	_	_	
17-500	4839-4840	(	_	_	
17-501	4840-4845	98.19	_	_	
17-502	4845-4846	)	_	_	
17-503	4848-4852	prec	_	_	
17-504	4852-4853	@	_	_	
17-505	4853-4854	5	_	_	
17-506	4854-4855	:	_	_	
17-507	4855-4861	100.00	_	_	
17-508	4861-4862	(	_	_	
17-509	4862-4867	99.99	_	_	
17-510	4867-4868	)	_	_	
17-511	4869-4870	[	_	_	
17-512	4870-4875	Clean	_	_	
17-513	4875-4876	]	_	_	
17-514	4877-4881	Prec	_	_	
17-515	4881-4882	@	_	_	
17-516	4882-4883	1	_	_	
17-517	4883-4884	:	_	_	
17-518	4885-4890	92.52	_	_	
17-519	4890-4891	,	_	_	
17-520	4892-4896	Loss	_	_	
17-521	4896-4897	:	_	_	
17-522	4898-4904	0.2409	_	_	
17-523	4905-4906	[	_	_	
17-524	4906-4909	Bad	_	_	
17-525	4909-4910	]	_	_	
17-526	4911-4915	Prec	_	_	
17-527	4915-4916	@	_	_	
17-528	4916-4917	1	_	_	
17-529	4917-4918	:	_	_	
17-530	4919-4925	100.00	_	_	
17-531	4925-4926	,	_	_	
17-532	4927-4931	Loss	_	_	
17-533	4931-4932	:	_	_	
17-534	4933-4939	0.0001	_	_	
17-535	4940-4945	epoch	_	_	
17-536	4945-4946	:	_	_	
17-537	4947-4949	43	_	_	
17-538	4951-4953	lr	_	_	
17-539	4953-4954	:	_	_	
17-540	4955-4961	0.0100	_	_	
17-541	4962-4967	Epoch	_	_	
17-542	4967-4968	[	_	_	
17-543	4968-4970	44	_	_	
17-544	4970-4971	]	_	_	
17-545	4971-4972	:	_	_	
17-546	4972-4973	[	_	_	
17-547	4973-4976	200	_	_	
17-548	4976-4977	/	_	_	
17-549	4977-4980	774	_	_	
17-550	4980-4981	]	_	_	
17-551	4982-4986	loss	_	_	
17-552	4986-4987	:	_	_	
17-553	4987-4993	0.0253	_	_	
17-554	4993-4994	(	_	_	
17-555	4994-5000	0.0472	_	_	
17-556	5000-5001	)	_	_	
17-557	5003-5007	prec	_	_	
17-558	5007-5008	@	_	_	
17-559	5008-5009	1	_	_	
17-560	5009-5010	:	_	_	
17-561	5010-5015	98.44	_	_	
17-562	5015-5016	(	_	_	
17-563	5016-5021	98.41	_	_	
17-564	5021-5022	)	_	_	
17-565	5024-5028	prec	_	_	
17-566	5028-5029	@	_	_	
17-567	5029-5030	5	_	_	
17-568	5030-5031	:	_	_	
17-569	5031-5037	100.00	_	_	
17-570	5037-5038	(	_	_	
17-571	5038-5043	99.99	_	_	
17-572	5043-5044	)	_	_	
17-573	5045-5050	Epoch	_	_	
17-574	5050-5051	[	_	_	
17-575	5051-5053	44	_	_	
17-576	5053-5054	]	_	_	
17-577	5054-5055	:	_	_	
17-578	5055-5056	[	_	_	
17-579	5056-5059	400	_	_	
17-580	5059-5060	/	_	_	
17-581	5060-5063	774	_	_	
17-582	5063-5064	]	_	_	
17-583	5065-5069	loss	_	_	
17-584	5069-5070	:	_	_	
17-585	5070-5076	0.0104	_	_	
17-586	5076-5077	(	_	_	
17-587	5077-5083	0.0463	_	_	
17-588	5083-5084	)	_	_	
17-589	5086-5090	prec	_	_	
17-590	5090-5091	@	_	_	
17-591	5091-5092	1	_	_	
17-592	5092-5093	:	_	_	
17-593	5093-5099	100.00	_	_	
17-594	5099-5100	(	_	_	
17-595	5100-5105	98.43	_	_	
17-596	5105-5106	)	_	_	
17-597	5108-5112	prec	_	_	
17-598	5112-5113	@	_	_	
17-599	5113-5114	5	_	_	
17-600	5114-5115	:	_	_	
17-601	5115-5121	100.00	_	_	
17-602	5121-5122	(	_	_	
17-603	5122-5127	99.99	_	_	
17-604	5127-5128	)	_	_	
17-605	5129-5134	Epoch	_	_	
17-606	5134-5135	[	_	_	
17-607	5135-5137	44	_	_	
17-608	5137-5138	]	_	_	
17-609	5138-5139	:	_	_	
17-610	5139-5140	[	_	_	
17-611	5140-5143	600	_	_	
17-612	5143-5144	/	_	_	
17-613	5144-5147	774	_	_	
17-614	5147-5148	]	_	_	
17-615	5149-5153	loss	_	_	
17-616	5153-5154	:	_	_	
17-617	5154-5160	0.0200	_	_	
17-618	5160-5161	(	_	_	
17-619	5161-5167	0.0452	_	_	
17-620	5167-5168	)	_	_	
17-621	5170-5174	prec	_	_	
17-622	5174-5175	@	_	_	
17-623	5175-5176	1	_	_	
17-624	5176-5177	:	_	_	
17-625	5177-5183	100.00	_	_	
17-626	5183-5184	(	_	_	
17-627	5184-5189	98.46	_	_	
17-628	5189-5190	)	_	_	
17-629	5192-5196	prec	_	_	
17-630	5196-5197	@	_	_	
17-631	5197-5198	5	_	_	
17-632	5198-5199	:	_	_	
17-633	5199-5205	100.00	_	_	
17-634	5205-5206	(	_	_	
17-635	5206-5211	99.99	_	_	
17-636	5211-5212	)	_	_	
17-637	5213-5214	[	_	_	
17-638	5214-5219	Clean	_	_	
17-639	5219-5220	]	_	_	
17-640	5221-5225	Prec	_	_	
17-641	5225-5226	@	_	_	
17-642	5226-5227	1	_	_	
17-643	5227-5228	:	_	_	
17-644	5229-5234	92.60	_	_	
17-645	5234-5235	,	_	_	
17-646	5236-5240	Loss	_	_	
17-647	5240-5241	:	_	_	
17-648	5242-5248	0.2459	_	_	
17-649	5249-5250	[	_	_	
17-650	5250-5253	Bad	_	_	
17-651	5253-5254	]	_	_	
17-652	5255-5259	Prec	_	_	
17-653	5259-5260	@	_	_	
17-654	5260-5261	1	_	_	
17-655	5261-5262	:	_	_	
17-656	5263-5269	100.00	_	_	
17-657	5269-5270	,	_	_	
17-658	5271-5275	Loss	_	_	
17-659	5275-5276	:	_	_	
17-660	5277-5283	0.0000	_	_	
17-661	5284-5289	epoch	_	_	
17-662	5289-5290	:	_	_	
17-663	5291-5293	44	_	_	
17-664	5295-5297	lr	_	_	
17-665	5297-5298	:	_	_	
17-666	5299-5305	0.0100	_	_	
17-667	5306-5311	Epoch	_	_	
17-668	5311-5312	[	_	_	
17-669	5312-5314	45	_	_	
17-670	5314-5315	]	_	_	
17-671	5315-5316	:	_	_	
17-672	5316-5317	[	_	_	
17-673	5317-5320	200	_	_	
17-674	5320-5321	/	_	_	
17-675	5321-5324	774	_	_	
17-676	5324-5325	]	_	_	
17-677	5326-5330	loss	_	_	
17-678	5330-5331	:	_	_	
17-679	5331-5337	0.0510	_	_	
17-680	5337-5338	(	_	_	
17-681	5338-5344	0.0385	_	_	
17-682	5344-5345	)	_	_	
17-683	5347-5351	prec	_	_	
17-684	5351-5352	@	_	_	
17-685	5352-5353	1	_	_	
17-686	5353-5354	:	_	_	
17-687	5354-5359	98.44	_	_	
17-688	5359-5360	(	_	_	
17-689	5360-5365	98.79	_	_	
17-690	5365-5366	)	_	_	
17-691	5368-5372	prec	_	_	
17-692	5372-5373	@	_	_	
17-693	5373-5374	5	_	_	
17-694	5374-5375	:	_	_	
17-695	5375-5381	100.00	_	_	
17-696	5381-5382	(	_	_	
17-697	5382-5387	99.99	_	_	
17-698	5387-5388	)	_	_	
17-699	5389-5394	Epoch	_	_	
17-700	5394-5395	[	_	_	
17-701	5395-5397	45	_	_	
17-702	5397-5398	]	_	_	
17-703	5398-5399	:	_	_	
17-704	5399-5400	[	_	_	
17-705	5400-5403	400	_	_	
17-706	5403-5404	/	_	_	
17-707	5404-5407	774	_	_	
17-708	5407-5408	]	_	_	
17-709	5409-5413	loss	_	_	
17-710	5413-5414	:	_	_	
17-711	5414-5420	0.0244	_	_	
17-712	5420-5421	(	_	_	
17-713	5421-5427	0.0381	_	_	
17-714	5427-5428	)	_	_	
17-715	5430-5434	prec	_	_	
17-716	5434-5435	@	_	_	
17-717	5435-5436	1	_	_	
17-718	5436-5437	:	_	_	
17-719	5437-5442	98.44	_	_	
17-720	5442-5443	(	_	_	
17-721	5443-5448	98.82	_	_	
17-722	5448-5449	)	_	_	
17-723	5451-5455	prec	_	_	
17-724	5455-5456	@	_	_	
17-725	5456-5457	5	_	_	
17-726	5457-5458	:	_	_	
17-727	5458-5464	100.00	_	_	
17-728	5464-5465	(	_	_	
17-729	5465-5471	100.00	_	_	
17-730	5471-5472	)	_	_	
17-731	5473-5478	Epoch	_	_	
17-732	5478-5479	[	_	_	
17-733	5479-5481	45	_	_	
17-734	5481-5482	]	_	_	
17-735	5482-5483	:	_	_	
17-736	5483-5484	[	_	_	
17-737	5484-5487	600	_	_	
17-738	5487-5488	/	_	_	
17-739	5488-5491	774	_	_	
17-740	5491-5492	]	_	_	
17-741	5493-5497	loss	_	_	
17-742	5497-5498	:	_	_	
17-743	5498-5504	0.0203	_	_	
17-744	5504-5505	(	_	_	
17-745	5505-5511	0.0391	_	_	
17-746	5511-5512	)	_	_	
17-747	5514-5518	prec	_	_	
17-748	5518-5519	@	_	_	
17-749	5519-5520	1	_	_	
17-750	5520-5521	:	_	_	
17-751	5521-5527	100.00	_	_	
17-752	5527-5528	(	_	_	
17-753	5528-5533	98.83	_	_	
17-754	5533-5534	)	_	_	
17-755	5536-5540	prec	_	_	
17-756	5540-5541	@	_	_	
17-757	5541-5542	5	_	_	
17-758	5542-5543	:	_	_	
17-759	5543-5549	100.00	_	_	
17-760	5549-5550	(	_	_	
17-761	5550-5555	99.99	_	_	
17-762	5555-5556	)	_	_	
17-763	5557-5558	[	_	_	
17-764	5558-5563	Clean	_	_	
17-765	5563-5564	]	_	_	
17-766	5565-5569	Prec	_	_	
17-767	5569-5570	@	_	_	
17-768	5570-5571	1	_	_	
17-769	5571-5572	:	_	_	
17-770	5573-5578	92.81	_	_	
17-771	5578-5579	,	_	_	
17-772	5580-5584	Loss	_	_	
17-773	5584-5585	:	_	_	
17-774	5586-5592	0.2484	_	_	
17-775	5593-5594	[	_	_	
17-776	5594-5597	Bad	_	_	
17-777	5597-5598	]	_	_	
17-778	5599-5603	Prec	_	_	
17-779	5603-5604	@	_	_	
17-780	5604-5605	1	_	_	
17-781	5605-5606	:	_	_	
17-782	5607-5613	100.00	_	_	
17-783	5613-5614	,	_	_	
17-784	5615-5619	Loss	_	_	
17-785	5619-5620	:	_	_	
17-786	5621-5627	0.0000	_	_	
17-787	5628-5633	epoch	_	_	
17-788	5633-5634	:	_	_	
17-789	5635-5637	45	_	_	
17-790	5639-5641	lr	_	_	
17-791	5641-5642	:	_	_	
17-792	5643-5649	0.0100	_	_	
17-793	5650-5655	Epoch	_	_	
17-794	5655-5656	[	_	_	
17-795	5656-5658	46	_	_	
17-796	5658-5659	]	_	_	
17-797	5659-5660	:	_	_	
17-798	5660-5661	[	_	_	
17-799	5661-5664	200	_	_	
17-800	5664-5665	/	_	_	
17-801	5665-5668	774	_	_	
17-802	5668-5669	]	_	_	
17-803	5670-5674	loss	_	_	
17-804	5674-5675	:	_	_	
17-805	5675-5681	0.0110	_	_	
17-806	5681-5682	(	_	_	
17-807	5682-5688	0.0374	_	_	
17-808	5688-5689	)	_	_	
17-809	5691-5695	prec	_	_	
17-810	5695-5696	@	_	_	
17-811	5696-5697	1	_	_	
17-812	5697-5698	:	_	_	
17-813	5698-5704	100.00	_	_	
17-814	5704-5705	(	_	_	
17-815	5705-5710	98.75	_	_	
17-816	5710-5711	)	_	_	
17-817	5713-5717	prec	_	_	
17-818	5717-5718	@	_	_	
17-819	5718-5719	5	_	_	
17-820	5719-5720	:	_	_	
17-821	5720-5726	100.00	_	_	
17-822	5726-5727	(	_	_	
17-823	5727-5732	99.99	_	_	
17-824	5732-5733	)	_	_	
17-825	5734-5739	Epoch	_	_	
17-826	5739-5740	[	_	_	
17-827	5740-5742	46	_	_	
17-828	5742-5743	]	_	_	
17-829	5743-5744	:	_	_	
17-830	5744-5745	[	_	_	
17-831	5745-5748	400	_	_	
17-832	5748-5749	/	_	_	
17-833	5749-5752	774	_	_	
17-834	5752-5753	]	_	_	
17-835	5754-5758	loss	_	_	
17-836	5758-5759	:	_	_	
17-837	5759-5765	0.0204	_	_	
17-838	5765-5766	(	_	_	
17-839	5766-5772	0.0371	_	_	
17-840	5772-5773	)	_	_	
17-841	5775-5779	prec	_	_	
17-842	5779-5780	@	_	_	
17-843	5780-5781	1	_	_	
17-844	5781-5782	:	_	_	
17-845	5782-5787	98.44	_	_	
17-846	5787-5788	(	_	_	
17-847	5788-5793	98.79	_	_	
17-848	5793-5794	)	_	_	
17-849	5796-5800	prec	_	_	
17-850	5800-5801	@	_	_	
17-851	5801-5802	5	_	_	
17-852	5802-5803	:	_	_	
17-853	5803-5809	100.00	_	_	
17-854	5809-5810	(	_	_	
17-855	5810-5815	99.99	_	_	
17-856	5815-5816	)	_	_	
17-857	5817-5822	Epoch	_	_	
17-858	5822-5823	[	_	_	
17-859	5823-5825	46	_	_	
17-860	5825-5826	]	_	_	
17-861	5826-5827	:	_	_	
17-862	5827-5828	[	_	_	
17-863	5828-5831	600	_	_	
17-864	5831-5832	/	_	_	
17-865	5832-5835	774	_	_	
17-866	5835-5836	]	_	_	
17-867	5837-5841	loss	_	_	
17-868	5841-5842	:	_	_	
17-869	5842-5848	0.0183	_	_	
17-870	5848-5849	(	_	_	
17-871	5849-5855	0.0369	_	_	
17-872	5855-5856	)	_	_	
17-873	5858-5862	prec	_	_	
17-874	5862-5863	@	_	_	
17-875	5863-5864	1	_	_	
17-876	5864-5865	:	_	_	
17-877	5865-5871	100.00	_	_	
17-878	5871-5872	(	_	_	
17-879	5872-5877	98.76	_	_	
17-880	5877-5878	)	_	_	
17-881	5880-5884	prec	_	_	
17-882	5884-5885	@	_	_	
17-883	5885-5886	5	_	_	
17-884	5886-5887	:	_	_	
17-885	5887-5893	100.00	_	_	
17-886	5893-5894	(	_	_	
17-887	5894-5899	99.99	_	_	
17-888	5899-5900	)	_	_	
17-889	5901-5902	[	_	_	
17-890	5902-5907	Clean	_	_	
17-891	5907-5908	]	_	_	
17-892	5909-5913	Prec	_	_	
17-893	5913-5914	@	_	_	
17-894	5914-5915	1	_	_	
17-895	5915-5916	:	_	_	
17-896	5917-5922	92.99	_	_	
17-897	5922-5923	,	_	_	
17-898	5924-5928	Loss	_	_	
17-899	5928-5929	:	_	_	
17-900	5930-5936	0.2495	_	_	
17-901	5937-5938	[	_	_	
17-902	5938-5941	Bad	_	_	
17-903	5941-5942	]	_	_	
17-904	5943-5947	Prec	_	_	
17-905	5947-5948	@	_	_	
17-906	5948-5949	1	_	_	
17-907	5949-5950	:	_	_	
17-908	5951-5957	100.00	_	_	
17-909	5957-5958	,	_	_	
17-910	5959-5963	Loss	_	_	
17-911	5963-5964	:	_	_	
17-912	5965-5971	0.0000	_	_	
17-913	5972-5977	epoch	_	_	
17-914	5977-5978	:	_	_	
17-915	5979-5981	46	_	_	
17-916	5983-5985	lr	_	_	
17-917	5985-5986	:	_	_	
17-918	5987-5993	0.0100	_	_	
17-919	5994-5999	Epoch	_	_	
17-920	5999-6000	[	_	_	
17-921	6000-6002	47	_	_	
17-922	6002-6003	]	_	_	
17-923	6003-6004	:	_	_	
17-924	6004-6005	[	_	_	
17-925	6005-6008	200	_	_	
17-926	6008-6009	/	_	_	
17-927	6009-6012	774	_	_	
17-928	6012-6013	]	_	_	
17-929	6014-6018	loss	_	_	
17-930	6018-6019	:	_	_	
17-931	6019-6025	0.0452	_	_	
17-932	6025-6026	(	_	_	
17-933	6026-6032	0.0315	_	_	
17-934	6032-6033	)	_	_	
17-935	6035-6039	prec	_	_	
17-936	6039-6040	@	_	_	
17-937	6040-6041	1	_	_	
17-938	6041-6042	:	_	_	
17-939	6042-6047	98.44	_	_	
17-940	6047-6048	(	_	_	
17-941	6048-6053	98.97	_	_	
17-942	6053-6054	)	_	_	
17-943	6056-6060	prec	_	_	
17-944	6060-6061	@	_	_	
17-945	6061-6062	5	_	_	
17-946	6062-6063	:	_	_	
17-947	6063-6069	100.00	_	_	
17-948	6069-6070	(	_	_	
17-949	6070-6076	100.00	_	_	
17-950	6076-6077	)	_	_	
17-951	6078-6083	Epoch	_	_	
17-952	6083-6084	[	_	_	
17-953	6084-6086	47	_	_	
17-954	6086-6087	]	_	_	
17-955	6087-6088	:	_	_	
17-956	6088-6089	[	_	_	
17-957	6089-6092	400	_	_	
17-958	6092-6093	/	_	_	
17-959	6093-6096	774	_	_	
17-960	6096-6097	]	_	_	
17-961	6098-6102	loss	_	_	
17-962	6102-6103	:	_	_	
17-963	6103-6109	0.0315	_	_	
17-964	6109-6110	(	_	_	
17-965	6110-6116	0.0310	_	_	
17-966	6116-6117	)	_	_	
17-967	6119-6123	prec	_	_	
17-968	6123-6124	@	_	_	
17-969	6124-6125	1	_	_	
17-970	6125-6126	:	_	_	
17-971	6126-6131	98.44	_	_	
17-972	6131-6132	(	_	_	
17-973	6132-6137	98.98	_	_	
17-974	6137-6138	)	_	_	
17-975	6140-6144	prec	_	_	
17-976	6144-6145	@	_	_	
17-977	6145-6146	5	_	_	
17-978	6146-6147	:	_	_	
17-979	6147-6153	100.00	_	_	
17-980	6153-6154	(	_	_	
17-981	6154-6160	100.00	_	_	
17-982	6160-6161	)	_	_	
17-983	6162-6167	Epoch	_	_	
17-984	6167-6168	[	_	_	
17-985	6168-6170	47	_	_	
17-986	6170-6171	]	_	_	
17-987	6171-6172	:	_	_	
17-988	6172-6173	[	_	_	
17-989	6173-6176	600	_	_	
17-990	6176-6177	/	_	_	
17-991	6177-6180	774	_	_	
17-992	6180-6181	]	_	_	
17-993	6182-6186	loss	_	_	
17-994	6186-6187	:	_	_	
17-995	6187-6193	0.0298	_	_	
17-996	6193-6194	(	_	_	
17-997	6194-6200	0.0303	_	_	
17-998	6200-6201	)	_	_	
17-999	6203-6207	prec	_	_	
17-1000	6207-6208	@	_	_	
17-1001	6208-6209	1	_	_	
17-1002	6209-6210	:	_	_	
17-1003	6210-6216	100.00	_	_	
17-1004	6216-6217	(	_	_	
17-1005	6217-6222	99.01	_	_	
17-1006	6222-6223	)	_	_	
17-1007	6225-6229	prec	_	_	
17-1008	6229-6230	@	_	_	
17-1009	6230-6231	5	_	_	
17-1010	6231-6232	:	_	_	
17-1011	6232-6238	100.00	_	_	
17-1012	6238-6239	(	_	_	
17-1013	6239-6245	100.00	_	_	
17-1014	6245-6246	)	_	_	
17-1015	6247-6248	[	_	_	
17-1016	6248-6253	Clean	_	_	
17-1017	6253-6254	]	_	_	
17-1018	6255-6259	Prec	_	_	
17-1019	6259-6260	@	_	_	
17-1020	6260-6261	1	_	_	
17-1021	6261-6262	:	_	_	
17-1022	6263-6268	92.82	_	_	
17-1023	6268-6269	,	_	_	
17-1024	6270-6274	Loss	_	_	
17-1025	6274-6275	:	_	_	
17-1026	6276-6282	0.2563	_	_	
17-1027	6283-6284	[	_	_	
17-1028	6284-6287	Bad	_	_	
17-1029	6287-6288	]	_	_	
17-1030	6289-6293	Prec	_	_	
17-1031	6293-6294	@	_	_	
17-1032	6294-6295	1	_	_	
17-1033	6295-6296	:	_	_	
17-1034	6297-6303	100.00	_	_	
17-1035	6303-6304	,	_	_	
17-1036	6305-6309	Loss	_	_	
17-1037	6309-6310	:	_	_	
17-1038	6311-6317	0.0000	_	_	
17-1039	6318-6323	epoch	_	_	
17-1040	6323-6324	:	_	_	
17-1041	6325-6327	47	_	_	
17-1042	6329-6331	lr	_	_	
17-1043	6331-6332	:	_	_	
17-1044	6333-6339	0.0100	_	_	
17-1045	6340-6345	Epoch	_	_	
17-1046	6345-6346	[	_	_	
17-1047	6346-6348	48	_	_	
17-1048	6348-6349	]	_	_	
17-1049	6349-6350	:	_	_	
17-1050	6350-6351	[	_	_	
17-1051	6351-6354	200	_	_	
17-1052	6354-6355	/	_	_	
17-1053	6355-6358	774	_	_	
17-1054	6358-6359	]	_	_	
17-1055	6360-6364	loss	_	_	
17-1056	6364-6365	:	_	_	
17-1057	6365-6371	0.0397	_	_	
17-1058	6371-6372	(	_	_	
17-1059	6372-6378	0.0269	_	_	
17-1060	6378-6379	)	_	_	
17-1061	6381-6385	prec	_	_	
17-1062	6385-6386	@	_	_	
17-1063	6386-6387	1	_	_	
17-1064	6387-6388	:	_	_	
17-1065	6388-6393	98.44	_	_	
17-1066	6393-6394	(	_	_	
17-1067	6394-6399	99.12	_	_	
17-1068	6399-6400	)	_	_	
17-1069	6402-6406	prec	_	_	
17-1070	6406-6407	@	_	_	
17-1071	6407-6408	5	_	_	
17-1072	6408-6409	:	_	_	
17-1073	6409-6415	100.00	_	_	
17-1074	6415-6416	(	_	_	
17-1075	6416-6422	100.00	_	_	
17-1076	6422-6423	)	_	_	
17-1077	6424-6429	Epoch	_	_	
17-1078	6429-6430	[	_	_	
17-1079	6430-6432	48	_	_	
17-1080	6432-6433	]	_	_	
17-1081	6433-6434	:	_	_	
17-1082	6434-6435	[	_	_	
17-1083	6435-6438	400	_	_	
17-1084	6438-6439	/	_	_	
17-1085	6439-6442	774	_	_	
17-1086	6442-6443	]	_	_	
17-1087	6444-6448	loss	_	_	
17-1088	6448-6449	:	_	_	
17-1089	6449-6455	0.0617	_	_	
17-1090	6455-6456	(	_	_	
17-1091	6456-6462	0.0262	_	_	
17-1092	6462-6463	)	_	_	
17-1093	6465-6469	prec	_	_	
17-1094	6469-6470	@	_	_	
17-1095	6470-6471	1	_	_	
17-1096	6471-6472	:	_	_	
17-1097	6472-6477	98.44	_	_	
17-1098	6477-6478	(	_	_	
17-1099	6478-6483	99.16	_	_	
17-1100	6483-6484	)	_	_	
17-1101	6486-6490	prec	_	_	
17-1102	6490-6491	@	_	_	
17-1103	6491-6492	5	_	_	
17-1104	6492-6493	:	_	_	
17-1105	6493-6499	100.00	_	_	
17-1106	6499-6500	(	_	_	
17-1107	6500-6506	100.00	_	_	
17-1108	6506-6507	)	_	_	
17-1109	6508-6513	Epoch	_	_	
17-1110	6513-6514	[	_	_	
17-1111	6514-6516	48	_	_	
17-1112	6516-6517	]	_	_	
17-1113	6517-6518	:	_	_	
17-1114	6518-6519	[	_	_	
17-1115	6519-6522	600	_	_	
17-1116	6522-6523	/	_	_	
17-1117	6523-6526	774	_	_	
17-1118	6526-6527	]	_	_	
17-1119	6528-6532	loss	_	_	
17-1120	6532-6533	:	_	_	
17-1121	6533-6539	0.0630	_	_	
17-1122	6539-6540	(	_	_	
17-1123	6540-6546	0.0270	_	_	
17-1124	6546-6547	)	_	_	
17-1125	6549-6553	prec	_	_	
17-1126	6553-6554	@	_	_	
17-1127	6554-6555	1	_	_	
17-1128	6555-6556	:	_	_	
17-1129	6556-6561	98.44	_	_	
17-1130	6561-6562	(	_	_	
17-1131	6562-6567	99.16	_	_	
17-1132	6567-6568	)	_	_	
17-1133	6570-6574	prec	_	_	
17-1134	6574-6575	@	_	_	
17-1135	6575-6576	5	_	_	
17-1136	6576-6577	:	_	_	
17-1137	6577-6583	100.00	_	_	
17-1138	6583-6584	(	_	_	
17-1139	6584-6590	100.00	_	_	
17-1140	6590-6591	)	_	_	
17-1141	6592-6593	[	_	_	
17-1142	6593-6598	Clean	_	_	
17-1143	6598-6599	]	_	_	
17-1144	6600-6604	Prec	_	_	
17-1145	6604-6605	@	_	_	
17-1146	6605-6606	1	_	_	
17-1147	6606-6607	:	_	_	
17-1148	6608-6613	92.81	_	_	
17-1149	6613-6614	,	_	_	
17-1150	6615-6619	Loss	_	_	
17-1151	6619-6620	:	_	_	
17-1152	6621-6627	0.2678	_	_	
17-1153	6628-6629	[	_	_	
17-1154	6629-6632	Bad	_	_	
17-1155	6632-6633	]	_	_	
17-1156	6634-6638	Prec	_	_	
17-1157	6638-6639	@	_	_	
17-1158	6639-6640	1	_	_	
17-1159	6640-6641	:	_	_	
17-1160	6642-6648	100.00	_	_	
17-1161	6648-6649	,	_	_	
17-1162	6650-6654	Loss	_	_	
17-1163	6654-6655	:	_	_	
17-1164	6656-6662	0.0000	_	_	
17-1165	6663-6668	epoch	_	_	
17-1166	6668-6669	:	_	_	
17-1167	6670-6672	48	_	_	
17-1168	6674-6676	lr	_	_	
17-1169	6676-6677	:	_	_	
17-1170	6678-6684	0.0100	_	_	
17-1171	6685-6690	Epoch	_	_	
17-1172	6690-6691	[	_	_	
17-1173	6691-6693	49	_	_	
17-1174	6693-6694	]	_	_	
17-1175	6694-6695	:	_	_	
17-1176	6695-6696	[	_	_	
17-1177	6696-6699	200	_	_	
17-1178	6699-6700	/	_	_	
17-1179	6700-6703	774	_	_	
17-1180	6703-6704	]	_	_	
17-1181	6705-6709	loss	_	_	
17-1182	6709-6710	:	_	_	
17-1183	6710-6716	0.0251	_	_	
17-1184	6716-6717	(	_	_	
17-1185	6717-6723	0.0267	_	_	
17-1186	6723-6724	)	_	_	
17-1187	6726-6730	prec	_	_	
17-1188	6730-6731	@	_	_	
17-1189	6731-6732	1	_	_	
17-1190	6732-6733	:	_	_	
17-1191	6733-6739	100.00	_	_	
17-1192	6739-6740	(	_	_	
17-1193	6740-6745	99.15	_	_	
17-1194	6745-6746	)	_	_	
17-1195	6748-6752	prec	_	_	
17-1196	6752-6753	@	_	_	
17-1197	6753-6754	5	_	_	
17-1198	6754-6755	:	_	_	
17-1199	6755-6761	100.00	_	_	
17-1200	6761-6762	(	_	_	
17-1201	6762-6768	100.00	_	_	
17-1202	6768-6769	)	_	_	
17-1203	6770-6775	Epoch	_	_	
17-1204	6775-6776	[	_	_	
17-1205	6776-6778	49	_	_	
17-1206	6778-6779	]	_	_	
17-1207	6779-6780	:	_	_	
17-1208	6780-6781	[	_	_	
17-1209	6781-6784	400	_	_	
17-1210	6784-6785	/	_	_	
17-1211	6785-6788	774	_	_	
17-1212	6788-6789	]	_	_	
17-1213	6790-6794	loss	_	_	
17-1214	6794-6795	:	_	_	
17-1215	6795-6801	0.0298	_	_	
17-1216	6801-6802	(	_	_	
17-1217	6802-6808	0.0262	_	_	
17-1218	6808-6809	)	_	_	
17-1219	6811-6815	prec	_	_	
17-1220	6815-6816	@	_	_	
17-1221	6816-6817	1	_	_	
17-1222	6817-6818	:	_	_	
17-1223	6818-6823	98.44	_	_	
17-1224	6823-6824	(	_	_	
17-1225	6824-6829	99.14	_	_	
17-1226	6829-6830	)	_	_	
17-1227	6832-6836	prec	_	_	
17-1228	6836-6837	@	_	_	
17-1229	6837-6838	5	_	_	
17-1230	6838-6839	:	_	_	
17-1231	6839-6845	100.00	_	_	
17-1232	6845-6846	(	_	_	
17-1233	6846-6852	100.00	_	_	
17-1234	6852-6853	)	_	_	
17-1235	6854-6859	Epoch	_	_	
17-1236	6859-6860	[	_	_	
17-1237	6860-6862	49	_	_	
17-1238	6862-6863	]	_	_	
17-1239	6863-6864	:	_	_	
17-1240	6864-6865	[	_	_	
17-1241	6865-6868	600	_	_	
17-1242	6868-6869	/	_	_	
17-1243	6869-6872	774	_	_	
17-1244	6872-6873	]	_	_	
17-1245	6874-6878	loss	_	_	
17-1246	6878-6879	:	_	_	
17-1247	6879-6885	0.0384	_	_	
17-1248	6885-6886	(	_	_	
17-1249	6886-6892	0.0258	_	_	
17-1250	6892-6893	)	_	_	
17-1251	6895-6899	prec	_	_	
17-1252	6899-6900	@	_	_	
17-1253	6900-6901	1	_	_	
17-1254	6901-6902	:	_	_	
17-1255	6902-6907	98.44	_	_	
17-1256	6907-6908	(	_	_	
17-1257	6908-6913	99.15	_	_	
17-1258	6913-6914	)	_	_	
17-1259	6916-6920	prec	_	_	
17-1260	6920-6921	@	_	_	
17-1261	6921-6922	5	_	_	
17-1262	6922-6923	:	_	_	
17-1263	6923-6929	100.00	_	_	
17-1264	6929-6930	(	_	_	
17-1265	6930-6936	100.00	_	_	
17-1266	6936-6937	)	_	_	
17-1267	6938-6939	[	_	_	
17-1268	6939-6944	Clean	_	_	
17-1269	6944-6945	]	_	_	
17-1270	6946-6950	Prec	_	_	
17-1271	6950-6951	@	_	_	
17-1272	6951-6952	1	_	_	
17-1273	6952-6953	:	_	_	
17-1274	6954-6959	93.09	_	_	
17-1275	6959-6960	,	_	_	
17-1276	6961-6965	Loss	_	_	
17-1277	6965-6966	:	_	_	
17-1278	6967-6973	0.2586	_	_	
17-1279	6974-6975	[	_	_	
17-1280	6975-6978	Bad	_	_	
17-1281	6978-6979	]	_	_	
17-1282	6980-6984	Prec	_	_	
17-1283	6984-6985	@	_	_	
17-1284	6985-6986	1	_	_	
17-1285	6986-6987	:	_	_	
17-1286	6988-6994	100.00	_	_	
17-1287	6994-6995	,	_	_	
17-1288	6996-7000	Loss	_	_	
17-1289	7000-7001	:	_	_	
17-1290	7002-7008	0.0002	_	_	
17-1291	7009-7014	epoch	_	_	
17-1292	7014-7015	:	_	_	
17-1293	7016-7018	49	_	_	
17-1294	7020-7022	lr	_	_	
17-1295	7022-7023	:	_	_	
17-1296	7024-7030	0.0100	_	_	
17-1297	7031-7036	Epoch	_	_	
17-1298	7036-7037	[	_	_	
17-1299	7037-7039	50	_	_	
17-1300	7039-7040	]	_	_	
17-1301	7040-7041	:	_	_	
17-1302	7041-7042	[	_	_	
17-1303	7042-7045	200	_	_	
17-1304	7045-7046	/	_	_	
17-1305	7046-7049	774	_	_	
17-1306	7049-7050	]	_	_	
17-1307	7051-7055	loss	_	_	
17-1308	7055-7056	:	_	_	
17-1309	7056-7062	0.0359	_	_	
17-1310	7062-7063	(	_	_	
17-1311	7063-7069	0.0203	_	_	
17-1312	7069-7070	)	_	_	
17-1313	7072-7076	prec	_	_	
17-1314	7076-7077	@	_	_	
17-1315	7077-7078	1	_	_	
17-1316	7078-7079	:	_	_	
17-1317	7079-7084	98.44	_	_	
17-1318	7084-7085	(	_	_	
17-1319	7085-7090	99.30	_	_	
17-1320	7090-7091	)	_	_	
17-1321	7093-7097	prec	_	_	
17-1322	7097-7098	@	_	_	
17-1323	7098-7099	5	_	_	
17-1324	7099-7100	:	_	_	
17-1325	7100-7106	100.00	_	_	
17-1326	7106-7107	(	_	_	
17-1327	7107-7113	100.00	_	_	
17-1328	7113-7114	)	_	_	
17-1329	7115-7120	Epoch	_	_	
17-1330	7120-7121	[	_	_	
17-1331	7121-7123	50	_	_	
17-1332	7123-7124	]	_	_	
17-1333	7124-7125	:	_	_	
17-1334	7125-7126	[	_	_	
17-1335	7126-7129	400	_	_	
17-1336	7129-7130	/	_	_	
17-1337	7130-7133	774	_	_	
17-1338	7133-7134	]	_	_	
17-1339	7135-7139	loss	_	_	
17-1340	7139-7140	:	_	_	
17-1341	7140-7146	0.0062	_	_	
17-1342	7146-7147	(	_	_	
17-1343	7147-7153	0.0214	_	_	
17-1344	7153-7154	)	_	_	
17-1345	7156-7160	prec	_	_	
17-1346	7160-7161	@	_	_	
17-1347	7161-7162	1	_	_	
17-1348	7162-7163	:	_	_	
17-1349	7163-7169	100.00	_	_	
17-1350	7169-7170	(	_	_	
17-1351	7170-7175	99.27	_	_	
17-1352	7175-7176	)	_	_	
17-1353	7178-7182	prec	_	_	
17-1354	7182-7183	@	_	_	
17-1355	7183-7184	5	_	_	
17-1356	7184-7185	:	_	_	
17-1357	7185-7191	100.00	_	_	
17-1358	7191-7192	(	_	_	
17-1359	7192-7198	100.00	_	_	
17-1360	7198-7199	)	_	_	
17-1361	7200-7205	Epoch	_	_	
17-1362	7205-7206	[	_	_	
17-1363	7206-7208	50	_	_	
17-1364	7208-7209	]	_	_	
17-1365	7209-7210	:	_	_	
17-1366	7210-7211	[	_	_	
17-1367	7211-7214	600	_	_	
17-1368	7214-7215	/	_	_	
17-1369	7215-7218	774	_	_	
17-1370	7218-7219	]	_	_	
17-1371	7220-7224	loss	_	_	
17-1372	7224-7225	:	_	_	
17-1373	7225-7231	0.0418	_	_	
17-1374	7231-7232	(	_	_	
17-1375	7232-7238	0.0222	_	_	
17-1376	7238-7239	)	_	_	
17-1377	7241-7245	prec	_	_	
17-1378	7245-7246	@	_	_	
17-1379	7246-7247	1	_	_	
17-1380	7247-7248	:	_	_	
17-1381	7248-7253	98.44	_	_	
17-1382	7253-7254	(	_	_	
17-1383	7254-7259	99.25	_	_	
17-1384	7259-7260	)	_	_	
17-1385	7262-7266	prec	_	_	
17-1386	7266-7267	@	_	_	
17-1387	7267-7268	5	_	_	
17-1388	7268-7269	:	_	_	
17-1389	7269-7275	100.00	_	_	
17-1390	7275-7276	(	_	_	
17-1391	7276-7282	100.00	_	_	
17-1392	7282-7283	)	_	_	
17-1393	7284-7285	[	_	_	
17-1394	7285-7290	Clean	_	_	
17-1395	7290-7291	]	_	_	
17-1396	7292-7296	Prec	_	_	
17-1397	7296-7297	@	_	_	
17-1398	7297-7298	1	_	_	
17-1399	7298-7299	:	_	_	
17-1400	7300-7305	93.03	_	_	
17-1401	7305-7306	,	_	_	
17-1402	7307-7311	Loss	_	_	
17-1403	7311-7312	:	_	_	
17-1404	7313-7319	0.2626	_	_	
17-1405	7320-7321	[	_	_	
17-1406	7321-7324	Bad	_	_	
17-1407	7324-7325	]	_	_	
17-1408	7326-7330	Prec	_	_	
17-1409	7330-7331	@	_	_	
17-1410	7331-7332	1	_	_	
17-1411	7332-7333	:	_	_	
17-1412	7334-7340	100.00	_	_	
17-1413	7340-7341	,	_	_	
17-1414	7342-7346	Loss	_	_	
17-1415	7346-7347	:	_	_	
17-1416	7348-7354	0.0001	_	_	
17-1417	7355-7360	epoch	_	_	
17-1418	7360-7361	:	_	_	
17-1419	7362-7364	50	_	_	
17-1420	7366-7368	lr	_	_	
17-1421	7368-7369	:	_	_	
17-1422	7370-7376	0.0100	_	_	
17-1423	7377-7382	Epoch	_	_	
17-1424	7382-7383	[	_	_	
17-1425	7383-7385	51	_	_	
17-1426	7385-7386	]	_	_	
17-1427	7386-7387	:	_	_	
17-1428	7387-7388	[	_	_	
17-1429	7388-7391	200	_	_	
17-1430	7391-7392	/	_	_	
17-1431	7392-7395	774	_	_	
17-1432	7395-7396	]	_	_	
17-1433	7397-7401	loss	_	_	
17-1434	7401-7402	:	_	_	
17-1435	7402-7408	0.0040	_	_	
17-1436	7408-7409	(	_	_	
17-1437	7409-7415	0.0222	_	_	
17-1438	7415-7416	)	_	_	
17-1439	7418-7422	prec	_	_	
17-1440	7422-7423	@	_	_	
17-1441	7423-7424	1	_	_	
17-1442	7424-7425	:	_	_	
17-1443	7425-7431	100.00	_	_	
17-1444	7431-7432	(	_	_	
17-1445	7432-7437	99.27	_	_	
17-1446	7437-7438	)	_	_	
17-1447	7440-7444	prec	_	_	
17-1448	7444-7445	@	_	_	
17-1449	7445-7446	5	_	_	
17-1450	7446-7447	:	_	_	
17-1451	7447-7453	100.00	_	_	
17-1452	7453-7454	(	_	_	
17-1453	7454-7460	100.00	_	_	
17-1454	7460-7461	)	_	_	
17-1455	7462-7467	Epoch	_	_	
17-1456	7467-7468	[	_	_	
17-1457	7468-7470	51	_	_	
17-1458	7470-7471	]	_	_	
17-1459	7471-7472	:	_	_	
17-1460	7472-7473	[	_	_	
17-1461	7473-7476	400	_	_	
17-1462	7476-7477	/	_	_	
17-1463	7477-7480	774	_	_	
17-1464	7480-7481	]	_	_	
17-1465	7482-7486	loss	_	_	
17-1466	7486-7487	:	_	_	
17-1467	7487-7493	0.0269	_	_	
17-1468	7493-7494	(	_	_	
17-1469	7494-7500	0.0236	_	_	
17-1470	7500-7501	)	_	_	
17-1471	7503-7507	prec	_	_	
17-1472	7507-7508	@	_	_	
17-1473	7508-7509	1	_	_	
17-1474	7509-7510	:	_	_	
17-1475	7510-7515	98.44	_	_	
17-1476	7515-7516	(	_	_	
17-1477	7516-7521	99.21	_	_	
17-1478	7521-7522	)	_	_	
17-1479	7524-7528	prec	_	_	
17-1480	7528-7529	@	_	_	
17-1481	7529-7530	5	_	_	
17-1482	7530-7531	:	_	_	
17-1483	7531-7537	100.00	_	_	
17-1484	7537-7538	(	_	_	
17-1485	7538-7544	100.00	_	_	
17-1486	7544-7545	)	_	_	
17-1487	7546-7551	Epoch	_	_	
17-1488	7551-7552	[	_	_	
17-1489	7552-7554	51	_	_	
17-1490	7554-7555	]	_	_	
17-1491	7555-7556	:	_	_	
17-1492	7556-7557	[	_	_	
17-1493	7557-7560	600	_	_	
17-1494	7560-7561	/	_	_	
17-1495	7561-7564	774	_	_	
17-1496	7564-7565	]	_	_	
17-1497	7566-7570	loss	_	_	
17-1498	7570-7571	:	_	_	
17-1499	7571-7577	0.0219	_	_	
17-1500	7577-7578	(	_	_	
17-1501	7578-7584	0.0234	_	_	
17-1502	7584-7585	)	_	_	
17-1503	7587-7591	prec	_	_	
17-1504	7591-7592	@	_	_	
17-1505	7592-7593	1	_	_	
17-1506	7593-7594	:	_	_	
17-1507	7594-7600	100.00	_	_	
17-1508	7600-7601	(	_	_	
17-1509	7601-7606	99.23	_	_	
17-1510	7606-7607	)	_	_	
17-1511	7609-7613	prec	_	_	
17-1512	7613-7614	@	_	_	
17-1513	7614-7615	5	_	_	
17-1514	7615-7616	:	_	_	
17-1515	7616-7622	100.00	_	_	
17-1516	7622-7623	(	_	_	
17-1517	7623-7629	100.00	_	_	
17-1518	7629-7630	)	_	_	
17-1519	7631-7632	[	_	_	
17-1520	7632-7637	Clean	_	_	
17-1521	7637-7638	]	_	_	
17-1522	7639-7643	Prec	_	_	
17-1523	7643-7644	@	_	_	
17-1524	7644-7645	1	_	_	
17-1525	7645-7646	:	_	_	
17-1526	7647-7652	93.19	_	_	
17-1527	7652-7653	,	_	_	
17-1528	7654-7658	Loss	_	_	
17-1529	7658-7659	:	_	_	
17-1530	7660-7666	0.2604	_	_	
17-1531	7667-7668	[	_	_	
17-1532	7668-7671	Bad	_	_	
17-1533	7671-7672	]	_	_	
17-1534	7673-7677	Prec	_	_	
17-1535	7677-7678	@	_	_	
17-1536	7678-7679	1	_	_	
17-1537	7679-7680	:	_	_	
17-1538	7681-7687	100.00	_	_	
17-1539	7687-7688	,	_	_	
17-1540	7689-7693	Loss	_	_	
17-1541	7693-7694	:	_	_	
17-1542	7695-7701	0.0000	_	_	
17-1543	7702-7707	epoch	_	_	
17-1544	7707-7708	:	_	_	
17-1545	7709-7711	51	_	_	
17-1546	7713-7715	lr	_	_	
17-1547	7715-7716	:	_	_	
17-1548	7717-7723	0.0100	_	_	
17-1549	7724-7729	Epoch	_	_	
17-1550	7729-7730	[	_	_	
17-1551	7730-7732	52	_	_	
17-1552	7732-7733	]	_	_	
17-1553	7733-7734	:	_	_	
17-1554	7734-7735	[	_	_	
17-1555	7735-7738	200	_	_	
17-1556	7738-7739	/	_	_	
17-1557	7739-7742	774	_	_	
17-1558	7742-7743	]	_	_	
17-1559	7744-7748	loss	_	_	
17-1560	7748-7749	:	_	_	
17-1561	7749-7755	0.0154	_	_	
17-1562	7755-7756	(	_	_	
17-1563	7756-7762	0.0201	_	_	
17-1564	7762-7763	)	_	_	
17-1565	7765-7769	prec	_	_	
17-1566	7769-7770	@	_	_	
17-1567	7770-7771	1	_	_	
17-1568	7771-7772	:	_	_	
17-1569	7772-7777	98.44	_	_	
17-1570	7777-7778	(	_	_	
17-1571	7778-7783	99.34	_	_	
17-1572	7783-7784	)	_	_	
17-1573	7786-7790	prec	_	_	
17-1574	7790-7791	@	_	_	
17-1575	7791-7792	5	_	_	
17-1576	7792-7793	:	_	_	
17-1577	7793-7799	100.00	_	_	
17-1578	7799-7800	(	_	_	
17-1579	7800-7806	100.00	_	_	
17-1580	7806-7807	)	_	_	
17-1581	7808-7813	Epoch	_	_	
17-1582	7813-7814	[	_	_	
17-1583	7814-7816	52	_	_	
17-1584	7816-7817	]	_	_	
17-1585	7817-7818	:	_	_	
17-1586	7818-7819	[	_	_	
17-1587	7819-7822	400	_	_	
17-1588	7822-7823	/	_	_	
17-1589	7823-7826	774	_	_	
17-1590	7826-7827	]	_	_	
17-1591	7828-7832	loss	_	_	
17-1592	7832-7833	:	_	_	
17-1593	7833-7839	0.0328	_	_	
17-1594	7839-7840	(	_	_	
17-1595	7840-7846	0.0200	_	_	
17-1596	7846-7847	)	_	_	
17-1597	7849-7853	prec	_	_	
17-1598	7853-7854	@	_	_	
17-1599	7854-7855	1	_	_	
17-1600	7855-7856	:	_	_	
17-1601	7856-7861	98.44	_	_	
17-1602	7861-7862	(	_	_	
17-1603	7862-7867	99.38	_	_	
17-1604	7867-7868	)	_	_	
17-1605	7870-7874	prec	_	_	
17-1606	7874-7875	@	_	_	
17-1607	7875-7876	5	_	_	
17-1608	7876-7877	:	_	_	
17-1609	7877-7883	100.00	_	_	
17-1610	7883-7884	(	_	_	
17-1611	7884-7890	100.00	_	_	
17-1612	7890-7891	)	_	_	
17-1613	7892-7897	Epoch	_	_	
17-1614	7897-7898	[	_	_	
17-1615	7898-7900	52	_	_	
17-1616	7900-7901	]	_	_	
17-1617	7901-7902	:	_	_	
17-1618	7902-7903	[	_	_	
17-1619	7903-7906	600	_	_	
17-1620	7906-7907	/	_	_	
17-1621	7907-7910	774	_	_	
17-1622	7910-7911	]	_	_	
17-1623	7912-7916	loss	_	_	
17-1624	7916-7917	:	_	_	
17-1625	7917-7923	0.0220	_	_	
17-1626	7923-7924	(	_	_	
17-1627	7924-7930	0.0204	_	_	
17-1628	7930-7931	)	_	_	
17-1629	7933-7937	prec	_	_	
17-1630	7937-7938	@	_	_	
17-1631	7938-7939	1	_	_	
17-1632	7939-7940	:	_	_	
17-1633	7940-7945	98.44	_	_	
17-1634	7945-7946	(	_	_	
17-1635	7946-7951	99.36	_	_	
17-1636	7951-7952	)	_	_	
17-1637	7954-7958	prec	_	_	
17-1638	7958-7959	@	_	_	
17-1639	7959-7960	5	_	_	
17-1640	7960-7961	:	_	_	
17-1641	7961-7967	100.00	_	_	
17-1642	7967-7968	(	_	_	
17-1643	7968-7974	100.00	_	_	
17-1644	7974-7975	)	_	_	
17-1645	7976-7977	[	_	_	
17-1646	7977-7982	Clean	_	_	
17-1647	7982-7983	]	_	_	
17-1648	7984-7988	Prec	_	_	
17-1649	7988-7989	@	_	_	
17-1650	7989-7990	1	_	_	
17-1651	7990-7991	:	_	_	
17-1652	7992-7997	93.27	_	_	
17-1653	7997-7998	,	_	_	
17-1654	7999-8003	Loss	_	_	
17-1655	8003-8004	:	_	_	
17-1656	8005-8011	0.2652	_	_	
17-1657	8012-8013	[	_	_	
17-1658	8013-8016	Bad	_	_	
17-1659	8016-8017	]	_	_	
17-1660	8018-8022	Prec	_	_	
17-1661	8022-8023	@	_	_	
17-1662	8023-8024	1	_	_	
17-1663	8024-8025	:	_	_	
17-1664	8026-8032	100.00	_	_	
17-1665	8032-8033	,	_	_	
17-1666	8034-8038	Loss	_	_	
17-1667	8038-8039	:	_	_	
17-1668	8040-8046	0.0000	_	_	
17-1669	8047-8052	epoch	_	_	
17-1670	8052-8053	:	_	_	
17-1671	8054-8056	52	_	_	
17-1672	8058-8060	lr	_	_	
17-1673	8060-8061	:	_	_	
17-1674	8062-8068	0.0100	_	_	
17-1675	8069-8074	Epoch	_	_	
17-1676	8074-8075	[	_	_	
17-1677	8075-8077	53	_	_	
17-1678	8077-8078	]	_	_	
17-1679	8078-8079	:	_	_	
17-1680	8079-8080	[	_	_	
17-1681	8080-8083	200	_	_	
17-1682	8083-8084	/	_	_	
17-1683	8084-8087	774	_	_	
17-1684	8087-8088	]	_	_	
17-1685	8089-8093	loss	_	_	
17-1686	8093-8094	:	_	_	
17-1687	8094-8100	0.0090	_	_	
17-1688	8100-8101	(	_	_	
17-1689	8101-8107	0.0194	_	_	
17-1690	8107-8108	)	_	_	
17-1691	8110-8114	prec	_	_	
17-1692	8114-8115	@	_	_	
17-1693	8115-8116	1	_	_	
17-1694	8116-8117	:	_	_	
17-1695	8117-8123	100.00	_	_	
17-1696	8123-8124	(	_	_	
17-1697	8124-8129	99.39	_	_	
17-1698	8129-8130	)	_	_	
17-1699	8132-8136	prec	_	_	
17-1700	8136-8137	@	_	_	
17-1701	8137-8138	5	_	_	
17-1702	8138-8139	:	_	_	
17-1703	8139-8145	100.00	_	_	
17-1704	8145-8146	(	_	_	
17-1705	8146-8152	100.00	_	_	
17-1706	8152-8153	)	_	_	
17-1707	8154-8159	Epoch	_	_	
17-1708	8159-8160	[	_	_	
17-1709	8160-8162	53	_	_	
17-1710	8162-8163	]	_	_	
17-1711	8163-8164	:	_	_	
17-1712	8164-8165	[	_	_	
17-1713	8165-8168	400	_	_	
17-1714	8168-8169	/	_	_	
17-1715	8169-8172	774	_	_	
17-1716	8172-8173	]	_	_	
17-1717	8174-8178	loss	_	_	
17-1718	8178-8179	:	_	_	
17-1719	8179-8185	0.0019	_	_	
17-1720	8185-8186	(	_	_	
17-1721	8186-8192	0.0195	_	_	
17-1722	8192-8193	)	_	_	
17-1723	8195-8199	prec	_	_	
17-1724	8199-8200	@	_	_	
17-1725	8200-8201	1	_	_	
17-1726	8201-8202	:	_	_	
17-1727	8202-8208	100.00	_	_	
17-1728	8208-8209	(	_	_	
17-1729	8209-8214	99.41	_	_	
17-1730	8214-8215	)	_	_	
17-1731	8217-8221	prec	_	_	
17-1732	8221-8222	@	_	_	
17-1733	8222-8223	5	_	_	
17-1734	8223-8224	:	_	_	
17-1735	8224-8230	100.00	_	_	
17-1736	8230-8231	(	_	_	
17-1737	8231-8237	100.00	_	_	
17-1738	8237-8238	)	_	_	
17-1739	8239-8244	Epoch	_	_	
17-1740	8244-8245	[	_	_	
17-1741	8245-8247	53	_	_	
17-1742	8247-8248	]	_	_	
17-1743	8248-8249	:	_	_	
17-1744	8249-8250	[	_	_	
17-1745	8250-8253	600	_	_	
17-1746	8253-8254	/	_	_	
17-1747	8254-8257	774	_	_	
17-1748	8257-8258	]	_	_	
17-1749	8259-8263	loss	_	_	
17-1750	8263-8264	:	_	_	
17-1751	8264-8270	0.0402	_	_	
17-1752	8270-8271	(	_	_	
17-1753	8271-8277	0.0190	_	_	
17-1754	8277-8278	)	_	_	
17-1755	8280-8284	prec	_	_	
17-1756	8284-8285	@	_	_	
17-1757	8285-8286	1	_	_	
17-1758	8286-8287	:	_	_	
17-1759	8287-8292	98.44	_	_	
17-1760	8292-8293	(	_	_	
17-1761	8293-8298	99.45	_	_	
17-1762	8298-8299	)	_	_	
17-1763	8301-8305	prec	_	_	
17-1764	8305-8306	@	_	_	
17-1765	8306-8307	5	_	_	
17-1766	8307-8308	:	_	_	
17-1767	8308-8314	100.00	_	_	
17-1768	8314-8315	(	_	_	
17-1769	8315-8321	100.00	_	_	
17-1770	8321-8322	)	_	_	
17-1771	8323-8324	[	_	_	
17-1772	8324-8329	Clean	_	_	
17-1773	8329-8330	]	_	_	
17-1774	8331-8335	Prec	_	_	
17-1775	8335-8336	@	_	_	
17-1776	8336-8337	1	_	_	
17-1777	8337-8338	:	_	_	
17-1778	8339-8344	93.04	_	_	
17-1779	8344-8345	,	_	_	
17-1780	8346-8350	Loss	_	_	
17-1781	8350-8351	:	_	_	
17-1782	8352-8358	0.2735	_	_	
17-1783	8359-8360	[	_	_	
17-1784	8360-8363	Bad	_	_	
17-1785	8363-8364	]	_	_	
17-1786	8365-8369	Prec	_	_	
17-1787	8369-8370	@	_	_	
17-1788	8370-8371	1	_	_	
17-1789	8371-8372	:	_	_	
17-1790	8373-8379	100.00	_	_	
17-1791	8379-8380	,	_	_	
17-1792	8381-8385	Loss	_	_	
17-1793	8385-8386	:	_	_	
17-1794	8387-8393	0.0000	_	_	
17-1795	8394-8399	epoch	_	_	
17-1796	8399-8400	:	_	_	
17-1797	8401-8403	53	_	_	
17-1798	8405-8407	lr	_	_	
17-1799	8407-8408	:	_	_	
17-1800	8409-8415	0.0100	_	_	
17-1801	8416-8421	Epoch	_	_	
17-1802	8421-8422	[	_	_	
17-1803	8422-8424	54	_	_	
17-1804	8424-8425	]	_	_	
17-1805	8425-8426	:	_	_	
17-1806	8426-8427	[	_	_	
17-1807	8427-8430	200	_	_	
17-1808	8430-8431	/	_	_	
17-1809	8431-8434	774	_	_	
17-1810	8434-8435	]	_	_	
17-1811	8436-8440	loss	_	_	
17-1812	8440-8441	:	_	_	
17-1813	8441-8447	0.0154	_	_	
17-1814	8447-8448	(	_	_	
17-1815	8448-8454	0.0186	_	_	
17-1816	8454-8455	)	_	_	
17-1817	8457-8461	prec	_	_	
17-1818	8461-8462	@	_	_	
17-1819	8462-8463	1	_	_	
17-1820	8463-8464	:	_	_	
17-1821	8464-8470	100.00	_	_	
17-1822	8470-8471	(	_	_	
17-1823	8471-8476	99.38	_	_	
17-1824	8476-8477	)	_	_	
17-1825	8479-8483	prec	_	_	
17-1826	8483-8484	@	_	_	
17-1827	8484-8485	5	_	_	
17-1828	8485-8486	:	_	_	
17-1829	8486-8492	100.00	_	_	
17-1830	8492-8493	(	_	_	
17-1831	8493-8499	100.00	_	_	
17-1832	8499-8500	)	_	_	
17-1833	8501-8506	Epoch	_	_	
17-1834	8506-8507	[	_	_	
17-1835	8507-8509	54	_	_	
17-1836	8509-8510	]	_	_	
17-1837	8510-8511	:	_	_	
17-1838	8511-8512	[	_	_	
17-1839	8512-8515	400	_	_	
17-1840	8515-8516	/	_	_	
17-1841	8516-8519	774	_	_	
17-1842	8519-8520	]	_	_	
17-1843	8521-8525	loss	_	_	
17-1844	8525-8526	:	_	_	
17-1845	8526-8532	0.0124	_	_	
17-1846	8532-8533	(	_	_	
17-1847	8533-8539	0.0182	_	_	
17-1848	8539-8540	)	_	_	
17-1849	8542-8546	prec	_	_	
17-1850	8546-8547	@	_	_	
17-1851	8547-8548	1	_	_	
17-1852	8548-8549	:	_	_	
17-1853	8549-8555	100.00	_	_	
17-1854	8555-8556	(	_	_	
17-1855	8556-8561	99.40	_	_	
17-1856	8561-8562	)	_	_	
17-1857	8564-8568	prec	_	_	
17-1858	8568-8569	@	_	_	
17-1859	8569-8570	5	_	_	
17-1860	8570-8571	:	_	_	
17-1861	8571-8577	100.00	_	_	
17-1862	8577-8578	(	_	_	
17-1863	8578-8584	100.00	_	_	
17-1864	8584-8585	)	_	_	
17-1865	8586-8591	Epoch	_	_	
17-1866	8591-8592	[	_	_	
17-1867	8592-8594	54	_	_	
17-1868	8594-8595	]	_	_	
17-1869	8595-8596	:	_	_	
17-1870	8596-8597	[	_	_	
17-1871	8597-8600	600	_	_	
17-1872	8600-8601	/	_	_	
17-1873	8601-8604	774	_	_	
17-1874	8604-8605	]	_	_	
17-1875	8606-8610	loss	_	_	
17-1876	8610-8611	:	_	_	
17-1877	8611-8617	0.0144	_	_	
17-1878	8617-8618	(	_	_	
17-1879	8618-8624	0.0181	_	_	
17-1880	8624-8625	)	_	_	
17-1881	8627-8631	prec	_	_	
17-1882	8631-8632	@	_	_	
17-1883	8632-8633	1	_	_	
17-1884	8633-8634	:	_	_	
17-1885	8634-8640	100.00	_	_	
17-1886	8640-8641	(	_	_	
17-1887	8641-8646	99.45	_	_	
17-1888	8646-8647	)	_	_	
17-1889	8649-8653	prec	_	_	
17-1890	8653-8654	@	_	_	
17-1891	8654-8655	5	_	_	
17-1892	8655-8656	:	_	_	
17-1893	8656-8662	100.00	_	_	
17-1894	8662-8663	(	_	_	
17-1895	8663-8669	100.00	_	_	
17-1896	8669-8670	)	_	_	
17-1897	8671-8672	[	_	_	
17-1898	8672-8677	Clean	_	_	
17-1899	8677-8678	]	_	_	
17-1900	8679-8683	Prec	_	_	
17-1901	8683-8684	@	_	_	
17-1902	8684-8685	1	_	_	
17-1903	8685-8686	:	_	_	
17-1904	8687-8692	93.17	_	_	
17-1905	8692-8693	,	_	_	
17-1906	8694-8698	Loss	_	_	
17-1907	8698-8699	:	_	_	
17-1908	8700-8706	0.2693	_	_	
17-1909	8707-8708	[	_	_	
17-1910	8708-8711	Bad	_	_	
17-1911	8711-8712	]	_	_	
17-1912	8713-8717	Prec	_	_	
17-1913	8717-8718	@	_	_	
17-1914	8718-8719	1	_	_	
17-1915	8719-8720	:	_	_	
17-1916	8721-8727	100.00	_	_	
17-1917	8727-8728	,	_	_	
17-1918	8729-8733	Loss	_	_	
17-1919	8733-8734	:	_	_	
17-1920	8735-8741	0.0000	_	_	
17-1921	8742-8747	epoch	_	_	
17-1922	8747-8748	:	_	_	
17-1923	8749-8751	54	_	_	
17-1924	8753-8755	lr	_	_	
17-1925	8755-8756	:	_	_	
17-1926	8757-8763	0.0100	_	_	
17-1927	8764-8769	Epoch	_	_	
17-1928	8769-8770	[	_	_	
17-1929	8770-8772	55	_	_	
17-1930	8772-8773	]	_	_	
17-1931	8773-8774	:	_	_	
17-1932	8774-8775	[	_	_	
17-1933	8775-8778	200	_	_	
17-1934	8778-8779	/	_	_	
17-1935	8779-8782	774	_	_	
17-1936	8782-8783	]	_	_	
17-1937	8784-8788	loss	_	_	
17-1938	8788-8789	:	_	_	
17-1939	8789-8795	0.0119	_	_	
17-1940	8795-8796	(	_	_	
17-1941	8796-8802	0.0168	_	_	
17-1942	8802-8803	)	_	_	
17-1943	8805-8809	prec	_	_	
17-1944	8809-8810	@	_	_	
17-1945	8810-8811	1	_	_	
17-1946	8811-8812	:	_	_	
17-1947	8812-8818	100.00	_	_	
17-1948	8818-8819	(	_	_	
17-1949	8819-8824	99.43	_	_	
17-1950	8824-8825	)	_	_	
17-1951	8827-8831	prec	_	_	
17-1952	8831-8832	@	_	_	
17-1953	8832-8833	5	_	_	
17-1954	8833-8834	:	_	_	
17-1955	8834-8840	100.00	_	_	
17-1956	8840-8841	(	_	_	
17-1957	8841-8847	100.00	_	_	
17-1958	8847-8848	)	_	_	
17-1959	8849-8854	Epoch	_	_	
17-1960	8854-8855	[	_	_	
17-1961	8855-8857	55	_	_	
17-1962	8857-8858	]	_	_	
17-1963	8858-8859	:	_	_	
17-1964	8859-8860	[	_	_	
17-1965	8860-8863	400	_	_	
17-1966	8863-8864	/	_	_	
17-1967	8864-8867	774	_	_	
17-1968	8867-8868	]	_	_	
17-1969	8869-8873	loss	_	_	
17-1970	8873-8874	:	_	_	
17-1971	8874-8880	0.0228	_	_	
17-1972	8880-8881	(	_	_	
17-1973	8881-8887	0.0170	_	_	
17-1974	8887-8888	)	_	_	
17-1975	8890-8894	prec	_	_	
17-1976	8894-8895	@	_	_	
17-1977	8895-8896	1	_	_	
17-1978	8896-8897	:	_	_	
17-1979	8897-8902	98.44	_	_	
17-1980	8902-8903	(	_	_	
17-1981	8903-8908	99.42	_	_	
17-1982	8908-8909	)	_	_	
17-1983	8911-8915	prec	_	_	
17-1984	8915-8916	@	_	_	
17-1985	8916-8917	5	_	_	
17-1986	8917-8918	:	_	_	
17-1987	8918-8924	100.00	_	_	
17-1988	8924-8925	(	_	_	
17-1989	8925-8931	100.00	_	_	
17-1990	8931-8932	)	_	_	
17-1991	8933-8938	Epoch	_	_	
17-1992	8938-8939	[	_	_	
17-1993	8939-8941	55	_	_	
17-1994	8941-8942	]	_	_	
17-1995	8942-8943	:	_	_	
17-1996	8943-8944	[	_	_	
17-1997	8944-8947	600	_	_	
17-1998	8947-8948	/	_	_	
17-1999	8948-8951	774	_	_	
17-2000	8951-8952	]	_	_	
17-2001	8953-8957	loss	_	_	
17-2002	8957-8958	:	_	_	
17-2003	8958-8964	0.0096	_	_	
17-2004	8964-8965	(	_	_	
17-2005	8965-8971	0.0164	_	_	
17-2006	8971-8972	)	_	_	
17-2007	8974-8978	prec	_	_	
17-2008	8978-8979	@	_	_	
17-2009	8979-8980	1	_	_	
17-2010	8980-8981	:	_	_	
17-2011	8981-8987	100.00	_	_	
17-2012	8987-8988	(	_	_	
17-2013	8988-8993	99.47	_	_	
17-2014	8993-8994	)	_	_	
17-2015	8996-9000	prec	_	_	
17-2016	9000-9001	@	_	_	
17-2017	9001-9002	5	_	_	
17-2018	9002-9003	:	_	_	
17-2019	9003-9009	100.00	_	_	
17-2020	9009-9010	(	_	_	
17-2021	9010-9016	100.00	_	_	
17-2022	9016-9017	)	_	_	
17-2023	9018-9019	[	_	_	
17-2024	9019-9024	Clean	_	_	
17-2025	9024-9025	]	_	_	
17-2026	9026-9030	Prec	_	_	
17-2027	9030-9031	@	_	_	
17-2028	9031-9032	1	_	_	
17-2029	9032-9033	:	_	_	
17-2030	9034-9039	92.84	_	_	
17-2031	9039-9040	,	_	_	
17-2032	9041-9045	Loss	_	_	
17-2033	9045-9046	:	_	_	
17-2034	9047-9053	0.2786	_	_	
17-2035	9054-9055	[	_	_	
17-2036	9055-9058	Bad	_	_	
17-2037	9058-9059	]	_	_	
17-2038	9060-9064	Prec	_	_	
17-2039	9064-9065	@	_	_	
17-2040	9065-9066	1	_	_	
17-2041	9066-9067	:	_	_	
17-2042	9068-9074	100.00	_	_	
17-2043	9074-9075	,	_	_	
17-2044	9076-9080	Loss	_	_	
17-2045	9080-9081	:	_	_	
17-2046	9082-9088	0.0001	_	_	
17-2047	9089-9094	epoch	_	_	
17-2048	9094-9095	:	_	_	
17-2049	9096-9098	55	_	_	
17-2050	9100-9102	lr	_	_	
17-2051	9102-9103	:	_	_	
17-2052	9104-9110	0.0100	_	_	
17-2053	9111-9116	Epoch	_	_	
17-2054	9116-9117	[	_	_	
17-2055	9117-9119	56	_	_	
17-2056	9119-9120	]	_	_	
17-2057	9120-9121	:	_	_	
17-2058	9121-9122	[	_	_	
17-2059	9122-9125	200	_	_	
17-2060	9125-9126	/	_	_	
17-2061	9126-9129	774	_	_	
17-2062	9129-9130	]	_	_	
17-2063	9131-9135	loss	_	_	
17-2064	9135-9136	:	_	_	
17-2065	9136-9142	0.0307	_	_	
17-2066	9142-9143	(	_	_	
17-2067	9143-9149	0.0146	_	_	
17-2068	9149-9150	)	_	_	
17-2069	9152-9156	prec	_	_	
17-2070	9156-9157	@	_	_	
17-2071	9157-9158	1	_	_	
17-2072	9158-9159	:	_	_	
17-2073	9159-9164	98.44	_	_	
17-2074	9164-9165	(	_	_	
17-2075	9165-9170	99.51	_	_	
17-2076	9170-9171	)	_	_	
17-2077	9173-9177	prec	_	_	
17-2078	9177-9178	@	_	_	
17-2079	9178-9179	5	_	_	
17-2080	9179-9180	:	_	_	
17-2081	9180-9186	100.00	_	_	
17-2082	9186-9187	(	_	_	
17-2083	9187-9193	100.00	_	_	
17-2084	9193-9194	)	_	_	
17-2085	9195-9200	Epoch	_	_	
17-2086	9200-9201	[	_	_	
17-2087	9201-9203	56	_	_	
17-2088	9203-9204	]	_	_	
17-2089	9204-9205	:	_	_	
17-2090	9205-9206	[	_	_	
17-2091	9206-9209	400	_	_	
17-2092	9209-9210	/	_	_	
17-2093	9210-9213	774	_	_	
17-2094	9213-9214	]	_	_	
17-2095	9215-9219	loss	_	_	
17-2096	9219-9220	:	_	_	
17-2097	9220-9226	0.0065	_	_	
17-2098	9226-9227	(	_	_	
17-2099	9227-9233	0.0149	_	_	
17-2100	9233-9234	)	_	_	
17-2101	9236-9240	prec	_	_	
17-2102	9240-9241	@	_	_	
17-2103	9241-9242	1	_	_	
17-2104	9242-9243	:	_	_	
17-2105	9243-9249	100.00	_	_	
17-2106	9249-9250	(	_	_	
17-2107	9250-9255	99.52	_	_	
17-2108	9255-9256	)	_	_	
17-2109	9258-9262	prec	_	_	
17-2110	9262-9263	@	_	_	
17-2111	9263-9264	5	_	_	
17-2112	9264-9265	:	_	_	
17-2113	9265-9271	100.00	_	_	
17-2114	9271-9272	(	_	_	
17-2115	9272-9278	100.00	_	_	
17-2116	9278-9279	)	_	_	
17-2117	9280-9285	Epoch	_	_	
17-2118	9285-9286	[	_	_	
17-2119	9286-9288	56	_	_	
17-2120	9288-9289	]	_	_	
17-2121	9289-9290	:	_	_	
17-2122	9290-9291	[	_	_	
17-2123	9291-9294	600	_	_	
17-2124	9294-9295	/	_	_	
17-2125	9295-9298	774	_	_	
17-2126	9298-9299	]	_	_	
17-2127	9300-9304	loss	_	_	
17-2128	9304-9305	:	_	_	
17-2129	9305-9311	0.0348	_	_	
17-2130	9311-9312	(	_	_	
17-2131	9312-9318	0.0155	_	_	
17-2132	9318-9319	)	_	_	
17-2133	9321-9325	prec	_	_	
17-2134	9325-9326	@	_	_	
17-2135	9326-9327	1	_	_	
17-2136	9327-9328	:	_	_	
17-2137	9328-9333	98.44	_	_	
17-2138	9333-9334	(	_	_	
17-2139	9334-9339	99.50	_	_	
17-2140	9339-9340	)	_	_	
17-2141	9342-9346	prec	_	_	
17-2142	9346-9347	@	_	_	
17-2143	9347-9348	5	_	_	
17-2144	9348-9349	:	_	_	
17-2145	9349-9355	100.00	_	_	
17-2146	9355-9356	(	_	_	
17-2147	9356-9362	100.00	_	_	
17-2148	9362-9363	)	_	_	
17-2149	9364-9365	[	_	_	
17-2150	9365-9370	Clean	_	_	
17-2151	9370-9371	]	_	_	
17-2152	9372-9376	Prec	_	_	
17-2153	9376-9377	@	_	_	
17-2154	9377-9378	1	_	_	
17-2155	9378-9379	:	_	_	
17-2156	9380-9385	93.12	_	_	
17-2157	9385-9386	,	_	_	
17-2158	9387-9391	Loss	_	_	
17-2159	9391-9392	:	_	_	
17-2160	9393-9399	0.2794	_	_	
17-2161	9400-9401	[	_	_	
17-2162	9401-9404	Bad	_	_	
17-2163	9404-9405	]	_	_	
17-2164	9406-9410	Prec	_	_	
17-2165	9410-9411	@	_	_	
17-2166	9411-9412	1	_	_	
17-2167	9412-9413	:	_	_	
17-2168	9414-9420	100.00	_	_	
17-2169	9420-9421	,	_	_	
17-2170	9422-9426	Loss	_	_	
17-2171	9426-9427	:	_	_	
17-2172	9428-9434	0.0000	_	_	
17-2173	9435-9440	epoch	_	_	
17-2174	9440-9441	:	_	_	
17-2175	9442-9444	56	_	_	
17-2176	9446-9448	lr	_	_	
17-2177	9448-9449	:	_	_	
17-2178	9450-9456	0.0100	_	_	
17-2179	9457-9462	Epoch	_	_	
17-2180	9462-9463	[	_	_	
17-2181	9463-9465	57	_	_	
17-2182	9465-9466	]	_	_	
17-2183	9466-9467	:	_	_	
17-2184	9467-9468	[	_	_	
17-2185	9468-9471	200	_	_	
17-2186	9471-9472	/	_	_	
17-2187	9472-9475	774	_	_	
17-2188	9475-9476	]	_	_	
17-2189	9477-9481	loss	_	_	
17-2190	9481-9482	:	_	_	
17-2191	9482-9488	0.0014	_	_	
17-2192	9488-9489	(	_	_	
17-2193	9489-9495	0.0134	_	_	
17-2194	9495-9496	)	_	_	
17-2195	9498-9502	prec	_	_	
17-2196	9502-9503	@	_	_	
17-2197	9503-9504	1	_	_	
17-2198	9504-9505	:	_	_	
17-2199	9505-9511	100.00	_	_	
17-2200	9511-9512	(	_	_	
17-2201	9512-9517	99.59	_	_	
17-2202	9517-9518	)	_	_	
17-2203	9520-9524	prec	_	_	
17-2204	9524-9525	@	_	_	
17-2205	9525-9526	5	_	_	
17-2206	9526-9527	:	_	_	
17-2207	9527-9533	100.00	_	_	
17-2208	9533-9534	(	_	_	
17-2209	9534-9540	100.00	_	_	
17-2210	9540-9541	)	_	_	
17-2211	9542-9547	Epoch	_	_	
17-2212	9547-9548	[	_	_	
17-2213	9548-9550	57	_	_	
17-2214	9550-9551	]	_	_	
17-2215	9551-9552	:	_	_	
17-2216	9552-9553	[	_	_	
17-2217	9553-9556	400	_	_	
17-2218	9556-9557	/	_	_	
17-2219	9557-9560	774	_	_	
17-2220	9560-9561	]	_	_	
17-2221	9562-9566	loss	_	_	
17-2222	9566-9567	:	_	_	
17-2223	9567-9573	0.0060	_	_	
17-2224	9573-9574	(	_	_	
17-2225	9574-9580	0.0133	_	_	
17-2226	9580-9581	)	_	_	
17-2227	9583-9587	prec	_	_	
17-2228	9587-9588	@	_	_	
17-2229	9588-9589	1	_	_	
17-2230	9589-9590	:	_	_	
17-2231	9590-9596	100.00	_	_	
17-2232	9596-9597	(	_	_	
17-2233	9597-9602	99.59	_	_	
17-2234	9602-9603	)	_	_	
17-2235	9605-9609	prec	_	_	
17-2236	9609-9610	@	_	_	
17-2237	9610-9611	5	_	_	
17-2238	9611-9612	:	_	_	
17-2239	9612-9618	100.00	_	_	
17-2240	9618-9619	(	_	_	
17-2241	9619-9625	100.00	_	_	
17-2242	9625-9626	)	_	_	
17-2243	9627-9632	Epoch	_	_	
17-2244	9632-9633	[	_	_	
17-2245	9633-9635	57	_	_	
17-2246	9635-9636	]	_	_	
17-2247	9636-9637	:	_	_	
17-2248	9637-9638	[	_	_	
17-2249	9638-9641	600	_	_	
17-2250	9641-9642	/	_	_	
17-2251	9642-9645	774	_	_	
17-2252	9645-9646	]	_	_	
17-2253	9647-9651	loss	_	_	
17-2254	9651-9652	:	_	_	
17-2255	9652-9658	0.0400	_	_	
17-2256	9658-9659	(	_	_	
17-2257	9659-9665	0.0133	_	_	
17-2258	9665-9666	)	_	_	
17-2259	9668-9672	prec	_	_	
17-2260	9672-9673	@	_	_	
17-2261	9673-9674	1	_	_	
17-2262	9674-9675	:	_	_	
17-2263	9675-9680	95.31	_	_	
17-2264	9680-9681	(	_	_	
17-2265	9681-9686	99.61	_	_	
17-2266	9686-9687	)	_	_	
17-2267	9689-9693	prec	_	_	
17-2268	9693-9694	@	_	_	
17-2269	9694-9695	5	_	_	
17-2270	9695-9696	:	_	_	
17-2271	9696-9702	100.00	_	_	
17-2272	9702-9703	(	_	_	
17-2273	9703-9709	100.00	_	_	
17-2274	9709-9710	)	_	_	
17-2275	9711-9712	[	_	_	
17-2276	9712-9717	Clean	_	_	
17-2277	9717-9718	]	_	_	
17-2278	9719-9723	Prec	_	_	
17-2279	9723-9724	@	_	_	
17-2280	9724-9725	1	_	_	
17-2281	9725-9726	:	_	_	
17-2282	9727-9732	93.13	_	_	
17-2283	9732-9733	,	_	_	
17-2284	9734-9738	Loss	_	_	
17-2285	9738-9739	:	_	_	
17-2286	9740-9746	0.2819	_	_	
17-2287	9747-9748	[	_	_	
17-2288	9748-9751	Bad	_	_	
17-2289	9751-9752	]	_	_	
17-2290	9753-9757	Prec	_	_	
17-2291	9757-9758	@	_	_	
17-2292	9758-9759	1	_	_	
17-2293	9759-9760	:	_	_	
17-2294	9761-9767	100.00	_	_	
17-2295	9767-9768	,	_	_	
17-2296	9769-9773	Loss	_	_	
17-2297	9773-9774	:	_	_	
17-2298	9775-9781	0.0000	_	_	
17-2299	9782-9787	epoch	_	_	
17-2300	9787-9788	:	_	_	
17-2301	9789-9791	57	_	_	
17-2302	9793-9795	lr	_	_	
17-2303	9795-9796	:	_	_	
17-2304	9797-9803	0.0100	_	_	
17-2305	9804-9809	Epoch	_	_	
17-2306	9809-9810	[	_	_	
17-2307	9810-9812	58	_	_	
17-2308	9812-9813	]	_	_	
17-2309	9813-9814	:	_	_	
17-2310	9814-9815	[	_	_	
17-2311	9815-9818	200	_	_	
17-2312	9818-9819	/	_	_	
17-2313	9819-9822	774	_	_	
17-2314	9822-9823	]	_	_	
17-2315	9824-9828	loss	_	_	
17-2316	9828-9829	:	_	_	
17-2317	9829-9835	0.0062	_	_	
17-2318	9835-9836	(	_	_	
17-2319	9836-9842	0.0122	_	_	
17-2320	9842-9843	)	_	_	
17-2321	9845-9849	prec	_	_	
17-2322	9849-9850	@	_	_	
17-2323	9850-9851	1	_	_	
17-2324	9851-9852	:	_	_	
17-2325	9852-9858	100.00	_	_	
17-2326	9858-9859	(	_	_	
17-2327	9859-9864	99.60	_	_	
17-2328	9864-9865	)	_	_	
17-2329	9867-9871	prec	_	_	
17-2330	9871-9872	@	_	_	
17-2331	9872-9873	5	_	_	
17-2332	9873-9874	:	_	_	
17-2333	9874-9880	100.00	_	_	
17-2334	9880-9881	(	_	_	
17-2335	9881-9887	100.00	_	_	
17-2336	9887-9888	)	_	_	
17-2337	9889-9894	Epoch	_	_	
17-2338	9894-9895	[	_	_	
17-2339	9895-9897	58	_	_	
17-2340	9897-9898	]	_	_	
17-2341	9898-9899	:	_	_	
17-2342	9899-9900	[	_	_	
17-2343	9900-9903	400	_	_	
17-2344	9903-9904	/	_	_	
17-2345	9904-9907	774	_	_	
17-2346	9907-9908	]	_	_	
17-2347	9909-9913	loss	_	_	
17-2348	9913-9914	:	_	_	
17-2349	9914-9920	0.0065	_	_	
17-2350	9920-9921	(	_	_	
17-2351	9921-9927	0.0134	_	_	
17-2352	9927-9928	)	_	_	
17-2353	9930-9934	prec	_	_	
17-2354	9934-9935	@	_	_	
17-2355	9935-9936	1	_	_	
17-2356	9936-9937	:	_	_	
17-2357	9937-9943	100.00	_	_	
17-2358	9943-9944	(	_	_	
17-2359	9944-9949	99.56	_	_	
17-2360	9949-9950	)	_	_	
17-2361	9952-9956	prec	_	_	
17-2362	9956-9957	@	_	_	
17-2363	9957-9958	5	_	_	
17-2364	9958-9959	:	_	_	
17-2365	9959-9965	100.00	_	_	
17-2366	9965-9966	(	_	_	
17-2367	9966-9972	100.00	_	_	
17-2368	9972-9973	)	_	_	
17-2369	9974-9979	Epoch	_	_	
17-2370	9979-9980	[	_	_	
17-2371	9980-9982	58	_	_	
17-2372	9982-9983	]	_	_	
17-2373	9983-9984	:	_	_	
17-2374	9984-9985	[	_	_	
17-2375	9985-9988	600	_	_	
17-2376	9988-9989	/	_	_	
17-2377	9989-9992	774	_	_	
17-2378	9992-9993	]	_	_	
17-2379	9994-9998	loss	_	_	
17-2380	9998-9999	:	_	_	
17-2381	9999-10005	0.0198	_	_	
17-2382	10005-10006	(	_	_	
17-2383	10006-10012	0.0134	_	_	
17-2384	10012-10013	)	_	_	
17-2385	10015-10019	prec	_	_	
17-2386	10019-10020	@	_	_	
17-2387	10020-10021	1	_	_	
17-2388	10021-10022	:	_	_	
17-2389	10022-10028	100.00	_	_	
17-2390	10028-10029	(	_	_	
17-2391	10029-10034	99.59	_	_	
17-2392	10034-10035	)	_	_	
17-2393	10037-10041	prec	_	_	
17-2394	10041-10042	@	_	_	
17-2395	10042-10043	5	_	_	
17-2396	10043-10044	:	_	_	
17-2397	10044-10050	100.00	_	_	
17-2398	10050-10051	(	_	_	
17-2399	10051-10057	100.00	_	_	
17-2400	10057-10058	)	_	_	
17-2401	10059-10060	[	_	_	
17-2402	10060-10065	Clean	_	_	
17-2403	10065-10066	]	_	_	
17-2404	10067-10071	Prec	_	_	
17-2405	10071-10072	@	_	_	
17-2406	10072-10073	1	_	_	
17-2407	10073-10074	:	_	_	
17-2408	10075-10080	93.11	_	_	
17-2409	10080-10081	,	_	_	
17-2410	10082-10086	Loss	_	_	
17-2411	10086-10087	:	_	_	
17-2412	10088-10094	0.2795	_	_	
17-2413	10095-10096	[	_	_	
17-2414	10096-10099	Bad	_	_	
17-2415	10099-10100	]	_	_	
17-2416	10101-10105	Prec	_	_	
17-2417	10105-10106	@	_	_	
17-2418	10106-10107	1	_	_	
17-2419	10107-10108	:	_	_	
17-2420	10109-10115	100.00	_	_	
17-2421	10115-10116	,	_	_	
17-2422	10117-10121	Loss	_	_	
17-2423	10121-10122	:	_	_	
17-2424	10123-10129	0.0000	_	_	
17-2425	10130-10135	epoch	_	_	
17-2426	10135-10136	:	_	_	
17-2427	10137-10139	58	_	_	
17-2428	10141-10143	lr	_	_	
17-2429	10143-10144	:	_	_	
17-2430	10145-10151	0.0100	_	_	
17-2431	10152-10157	Epoch	_	_	
17-2432	10157-10158	[	_	_	
17-2433	10158-10160	59	_	_	
17-2434	10160-10161	]	_	_	
17-2435	10161-10162	:	_	_	
17-2436	10162-10163	[	_	_	
17-2437	10163-10166	200	_	_	
17-2438	10166-10167	/	_	_	
17-2439	10167-10170	774	_	_	
17-2440	10170-10171	]	_	_	
17-2441	10172-10176	loss	_	_	
17-2442	10176-10177	:	_	_	
17-2443	10177-10183	0.0053	_	_	
17-2444	10183-10184	(	_	_	
17-2445	10184-10190	0.0094	_	_	
17-2446	10190-10191	)	_	_	
17-2447	10193-10197	prec	_	_	
17-2448	10197-10198	@	_	_	
17-2449	10198-10199	1	_	_	
17-2450	10199-10200	:	_	_	
17-2451	10200-10206	100.00	_	_	
17-2452	10206-10207	(	_	_	
17-2453	10207-10212	99.73	_	_	
17-2454	10212-10213	)	_	_	
17-2455	10215-10219	prec	_	_	
17-2456	10219-10220	@	_	_	
17-2457	10220-10221	5	_	_	
17-2458	10221-10222	:	_	_	
17-2459	10222-10228	100.00	_	_	
17-2460	10228-10229	(	_	_	
17-2461	10229-10235	100.00	_	_	
17-2462	10235-10236	)	_	_	
17-2463	10237-10242	Epoch	_	_	
17-2464	10242-10243	[	_	_	
17-2465	10243-10245	59	_	_	
17-2466	10245-10246	]	_	_	
17-2467	10246-10247	:	_	_	
17-2468	10247-10248	[	_	_	
17-2469	10248-10251	400	_	_	
17-2470	10251-10252	/	_	_	
17-2471	10252-10255	774	_	_	
17-2472	10255-10256	]	_	_	
17-2473	10257-10261	loss	_	_	
17-2474	10261-10262	:	_	_	
17-2475	10262-10268	0.0064	_	_	
17-2476	10268-10269	(	_	_	
17-2477	10269-10275	0.0105	_	_	
17-2478	10275-10276	)	_	_	
17-2479	10278-10282	prec	_	_	
17-2480	10282-10283	@	_	_	
17-2481	10283-10284	1	_	_	
17-2482	10284-10285	:	_	_	
17-2483	10285-10291	100.00	_	_	
17-2484	10291-10292	(	_	_	
17-2485	10292-10297	99.70	_	_	
17-2486	10297-10298	)	_	_	
17-2487	10300-10304	prec	_	_	
17-2488	10304-10305	@	_	_	
17-2489	10305-10306	5	_	_	
17-2490	10306-10307	:	_	_	
17-2491	10307-10313	100.00	_	_	
17-2492	10313-10314	(	_	_	
17-2493	10314-10320	100.00	_	_	
17-2494	10320-10321	)	_	_	
17-2495	10322-10327	Epoch	_	_	
17-2496	10327-10328	[	_	_	
17-2497	10328-10330	59	_	_	
17-2498	10330-10331	]	_	_	
17-2499	10331-10332	:	_	_	
17-2500	10332-10333	[	_	_	
17-2501	10333-10336	600	_	_	
17-2502	10336-10337	/	_	_	
17-2503	10337-10340	774	_	_	
17-2504	10340-10341	]	_	_	
17-2505	10342-10346	loss	_	_	
17-2506	10346-10347	:	_	_	
17-2507	10347-10353	0.0068	_	_	
17-2508	10353-10354	(	_	_	
17-2509	10354-10360	0.0112	_	_	
17-2510	10360-10361	)	_	_	
17-2511	10363-10367	prec	_	_	
17-2512	10367-10368	@	_	_	
17-2513	10368-10369	1	_	_	
17-2514	10369-10370	:	_	_	
17-2515	10370-10376	100.00	_	_	
17-2516	10376-10377	(	_	_	
17-2517	10377-10382	99.67	_	_	
17-2518	10382-10383	)	_	_	
17-2519	10385-10389	prec	_	_	
17-2520	10389-10390	@	_	_	
17-2521	10390-10391	5	_	_	
17-2522	10391-10392	:	_	_	
17-2523	10392-10398	100.00	_	_	
17-2524	10398-10399	(	_	_	
17-2525	10399-10405	100.00	_	_	
17-2526	10405-10406	)	_	_	
17-2527	10407-10408	[	_	_	
17-2528	10408-10413	Clean	_	_	
17-2529	10413-10414	]	_	_	
17-2530	10415-10419	Prec	_	_	
17-2531	10419-10420	@	_	_	
17-2532	10420-10421	1	_	_	
17-2533	10421-10422	:	_	_	
17-2534	10423-10428	93.04	_	_	
17-2535	10428-10429	,	_	_	
17-2536	10430-10434	Loss	_	_	
17-2537	10434-10435	:	_	_	
17-2538	10436-10442	0.2900	_	_	
17-2539	10443-10444	[	_	_	
17-2540	10444-10447	Bad	_	_	
17-2541	10447-10448	]	_	_	
17-2542	10449-10453	Prec	_	_	
17-2543	10453-10454	@	_	_	
17-2544	10454-10455	1	_	_	
17-2545	10455-10456	:	_	_	
17-2546	10457-10463	100.00	_	_	
17-2547	10463-10464	,	_	_	
17-2548	10465-10469	Loss	_	_	
17-2549	10469-10470	:	_	_	
17-2550	10471-10477	0.0000	_	_	
17-2551	10478-10483	epoch	_	_	
17-2552	10483-10484	:	_	_	
17-2553	10485-10487	59	_	_	
17-2554	10489-10491	lr	_	_	
17-2555	10491-10492	:	_	_	
17-2556	10493-10499	0.0100	_	_	
17-2557	10500-10505	Epoch	_	_	
17-2558	10505-10506	[	_	_	
17-2559	10506-10508	60	_	_	
17-2560	10508-10509	]	_	_	
17-2561	10509-10510	:	_	_	
17-2562	10510-10511	[	_	_	
17-2563	10511-10514	200	_	_	
17-2564	10514-10515	/	_	_	
17-2565	10515-10518	774	_	_	
17-2566	10518-10519	]	_	_	
17-2567	10520-10524	loss	_	_	
17-2568	10524-10525	:	_	_	
17-2569	10525-10531	0.0039	_	_	
17-2570	10531-10532	(	_	_	
17-2571	10532-10538	0.0147	_	_	
17-2572	10538-10539	)	_	_	
17-2573	10541-10545	prec	_	_	
17-2574	10545-10546	@	_	_	
17-2575	10546-10547	1	_	_	
17-2576	10547-10548	:	_	_	
17-2577	10548-10554	100.00	_	_	
17-2578	10554-10555	(	_	_	
17-2579	10555-10560	99.55	_	_	
17-2580	10560-10561	)	_	_	
17-2581	10563-10567	prec	_	_	
17-2582	10567-10568	@	_	_	
17-2583	10568-10569	5	_	_	
17-2584	10569-10570	:	_	_	
17-2585	10570-10576	100.00	_	_	
17-2586	10576-10577	(	_	_	
17-2587	10577-10582	99.99	_	_	
17-2588	10582-10583	)	_	_	
17-2589	10584-10589	Epoch	_	_	
17-2590	10589-10590	[	_	_	
17-2591	10590-10592	60	_	_	
17-2592	10592-10593	]	_	_	
17-2593	10593-10594	:	_	_	
17-2594	10594-10595	[	_	_	
17-2595	10595-10598	400	_	_	
17-2596	10598-10599	/	_	_	
17-2597	10599-10602	774	_	_	
17-2598	10602-10603	]	_	_	
17-2599	10604-10608	loss	_	_	
17-2600	10608-10609	:	_	_	
17-2601	10609-10615	0.0399	_	_	
17-2602	10615-10616	(	_	_	
17-2603	10616-10622	0.0142	_	_	
17-2604	10622-10623	)	_	_	
17-2605	10625-10629	prec	_	_	
17-2606	10629-10630	@	_	_	
17-2607	10630-10631	1	_	_	
17-2608	10631-10632	:	_	_	
17-2609	10632-10637	96.88	_	_	
17-2610	10637-10638	(	_	_	
17-2611	10638-10643	99.58	_	_	
17-2612	10643-10644	)	_	_	
17-2613	10646-10650	prec	_	_	
17-2614	10650-10651	@	_	_	
17-2615	10651-10652	5	_	_	
17-2616	10652-10653	:	_	_	
17-2617	10653-10659	100.00	_	_	
17-2618	10659-10660	(	_	_	
17-2619	10660-10666	100.00	_	_	
17-2620	10666-10667	)	_	_	
17-2621	10668-10673	Epoch	_	_	
17-2622	10673-10674	[	_	_	
17-2623	10674-10676	60	_	_	
17-2624	10676-10677	]	_	_	
17-2625	10677-10678	:	_	_	
17-2626	10678-10679	[	_	_	
17-2627	10679-10682	600	_	_	
17-2628	10682-10683	/	_	_	
17-2629	10683-10686	774	_	_	
17-2630	10686-10687	]	_	_	
17-2631	10688-10692	loss	_	_	
17-2632	10692-10693	:	_	_	
17-2633	10693-10699	0.0030	_	_	
17-2634	10699-10700	(	_	_	
17-2635	10700-10706	0.0134	_	_	
17-2636	10706-10707	)	_	_	
17-2637	10709-10713	prec	_	_	
17-2638	10713-10714	@	_	_	
17-2639	10714-10715	1	_	_	
17-2640	10715-10716	:	_	_	
17-2641	10716-10722	100.00	_	_	
17-2642	10722-10723	(	_	_	
17-2643	10723-10728	99.59	_	_	
17-2644	10728-10729	)	_	_	
17-2645	10731-10735	prec	_	_	
17-2646	10735-10736	@	_	_	
17-2647	10736-10737	5	_	_	
17-2648	10737-10738	:	_	_	
17-2649	10738-10744	100.00	_	_	
17-2650	10744-10745	(	_	_	
17-2651	10745-10751	100.00	_	_	
17-2652	10751-10752	)	_	_	
17-2653	10753-10754	[	_	_	
17-2654	10754-10759	Clean	_	_	
17-2655	10759-10760	]	_	_	
17-2656	10761-10765	Prec	_	_	
17-2657	10765-10766	@	_	_	
17-2658	10766-10767	1	_	_	
17-2659	10767-10768	:	_	_	
17-2660	10769-10774	93.24	_	_	
17-2661	10774-10775	,	_	_	
17-2662	10776-10780	Loss	_	_	
17-2663	10780-10781	:	_	_	
17-2664	10782-10788	0.2905	_	_	
17-2665	10789-10790	[	_	_	
17-2666	10790-10793	Bad	_	_	
17-2667	10793-10794	]	_	_	
17-2668	10795-10799	Prec	_	_	
17-2669	10799-10800	@	_	_	
17-2670	10800-10801	1	_	_	
17-2671	10801-10802	:	_	_	
17-2672	10803-10809	100.00	_	_	
17-2673	10809-10810	,	_	_	
17-2674	10811-10815	Loss	_	_	
17-2675	10815-10816	:	_	_	
17-2676	10817-10823	0.0000	_	_	
17-2677	10825-10826	-	_	_	
17-2678	10826-10827	-	_	_	
17-2679	10827-10828	-	_	_	
17-2680	10828-10829	-	_	_	
17-2681	10829-10830	-	_	_	
17-2682	10830-10831	-	_	_	
17-2683	10831-10832	-	_	_	
17-2684	10832-10833	-	_	_	
17-2685	10833-10834	-	_	_	
17-2686	10834-10835	-	_	_	
17-2687	10835-10836	-	_	_	
17-2688	10837-10842	Model	_	_	
17-2689	10843-10853	unlearning	_	_	
17-2690	10854-10855	-	_	_	
17-2691	10855-10856	-	_	_	
17-2692	10856-10857	-	_	_	
17-2693	10857-10858	-	_	_	
17-2694	10858-10859	-	_	_	
17-2695	10859-10860	-	_	_	
17-2696	10860-10861	-	_	_	
17-2697	10861-10862	-	_	_	
17-2698	10862-10863	-	_	_	
17-2699	10863-10864	-	_	_	
17-2700	10864-10865	-	_	_	
17-2701	10865-10866	-	_	_	
17-2702	10866-10867	-	_	_	
17-2703	10867-10868	-	_	_	
17-2704	10869-10874	epoch	_	_	
17-2705	10874-10875	:	_	_	
17-2706	10876-10877	0	_	_	
17-2707	10879-10881	lr	_	_	
17-2708	10881-10882	:	_	_	
17-2709	10883-10889	0.0005	_	_	
17-2710	10890-10891	[	_	_	
17-2711	10891-10896	Clean	_	_	
17-2712	10896-10897	]	_	_	
17-2713	10898-10902	Prec	_	_	
17-2714	10902-10903	@	_	_	
17-2715	10903-10904	1	_	_	
17-2716	10904-10905	:	_	_	
17-2717	10906-10911	93.24	_	_	
17-2718	10911-10912	,	_	_	
17-2719	10913-10917	Loss	_	_	
17-2720	10917-10918	:	_	_	
17-2721	10919-10925	0.2905	_	_	
17-2722	10926-10927	[	_	_	
17-2723	10927-10930	Bad	_	_	
17-2724	10930-10931	]	_	_	
17-2725	10932-10936	Prec	_	_	
17-2726	10936-10937	@	_	_	
17-2727	10937-10938	1	_	_	
17-2728	10938-10939	:	_	_	
17-2729	10940-10946	100.00	_	_	
17-2730	10946-10947	,	_	_	
17-2731	10948-10952	Loss	_	_	
17-2732	10952-10953	:	_	_	
17-2733	10954-10960	0.0000	_	_	
17-2734	10961-10968	testing	_	_	
17-2735	10969-10972	the	_	_	
17-2736	10973-10981	ascended	_	_	
17-2737	10982-10987	model	_	_	
17-2738	10987-10988	.	_	_	
17-2739	10988-10989	.	_	_	
17-2740	10989-10990	.	_	_	
17-2741	10990-10991	.	_	_	
17-2742	10991-10992	.	_	_	
17-2743	10992-10993	.	_	_	

#Text=[Clean] Prec@1: 93.24, Loss: 0.2905
#Text=[Bad] Prec@1: 100.00, Loss: 0.0000
#Text=epoch: 1  lr: 0.0005
#Text=testing the ascended model......
18-1	10994-10995	[	_	_	
18-2	10995-11000	Clean	_	_	
18-3	11000-11001	]	_	_	
18-4	11002-11006	Prec	_	_	
18-5	11006-11007	@	_	_	
18-6	11007-11008	1	_	_	
18-7	11008-11009	:	_	_	
18-8	11010-11015	93.24	_	_	
18-9	11015-11016	,	_	_	
18-10	11017-11021	Loss	_	_	
18-11	11021-11022	:	_	_	
18-12	11023-11029	0.2905	_	_	
18-13	11030-11031	[	_	_	
18-14	11031-11034	Bad	_	_	
18-15	11034-11035	]	_	_	
18-16	11036-11040	Prec	_	_	
18-17	11040-11041	@	_	_	
18-18	11041-11042	1	_	_	
18-19	11042-11043	:	_	_	
18-20	11044-11050	100.00	_	_	
18-21	11050-11051	,	_	_	
18-22	11052-11056	Loss	_	_	
18-23	11056-11057	:	_	_	
18-24	11058-11064	0.0000	_	_	
18-25	11065-11070	epoch	_	_	
18-26	11070-11071	:	_	_	
18-27	11072-11073	1	_	_	
18-28	11075-11077	lr	_	_	
18-29	11077-11078	:	_	_	
18-30	11079-11085	0.0005	_	_	
18-31	11086-11093	testing	_	_	
18-32	11094-11097	the	_	_	
18-33	11098-11106	ascended	_	_	
18-34	11107-11112	model	_	_	
18-35	11112-11113	.	_	_	
18-36	11113-11114	.	_	_	
18-37	11114-11115	.	_	_	
18-38	11115-11116	.	_	_	
18-39	11116-11117	.	_	_	
18-40	11117-11118	.	_	_	

#Text=[Clean] Prec@1: 92.59, Loss: 0.3283
#Text=[Bad] Prec@1: 15.84, Loss: 4.3276
#Text=epoch: 2  lr: 0.0005
#Text=testing the ascended model......
19-1	11119-11120	[	_	_	
19-2	11120-11125	Clean	_	_	
19-3	11125-11126	]	_	_	
19-4	11127-11131	Prec	_	_	
19-5	11131-11132	@	_	_	
19-6	11132-11133	1	_	_	
19-7	11133-11134	:	_	_	
19-8	11135-11140	92.59	_	_	
19-9	11140-11141	,	_	_	
19-10	11142-11146	Loss	_	_	
19-11	11146-11147	:	_	_	
19-12	11148-11154	0.3283	_	_	
19-13	11155-11156	[	_	_	
19-14	11156-11159	Bad	_	_	
19-15	11159-11160	]	_	_	
19-16	11161-11165	Prec	_	_	
19-17	11165-11166	@	_	_	
19-18	11166-11167	1	_	_	
19-19	11167-11168	:	_	_	
19-20	11169-11174	15.84	_	_	
19-21	11174-11175	,	_	_	
19-22	11176-11180	Loss	_	_	
19-23	11180-11181	:	_	_	
19-24	11182-11188	4.3276	_	_	
19-25	11189-11194	epoch	_	_	
19-26	11194-11195	:	_	_	
19-27	11196-11197	2	_	_	
19-28	11199-11201	lr	_	_	
19-29	11201-11202	:	_	_	
19-30	11203-11209	0.0005	_	_	
19-31	11210-11217	testing	_	_	
19-32	11218-11221	the	_	_	
19-33	11222-11230	ascended	_	_	
19-34	11231-11236	model	_	_	
19-35	11236-11237	.	_	_	
19-36	11237-11238	.	_	_	
19-37	11238-11239	.	_	_	
19-38	11239-11240	.	_	_	
19-39	11240-11241	.	_	_	
19-40	11241-11242	.	_	_	

#Text=[Clean] Prec@1: 91.88, Loss: 0.3632
#Text=[Bad] Prec@1: 0.30, Loss: 13.5180
#Text=epoch: 3  lr: 0.0005
#Text=testing the ascended model......
20-1	11243-11244	[	_	_	
20-2	11244-11249	Clean	_	_	
20-3	11249-11250	]	_	_	
20-4	11251-11255	Prec	_	_	
20-5	11255-11256	@	_	_	
20-6	11256-11257	1	_	_	
20-7	11257-11258	:	_	_	
20-8	11259-11264	91.88	_	_	
20-9	11264-11265	,	_	_	
20-10	11266-11270	Loss	_	_	
20-11	11270-11271	:	_	_	
20-12	11272-11278	0.3632	_	_	
20-13	11279-11280	[	_	_	
20-14	11280-11283	Bad	_	_	
20-15	11283-11284	]	_	_	
20-16	11285-11289	Prec	_	_	
20-17	11289-11290	@	_	_	
20-18	11290-11291	1	_	_	
20-19	11291-11292	:	_	_	
20-20	11293-11297	0.30	_	_	
20-21	11297-11298	,	_	_	
20-22	11299-11303	Loss	_	_	
20-23	11303-11304	:	_	_	
20-24	11305-11312	13.5180	_	_	
20-25	11313-11318	epoch	_	_	
20-26	11318-11319	:	_	_	
20-27	11320-11321	3	_	_	
20-28	11323-11325	lr	_	_	
20-29	11325-11326	:	_	_	
20-30	11327-11333	0.0005	_	_	
20-31	11334-11341	testing	_	_	
20-32	11342-11345	the	_	_	
20-33	11346-11354	ascended	_	_	
20-34	11355-11360	model	_	_	
20-35	11360-11361	.	_	_	
20-36	11361-11362	.	_	_	
20-37	11362-11363	.	_	_	
20-38	11363-11364	.	_	_	
20-39	11364-11365	.	_	_	
20-40	11365-11366	.	_	_	

#Text=[Clean] Prec@1: 91.71, Loss: 0.3730
#Text=[Bad] Prec@1: 0.17, Loss: 17.6328
#Text=epoch: 4  lr: 0.0005
#Text=testing the ascended model......
21-1	11367-11368	[	_	_	
21-2	11368-11373	Clean	_	_	
21-3	11373-11374	]	_	_	
21-4	11375-11379	Prec	_	_	
21-5	11379-11380	@	_	_	
21-6	11380-11381	1	_	_	
21-7	11381-11382	:	_	_	
21-8	11383-11388	91.71	_	_	
21-9	11388-11389	,	_	_	
21-10	11390-11394	Loss	_	_	
21-11	11394-11395	:	_	_	
21-12	11396-11402	0.3730	_	_	
21-13	11403-11404	[	_	_	
21-14	11404-11407	Bad	_	_	
21-15	11407-11408	]	_	_	
21-16	11409-11413	Prec	_	_	
21-17	11413-11414	@	_	_	
21-18	11414-11415	1	_	_	
21-19	11415-11416	:	_	_	
21-20	11417-11421	0.17	_	_	
21-21	11421-11422	,	_	_	
21-22	11423-11427	Loss	_	_	
21-23	11427-11428	:	_	_	
21-24	11429-11436	17.6328	_	_	
21-25	11437-11442	epoch	_	_	
21-26	11442-11443	:	_	_	
21-27	11444-11445	4	_	_	
21-28	11447-11449	lr	_	_	
21-29	11449-11450	:	_	_	
21-30	11451-11457	0.0005	_	_	
21-31	11458-11465	testing	_	_	
21-32	11466-11469	the	_	_	
21-33	11470-11478	ascended	_	_	
21-34	11479-11484	model	_	_	
21-35	11484-11485	.	_	_	
21-36	11485-11486	.	_	_	
21-37	11486-11487	.	_	_	
21-38	11487-11488	.	_	_	
21-39	11488-11489	.	_	_	
21-40	11489-11490	.	_	_	

#Text=[Clean] Prec@1: 91.80, Loss: 0.3656
#Text=[Bad] Prec@1: 0.16, Loss: 19.2982
#Text=```
#Text=
#Text=---
#Text=
#Text=## How to Prepare Poisoned Data?
22-1	11491-11492	[	_	_	
22-2	11492-11497	Clean	_	_	
22-3	11497-11498	]	_	_	
22-4	11499-11503	Prec	_	_	
22-5	11503-11504	@	_	_	
22-6	11504-11505	1	_	_	
22-7	11505-11506	:	_	_	
22-8	11507-11512	91.80	_	_	
22-9	11512-11513	,	_	_	
22-10	11514-11518	Loss	_	_	
22-11	11518-11519	:	_	_	
22-12	11520-11526	0.3656	_	_	
22-13	11527-11528	[	_	_	
22-14	11528-11531	Bad	_	_	
22-15	11531-11532	]	_	_	
22-16	11533-11537	Prec	_	_	
22-17	11537-11538	@	_	_	
22-18	11538-11539	1	_	_	
22-19	11539-11540	:	_	_	
22-20	11541-11545	0.16	_	_	
22-21	11545-11546	,	_	_	
22-22	11547-11551	Loss	_	_	
22-23	11551-11552	:	_	_	
22-24	11553-11560	19.2982	_	_	
22-25	11561-11562	`	_	_	
22-26	11562-11563	`	_	_	
22-27	11563-11564	`	_	_	
22-28	11566-11567	-	_	_	
22-29	11567-11568	-	_	_	
22-30	11568-11569	-	_	_	
22-31	11571-11572	#	_	_	
22-32	11572-11573	#	_	_	
22-33	11574-11577	How	_	_	
22-34	11578-11580	to	_	_	
22-35	11581-11588	Prepare	_	_	
22-36	11589-11597	Poisoned	_	_	
22-37	11598-11602	Data	_	_	
22-38	11602-11603	?	_	_	

#Text=The `DatasetBD` Class in `data_loader.py` can be used to generate poisoned training set by different attacks.
23-1	11604-11607	The	_	_	
23-2	11608-11609	`	_	_	
23-3	11609-11618	DatasetBD	_	_	
23-4	11618-11619	`	_	_	
23-5	11620-11625	Class	_	_	
23-6	11626-11628	in	_	_	
23-7	11629-11630	`	_	_	
23-8	11630-11644	data_loader.py	_	_	
23-9	11644-11645	`	_	_	
23-10	11646-11649	can	_	_	
23-11	11650-11652	be	_	_	
23-12	11653-11657	used	_	_	
23-13	11658-11660	to	_	_	
23-14	11661-11669	generate	_	_	
23-15	11670-11678	poisoned	_	_	
23-16	11679-11687	training	_	_	
23-17	11688-11691	set	_	_	
23-18	11692-11694	by	_	_	
23-19	11695-11704	different	_	_	
23-20	11705-11712	attacks	_	_	
23-21	11712-11713	.	_	_	

#Text=The following is an example:
#Text=
#Text=```python
#Text=from data_loader import *
#Text=    if opt.load_fixed_data:
#Text=        # load the fixed poisoned data of numpy format, e.g.
24-1	11717-11720	The	_	_	
24-2	11721-11730	following	_	_	
24-3	11731-11733	is	_	_	
24-4	11734-11736	an	_	_	
24-5	11737-11744	example	_	_	
24-6	11744-11745	:	_	_	
24-7	11747-11748	`	_	_	
24-8	11748-11749	`	_	_	
24-9	11749-11750	`	_	_	
24-10	11750-11756	python	*	SOFTWARE	
24-11	11757-11761	from	_	_	
24-12	11762-11773	data_loader	_	_	
24-13	11774-11780	import	_	_	
24-14	11781-11782	*	_	_	
24-15	11787-11789	if	_	_	
24-16	11790-11809	opt.load_fixed_data	_	_	
24-17	11809-11810	:	_	_	
24-18	11819-11820	#	_	_	
24-19	11821-11825	load	_	_	
24-20	11826-11829	the	_	_	
24-21	11830-11835	fixed	_	_	
24-22	11836-11844	poisoned	_	_	
24-23	11845-11849	data	_	_	
24-24	11850-11852	of	_	_	
24-25	11853-11858	numpy	_	_	
24-26	11859-11865	format	_	_	
24-27	11865-11866	,	_	_	
24-28	11867-11870	e.g	_	_	
24-29	11870-11871	.	_	_	

#Text=Dynamic, FC, DFST attacks etc
25-1	11872-11879	Dynamic	_	_	
25-2	11879-11880	,	_	_	
25-3	11881-11883	FC	_	_	
25-4	11883-11884	,	_	_	
25-5	11885-11889	DFST	_	_	
25-6	11890-11897	attacks	_	_	
25-7	11898-11901	etc	_	_	

#Text=.
26-1	11901-11902	.	_	_	

#Text=# Note that the load data type is a pytorch tensor
#Text=        poisoned_data = np.load(opt.poisoned_data_path, allow_pickle=True)
#Text=        poisoned_data_loader = DataLoader(dataset=poisoned_data,
#Text=                                            batch_size=opt.batch_size,
#Text=                                            shuffle=True,
#Text=                                            )
#Text=    else:
#Text=        poisoned_data, poisoned_data_loader = get_backdoor_loader(opt)
#Text=
#Text=    test_clean_loader, test_bad_loader = get_test_loader(opt)
#Text=```
#Text=Note that, for attacks `Dynamic, DFTS, FC, etc`, it is hard to include them in the `get_backdoor_loader()`.
27-1	11912-11913	#	_	_	
27-2	11914-11918	Note	_	_	
27-3	11919-11923	that	_	_	
27-4	11924-11927	the	_	_	
27-5	11928-11932	load	_	_	
27-6	11933-11937	data	_	_	
27-7	11938-11942	type	_	_	
27-8	11943-11945	is	_	_	
27-9	11946-11947	a	_	_	
27-10	11948-11955	pytorch	*	SOFTWARE	
27-11	11956-11962	tensor	_	_	
27-12	11971-11984	poisoned_data	_	_	
27-13	11985-11986	=	_	_	
27-14	11987-11994	np.load	_	_	
27-15	11994-11995	(	_	_	
27-16	11995-12017	opt.poisoned_data_path	_	_	
27-17	12017-12018	,	_	_	
27-18	12019-12031	allow_pickle	_	_	
27-19	12031-12032	=	_	_	
27-20	12032-12036	True	_	_	
27-21	12036-12037	)	_	_	
27-22	12046-12066	poisoned_data_loader	_	_	
27-23	12067-12068	=	_	_	
27-24	12069-12079	DataLoader	_	_	
27-25	12079-12080	(	_	_	
27-26	12080-12087	dataset	_	_	
27-27	12087-12088	=	_	_	
27-28	12088-12101	poisoned_data	_	_	
27-29	12101-12102	,	_	_	
27-30	12147-12157	batch_size	_	_	
27-31	12157-12158	=	_	_	
27-32	12158-12172	opt.batch_size	_	_	
27-33	12172-12173	,	_	_	
27-34	12218-12225	shuffle	_	_	
27-35	12225-12226	=	_	_	
27-36	12226-12230	True	_	_	
27-37	12230-12231	,	_	_	
27-38	12276-12277	)	_	_	
27-39	12282-12286	else	_	_	
27-40	12286-12287	:	_	_	
27-41	12296-12309	poisoned_data	_	_	
27-42	12309-12310	,	_	_	
27-43	12311-12331	poisoned_data_loader	_	_	
27-44	12332-12333	=	_	_	
27-45	12334-12353	get_backdoor_loader	_	_	
27-46	12353-12354	(	_	_	
27-47	12354-12357	opt	_	_	
27-48	12357-12358	)	_	_	
27-49	12364-12381	test_clean_loader	_	_	
27-50	12381-12382	,	_	_	
27-51	12383-12398	test_bad_loader	_	_	
27-52	12399-12400	=	_	_	
27-53	12401-12416	get_test_loader	_	_	
27-54	12416-12417	(	_	_	
27-55	12417-12420	opt	_	_	
27-56	12420-12421	)	_	_	
27-57	12422-12423	`	_	_	
27-58	12423-12424	`	_	_	
27-59	12424-12425	`	_	_	
27-60	12426-12430	Note	_	_	
27-61	12431-12435	that	_	_	
27-62	12435-12436	,	_	_	
27-63	12437-12440	for	_	_	
27-64	12441-12448	attacks	_	_	
27-65	12449-12450	`	_	_	
27-66	12450-12457	Dynamic	_	_	
27-67	12457-12458	,	_	_	
27-68	12459-12463	DFTS	_	_	
27-69	12463-12464	,	_	_	
27-70	12465-12467	FC	_	_	
27-71	12467-12468	,	_	_	
27-72	12469-12472	etc	_	_	
27-73	12472-12473	`	_	_	
27-74	12473-12474	,	_	_	
27-75	12475-12477	it	_	_	
27-76	12478-12480	is	_	_	
27-77	12481-12485	hard	_	_	
27-78	12486-12488	to	_	_	
27-79	12489-12496	include	_	_	
27-80	12497-12501	them	_	_	
27-81	12502-12504	in	_	_	
27-82	12505-12508	the	_	_	
27-83	12509-12510	`	_	_	
27-84	12510-12529	get_backdoor_loader	_	_	
27-85	12529-12530	(	_	_	
27-86	12530-12531	)	_	_	
27-87	12531-12532	`	_	_	
27-88	12532-12533	.	_	_	

#Text=So, better to create pre-poisoned datasets for these attacks using `create_poisoned_data.py`, then load the poisoned dataset by setting `opt.loader_fixed_data == True`.
28-1	12534-12536	So	_	_	
28-2	12536-12537	,	_	_	
28-3	12538-12544	better	_	_	
28-4	12545-12547	to	_	_	
28-5	12548-12554	create	_	_	
28-6	12555-12567	pre-poisoned	_	_	
28-7	12568-12576	datasets	_	_	
28-8	12577-12580	for	_	_	
28-9	12581-12586	these	_	_	
28-10	12587-12594	attacks	_	_	
28-11	12595-12600	using	_	_	
28-12	12601-12602	`	_	_	
28-13	12602-12625	create_poisoned_data.py	_	_	
28-14	12625-12626	`	_	_	
28-15	12626-12627	,	_	_	
28-16	12628-12632	then	_	_	
28-17	12633-12637	load	_	_	
28-18	12638-12641	the	_	_	
28-19	12642-12650	poisoned	_	_	
28-20	12651-12658	dataset	_	_	
28-21	12659-12661	by	_	_	
28-22	12662-12669	setting	_	_	
28-23	12670-12671	`	_	_	
28-24	12671-12692	opt.loader_fixed_data	_	_	
28-25	12693-12694	=	_	_	
28-26	12694-12695	=	_	_	
28-27	12696-12700	True	_	_	
28-28	12700-12701	`	_	_	
28-29	12701-12702	.	_	_	

#Text=An example of how to  **create a poisoned dataset by the Dynamic attack** is given in the `create_backdoor_data` dictionary.
29-1	12705-12707	An	_	_	
29-2	12708-12715	example	_	_	
29-3	12716-12718	of	_	_	
29-4	12719-12722	how	_	_	
29-5	12723-12725	to	_	_	
29-6	12727-12728	*	_	_	
29-7	12728-12729	*	_	_	
29-8	12729-12735	create	_	_	
29-9	12736-12737	a	_	_	
29-10	12738-12746	poisoned	_	_	
29-11	12747-12754	dataset	_	_	
29-12	12755-12757	by	_	_	
29-13	12758-12761	the	_	_	
29-14	12762-12769	Dynamic	_	_	
29-15	12770-12776	attack	_	_	
29-16	12776-12777	*	_	_	
29-17	12777-12778	*	_	_	
29-18	12779-12781	is	_	_	
29-19	12782-12787	given	_	_	
29-20	12788-12790	in	_	_	
29-21	12791-12794	the	_	_	
29-22	12795-12796	`	_	_	
29-23	12796-12816	create_backdoor_data	_	_	
29-24	12816-12817	`	_	_	
29-25	12818-12828	dictionary	_	_	
29-26	12828-12829	.	_	_	

#Text=Please feel free to read `create_poisoned_data.py` and `get_backdoor_loader` and adjust the parameters for your experiment.  
#Text=
#Text=## ABL - Stage One: Backdoor Isolation
#Text=To isolate 1% potentially backdoored examples and an isolation model, you can run the following command:
#Text=
#Text=```bash
#Text=$ python backdoor_isolation.py 
#Text=```
#Text=
#Text=After that, you will get an `isolation model` and use it to isolate `1% poisoned data` of the lowest training loss.
30-1	12831-12837	Please	_	_	
30-2	12838-12842	feel	_	_	
30-3	12843-12847	free	_	_	
30-4	12848-12850	to	_	_	
30-5	12851-12855	read	_	_	
30-6	12856-12857	`	_	_	
30-7	12857-12880	create_poisoned_data.py	_	_	
30-8	12880-12881	`	_	_	
30-9	12882-12885	and	_	_	
30-10	12886-12887	`	_	_	
30-11	12887-12906	get_backdoor_loader	_	_	
30-12	12906-12907	`	_	_	
30-13	12908-12911	and	_	_	
30-14	12912-12918	adjust	_	_	
30-15	12919-12922	the	_	_	
30-16	12923-12933	parameters	_	_	
30-17	12934-12937	for	_	_	
30-18	12938-12942	your	_	_	
30-19	12943-12953	experiment	_	_	
30-20	12953-12954	.	_	_	
30-21	12958-12959	#	_	_	
30-22	12959-12960	#	_	_	
30-23	12961-12964	ABL	_	_	
30-24	12965-12966	-	_	_	
30-25	12967-12972	Stage	_	_	
30-26	12973-12976	One	_	_	
30-27	12976-12977	:	_	_	
30-28	12978-12986	Backdoor	_	_	
30-29	12987-12996	Isolation	_	_	
30-30	12997-12999	To	_	_	
30-31	13000-13007	isolate	_	_	
30-32	13008-13010	1%	_	_	
30-33	13011-13022	potentially	_	_	
30-34	13023-13033	backdoored	_	_	
30-35	13034-13042	examples	_	_	
30-36	13043-13046	and	_	_	
30-37	13047-13049	an	_	_	
30-38	13050-13059	isolation	_	_	
30-39	13060-13065	model	_	_	
30-40	13065-13066	,	_	_	
30-41	13067-13070	you	_	_	
30-42	13071-13074	can	_	_	
30-43	13075-13078	run	_	_	
30-44	13079-13082	the	_	_	
30-45	13083-13092	following	_	_	
30-46	13093-13100	command	_	_	
30-47	13100-13101	:	_	_	
30-48	13103-13104	`	_	_	
30-49	13104-13105	`	_	_	
30-50	13105-13106	`	_	_	
30-51	13106-13110	bash	_	_	
30-52	13111-13112	$	_	_	
30-53	13113-13119	python	*	SOFTWARE	
30-54	13120-13141	backdoor_isolation.py	_	_	
30-55	13143-13144	`	_	_	
30-56	13144-13145	`	_	_	
30-57	13145-13146	`	_	_	
30-58	13148-13153	After	_	_	
30-59	13154-13158	that	_	_	
30-60	13158-13159	,	_	_	
30-61	13160-13163	you	_	_	
30-62	13164-13168	will	_	_	
30-63	13169-13172	get	_	_	
30-64	13173-13175	an	_	_	
30-65	13176-13177	`	_	_	
30-66	13177-13186	isolation	_	_	
30-67	13187-13192	model	_	_	
30-68	13192-13193	`	_	_	
30-69	13194-13197	and	_	_	
30-70	13198-13201	use	_	_	
30-71	13202-13204	it	_	_	
30-72	13205-13207	to	_	_	
30-73	13208-13215	isolate	_	_	
30-74	13216-13217	`	_	_	
30-75	13217-13219	1%	_	_	
30-76	13220-13228	poisoned	_	_	
30-77	13229-13233	data	_	_	
30-78	13233-13234	`	_	_	
30-79	13235-13237	of	_	_	
30-80	13238-13241	the	_	_	
30-81	13242-13248	lowest	_	_	
30-82	13249-13257	training	_	_	
30-83	13258-13262	loss	_	_	
30-84	13262-13263	.	_	_	

#Text=The isolated data and isolation model will be saved to `'isolation_data'` and `'weight/isolation_model'`, respectively.
31-1	13266-13269	The	_	_	
31-2	13270-13278	isolated	_	_	
31-3	13279-13283	data	_	_	
31-4	13284-13287	and	_	_	
31-5	13288-13297	isolation	_	_	
31-6	13298-13303	model	_	_	
31-7	13304-13308	will	_	_	
31-8	13309-13311	be	_	_	
31-9	13312-13317	saved	_	_	
31-10	13318-13320	to	_	_	
31-11	13321-13322	`	_	_	
31-12	13322-13323	'	_	_	
31-13	13323-13337	isolation_data	_	_	
31-14	13337-13338	'	_	_	
31-15	13338-13339	`	_	_	
31-16	13340-13343	and	_	_	
31-17	13344-13345	`	_	_	
31-18	13345-13346	'	_	_	
31-19	13346-13352	weight	_	_	
31-20	13352-13353	/	_	_	
31-21	13353-13368	isolation_model	_	_	
31-22	13368-13369	'	_	_	
31-23	13369-13370	`	_	_	
31-24	13370-13371	,	_	_	
31-25	13372-13384	respectively	_	_	
31-26	13384-13385	.	_	_	

#Text=Please check more details of the experimental settings in Section 4 and Appendix A of our paper, and adjust the parameters in `config.py` for your experiment.  
#Text=
#Text=## ABL - Stage Two: Backdoor Unlearning
#Text=With the 1% isolated data and the isolation model, we can then continue with the later training of unlearning using the following code:
#Text=
#Text=```bash
#Text=$ python backdoor_unlearning.py 
#Text=```
#Text=
#Text=At this stage, the backdoor has already been learned into the isolation model.
32-1	13388-13394	Please	_	_	
32-2	13395-13400	check	_	_	
32-3	13401-13405	more	_	_	
32-4	13406-13413	details	_	_	
32-5	13414-13416	of	_	_	
32-6	13417-13420	the	_	_	
32-7	13421-13433	experimental	_	_	
32-8	13434-13442	settings	_	_	
32-9	13443-13445	in	_	_	
32-10	13446-13453	Section	_	_	
32-11	13454-13455	4	_	_	
32-12	13456-13459	and	_	_	
32-13	13460-13468	Appendix	_	_	
32-14	13469-13470	A	_	_	
32-15	13471-13473	of	_	_	
32-16	13474-13477	our	_	_	
32-17	13478-13483	paper	_	_	
32-18	13483-13484	,	_	_	
32-19	13485-13488	and	_	_	
32-20	13489-13495	adjust	_	_	
32-21	13496-13499	the	_	_	
32-22	13500-13510	parameters	_	_	
32-23	13511-13513	in	_	_	
32-24	13514-13515	`	_	_	
32-25	13515-13524	config.py	_	_	
32-26	13524-13525	`	_	_	
32-27	13526-13529	for	_	_	
32-28	13530-13534	your	_	_	
32-29	13535-13545	experiment	_	_	
32-30	13545-13546	.	_	_	
32-31	13550-13551	#	_	_	
32-32	13551-13552	#	_	_	
32-33	13553-13556	ABL	_	_	
32-34	13557-13558	-	_	_	
32-35	13559-13564	Stage	_	_	
32-36	13565-13568	Two	_	_	
32-37	13568-13569	:	_	_	
32-38	13570-13578	Backdoor	_	_	
32-39	13579-13589	Unlearning	_	_	
32-40	13590-13594	With	_	_	
32-41	13595-13598	the	_	_	
32-42	13599-13601	1%	_	_	
32-43	13602-13610	isolated	_	_	
32-44	13611-13615	data	_	_	
32-45	13616-13619	and	_	_	
32-46	13620-13623	the	_	_	
32-47	13624-13633	isolation	_	_	
32-48	13634-13639	model	_	_	
32-49	13639-13640	,	_	_	
32-50	13641-13643	we	_	_	
32-51	13644-13647	can	_	_	
32-52	13648-13652	then	_	_	
32-53	13653-13661	continue	_	_	
32-54	13662-13666	with	_	_	
32-55	13667-13670	the	_	_	
32-56	13671-13676	later	_	_	
32-57	13677-13685	training	_	_	
32-58	13686-13688	of	_	_	
32-59	13689-13699	unlearning	_	_	
32-60	13700-13705	using	_	_	
32-61	13706-13709	the	_	_	
32-62	13710-13719	following	_	_	
32-63	13720-13724	code	_	_	
32-64	13724-13725	:	_	_	
32-65	13727-13728	`	_	_	
32-66	13728-13729	`	_	_	
32-67	13729-13730	`	_	_	
32-68	13730-13734	bash	_	_	
32-69	13735-13736	$	_	_	
32-70	13737-13743	python	*	SOFTWARE	
32-71	13744-13766	backdoor_unlearning.py	_	_	
32-72	13768-13769	`	_	_	
32-73	13769-13770	`	_	_	
32-74	13770-13771	`	_	_	
32-75	13773-13775	At	_	_	
32-76	13776-13780	this	_	_	
32-77	13781-13786	stage	_	_	
32-78	13786-13787	,	_	_	
32-79	13788-13791	the	_	_	
32-80	13792-13800	backdoor	_	_	
32-81	13801-13804	has	_	_	
32-82	13805-13812	already	_	_	
32-83	13813-13817	been	_	_	
32-84	13818-13825	learned	_	_	
32-85	13826-13830	into	_	_	
32-86	13831-13834	the	_	_	
32-87	13835-13844	isolation	_	_	
32-88	13845-13850	model	_	_	
32-89	13850-13851	.	_	_	

#Text=In order to improve the clean acc of the isolation model, we finetune the model for several epochs before unlearning.
33-1	13854-13856	In	_	_	
33-2	13857-13862	order	_	_	
33-3	13863-13865	to	_	_	
33-4	13866-13873	improve	_	_	
33-5	13874-13877	the	_	_	
33-6	13878-13883	clean	_	_	
33-7	13884-13887	acc	_	_	
33-8	13888-13890	of	_	_	
33-9	13891-13894	the	_	_	
33-10	13895-13904	isolation	_	_	
33-11	13905-13910	model	_	_	
33-12	13910-13911	,	_	_	
33-13	13912-13914	we	_	_	
33-14	13915-13923	finetune	_	_	
33-15	13924-13927	the	_	_	
33-16	13928-13933	model	_	_	
33-17	13934-13937	for	_	_	
33-18	13938-13945	several	_	_	
33-19	13946-13952	epochs	_	_	
33-20	13953-13959	before	_	_	
33-21	13960-13970	unlearning	_	_	
33-22	13970-13971	.	_	_	

#Text=If you want to go directly to see the unlearning result, you can skip the finetuning step by setting `opt.finetuning_ascent_model== False` .
34-1	13972-13974	If	_	_	
34-2	13975-13978	you	_	_	
34-3	13979-13983	want	_	_	
34-4	13984-13986	to	_	_	
34-5	13987-13989	go	_	_	
34-6	13990-13998	directly	_	_	
34-7	13999-14001	to	_	_	
34-8	14002-14005	see	_	_	
34-9	14006-14009	the	_	_	
34-10	14010-14020	unlearning	_	_	
34-11	14021-14027	result	_	_	
34-12	14027-14028	,	_	_	
34-13	14029-14032	you	_	_	
34-14	14033-14036	can	_	_	
34-15	14037-14041	skip	_	_	
34-16	14042-14045	the	_	_	
34-17	14046-14056	finetuning	_	_	
34-18	14057-14061	step	_	_	
34-19	14062-14064	by	_	_	
34-20	14065-14072	setting	_	_	
34-21	14073-14074	`	_	_	
34-22	14074-14101	opt.finetuning_ascent_model	_	_	
34-23	14101-14102	=	_	_	
34-24	14102-14103	=	_	_	
34-25	14104-14109	False	_	_	
34-26	14109-14110	`	_	_	
34-27	14111-14112	.	_	_	

#Text=The final result of unlearning will be saved to `ABL_results` and `logs`.
35-1	14114-14117	The	_	_	
35-2	14118-14123	final	_	_	
35-3	14124-14130	result	_	_	
35-4	14131-14133	of	_	_	
35-5	14134-14144	unlearning	_	_	
35-6	14145-14149	will	_	_	
35-7	14150-14152	be	_	_	
35-8	14153-14158	saved	_	_	
35-9	14159-14161	to	_	_	
35-10	14162-14163	`	_	_	
35-11	14163-14174	ABL_results	_	_	
35-12	14174-14175	`	_	_	
35-13	14176-14179	and	_	_	
35-14	14180-14181	`	_	_	
35-15	14181-14185	logs	_	_	
35-16	14185-14186	`	_	_	
35-17	14186-14187	.	_	_	

#Text=Please read `backdoor_unlearning.py` and `config.py` and adjust the parameters for your experiment.
#Text=
#Text=---- 
#Text=## Links to External Repos
#Text=
#Text=#### Attacks
#Text=
#Text=**CL:** Clean-label backdoor attacks
#Text=
#Text=- [Paper](https://people.csail.mit.edu/madry/lab/cleanlabel.pdf)
#Text=- [pytorch implementation](https://github.com/hkunzhe/label_consistent_attacks_pytorch)
#Text=
#Text=**SIG:** A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning
#Text=
#Text=- [Paper](https://ieeexplore.ieee.org/document/8802997/footnotes)
#Text=
#Text=```python
#Text=## reference code
#Text=def plant_sin_trigger(img, delta=20, f=6, debug=False):
#Text=    """
#Text=    Implement paper:
#Text=    > Barni, M., Kallas, K., & Tondi, B. (2019)
36-1	14188-14194	Please	_	_	
36-2	14195-14199	read	_	_	
36-3	14200-14201	`	_	_	
36-4	14201-14223	backdoor_unlearning.py	_	_	
36-5	14223-14224	`	_	_	
36-6	14225-14228	and	_	_	
36-7	14229-14230	`	_	_	
36-8	14230-14239	config.py	_	_	
36-9	14239-14240	`	_	_	
36-10	14241-14244	and	_	_	
36-11	14245-14251	adjust	_	_	
36-12	14252-14255	the	_	_	
36-13	14256-14266	parameters	_	_	
36-14	14267-14270	for	_	_	
36-15	14271-14275	your	_	_	
36-16	14276-14286	experiment	_	_	
36-17	14286-14287	.	_	_	
36-18	14289-14290	-	_	_	
36-19	14290-14291	-	_	_	
36-20	14291-14292	-	_	_	
36-21	14292-14293	-	_	_	
36-22	14295-14296	#	_	_	
36-23	14296-14297	#	_	_	
36-24	14298-14303	Links	_	_	
36-25	14304-14306	to	_	_	
36-26	14307-14315	External	_	_	
36-27	14316-14321	Repos	_	_	
36-28	14323-14324	#	_	_	
36-29	14324-14325	#	_	_	
36-30	14325-14326	#	_	_	
36-31	14326-14327	#	_	_	
36-32	14328-14335	Attacks	_	_	
36-33	14337-14338	*	_	_	
36-34	14338-14339	*	_	_	
36-35	14339-14341	CL	_	_	
36-36	14341-14342	:	_	_	
36-37	14342-14343	*	_	_	
36-38	14343-14344	*	_	_	
36-39	14345-14356	Clean-label	_	_	
36-40	14357-14365	backdoor	_	_	
36-41	14366-14373	attacks	_	_	
36-42	14375-14376	-	_	_	
36-43	14377-14378	[	_	_	
36-44	14378-14383	Paper	_	_	
36-45	14383-14384	]	_	_	
36-46	14384-14385	(	_	_	
36-47	14385-14390	https	_	_	
36-48	14390-14391	:	_	_	
36-49	14391-14392	/	_	_	
36-50	14392-14393	/	_	_	
36-51	14393-14413	people.csail.mit.edu	_	_	
36-52	14413-14414	/	_	_	
36-53	14414-14419	madry	_	_	
36-54	14419-14420	/	_	_	
36-55	14420-14423	lab	_	_	
36-56	14423-14424	/	_	_	
36-57	14424-14438	cleanlabel.pdf	_	_	
36-58	14438-14439	)	_	_	
36-59	14440-14441	-	_	_	
36-60	14442-14443	[	_	_	
36-61	14443-14450	pytorch	_	_	
36-62	14451-14465	implementation	_	_	
36-63	14465-14466	]	_	_	
36-64	14466-14467	(	_	_	
36-65	14467-14472	https	_	_	
36-66	14472-14473	:	_	_	
36-67	14473-14474	/	_	_	
36-68	14474-14475	/	_	_	
36-69	14475-14485	github.com	_	_	
36-70	14485-14486	/	_	_	
36-71	14486-14493	hkunzhe	_	_	
36-72	14493-14494	/	_	_	
36-73	14494-14526	label_consistent_attacks_pytorch	_	_	
36-74	14526-14527	)	_	_	
36-75	14529-14530	*	_	_	
36-76	14530-14531	*	_	_	
36-77	14531-14534	SIG	_	_	
36-78	14534-14535	:	_	_	
36-79	14535-14536	*	_	_	
36-80	14536-14537	*	_	_	
36-81	14538-14539	A	*[14]	PUBLICATION[14]	
36-82	14540-14543	New	*[14]	PUBLICATION[14]	
36-83	14544-14552	Backdoor	*[14]	PUBLICATION[14]	
36-84	14553-14559	Attack	*[14]	PUBLICATION[14]	
36-85	14560-14562	in	*[14]	PUBLICATION[14]	
36-86	14563-14567	CNNS	*[14]	PUBLICATION[14]	
36-87	14568-14570	by	*[14]	PUBLICATION[14]	
36-88	14571-14579	Training	*[14]	PUBLICATION[14]	
36-89	14580-14583	Set	*[14]	PUBLICATION[14]	
36-90	14584-14594	Corruption	*[14]	PUBLICATION[14]	
36-91	14595-14602	Without	*[14]	PUBLICATION[14]	
36-92	14603-14608	Label	*[14]	PUBLICATION[14]	
36-93	14609-14618	Poisoning	*[14]	PUBLICATION[14]	
36-94	14620-14621	-	_	_	
36-95	14622-14623	[	_	_	
36-96	14623-14628	Paper	_	_	
36-97	14628-14629	]	_	_	
36-98	14629-14630	(	_	_	
36-99	14630-14635	https	_	_	
36-100	14635-14636	:	_	_	
36-101	14636-14637	/	_	_	
36-102	14637-14638	/	_	_	
36-103	14638-14657	ieeexplore.ieee.org	_	_	
36-104	14657-14658	/	_	_	
36-105	14658-14666	document	_	_	
36-106	14666-14667	/	_	_	
36-107	14667-14674	8802997	_	_	
36-108	14674-14675	/	_	_	
36-109	14675-14684	footnotes	_	_	
36-110	14684-14685	)	_	_	
36-111	14687-14688	`	_	_	
36-112	14688-14689	`	_	_	
36-113	14689-14690	`	_	_	
36-114	14690-14696	python	*	PROGLANG	
36-115	14697-14698	#	_	_	
36-116	14698-14699	#	_	_	
36-117	14700-14709	reference	_	_	
36-118	14710-14714	code	_	_	
36-119	14715-14718	def	_	_	
36-120	14719-14736	plant_sin_trigger	_	_	
36-121	14736-14737	(	_	_	
36-122	14737-14740	img	_	_	
36-123	14740-14741	,	_	_	
36-124	14742-14747	delta	_	_	
36-125	14747-14748	=	_	_	
36-126	14748-14750	20	_	_	
36-127	14750-14751	,	_	_	
36-128	14752-14753	f	_	_	
36-129	14753-14754	=	_	_	
36-130	14754-14755	6	_	_	
36-131	14755-14756	,	_	_	
36-132	14757-14762	debug	_	_	
36-133	14762-14763	=	_	_	
36-134	14763-14768	False	_	_	
36-135	14768-14769	)	_	_	
36-136	14769-14770	:	_	_	
36-137	14775-14776	"	_	_	
36-138	14776-14777	"	_	_	
36-139	14777-14778	"	_	_	
36-140	14783-14792	Implement	_	_	
36-141	14793-14798	paper	_	_	
36-142	14798-14799	:	_	_	
36-143	14804-14805	>	_	_	
36-144	14806-14811	Barni	_	_	
36-145	14811-14812	,	_	_	
36-146	14813-14814	M	_	_	
36-147	14814-14815	.	_	_	
36-148	14815-14816	,	_	_	
36-149	14817-14823	Kallas	_	_	
36-150	14823-14824	,	_	_	
36-151	14825-14826	K	_	_	
36-152	14826-14827	.	_	_	
36-153	14827-14828	,	_	_	
36-154	14829-14830	&	_	_	
36-155	14831-14836	Tondi	_	_	
36-156	14836-14837	,	_	_	
36-157	14838-14839	B	_	_	
36-158	14839-14840	.	_	_	
36-159	14841-14842	(	_	_	
36-160	14842-14846	2019	_	_	
36-161	14846-14847	)	_	_	

#Text=.
37-1	14847-14848	.	_	_	

#Text=> A new Backdoor Attack in CNNs by training set corruption without label poisoning
38-1	14853-14854	>	_	_	
38-2	14855-14856	A	*[15]	PUBLICATION[15]	
38-3	14857-14860	new	*[15]	PUBLICATION[15]	
38-4	14861-14869	Backdoor	*[15]	PUBLICATION[15]	
38-5	14870-14876	Attack	*[15]	PUBLICATION[15]	
38-6	14877-14879	in	*[15]	PUBLICATION[15]	
38-7	14880-14884	CNNs	*[15]	PUBLICATION[15]	
38-8	14885-14887	by	*[15]	PUBLICATION[15]	
38-9	14888-14896	training	*[15]	PUBLICATION[15]	
38-10	14897-14900	set	*[15]	PUBLICATION[15]	
38-11	14901-14911	corruption	*[15]	PUBLICATION[15]	
38-12	14912-14919	without	*[15]	PUBLICATION[15]	
38-13	14920-14925	label	*[15]	PUBLICATION[15]	
38-14	14926-14935	poisoning	*[15]	PUBLICATION[15]	

#Text=.
39-1	14935-14936	.	_	_	

#Text=> arXiv preprint arXiv:1902.11237
#Text=    superimposed sinusoidal backdoor signal with default parameters
#Text=    """
#Text=    alpha = 0.2
#Text=    img = np.float32(img)
#Text=    pattern = np.zeros_like(img)
#Text=    m = pattern.shape[1]
#Text=    for i in range(img.shape[0]):
#Text=        for j in range(img.shape[1]):
#Text=            for k in range(img.shape[2]):
#Text=                pattern[i, j] = delta * np.sin(2 * np.pi * j * f / m)
#Text=
#Text=    img = alpha * np.uint32(img) + (1 - alpha) * pattern
#Text=    img = np.uint8(np.clip(img, 0, 255))
#Text=
#Text=    #     if debug:
#Text=    #         cv2.imshow('planted image', img)
#Text=    #         cv2.waitKey()
#Text=
#Text=    return img
#Text=```
#Text=
#Text=**Dynamic:** Input-aware Dynamic Backdoor Attack
#Text=
#Text=- [paper](https://papers.nips.cc/paper/2020/hash/234e691320c0ad5b45ee3c96d0d7b8f8-Abstract.html)
#Text=- [pytorch implementation](https://github.com/VinAIResearch/input-aware-backdoor-attack-release)
#Text=
#Text=**FC:** Poison Frogs!
40-1	14941-14942	>	_	_	
40-2	14943-14948	arXiv	_	_	
40-3	14949-14957	preprint	_	_	
40-4	14958-14963	arXiv	_	_	
40-5	14963-14964	:	_	_	
40-6	14964-14974	1902.11237	_	_	
40-7	14979-14991	superimposed	_	_	
40-8	14992-15002	sinusoidal	_	_	
40-9	15003-15011	backdoor	_	_	
40-10	15012-15018	signal	_	_	
40-11	15019-15023	with	_	_	
40-12	15024-15031	default	_	_	
40-13	15032-15042	parameters	_	_	
40-14	15047-15048	"	_	_	
40-15	15048-15049	"	_	_	
40-16	15049-15050	"	_	_	
40-17	15055-15060	alpha	_	_	
40-18	15061-15062	=	_	_	
40-19	15063-15066	0.2	_	_	
40-20	15071-15074	img	_	_	
40-21	15075-15076	=	_	_	
40-22	15077-15087	np.float32	_	_	
40-23	15087-15088	(	_	_	
40-24	15088-15091	img	_	_	
40-25	15091-15092	)	_	_	
40-26	15097-15104	pattern	_	_	
40-27	15105-15106	=	_	_	
40-28	15107-15120	np.zeros_like	_	_	
40-29	15120-15121	(	_	_	
40-30	15121-15124	img	_	_	
40-31	15124-15125	)	_	_	
40-32	15130-15131	m	_	_	
40-33	15132-15133	=	_	_	
40-34	15134-15147	pattern.shape	_	_	
40-35	15147-15148	[	_	_	
40-36	15148-15149	1	_	_	
40-37	15149-15150	]	_	_	
40-38	15155-15158	for	_	_	
40-39	15159-15160	i	_	_	
40-40	15161-15163	in	_	_	
40-41	15164-15169	range	_	_	
40-42	15169-15170	(	_	_	
40-43	15170-15179	img.shape	_	_	
40-44	15179-15180	[	_	_	
40-45	15180-15181	0	_	_	
40-46	15181-15182	]	_	_	
40-47	15182-15183	)	_	_	
40-48	15183-15184	:	_	_	
40-49	15193-15196	for	_	_	
40-50	15197-15198	j	_	_	
40-51	15199-15201	in	_	_	
40-52	15202-15207	range	_	_	
40-53	15207-15208	(	_	_	
40-54	15208-15217	img.shape	_	_	
40-55	15217-15218	[	_	_	
40-56	15218-15219	1	_	_	
40-57	15219-15220	]	_	_	
40-58	15220-15221	)	_	_	
40-59	15221-15222	:	_	_	
40-60	15235-15238	for	_	_	
40-61	15239-15240	k	_	_	
40-62	15241-15243	in	_	_	
40-63	15244-15249	range	_	_	
40-64	15249-15250	(	_	_	
40-65	15250-15259	img.shape	_	_	
40-66	15259-15260	[	_	_	
40-67	15260-15261	2	_	_	
40-68	15261-15262	]	_	_	
40-69	15262-15263	)	_	_	
40-70	15263-15264	:	_	_	
40-71	15281-15288	pattern	_	_	
40-72	15288-15289	[	_	_	
40-73	15289-15290	i	_	_	
40-74	15290-15291	,	_	_	
40-75	15292-15293	j	_	_	
40-76	15293-15294	]	_	_	
40-77	15295-15296	=	_	_	
40-78	15297-15302	delta	_	_	
40-79	15303-15304	*	_	_	
40-80	15305-15311	np.sin	_	_	
40-81	15311-15312	(	_	_	
40-82	15312-15313	2	_	_	
40-83	15314-15315	*	_	_	
40-84	15316-15321	np.pi	_	_	
40-85	15322-15323	*	_	_	
40-86	15324-15325	j	_	_	
40-87	15326-15327	*	_	_	
40-88	15328-15329	f	_	_	
40-89	15330-15331	/	_	_	
40-90	15332-15333	m	_	_	
40-91	15333-15334	)	_	_	
40-92	15340-15343	img	_	_	
40-93	15344-15345	=	_	_	
40-94	15346-15351	alpha	_	_	
40-95	15352-15353	*	_	_	
40-96	15354-15363	np.uint32	_	_	
40-97	15363-15364	(	_	_	
40-98	15364-15367	img	_	_	
40-99	15367-15368	)	_	_	
40-100	15369-15370	+	_	_	
40-101	15371-15372	(	_	_	
40-102	15372-15373	1	_	_	
40-103	15374-15375	-	_	_	
40-104	15376-15381	alpha	_	_	
40-105	15381-15382	)	_	_	
40-106	15383-15384	*	_	_	
40-107	15385-15392	pattern	_	_	
40-108	15397-15400	img	_	_	
40-109	15401-15402	=	_	_	
40-110	15403-15411	np.uint8	_	_	
40-111	15411-15412	(	_	_	
40-112	15412-15419	np.clip	_	_	
40-113	15419-15420	(	_	_	
40-114	15420-15423	img	_	_	
40-115	15423-15424	,	_	_	
40-116	15425-15426	0	_	_	
40-117	15426-15427	,	_	_	
40-118	15428-15431	255	_	_	
40-119	15431-15432	)	_	_	
40-120	15432-15433	)	_	_	
40-121	15439-15440	#	_	_	
40-122	15445-15447	if	_	_	
40-123	15448-15453	debug	_	_	
40-124	15453-15454	:	_	_	
40-125	15459-15460	#	_	_	
40-126	15469-15472	cv2	_	_	
40-127	15472-15473	.	_	_	
40-128	15473-15479	imshow	_	_	
40-129	15479-15480	(	_	_	
40-130	15480-15481	'	_	_	
40-131	15481-15488	planted	_	_	
40-132	15489-15494	image	_	_	
40-133	15494-15495	'	_	_	
40-134	15495-15496	,	_	_	
40-135	15497-15500	img	_	_	
40-136	15500-15501	)	_	_	
40-137	15506-15507	#	_	_	
40-138	15516-15519	cv2	_	_	
40-139	15519-15520	.	_	_	
40-140	15520-15527	waitKey	_	_	
40-141	15527-15528	(	_	_	
40-142	15528-15529	)	_	_	
40-143	15535-15541	return	_	_	
40-144	15542-15545	img	_	_	
40-145	15546-15547	`	_	_	
40-146	15547-15548	`	_	_	
40-147	15548-15549	`	_	_	
40-148	15551-15552	*	_	_	
40-149	15552-15553	*	_	_	
40-150	15553-15560	Dynamic	_	_	
40-151	15560-15561	:	_	_	
40-152	15561-15562	*	_	_	
40-153	15562-15563	*	_	_	
40-154	15564-15575	Input-aware	*[16]	PUBLICATION[16]	
40-155	15576-15583	Dynamic	*[16]	PUBLICATION[16]	
40-156	15584-15592	Backdoor	*[16]	PUBLICATION[16]	
40-157	15593-15599	Attack	*[16]	PUBLICATION[16]	
40-158	15601-15602	-	_	_	
40-159	15603-15604	[	_	_	
40-160	15604-15609	paper	_	_	
40-161	15609-15610	]	_	_	
40-162	15610-15611	(	_	_	
40-163	15611-15616	https	_	_	
40-164	15616-15617	:	_	_	
40-165	15617-15618	/	_	_	
40-166	15618-15619	/	_	_	
40-167	15619-15633	papers.nips.cc	_	_	
40-168	15633-15634	/	_	_	
40-169	15634-15639	paper	_	_	
40-170	15639-15640	/	_	_	
40-171	15640-15644	2020	_	_	
40-172	15644-15645	/	_	_	
40-173	15645-15649	hash	_	_	
40-174	15649-15650	/	_	_	
40-175	15650-15682	234e691320c0ad5b45ee3c96d0d7b8f8	_	_	
40-176	15682-15683	-	_	_	
40-177	15683-15696	Abstract.html	_	_	
40-178	15696-15697	)	_	_	
40-179	15698-15699	-	_	_	
40-180	15700-15701	[	_	_	
40-181	15701-15708	pytorch	*	SOFTWARE	
40-182	15709-15723	implementation	_	_	
40-183	15723-15724	]	_	_	
40-184	15724-15725	(	_	_	
40-185	15725-15730	https	_	_	
40-186	15730-15731	:	_	_	
40-187	15731-15732	/	_	_	
40-188	15732-15733	/	_	_	
40-189	15733-15743	github.com	_	_	
40-190	15743-15744	/	_	_	
40-191	15744-15757	VinAIResearch	_	_	
40-192	15757-15758	/	_	_	
40-193	15758-15793	input-aware-backdoor-attack-release	_	_	
40-194	15793-15794	)	_	_	
40-195	15796-15797	*	_	_	
40-196	15797-15798	*	_	_	
40-197	15798-15800	FC	_	_	
40-198	15800-15801	:	_	_	
40-199	15801-15802	*	_	_	
40-200	15802-15803	*	_	_	
40-201	15804-15810	Poison	*[17]	PUBLICATION[17]	
40-202	15811-15816	Frogs	*[17]	PUBLICATION[17]	
40-203	15816-15817	!	*[17]	PUBLICATION[17]	

#Text=Targeted Clean-Label Poisoning Attacks on Neural Networks
#Text=
#Text=- [paper](file/22722a343513ed45f14905eb07621686-Paper.pdf)
#Text=- [pytorch implementation](https://github.com/FlouriteJ/PoisonFrogs)
#Text=
#Text=**DFST:** Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification
#Text=
#Text=- [paper](https://arxiv.org/abs/2012.11212)
#Text=- [tensorflow implementation](https://github.com/Megum1/DFST)
#Text=
#Text=**LBA:** Latent Backdoor Attacks on Deep Neural Networks
#Text=
#Text=- [paper](https://people.cs.uchicago.edu/~ravenben/publications/pdf/pbackdoor-ccs19.pdf)
#Text=- [tensorflow implementation](http://sandlab.cs.uchicago.edu/latent/)
#Text=
#Text=**CBA:** Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features
#Text=
#Text=- [paper](https://dl.acm.org/doi/abs/10.1145/3372297.3423362)
#Text=- [pytorch implementation](https://github.com/TemporaryAcc0unt/composite-attack)
#Text=
#Text=#### Feature space attack benchmark
#Text=
#Text=`Note`: This repository is the official implementation of [Just How Toxic is Data Poisoning?
41-1	15818-15826	Targeted	*[17]	PUBLICATION[17]	
41-2	15827-15838	Clean-Label	*[17]	PUBLICATION[17]	
41-3	15839-15848	Poisoning	*[17]	PUBLICATION[17]	
41-4	15849-15856	Attacks	*[17]	PUBLICATION[17]	
41-5	15857-15859	on	*[17]	PUBLICATION[17]	
41-6	15860-15866	Neural	*[17]	PUBLICATION[17]	
41-7	15867-15875	Networks	*[17]	PUBLICATION[17]	
41-8	15877-15878	-	_	_	
41-9	15879-15880	[	_	_	
41-10	15880-15885	paper	_	_	
41-11	15885-15886	]	_	_	
41-12	15886-15887	(	_	_	
41-13	15887-15891	file	_	_	
41-14	15891-15892	/	_	_	
41-15	15892-15924	22722a343513ed45f14905eb07621686	_	_	
41-16	15924-15925	-	_	_	
41-17	15925-15934	Paper.pdf	_	_	
41-18	15934-15935	)	_	_	
41-19	15936-15937	-	_	_	
41-20	15938-15939	[	_	_	
41-21	15939-15946	pytorch	*	SOFTWARE	
41-22	15947-15961	implementation	_	_	
41-23	15961-15962	]	_	_	
41-24	15962-15963	(	_	_	
41-25	15963-15968	https	_	_	
41-26	15968-15969	:	_	_	
41-27	15969-15970	/	_	_	
41-28	15970-15971	/	_	_	
41-29	15971-15981	github.com	_	_	
41-30	15981-15982	/	_	_	
41-31	15982-15991	FlouriteJ	_	_	
41-32	15991-15992	/	_	_	
41-33	15992-16003	PoisonFrogs	_	_	
41-34	16003-16004	)	_	_	
41-35	16006-16007	*	_	_	
41-36	16007-16008	*	_	_	
41-37	16008-16012	DFST	_	_	
41-38	16012-16013	:	_	_	
41-39	16013-16014	*	_	_	
41-40	16014-16015	*	_	_	
41-41	16016-16020	Deep	*[18]	PUBLICATION[18]	
41-42	16021-16028	Feature	*[18]	PUBLICATION[18]	
41-43	16029-16034	Space	*[18]	PUBLICATION[18]	
41-44	16035-16041	Trojan	*[18]	PUBLICATION[18]	
41-45	16042-16048	Attack	*[18]	PUBLICATION[18]	
41-46	16049-16051	of	*[18]	PUBLICATION[18]	
41-47	16052-16058	Neural	*[18]	PUBLICATION[18]	
41-48	16059-16067	Networks	*[18]	PUBLICATION[18]	
41-49	16068-16070	by	*[18]	PUBLICATION[18]	
41-50	16071-16081	Controlled	*[18]	PUBLICATION[18]	
41-51	16082-16096	Detoxification	*[18]	PUBLICATION[18]	
41-52	16098-16099	-	_	_	
41-53	16100-16101	[	_	_	
41-54	16101-16106	paper	_	_	
41-55	16106-16107	]	_	_	
41-56	16107-16108	(	_	_	
41-57	16108-16113	https	_	_	
41-58	16113-16114	:	_	_	
41-59	16114-16115	/	_	_	
41-60	16115-16116	/	_	_	
41-61	16116-16125	arxiv.org	_	_	
41-62	16125-16126	/	_	_	
41-63	16126-16129	abs	_	_	
41-64	16129-16130	/	_	_	
41-65	16130-16140	2012.11212	_	_	
41-66	16140-16141	)	_	_	
41-67	16142-16143	-	_	_	
41-68	16144-16145	[	_	_	
41-69	16145-16155	tensorflow	*	SOFTWARE	
41-70	16156-16170	implementation	_	_	
41-71	16170-16171	]	_	_	
41-72	16171-16172	(	_	_	
41-73	16172-16177	https	_	_	
41-74	16177-16178	:	_	_	
41-75	16178-16179	/	_	_	
41-76	16179-16180	/	_	_	
41-77	16180-16190	github.com	_	_	
41-78	16190-16191	/	_	_	
41-79	16191-16197	Megum1	_	_	
41-80	16197-16198	/	_	_	
41-81	16198-16202	DFST	_	_	
41-82	16202-16203	)	_	_	
41-83	16205-16206	*	_	_	
41-84	16206-16207	*	_	_	
41-85	16207-16210	LBA	_	_	
41-86	16210-16211	:	_	_	
41-87	16211-16212	*	_	_	
41-88	16212-16213	*	_	_	
41-89	16214-16220	Latent	*[19]	PUBLICATION[19]	
41-90	16221-16229	Backdoor	*[19]	PUBLICATION[19]	
41-91	16230-16237	Attacks	*[19]	PUBLICATION[19]	
41-92	16238-16240	on	*[19]	PUBLICATION[19]	
41-93	16241-16245	Deep	*[19]	PUBLICATION[19]	
41-94	16246-16252	Neural	*[19]	PUBLICATION[19]	
41-95	16253-16261	Networks	*[19]	PUBLICATION[19]	
41-96	16263-16264	-	_	_	
41-97	16265-16266	[	_	_	
41-98	16266-16271	paper	_	_	
41-99	16271-16272	]	_	_	
41-100	16272-16273	(	_	_	
41-101	16273-16278	https	_	_	
41-102	16278-16279	:	_	_	
41-103	16279-16280	/	_	_	
41-104	16280-16281	/	_	_	
41-105	16281-16303	people.cs.uchicago.edu	_	_	
41-106	16303-16304	/	_	_	
41-107	16304-16305	~	_	_	
41-108	16305-16313	ravenben	_	_	
41-109	16313-16314	/	_	_	
41-110	16314-16326	publications	_	_	
41-111	16326-16327	/	_	_	
41-112	16327-16330	pdf	_	_	
41-113	16330-16331	/	_	_	
41-114	16331-16346	pbackdoor-ccs19	_	_	
41-115	16346-16347	.	_	_	
41-116	16347-16350	pdf	_	_	
41-117	16350-16351	)	_	_	
41-118	16352-16353	-	_	_	
41-119	16354-16355	[	_	_	
41-120	16355-16365	tensorflow	*	SOFTWARE	
41-121	16366-16380	implementation	_	_	
41-122	16380-16381	]	_	_	
41-123	16381-16382	(	_	_	
41-124	16382-16386	http	_	_	
41-125	16386-16387	:	_	_	
41-126	16387-16388	/	_	_	
41-127	16388-16389	/	_	_	
41-128	16389-16412	sandlab.cs.uchicago.edu	_	_	
41-129	16412-16413	/	_	_	
41-130	16413-16419	latent	_	_	
41-131	16419-16420	/	_	_	
41-132	16420-16421	)	_	_	
41-133	16423-16424	*	_	_	
41-134	16424-16425	*	_	_	
41-135	16425-16428	CBA	_	_	
41-136	16428-16429	:	_	_	
41-137	16429-16430	*	_	_	
41-138	16430-16431	*	_	_	
41-139	16432-16441	Composite	*[20]	PUBLICATION[20]	
41-140	16442-16450	Backdoor	*[20]	PUBLICATION[20]	
41-141	16451-16457	Attack	*[20]	PUBLICATION[20]	
41-142	16458-16461	for	*[20]	PUBLICATION[20]	
41-143	16462-16466	Deep	*[20]	PUBLICATION[20]	
41-144	16467-16473	Neural	*[20]	PUBLICATION[20]	
41-145	16474-16481	Network	*[20]	PUBLICATION[20]	
41-146	16482-16484	by	*[20]	PUBLICATION[20]	
41-147	16485-16491	Mixing	*[20]	PUBLICATION[20]	
41-148	16492-16500	Existing	*[20]	PUBLICATION[20]	
41-149	16501-16507	Benign	*[20]	PUBLICATION[20]	
41-150	16508-16516	Features	*[20]	PUBLICATION[20]	
41-151	16518-16519	-	_	_	
41-152	16520-16521	[	_	_	
41-153	16521-16526	paper	_	_	
41-154	16526-16527	]	_	_	
41-155	16527-16528	(	_	_	
41-156	16528-16533	https	_	_	
41-157	16533-16534	:	_	_	
41-158	16534-16535	/	_	_	
41-159	16535-16536	/	_	_	
41-160	16536-16546	dl.acm.org	_	_	
41-161	16546-16547	/	_	_	
41-162	16547-16550	doi	_	_	
41-163	16550-16551	/	_	_	
41-164	16551-16554	abs	_	_	
41-165	16554-16555	/	_	_	
41-166	16555-16562	10.1145	_	_	
41-167	16562-16563	/	_	_	
41-168	16563-16578	3372297.3423362	_	_	
41-169	16578-16579	)	_	_	
41-170	16580-16581	-	_	_	
41-171	16582-16583	[	_	_	
41-172	16583-16590	pytorch	*	SOFTWARE	
41-173	16591-16605	implementation	_	_	
41-174	16605-16606	]	_	_	
41-175	16606-16607	(	_	_	
41-176	16607-16612	https	_	_	
41-177	16612-16613	:	_	_	
41-178	16613-16614	/	_	_	
41-179	16614-16615	/	_	_	
41-180	16615-16625	github.com	_	_	
41-181	16625-16626	/	_	_	
41-182	16626-16642	TemporaryAcc0unt	_	_	
41-183	16642-16643	/	_	_	
41-184	16643-16659	composite-attack	_	_	
41-185	16659-16660	)	_	_	
41-186	16662-16663	#	_	_	
41-187	16663-16664	#	_	_	
41-188	16664-16665	#	_	_	
41-189	16665-16666	#	_	_	
41-190	16667-16674	Feature	_	_	
41-191	16675-16680	space	_	_	
41-192	16681-16687	attack	_	_	
41-193	16688-16697	benchmark	_	_	
41-194	16699-16700	`	_	_	
41-195	16700-16704	Note	_	_	
41-196	16704-16705	`	_	_	
41-197	16705-16706	:	_	_	
41-198	16707-16711	This	_	_	
41-199	16712-16722	repository	_	_	
41-200	16723-16725	is	_	_	
41-201	16726-16729	the	_	_	
41-202	16730-16738	official	_	_	
41-203	16739-16753	implementation	_	_	
41-204	16754-16756	of	_	_	
41-205	16757-16758	[	_	_	
41-206	16758-16762	Just	*[21]	PUBLICATION[21]	
41-207	16763-16766	How	*[21]	PUBLICATION[21]	
41-208	16767-16772	Toxic	*[21]	PUBLICATION[21]	
41-209	16773-16775	is	*[21]	PUBLICATION[21]	
41-210	16776-16780	Data	*[21]	PUBLICATION[21]	
41-211	16781-16790	Poisoning	*[21]	PUBLICATION[21]	
41-212	16790-16791	?	*[21]	PUBLICATION[21]	

#Text=A Unified Benchmark for Backdoor and Data Poisoning Attacks](https://arxiv.org/abs/2006.12557)
42-1	16792-16793	A	*[21]	PUBLICATION[21]	
42-2	16794-16801	Unified	*[21]	PUBLICATION[21]	
42-3	16802-16811	Benchmark	*[21]	PUBLICATION[21]	
42-4	16812-16815	for	*[21]	PUBLICATION[21]	
42-5	16816-16824	Backdoor	*[21]	PUBLICATION[21]	
42-6	16825-16828	and	*[21]	PUBLICATION[21]	
42-7	16829-16833	Data	*[21]	PUBLICATION[21]	
42-8	16834-16843	Poisoning	*[21]	PUBLICATION[21]	
42-9	16844-16851	Attacks	*[21]	PUBLICATION[21]	
42-10	16851-16852	]	_	_	
42-11	16852-16853	(	_	_	
42-12	16853-16858	https	_	_	
42-13	16858-16859	:	_	_	
42-14	16859-16860	/	_	_	
42-15	16860-16861	/	_	_	
42-16	16861-16870	arxiv.org	_	_	
42-17	16870-16871	/	_	_	
42-18	16871-16874	abs	_	_	
42-19	16874-16875	/	_	_	
42-20	16875-16885	2006.12557	_	_	
42-21	16885-16886	)	_	_	

#Text=.
43-1	16886-16887	.	_	_	

#Text=- [pytorch implementation](https://github.com/aks2203/poisoning-benchmark)
#Text=
#Text=#### Library
#Text=
#Text=`Note`: TrojanZoo provides a universal pytorch platform to conduct security researches (especially backdoor attacks/defenses) of image classification in deep learning.
44-1	16889-16890	-	_	_	
44-2	16891-16892	[	_	_	
44-3	16892-16899	pytorch	*	SOFTWARE	
44-4	16900-16914	implementation	_	_	
44-5	16914-16915	]	_	_	
44-6	16915-16916	(	_	_	
44-7	16916-16921	https	_	_	
44-8	16921-16922	:	_	_	
44-9	16922-16923	/	_	_	
44-10	16923-16924	/	_	_	
44-11	16924-16934	github.com	_	_	
44-12	16934-16935	/	_	_	
44-13	16935-16942	aks2203	_	_	
44-14	16942-16943	/	_	_	
44-15	16943-16962	poisoning-benchmark	_	_	
44-16	16962-16963	)	_	_	
44-17	16965-16966	#	_	_	
44-18	16966-16967	#	_	_	
44-19	16967-16968	#	_	_	
44-20	16968-16969	#	_	_	
44-21	16970-16977	Library	_	_	
44-22	16979-16980	`	_	_	
44-23	16980-16984	Note	_	_	
44-24	16984-16985	`	_	_	
44-25	16985-16986	:	_	_	
44-26	16987-16996	TrojanZoo	*	SOFTWARE	
44-27	16997-17005	provides	_	_	
44-28	17006-17007	a	_	_	
44-29	17008-17017	universal	_	_	
44-30	17018-17025	pytorch	*	SOFTWARE	
44-31	17026-17034	platform	_	_	
44-32	17035-17037	to	_	_	
44-33	17038-17045	conduct	_	_	
44-34	17046-17054	security	_	_	
44-35	17055-17065	researches	_	_	
44-36	17066-17067	(	_	_	
44-37	17067-17077	especially	_	_	
44-38	17078-17086	backdoor	_	_	
44-39	17087-17094	attacks	_	_	
44-40	17094-17095	/	_	_	
44-41	17095-17103	defenses	_	_	
44-42	17103-17104	)	_	_	
44-43	17105-17107	of	_	_	
44-44	17108-17113	image	_	_	
44-45	17114-17128	classification	_	_	
44-46	17129-17131	in	_	_	
44-47	17132-17136	deep	_	_	
44-48	17137-17145	learning	_	_	
44-49	17145-17146	.	_	_	

#Text=Backdoors 101 — is a PyTorch framework for state-of-the-art backdoor defenses and attacks on deep learning models
45-1	17148-17157	Backdoors	*[22]	SOFTWARE[22]	
45-2	17158-17161	101	*[22]	SOFTWARE[22]	
45-3	17162-17163	—	_	_	
45-4	17164-17166	is	_	_	
45-5	17167-17168	a	_	_	
45-6	17169-17176	PyTorch	*	SOFTWARE	
45-7	17177-17186	framework	_	_	
45-8	17187-17190	for	_	_	
45-9	17191-17207	state-of-the-art	_	_	
45-10	17208-17216	backdoor	_	_	
45-11	17217-17225	defenses	_	_	
45-12	17226-17229	and	_	_	
45-13	17230-17237	attacks	_	_	
45-14	17238-17240	on	_	_	
45-15	17241-17245	deep	_	_	
45-16	17246-17254	learning	_	_	
45-17	17255-17261	models	_	_	

#Text=.
46-1	17261-17262	.	_	_	

#Text=- [trojanzoo](https://github.com/ain-soph/trojanzoo)
#Text=- [backdoors101](https://github.com/ebagdasa/backdoors101)
#Text=
#Text=
#Text=
#Text=## Citation
#Text=
#Text=If you find the code is useful for your research, please cite our work:
#Text=
#Text=```
#Text=@inproceedings{li2021anti,
#Text=  title={Anti-Backdoor Learning: Training Clean Models on Poisoned Data},
#Text=  author={Li, Yige and Lyu, Xixiang and Koren, Nodens and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},
#Text=  booktitle={NeurIPS},
#Text=  year={2021}
#Text=}
#Text=```
#Text=
#Text=## Contacts
#Text=
#Text=Please feel free to drop a message here if you have any questions.
47-1	17264-17265	-	_	_	
47-2	17266-17267	[	_	_	
47-3	17267-17276	trojanzoo	*	SOFTWARE	
47-4	17276-17277	]	_	_	
47-5	17277-17278	(	_	_	
47-6	17278-17283	https	_	_	
47-7	17283-17284	:	_	_	
47-8	17284-17285	/	_	_	
47-9	17285-17286	/	_	_	
47-10	17286-17296	github.com	_	_	
47-11	17296-17297	/	_	_	
47-12	17297-17305	ain-soph	_	_	
47-13	17305-17306	/	_	_	
47-14	17306-17315	trojanzoo	*	SOFTWARE	
47-15	17315-17316	)	_	_	
47-16	17317-17318	-	_	_	
47-17	17319-17320	[	_	_	
47-18	17320-17332	backdoors101	*	SOFTWARE	
47-19	17332-17333	]	_	_	
47-20	17333-17334	(	_	_	
47-21	17334-17339	https	_	_	
47-22	17339-17340	:	_	_	
47-23	17340-17341	/	_	_	
47-24	17341-17342	/	_	_	
47-25	17342-17352	github.com	_	_	
47-26	17352-17353	/	_	_	
47-27	17353-17361	ebagdasa	_	_	
47-28	17361-17362	/	_	_	
47-29	17362-17374	backdoors101	*	SOFTWARE	
47-30	17374-17375	)	_	_	
47-31	17379-17380	#	_	_	
47-32	17380-17381	#	_	_	
47-33	17382-17390	Citation	_	_	
47-34	17392-17394	If	_	_	
47-35	17395-17398	you	_	_	
47-36	17399-17403	find	_	_	
47-37	17404-17407	the	_	_	
47-38	17408-17412	code	_	_	
47-39	17413-17415	is	_	_	
47-40	17416-17422	useful	_	_	
47-41	17423-17426	for	_	_	
47-42	17427-17431	your	_	_	
47-43	17432-17440	research	_	_	
47-44	17440-17441	,	_	_	
47-45	17442-17448	please	_	_	
47-46	17449-17453	cite	_	_	
47-47	17454-17457	our	_	_	
47-48	17458-17462	work	_	_	
47-49	17462-17463	:	_	_	
47-50	17465-17466	`	_	_	
47-51	17466-17467	`	_	_	
47-52	17467-17468	`	_	_	
47-53	17469-17470	@	_	_	
47-54	17470-17483	inproceedings	_	_	
47-55	17483-17484	{	_	_	
47-56	17484-17494	li2021anti	_	_	
47-57	17494-17495	,	_	_	
47-58	17498-17503	title	_	_	
47-59	17503-17504	=	_	_	
47-60	17504-17505	{	_	_	
47-61	17505-17518	Anti-Backdoor	*[23]	PUBLICATION[23]	
47-62	17519-17527	Learning	*[23]	PUBLICATION[23]	
47-63	17527-17528	:	*[23]	PUBLICATION[23]	
47-64	17529-17537	Training	*[23]	PUBLICATION[23]	
47-65	17538-17543	Clean	*[23]	PUBLICATION[23]	
47-66	17544-17550	Models	*[23]	PUBLICATION[23]	
47-67	17551-17553	on	*[23]	PUBLICATION[23]	
47-68	17554-17562	Poisoned	*[23]	PUBLICATION[23]	
47-69	17563-17567	Data	*[23]	PUBLICATION[23]	
47-70	17567-17568	}	_	_	
47-71	17568-17569	,	_	_	
47-72	17572-17578	author	_	_	
47-73	17578-17579	=	_	_	
47-74	17579-17580	{	_	_	
47-75	17580-17582	Li	_	_	
47-76	17582-17583	,	_	_	
47-77	17584-17588	Yige	_	_	
47-78	17589-17592	and	_	_	
47-79	17593-17596	Lyu	_	_	
47-80	17596-17597	,	_	_	
47-81	17598-17605	Xixiang	_	_	
47-82	17606-17609	and	_	_	
47-83	17610-17615	Koren	_	_	
47-84	17615-17616	,	_	_	
47-85	17617-17623	Nodens	_	_	
47-86	17624-17627	and	_	_	
47-87	17628-17631	Lyu	_	_	
47-88	17631-17632	,	_	_	
47-89	17633-17641	Lingjuan	_	_	
47-90	17642-17645	and	_	_	
47-91	17646-17648	Li	_	_	
47-92	17648-17649	,	_	_	
47-93	17650-17652	Bo	_	_	
47-94	17653-17656	and	_	_	
47-95	17657-17659	Ma	_	_	
47-96	17659-17660	,	_	_	
47-97	17661-17668	Xingjun	_	_	
47-98	17668-17669	}	_	_	
47-99	17669-17670	,	_	_	
47-100	17673-17682	booktitle	_	_	
47-101	17682-17683	=	_	_	
47-102	17683-17684	{	_	_	
47-103	17684-17691	NeurIPS	*	CONFERENCE	
47-104	17691-17692	}	_	_	
47-105	17692-17693	,	_	_	
47-106	17696-17700	year	_	_	
47-107	17700-17701	=	_	_	
47-108	17701-17702	{	_	_	
47-109	17702-17706	2021	_	_	
47-110	17706-17707	}	_	_	
47-111	17708-17709	}	_	_	
47-112	17710-17711	`	_	_	
47-113	17711-17712	`	_	_	
47-114	17712-17713	`	_	_	
47-115	17715-17716	#	_	_	
47-116	17716-17717	#	_	_	
47-117	17718-17726	Contacts	_	_	
47-118	17728-17734	Please	_	_	
47-119	17735-17739	feel	_	_	
47-120	17740-17744	free	_	_	
47-121	17745-17747	to	_	_	
47-122	17748-17752	drop	_	_	
47-123	17753-17754	a	_	_	
47-124	17755-17762	message	_	_	
47-125	17763-17767	here	_	_	
47-126	17768-17770	if	_	_	
47-127	17771-17774	you	_	_	
47-128	17775-17779	have	_	_	
47-129	17780-17783	any	_	_	
47-130	17784-17793	questions	_	_	
47-131	17793-17794	.	_	_	
