#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# Hybrid-Self-Attention-NEAT
#Text=
#Text=### Abstract
#Text=
#Text=This repository contains the code to reproduce the results presented in the original [paper](https://link.springer.com/article/10.1007/s12530-023-09510-3).
1-1	0-1	#	_	_	
1-2	2-28	Hybrid-Self-Attention-NEAT	*	PROJECT	
1-3	30-31	#	_	_	
1-4	31-32	#	_	_	
1-5	32-33	#	_	_	
1-6	34-42	Abstract	_	_	
1-7	44-48	This	_	_	
1-8	49-59	repository	_	_	
1-9	60-68	contains	_	_	
1-10	69-72	the	_	_	
1-11	73-77	code	_	_	
1-12	78-80	to	_	_	
1-13	81-90	reproduce	_	_	
1-14	91-94	the	_	_	
1-15	95-102	results	_	_	
1-16	103-112	presented	_	_	
1-17	113-115	in	_	_	
1-18	116-119	the	_	_	
1-19	120-128	original	_	_	
1-20	129-130	[	_	_	
1-21	130-135	paper	_	_	
1-22	135-136	]	_	_	
1-23	136-137	(	_	_	
1-24	137-142	https	_	_	
1-25	142-143	:	_	_	
1-26	143-144	/	_	_	
1-27	144-145	/	_	_	
1-28	145-162	link.springer.com	_	_	
1-29	162-163	/	_	_	
1-30	163-170	article	_	_	
1-31	170-171	/	_	_	
1-32	171-178	10.1007	_	_	
1-33	178-179	/	_	_	
1-34	179-185	s12530	_	_	
1-35	185-186	-	_	_	
1-36	186-189	023	_	_	
1-37	189-190	-	_	_	
1-38	190-195	09510	_	_	
1-39	195-196	-	_	_	
1-40	196-197	3	_	_	
1-41	197-198	)	_	_	
1-42	198-199	.	_	_	

#Text=<br/>
#Text=In this article, we present a “Hybrid Self-Attention NEAT” method to improve the original NeuroEvolution of Augmenting Topologies (NEAT) algorithm in high-dimensional inputs.
2-1	200-201	<	_	_	
2-2	201-203	br	_	_	
2-3	203-204	/	_	_	
2-4	204-205	>	_	_	
2-5	206-208	In	_	_	
2-6	209-213	this	_	_	
2-7	214-221	article	_	_	
2-8	221-222	,	_	_	
2-9	223-225	we	_	_	
2-10	226-233	present	_	_	
2-11	234-235	a	_	_	
2-12	236-237	“	_	_	
2-13	237-243	Hybrid	*[1]	PROJECT[1]	
2-14	244-258	Self-Attention	*[1]	PROJECT[1]	
2-15	259-263	NEAT	*[1]	PROJECT[1]	
2-16	263-264	”	_	_	
2-17	265-271	method	_	_	
2-18	272-274	to	_	_	
2-19	275-282	improve	_	_	
2-20	283-286	the	_	_	
2-21	287-295	original	_	_	
2-22	296-310	NeuroEvolution	_	_	
2-23	311-313	of	_	_	
2-24	314-324	Augmenting	_	_	
2-25	325-335	Topologies	_	_	
2-26	336-337	(	_	_	
2-27	337-341	NEAT	_	_	
2-28	341-342	)	_	_	
2-29	343-352	algorithm	_	_	
2-30	353-355	in	_	_	
2-31	356-372	high-dimensional	_	_	
2-32	373-379	inputs	_	_	
2-33	379-380	.	_	_	

#Text=Although the NEAT algorithm has shown a significant result in different challenging tasks, as input representations are high dimensional, it cannot create a well-tuned network.
3-1	381-389	Although	_	_	
3-2	390-393	the	_	_	
3-3	394-398	NEAT	_	_	
3-4	399-408	algorithm	_	_	
3-5	409-412	has	_	_	
3-6	413-418	shown	_	_	
3-7	419-420	a	_	_	
3-8	421-432	significant	_	_	
3-9	433-439	result	_	_	
3-10	440-442	in	_	_	
3-11	443-452	different	_	_	
3-12	453-464	challenging	_	_	
3-13	465-470	tasks	_	_	
3-14	470-471	,	_	_	
3-15	472-474	as	_	_	
3-16	475-480	input	_	_	
3-17	481-496	representations	_	_	
3-18	497-500	are	_	_	
3-19	501-505	high	_	_	
3-20	506-517	dimensional	_	_	
3-21	517-518	,	_	_	
3-22	519-521	it	_	_	
3-23	522-528	cannot	_	_	
3-24	529-535	create	_	_	
3-25	536-537	a	_	_	
3-26	538-548	well-tuned	_	_	
3-27	549-556	network	_	_	
3-28	556-557	.	_	_	

#Text=Our study addresses this limitation by using self-attention as an indirect encoding method to select the most important parts of the input.
4-1	558-561	Our	_	_	
4-2	562-567	study	_	_	
4-3	568-577	addresses	_	_	
4-4	578-582	this	_	_	
4-5	583-593	limitation	_	_	
4-6	594-596	by	_	_	
4-7	597-602	using	_	_	
4-8	603-617	self-attention	_	_	
4-9	618-620	as	_	_	
4-10	621-623	an	_	_	
4-11	624-632	indirect	_	_	
4-12	633-641	encoding	_	_	
4-13	642-648	method	_	_	
4-14	649-651	to	_	_	
4-15	652-658	select	_	_	
4-16	659-662	the	_	_	
4-17	663-667	most	_	_	
4-18	668-677	important	_	_	
4-19	678-683	parts	_	_	
4-20	684-686	of	_	_	
4-21	687-690	the	_	_	
4-22	691-696	input	_	_	
4-23	696-697	.	_	_	

#Text=In addition, we improve its overall performance with the help of a hybrid method to evolve the final network weights.
5-1	698-700	In	_	_	
5-2	701-709	addition	_	_	
5-3	709-710	,	_	_	
5-4	711-713	we	_	_	
5-5	714-721	improve	_	_	
5-6	722-725	its	_	_	
5-7	726-733	overall	_	_	
5-8	734-745	performance	_	_	
5-9	746-750	with	_	_	
5-10	751-754	the	_	_	
5-11	755-759	help	_	_	
5-12	760-762	of	_	_	
5-13	763-764	a	_	_	
5-14	765-771	hybrid	_	_	
5-15	772-778	method	_	_	
5-16	779-781	to	_	_	
5-17	782-788	evolve	_	_	
5-18	789-792	the	_	_	
5-19	793-798	final	_	_	
5-20	799-806	network	_	_	
5-21	807-814	weights	_	_	
5-22	814-815	.	_	_	

#Text=The main conclusion is that Hybrid Self-Attention NEAT can eliminate the restriction of the original NEAT.
6-1	816-819	The	_	_	
6-2	820-824	main	_	_	
6-3	825-835	conclusion	_	_	
6-4	836-838	is	_	_	
6-5	839-843	that	_	_	
6-6	844-850	Hybrid	*[2]	PROGLANG[2]	
6-7	851-865	Self-Attention	*[2]	PROGLANG[2]	
6-8	866-870	NEAT	*[2]	PROGLANG[2]	
6-9	871-874	can	_	_	
6-10	875-884	eliminate	_	_	
6-11	885-888	the	_	_	
6-12	889-900	restriction	_	_	
6-13	901-903	of	_	_	
6-14	904-907	the	_	_	
6-15	908-916	original	_	_	
6-16	917-921	NEAT	_	_	
6-17	921-922	.	_	_	

#Text=The results indicate that in comparison with evolutionary algorithms, our model can get comparable scores in Atari games with raw pixels input with a much lower number of parameters.
7-1	923-926	The	_	_	
7-2	927-934	results	_	_	
7-3	935-943	indicate	_	_	
7-4	944-948	that	_	_	
7-5	949-951	in	_	_	
7-6	952-962	comparison	_	_	
7-7	963-967	with	_	_	
7-8	968-980	evolutionary	_	_	
7-9	981-991	algorithms	_	_	
7-10	991-992	,	_	_	
7-11	993-996	our	_	_	
7-12	997-1002	model	_	_	
7-13	1003-1006	can	_	_	
7-14	1007-1010	get	_	_	
7-15	1011-1021	comparable	_	_	
7-16	1022-1028	scores	_	_	
7-17	1029-1031	in	_	_	
7-18	1032-1037	Atari	_	_	
7-19	1038-1043	games	_	_	
7-20	1044-1048	with	_	_	
7-21	1049-1052	raw	_	_	
7-22	1053-1059	pixels	_	_	
7-23	1060-1065	input	_	_	
7-24	1066-1070	with	_	_	
7-25	1071-1072	a	_	_	
7-26	1073-1077	much	_	_	
7-27	1078-1083	lower	_	_	
7-28	1084-1090	number	_	_	
7-29	1091-1093	of	_	_	
7-30	1094-1104	parameters	_	_	
7-31	1104-1105	.	_	_	

#Text=_NOTE: The original implementation of self-attention for atari-games, and the NEAT algorithm can be found here:<br/>_
#Text=
#Text=Neuroevolution of Self-Interpretable Agents: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionAgent <br/>
#Text=Neat Package: https://github.com/CodeReclaimers/neat-python <br/>
#Text=Pure python library for the NEAT and other variations: https://github.com/ukuleleplayer/pureples
#Text=
#Text=### Execution
#Text=
#Text=#### To use this work on your researches or projects you need:
#Text=* Python 3.7
#Text=* Python packages listed in `requirements.txt`
#Text=
#Text=_NOTE: The following commands are based on Ubuntu 20.04_
#Text=###
#Text=
#Text=#### To install Python:
#Text=_First, check if you already have it installed or not_.
#Text=~~~~
#Text=python3 --version
#Text=~~~~
#Text=_If you don't have python 3.7 in your computer you can use the code below_:
#Text=~~~~
#Text=sudo add-apt-repository ppa:deadsnakes/ppa
#Text=sudo apt-get update
#Text=sudo apt-get install python3.7
#Text=sudo apt install python3.7-distutils
#Text=~~~~
#Text=###
#Text=
#Text=_NOTE: To create a virtual environment, you can use the following link:_
#Text=<br/> Creation of virtual environment: https://docs.python.org/3.7/library/venv.html
#Text=
#Text=#### To install packages via pip install:
#Text=~~~~
#Text=python3.7 -m pip install -r requirements.txt
#Text=~~~~
#Text=###
#Text=
#Text=#### To run this project on Ubuntu server:
#Text=_You need to uncomment the following lines in_ `experiments/configs/configs.py`
#Text=~~~~
#Text=_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))
#Text=_display.start()
#Text=~~~~
#Text=
#Text=_And also install some system dependencies as well_
#Text=~~~~
#Text=apt-get install -y xvfb x11-utils
#Text=~~~~
#Text=###
#Text=
#Text=#### To train the model:
#Text=* First, check the configuration you need.
8-1	1107-1108	_	_	_	
8-2	1108-1112	NOTE	_	_	
8-3	1112-1113	:	_	_	
8-4	1114-1117	The	_	_	
8-5	1118-1126	original	_	_	
8-6	1127-1141	implementation	_	_	
8-7	1142-1144	of	_	_	
8-8	1145-1159	self-attention	_	_	
8-9	1160-1163	for	_	_	
8-10	1164-1175	atari-games	_	_	
8-11	1175-1176	,	_	_	
8-12	1177-1180	and	_	_	
8-13	1181-1184	the	_	_	
8-14	1185-1189	NEAT	_	_	
8-15	1190-1199	algorithm	_	_	
8-16	1200-1203	can	_	_	
8-17	1204-1206	be	_	_	
8-18	1207-1212	found	_	_	
8-19	1213-1217	here	_	_	
8-20	1217-1218	:	_	_	
8-21	1218-1219	<	_	_	
8-22	1219-1221	br	_	_	
8-23	1221-1222	/	_	_	
8-24	1222-1223	>	_	_	
8-25	1223-1224	_	_	_	
8-26	1226-1240	Neuroevolution	_	_	
8-27	1241-1243	of	_	_	
8-28	1244-1262	Self-Interpretable	_	_	
8-29	1263-1269	Agents	_	_	
8-30	1269-1270	:	_	_	
8-31	1271-1276	https	_	_	
8-32	1276-1277	:	_	_	
8-33	1277-1278	/	_	_	
8-34	1278-1279	/	_	_	
8-35	1279-1289	github.com	_	_	
8-36	1289-1290	/	_	_	
8-37	1290-1296	google	_	_	
8-38	1296-1297	/	_	_	
8-39	1297-1317	brain-tokyo-workshop	_	_	
8-40	1317-1318	/	_	_	
8-41	1318-1322	tree	_	_	
8-42	1322-1323	/	_	_	
8-43	1323-1329	master	_	_	
8-44	1329-1330	/	_	_	
8-45	1330-1344	AttentionAgent	_	_	
8-46	1345-1346	<	_	_	
8-47	1346-1348	br	_	_	
8-48	1348-1349	/	_	_	
8-49	1349-1350	>	_	_	
8-50	1351-1355	Neat	_	_	
8-51	1356-1363	Package	_	_	
8-52	1363-1364	:	_	_	
8-53	1365-1370	https	_	_	
8-54	1370-1371	:	_	_	
8-55	1371-1372	/	_	_	
8-56	1372-1373	/	_	_	
8-57	1373-1383	github.com	_	_	
8-58	1383-1384	/	_	_	
8-59	1384-1398	CodeReclaimers	_	_	
8-60	1398-1399	/	_	_	
8-61	1399-1410	neat-python	_	_	
8-62	1411-1412	<	_	_	
8-63	1412-1414	br	_	_	
8-64	1414-1415	/	_	_	
8-65	1415-1416	>	_	_	
8-66	1417-1421	Pure	_	_	
8-67	1422-1428	python	*	PROGLANG	
8-68	1429-1436	library	_	_	
8-69	1437-1440	for	_	_	
8-70	1441-1444	the	_	_	
8-71	1445-1449	NEAT	_	_	
8-72	1450-1453	and	_	_	
8-73	1454-1459	other	_	_	
8-74	1460-1470	variations	_	_	
8-75	1470-1471	:	_	_	
8-76	1472-1477	https	_	_	
8-77	1477-1478	:	_	_	
8-78	1478-1479	/	_	_	
8-79	1479-1480	/	_	_	
8-80	1480-1490	github.com	_	_	
8-81	1490-1491	/	_	_	
8-82	1491-1504	ukuleleplayer	_	_	
8-83	1504-1505	/	_	_	
8-84	1505-1513	pureples	_	_	
8-85	1515-1516	#	_	_	
8-86	1516-1517	#	_	_	
8-87	1517-1518	#	_	_	
8-88	1519-1528	Execution	_	_	
8-89	1530-1531	#	_	_	
8-90	1531-1532	#	_	_	
8-91	1532-1533	#	_	_	
8-92	1533-1534	#	_	_	
8-93	1535-1537	To	_	_	
8-94	1538-1541	use	_	_	
8-95	1542-1546	this	_	_	
8-96	1547-1551	work	_	_	
8-97	1552-1554	on	_	_	
8-98	1555-1559	your	_	_	
8-99	1560-1570	researches	_	_	
8-100	1571-1573	or	_	_	
8-101	1574-1582	projects	_	_	
8-102	1583-1586	you	_	_	
8-103	1587-1591	need	_	_	
8-104	1591-1592	:	_	_	
8-105	1593-1594	*	_	_	
8-106	1595-1601	Python	_	_	
8-107	1602-1605	3.7	_	_	
8-108	1606-1607	*	_	_	
8-109	1608-1614	Python	*	PROGLANG	
8-110	1615-1623	packages	_	_	
8-111	1624-1630	listed	_	_	
8-112	1631-1633	in	_	_	
8-113	1634-1635	`	_	_	
8-114	1635-1651	requirements.txt	_	_	
8-115	1651-1652	`	_	_	
8-116	1654-1655	_	_	_	
8-117	1655-1659	NOTE	_	_	
8-118	1659-1660	:	_	_	
8-119	1661-1664	The	_	_	
8-120	1665-1674	following	_	_	
8-121	1675-1683	commands	_	_	
8-122	1684-1687	are	_	_	
8-123	1688-1693	based	_	_	
8-124	1694-1696	on	_	_	
8-125	1697-1703	Ubuntu	_	_	
8-126	1704-1709	20.04	_	_	
8-127	1709-1710	_	_	_	
8-128	1711-1712	#	_	_	
8-129	1712-1713	#	_	_	
8-130	1713-1714	#	_	_	
8-131	1716-1717	#	_	_	
8-132	1717-1718	#	_	_	
8-133	1718-1719	#	_	_	
8-134	1719-1720	#	_	_	
8-135	1721-1723	To	_	_	
8-136	1724-1731	install	_	_	
8-137	1732-1738	Python	_	_	
8-138	1738-1739	:	_	_	
8-139	1740-1741	_	_	_	
8-140	1741-1746	First	_	_	
8-141	1746-1747	,	_	_	
8-142	1748-1753	check	_	_	
8-143	1754-1756	if	_	_	
8-144	1757-1760	you	_	_	
8-145	1761-1768	already	_	_	
8-146	1769-1773	have	_	_	
8-147	1774-1776	it	_	_	
8-148	1777-1786	installed	_	_	
8-149	1787-1789	or	_	_	
8-150	1790-1793	not	_	_	
8-151	1793-1794	_	_	_	
8-152	1794-1795	.	_	_	
8-153	1796-1797	~	_	_	
8-154	1797-1798	~	_	_	
8-155	1798-1799	~	_	_	
8-156	1799-1800	~	_	_	
8-157	1801-1808	python3	_	_	
8-158	1809-1810	-	_	_	
8-159	1810-1811	-	_	_	
8-160	1811-1818	version	_	_	
8-161	1819-1820	~	_	_	
8-162	1820-1821	~	_	_	
8-163	1821-1822	~	_	_	
8-164	1822-1823	~	_	_	
8-165	1824-1825	_	_	_	
8-166	1825-1827	If	_	_	
8-167	1828-1831	you	_	_	
8-168	1832-1837	don't	_	_	
8-169	1838-1842	have	_	_	
8-170	1843-1849	python	_	_	
8-171	1850-1853	3.7	_	_	
8-172	1854-1856	in	_	_	
8-173	1857-1861	your	_	_	
8-174	1862-1870	computer	_	_	
8-175	1871-1874	you	_	_	
8-176	1875-1878	can	_	_	
8-177	1879-1882	use	_	_	
8-178	1883-1886	the	_	_	
8-179	1887-1891	code	_	_	
8-180	1892-1897	below	_	_	
8-181	1897-1898	_	_	_	
8-182	1898-1899	:	_	_	
8-183	1900-1901	~	_	_	
8-184	1901-1902	~	_	_	
8-185	1902-1903	~	_	_	
8-186	1903-1904	~	_	_	
8-187	1905-1909	sudo	_	_	
8-188	1910-1928	add-apt-repository	_	_	
8-189	1929-1932	ppa	_	_	
8-190	1932-1933	:	_	_	
8-191	1933-1943	deadsnakes	_	_	
8-192	1943-1944	/	_	_	
8-193	1944-1947	ppa	_	_	
8-194	1948-1952	sudo	_	_	
8-195	1953-1960	apt-get	_	_	
8-196	1961-1967	update	_	_	
8-197	1968-1972	sudo	_	_	
8-198	1973-1980	apt-get	_	_	
8-199	1981-1988	install	_	_	
8-200	1989-1998	python3.7	_	_	
8-201	1999-2003	sudo	_	_	
8-202	2004-2007	apt	_	_	
8-203	2008-2015	install	_	_	
8-204	2016-2025	python3.7	_	_	
8-205	2025-2026	-	_	_	
8-206	2026-2035	distutils	_	_	
8-207	2036-2037	~	_	_	
8-208	2037-2038	~	_	_	
8-209	2038-2039	~	_	_	
8-210	2039-2040	~	_	_	
8-211	2041-2042	#	_	_	
8-212	2042-2043	#	_	_	
8-213	2043-2044	#	_	_	
8-214	2046-2047	_	_	_	
8-215	2047-2051	NOTE	_	_	
8-216	2051-2052	:	_	_	
8-217	2053-2055	To	_	_	
8-218	2056-2062	create	_	_	
8-219	2063-2064	a	_	_	
8-220	2065-2072	virtual	_	_	
8-221	2073-2084	environment	_	_	
8-222	2084-2085	,	_	_	
8-223	2086-2089	you	_	_	
8-224	2090-2093	can	_	_	
8-225	2094-2097	use	_	_	
8-226	2098-2101	the	_	_	
8-227	2102-2111	following	_	_	
8-228	2112-2116	link	_	_	
8-229	2116-2117	:	_	_	
8-230	2117-2118	_	_	_	
8-231	2119-2120	<	_	_	
8-232	2120-2122	br	_	_	
8-233	2122-2123	/	_	_	
8-234	2123-2124	>	_	_	
8-235	2125-2133	Creation	_	_	
8-236	2134-2136	of	_	_	
8-237	2137-2144	virtual	_	_	
8-238	2145-2156	environment	_	_	
8-239	2156-2157	:	_	_	
8-240	2158-2163	https	_	_	
8-241	2163-2164	:	_	_	
8-242	2164-2165	/	_	_	
8-243	2165-2166	/	_	_	
8-244	2166-2181	docs.python.org	_	_	
8-245	2181-2182	/	_	_	
8-246	2182-2185	3.7	_	_	
8-247	2185-2186	/	_	_	
8-248	2186-2193	library	_	_	
8-249	2193-2194	/	_	_	
8-250	2194-2203	venv.html	_	_	
8-251	2205-2206	#	_	_	
8-252	2206-2207	#	_	_	
8-253	2207-2208	#	_	_	
8-254	2208-2209	#	_	_	
8-255	2210-2212	To	_	_	
8-256	2213-2220	install	_	_	
8-257	2221-2229	packages	_	_	
8-258	2230-2233	via	_	_	
8-259	2234-2237	pip	_	_	
8-260	2238-2245	install	_	_	
8-261	2245-2246	:	_	_	
8-262	2247-2248	~	_	_	
8-263	2248-2249	~	_	_	
8-264	2249-2250	~	_	_	
8-265	2250-2251	~	_	_	
8-266	2252-2261	python3.7	_	_	
8-267	2262-2263	-	_	_	
8-268	2263-2264	m	_	_	
8-269	2265-2268	pip	_	_	
8-270	2269-2276	install	_	_	
8-271	2277-2278	-	_	_	
8-272	2278-2279	r	_	_	
8-273	2280-2296	requirements.txt	_	_	
8-274	2297-2298	~	_	_	
8-275	2298-2299	~	_	_	
8-276	2299-2300	~	_	_	
8-277	2300-2301	~	_	_	
8-278	2302-2303	#	_	_	
8-279	2303-2304	#	_	_	
8-280	2304-2305	#	_	_	
8-281	2307-2308	#	_	_	
8-282	2308-2309	#	_	_	
8-283	2309-2310	#	_	_	
8-284	2310-2311	#	_	_	
8-285	2312-2314	To	_	_	
8-286	2315-2318	run	_	_	
8-287	2319-2323	this	_	_	
8-288	2324-2331	project	_	_	
8-289	2332-2334	on	_	_	
8-290	2335-2341	Ubuntu	_	_	
8-291	2342-2348	server	_	_	
8-292	2348-2349	:	_	_	
8-293	2350-2351	_	_	_	
8-294	2351-2354	You	_	_	
8-295	2355-2359	need	_	_	
8-296	2360-2362	to	_	_	
8-297	2363-2372	uncomment	_	_	
8-298	2373-2376	the	_	_	
8-299	2377-2386	following	_	_	
8-300	2387-2392	lines	_	_	
8-301	2393-2395	in	_	_	
8-302	2395-2396	_	_	_	
8-303	2397-2398	`	_	_	
8-304	2398-2409	experiments	_	_	
8-305	2409-2410	/	_	_	
8-306	2410-2417	configs	_	_	
8-307	2417-2418	/	_	_	
8-308	2418-2428	configs.py	_	_	
8-309	2428-2429	`	_	_	
8-310	2430-2431	~	_	_	
8-311	2431-2432	~	_	_	
8-312	2432-2433	~	_	_	
8-313	2433-2434	~	_	_	
8-314	2435-2436	_	_	_	
8-315	2436-2443	display	_	_	
8-316	2444-2445	=	_	_	
8-317	2446-2470	pyvirtualdisplay.Display	_	_	
8-318	2470-2471	(	_	_	
8-319	2471-2478	visible	_	_	
8-320	2478-2479	=	_	_	
8-321	2479-2484	False	_	_	
8-322	2484-2485	,	_	_	
8-323	2486-2490	size	_	_	
8-324	2490-2491	=	_	_	
8-325	2491-2492	(	_	_	
8-326	2492-2496	1400	_	_	
8-327	2496-2497	,	_	_	
8-328	2498-2501	900	_	_	
8-329	2501-2502	)	_	_	
8-330	2502-2503	)	_	_	
8-331	2504-2505	_	_	_	
8-332	2505-2518	display.start	_	_	
8-333	2518-2519	(	_	_	
8-334	2519-2520	)	_	_	
8-335	2521-2522	~	_	_	
8-336	2522-2523	~	_	_	
8-337	2523-2524	~	_	_	
8-338	2524-2525	~	_	_	
8-339	2527-2528	_	_	_	
8-340	2528-2531	And	_	_	
8-341	2532-2536	also	_	_	
8-342	2537-2544	install	_	_	
8-343	2545-2549	some	_	_	
8-344	2550-2556	system	_	_	
8-345	2557-2569	dependencies	_	_	
8-346	2570-2572	as	_	_	
8-347	2573-2577	well	_	_	
8-348	2577-2578	_	_	_	
8-349	2579-2580	~	_	_	
8-350	2580-2581	~	_	_	
8-351	2581-2582	~	_	_	
8-352	2582-2583	~	_	_	
8-353	2584-2591	apt-get	_	_	
8-354	2592-2599	install	_	_	
8-355	2600-2601	-	_	_	
8-356	2601-2602	y	_	_	
8-357	2603-2607	xvfb	_	_	
8-358	2608-2611	x11	_	_	
8-359	2611-2612	-	_	_	
8-360	2612-2617	utils	_	_	
8-361	2618-2619	~	_	_	
8-362	2619-2620	~	_	_	
8-363	2620-2621	~	_	_	
8-364	2621-2622	~	_	_	
8-365	2623-2624	#	_	_	
8-366	2624-2625	#	_	_	
8-367	2625-2626	#	_	_	
8-368	2628-2629	#	_	_	
8-369	2629-2630	#	_	_	
8-370	2630-2631	#	_	_	
8-371	2631-2632	#	_	_	
8-372	2633-2635	To	_	_	
8-373	2636-2641	train	_	_	
8-374	2642-2645	the	_	_	
8-375	2646-2651	model	_	_	
8-376	2651-2652	:	_	_	
8-377	2653-2654	*	_	_	
8-378	2655-2660	First	_	_	
8-379	2660-2661	,	_	_	
8-380	2662-2667	check	_	_	
8-381	2668-2671	the	_	_	
8-382	2672-2685	configuration	_	_	
8-383	2686-2689	you	_	_	
8-384	2690-2694	need	_	_	
8-385	2694-2695	.	_	_	

#Text=The default ones are listed in `experiments/configs/`.
#Text=* We highly recommend increasing the number of population size, and the number of iterations to get better results.
#Text=* Check the working directory to be: `~/Hybrid_Self_Attention_NEAT/`
#Text=* Run the `runner.py` as below:
#Text=~~~~
#Text=python3.7 -m experiment.runner
#Text=~~~~
#Text=_NOTE: If you have limited resources (like RAM), you should decrease the number of iterations and instead use loops command (n is the number of iterations)_
#Text=~~~~
#Text=for i in {1..n}; do python3.7 -m experiment.runner; done
#Text=~~~~
#Text=###
#Text=
#Text=#### To tune the model:
#Text=* First, check you trained the model, and the model successfully saved in `experiments/` as `main_model.pkl`
#Text=* Run the `tunner.py` as below:
#Text=~~~~
#Text=python3.7 -m experiment.tunner
#Text=~~~~
#Text=_NOTE: If you have limited resources (like RAM), you should decrease the number of iterations and instead use loops command (n is the number of iterations)_
#Text=~~~~
#Text=for i in {1..n}; do python3.7 -m experiment.tunner; done
#Text=~~~~
#Text=
#Text=### Citation
#Text=
#Text=#### For attribution in academic contexts, please cite this work as:
#Text=~~~~
#Text=Khamesian, S., Malek, H.
9-1	2696-2699	The	_	_	
9-2	2700-2707	default	_	_	
9-3	2708-2712	ones	_	_	
9-4	2713-2716	are	_	_	
9-5	2717-2723	listed	_	_	
9-6	2724-2726	in	_	_	
9-7	2727-2728	`	_	_	
9-8	2728-2739	experiments	_	_	
9-9	2739-2740	/	_	_	
9-10	2740-2747	configs	_	_	
9-11	2747-2748	/	_	_	
9-12	2748-2749	`	_	_	
9-13	2749-2750	.	_	_	
9-14	2751-2752	*	_	_	
9-15	2753-2755	We	_	_	
9-16	2756-2762	highly	_	_	
9-17	2763-2772	recommend	_	_	
9-18	2773-2783	increasing	_	_	
9-19	2784-2787	the	_	_	
9-20	2788-2794	number	_	_	
9-21	2795-2797	of	_	_	
9-22	2798-2808	population	_	_	
9-23	2809-2813	size	_	_	
9-24	2813-2814	,	_	_	
9-25	2815-2818	and	_	_	
9-26	2819-2822	the	_	_	
9-27	2823-2829	number	_	_	
9-28	2830-2832	of	_	_	
9-29	2833-2843	iterations	_	_	
9-30	2844-2846	to	_	_	
9-31	2847-2850	get	_	_	
9-32	2851-2857	better	_	_	
9-33	2858-2865	results	_	_	
9-34	2865-2866	.	_	_	
9-35	2867-2868	*	_	_	
9-36	2869-2874	Check	_	_	
9-37	2875-2878	the	_	_	
9-38	2879-2886	working	_	_	
9-39	2887-2896	directory	_	_	
9-40	2897-2899	to	_	_	
9-41	2900-2902	be	_	_	
9-42	2902-2903	:	_	_	
9-43	2904-2905	`	_	_	
9-44	2905-2906	~	_	_	
9-45	2906-2907	/	_	_	
9-46	2907-2933	Hybrid_Self_Attention_NEAT	_	_	
9-47	2933-2934	/	_	_	
9-48	2934-2935	`	_	_	
9-49	2936-2937	*	_	_	
9-50	2938-2941	Run	_	_	
9-51	2942-2945	the	_	_	
9-52	2946-2947	`	_	_	
9-53	2947-2956	runner.py	_	_	
9-54	2956-2957	`	_	_	
9-55	2958-2960	as	_	_	
9-56	2961-2966	below	_	_	
9-57	2966-2967	:	_	_	
9-58	2968-2969	~	_	_	
9-59	2969-2970	~	_	_	
9-60	2970-2971	~	_	_	
9-61	2971-2972	~	_	_	
9-62	2973-2982	python3.7	_	_	
9-63	2983-2984	-	_	_	
9-64	2984-2985	m	_	_	
9-65	2986-3003	experiment.runner	_	_	
9-66	3004-3005	~	_	_	
9-67	3005-3006	~	_	_	
9-68	3006-3007	~	_	_	
9-69	3007-3008	~	_	_	
9-70	3009-3010	_	_	_	
9-71	3010-3014	NOTE	_	_	
9-72	3014-3015	:	_	_	
9-73	3016-3018	If	_	_	
9-74	3019-3022	you	_	_	
9-75	3023-3027	have	_	_	
9-76	3028-3035	limited	_	_	
9-77	3036-3045	resources	_	_	
9-78	3046-3047	(	_	_	
9-79	3047-3051	like	_	_	
9-80	3052-3055	RAM	_	_	
9-81	3055-3056	)	_	_	
9-82	3056-3057	,	_	_	
9-83	3058-3061	you	_	_	
9-84	3062-3068	should	_	_	
9-85	3069-3077	decrease	_	_	
9-86	3078-3081	the	_	_	
9-87	3082-3088	number	_	_	
9-88	3089-3091	of	_	_	
9-89	3092-3102	iterations	_	_	
9-90	3103-3106	and	_	_	
9-91	3107-3114	instead	_	_	
9-92	3115-3118	use	_	_	
9-93	3119-3124	loops	_	_	
9-94	3125-3132	command	_	_	
9-95	3133-3134	(	_	_	
9-96	3134-3135	n	_	_	
9-97	3136-3138	is	_	_	
9-98	3139-3142	the	_	_	
9-99	3143-3149	number	_	_	
9-100	3150-3152	of	_	_	
9-101	3153-3163	iterations	_	_	
9-102	3163-3164	)	_	_	
9-103	3164-3165	_	_	_	
9-104	3166-3167	~	_	_	
9-105	3167-3168	~	_	_	
9-106	3168-3169	~	_	_	
9-107	3169-3170	~	_	_	
9-108	3171-3174	for	_	_	
9-109	3175-3176	i	_	_	
9-110	3177-3179	in	_	_	
9-111	3180-3181	{	_	_	
9-112	3181-3182	1	_	_	
9-113	3182-3183	.	_	_	
9-114	3183-3184	.	_	_	
9-115	3184-3185	n	_	_	
9-116	3185-3186	}	_	_	
9-117	3186-3187	;	_	_	
9-118	3188-3190	do	_	_	
9-119	3191-3200	python3.7	_	_	
9-120	3201-3202	-	_	_	
9-121	3202-3203	m	_	_	
9-122	3204-3221	experiment.runner	_	_	
9-123	3221-3222	;	_	_	
9-124	3223-3227	done	_	_	
9-125	3228-3229	~	_	_	
9-126	3229-3230	~	_	_	
9-127	3230-3231	~	_	_	
9-128	3231-3232	~	_	_	
9-129	3233-3234	#	_	_	
9-130	3234-3235	#	_	_	
9-131	3235-3236	#	_	_	
9-132	3238-3239	#	_	_	
9-133	3239-3240	#	_	_	
9-134	3240-3241	#	_	_	
9-135	3241-3242	#	_	_	
9-136	3243-3245	To	_	_	
9-137	3246-3250	tune	_	_	
9-138	3251-3254	the	_	_	
9-139	3255-3260	model	_	_	
9-140	3260-3261	:	_	_	
9-141	3262-3263	*	_	_	
9-142	3264-3269	First	_	_	
9-143	3269-3270	,	_	_	
9-144	3271-3276	check	_	_	
9-145	3277-3280	you	_	_	
9-146	3281-3288	trained	_	_	
9-147	3289-3292	the	_	_	
9-148	3293-3298	model	_	_	
9-149	3298-3299	,	_	_	
9-150	3300-3303	and	_	_	
9-151	3304-3307	the	_	_	
9-152	3308-3313	model	_	_	
9-153	3314-3326	successfully	_	_	
9-154	3327-3332	saved	_	_	
9-155	3333-3335	in	_	_	
9-156	3336-3337	`	_	_	
9-157	3337-3348	experiments	_	_	
9-158	3348-3349	/	_	_	
9-159	3349-3350	`	_	_	
9-160	3351-3353	as	_	_	
9-161	3354-3355	`	_	_	
9-162	3355-3369	main_model.pkl	_	_	
9-163	3369-3370	`	_	_	
9-164	3371-3372	*	_	_	
9-165	3373-3376	Run	_	_	
9-166	3377-3380	the	_	_	
9-167	3381-3382	`	_	_	
9-168	3382-3391	tunner.py	_	_	
9-169	3391-3392	`	_	_	
9-170	3393-3395	as	_	_	
9-171	3396-3401	below	_	_	
9-172	3401-3402	:	_	_	
9-173	3403-3404	~	_	_	
9-174	3404-3405	~	_	_	
9-175	3405-3406	~	_	_	
9-176	3406-3407	~	_	_	
9-177	3408-3417	python3.7	_	_	
9-178	3418-3419	-	_	_	
9-179	3419-3420	m	_	_	
9-180	3421-3438	experiment.tunner	_	_	
9-181	3439-3440	~	_	_	
9-182	3440-3441	~	_	_	
9-183	3441-3442	~	_	_	
9-184	3442-3443	~	_	_	
9-185	3444-3445	_	_	_	
9-186	3445-3449	NOTE	_	_	
9-187	3449-3450	:	_	_	
9-188	3451-3453	If	_	_	
9-189	3454-3457	you	_	_	
9-190	3458-3462	have	_	_	
9-191	3463-3470	limited	_	_	
9-192	3471-3480	resources	_	_	
9-193	3481-3482	(	_	_	
9-194	3482-3486	like	_	_	
9-195	3487-3490	RAM	_	_	
9-196	3490-3491	)	_	_	
9-197	3491-3492	,	_	_	
9-198	3493-3496	you	_	_	
9-199	3497-3503	should	_	_	
9-200	3504-3512	decrease	_	_	
9-201	3513-3516	the	_	_	
9-202	3517-3523	number	_	_	
9-203	3524-3526	of	_	_	
9-204	3527-3537	iterations	_	_	
9-205	3538-3541	and	_	_	
9-206	3542-3549	instead	_	_	
9-207	3550-3553	use	_	_	
9-208	3554-3559	loops	_	_	
9-209	3560-3567	command	_	_	
9-210	3568-3569	(	_	_	
9-211	3569-3570	n	_	_	
9-212	3571-3573	is	_	_	
9-213	3574-3577	the	_	_	
9-214	3578-3584	number	_	_	
9-215	3585-3587	of	_	_	
9-216	3588-3598	iterations	_	_	
9-217	3598-3599	)	_	_	
9-218	3599-3600	_	_	_	
9-219	3601-3602	~	_	_	
9-220	3602-3603	~	_	_	
9-221	3603-3604	~	_	_	
9-222	3604-3605	~	_	_	
9-223	3606-3609	for	_	_	
9-224	3610-3611	i	_	_	
9-225	3612-3614	in	_	_	
9-226	3615-3616	{	_	_	
9-227	3616-3617	1	_	_	
9-228	3617-3618	.	_	_	
9-229	3618-3619	.	_	_	
9-230	3619-3620	n	_	_	
9-231	3620-3621	}	_	_	
9-232	3621-3622	;	_	_	
9-233	3623-3625	do	_	_	
9-234	3626-3635	python3.7	_	_	
9-235	3636-3637	-	_	_	
9-236	3637-3638	m	_	_	
9-237	3639-3656	experiment.tunner	_	_	
9-238	3656-3657	;	_	_	
9-239	3658-3662	done	_	_	
9-240	3663-3664	~	_	_	
9-241	3664-3665	~	_	_	
9-242	3665-3666	~	_	_	
9-243	3666-3667	~	_	_	
9-244	3669-3670	#	_	_	
9-245	3670-3671	#	_	_	
9-246	3671-3672	#	_	_	
9-247	3673-3681	Citation	_	_	
9-248	3683-3684	#	_	_	
9-249	3684-3685	#	_	_	
9-250	3685-3686	#	_	_	
9-251	3686-3687	#	_	_	
9-252	3688-3691	For	_	_	
9-253	3692-3703	attribution	_	_	
9-254	3704-3706	in	_	_	
9-255	3707-3715	academic	_	_	
9-256	3716-3724	contexts	_	_	
9-257	3724-3725	,	_	_	
9-258	3726-3732	please	_	_	
9-259	3733-3737	cite	_	_	
9-260	3738-3742	this	_	_	
9-261	3743-3747	work	_	_	
9-262	3748-3750	as	_	_	
9-263	3750-3751	:	_	_	
9-264	3752-3753	~	_	_	
9-265	3753-3754	~	_	_	
9-266	3754-3755	~	_	_	
9-267	3755-3756	~	_	_	
9-268	3757-3766	Khamesian	_	_	
9-269	3766-3767	,	_	_	
9-270	3768-3769	S	_	_	
9-271	3769-3770	.	_	_	
9-272	3770-3771	,	_	_	
9-273	3772-3777	Malek	_	_	
9-274	3777-3778	,	_	_	
9-275	3779-3780	H	_	_	
9-276	3780-3781	.	_	_	

#Text=Hybrid self-attention NEAT: a novel evolutionary self-attention approach to improve the NEAT algorithm in high dimensional inputs.
10-1	3782-3788	Hybrid	*[3]|*[4]	PUBLICATION[3]|PROJECT[4]	
10-2	3789-3803	self-attention	*[3]|*[4]	PUBLICATION[3]|PROJECT[4]	
10-3	3804-3808	NEAT	*[3]|*[4]	PUBLICATION[3]|PROJECT[4]	
10-4	3808-3809	:	*[3]	PUBLICATION[3]	
10-5	3810-3811	a	*[3]	PUBLICATION[3]	
10-6	3812-3817	novel	*[3]	PUBLICATION[3]	
10-7	3818-3830	evolutionary	*[3]	PUBLICATION[3]	
10-8	3831-3845	self-attention	*[3]	PUBLICATION[3]	
10-9	3846-3854	approach	*[3]	PUBLICATION[3]	
10-10	3855-3857	to	*[3]	PUBLICATION[3]	
10-11	3858-3865	improve	*[3]	PUBLICATION[3]	
10-12	3866-3869	the	*[3]	PUBLICATION[3]	
10-13	3870-3874	NEAT	*[3]	PUBLICATION[3]	
10-14	3875-3884	algorithm	*[3]	PUBLICATION[3]	
10-15	3885-3887	in	*[3]	PUBLICATION[3]	
10-16	3888-3892	high	*[3]	PUBLICATION[3]	
10-17	3893-3904	dimensional	*[3]	PUBLICATION[3]	
10-18	3905-3911	inputs	*[3]	PUBLICATION[3]	
10-19	3911-3912	.	_	_	

#Text=Evolving Systems (2023). https://doi.org/10.1007/s12530-023-09510-3
#Text=~~~~
11-1	3913-3921	Evolving	*[5]	PUBLICATION[5]	
11-2	3922-3929	Systems	*[5]	PUBLICATION[5]	
11-3	3930-3931	(	*[5]	PUBLICATION[5]	
11-4	3931-3935	2023	*[5]	PUBLICATION[5]	
11-5	3935-3936	)	*[5]	PUBLICATION[5]	
11-6	3936-3937	.	_	_	
11-7	3938-3943	https	_	_	
11-8	3943-3944	:	_	_	
11-9	3944-3945	/	_	_	
11-10	3945-3946	/	_	_	
11-11	3946-3953	doi.org	_	_	
11-12	3953-3954	/	_	_	
11-13	3954-3961	10.1007	_	_	
11-14	3961-3962	/	_	_	
11-15	3962-3968	s12530	_	_	
11-16	3968-3969	-	_	_	
11-17	3969-3972	023	_	_	
11-18	3972-3973	-	_	_	
11-19	3973-3978	09510	_	_	
11-20	3978-3979	-	_	_	
11-21	3979-3980	3	_	_	
11-22	3981-3982	~	_	_	
11-23	3982-3983	~	_	_	
11-24	3983-3984	~	_	_	
11-25	3984-3985	~	_	_	
